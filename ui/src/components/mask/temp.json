{
  "585665": "11pt,a4paperarticle tablexcolor times,latexsym url T1fontenc",
  "585666": "times latexsym CJKutf8 xspace,mfirstuc,tabulary amsmath,bm multirow color multicol tabularx booktabs pbox framed array enumitem chngpage threeparttable dsfont graphicx enumitem epigraph CJKutf8 amsfonts arydshln csquotes subfig circledsteps",
  "585667": "TableShadegray0.96 lightgreyRGB253, 253, 253 newblueRGB218, 232, 252 newgreyRGB240, 240, 240",
  "585668": "width 0pt height.9depth.9 1     0pt  newgrey#1\\/",
  "585669": "Bib 1cyan#1 e.g. i.e. vs.  1darkblue#1 C EQUATION_DEPTH_0_START ^3 EQUATION_DEPTH_0_END  1 2",
  "585670": "EQUATION_DEPTH_0_START {\\text C}_{\\text M}^{3} EQUATION_DEPTH_0_END   EQUATION_DEPTH_0_START {\\text C}_{\\text D}^{3} EQUATION_DEPTH_0_END  1   EQUATION_DEPTH_0_START \\circ EQUATION_DEPTH_0_END",
  "585671": "1     #1  footnote-1",
  "585672": "acceptedWithAtacl2018v2",
  "585673": "xspace,mfirstuc,tabulary Sept. 20, 2018 tacl2018v2",
  "585674": "1#1",
  "585675": "(No author info supplied here, for consistency with TACL-submission anonymization requirements)",
  "585676": "final version final versions Final version Final versions Final Versions  submission s Submission s Submissions",
  "585677": "Self-Teaching Machines to Read and Comprehend with",
  "585678": "Large-Scale Multi-Subject Question-Answering Data",
  "585679": "Dian Yu1   Kai Sun2   Dong Yu1   Claire Cardie2",
  "585680": "1Tencent AI Lab, Bellevue, WA",
  "585681": "2Cornell University, Ithaca, NY",
  "585682": "\\{yudian, dyu\\}@tencent.com, ks985@cornell.edu, cardie@cs.cornell.edu",
  "585683": "In spite of much recent research in the area, it is still unclear whether subject-area question-answering data is useful for machine reading comprehension (MRC) tasks.",
  "585684": "In this paper, we investigate this question.",
  "585685": "We collect a large-scale multi-subject multiple-choice question-answering dataset, ExamQA, and use incomplete and noisy snippets returned by a web search engine as the relevant context for each question-answering instance to convert it into a weakly-labeled MRC instance.",
  "585686": "We then propose a self-teaching paradigm to better use the generated weakly-labeled MRC instances to improve a target MRC task.",
  "585687": "Experimental results show that we can obtain EQUATION_DEPTH_0_START +5.1\\% EQUATION_DEPTH_0_END in accuracy on a multiple-choice MRC dataset, , and EQUATION_DEPTH_0_START +3.8\\% EQUATION_DEPTH_0_END in exact match on an extractive MRC dataset, CMRC 2018 over state-of-the-art MRC baselines, demonstrating the effectiveness of our framework and the usefulness of large-scale subject-area question-answering data for different types of machine reading comprehension tasks.",
  "585688": "Introduction.",
  "585689": "At some level, machine reading comprehension (MRC) and question answering (QA) seem to be quite related tasks: machine reading comprehension aims to answer questions derived from a given document Citation (richardson2013mctest,hermann2015teaching,rodrigo2015overview),  while the standard question answering formulation Citation (voorhees2000building,burger2001issues,fukumoto2001overview) requires retrieval of snippets of text from a large corpus that answer a given question.",
  "585690": "th!",
  "585691": "width=0.4Figures/Examnet-mrc_to_qa.pdf       A typical framework for using medium-scale MRC data to improve small-scale QA.",
  "585692": "(Label fig:mrc_qa)",
  "585693": "And it has been demonstrated that medium-scale MRC datasets can be employed to improve performance on small-scale question-answering systems.",
  "585694": "sun2018improving and pan-2019-improving-question, for example, obtain performance gains on subject-area question-answering datasets about science such as ARC Citation (clark2016combining,clark2018think) and OpenBookQA Citation (mihaylov2018can) by pre-training the QA models on MRC data (, RACE Citation (lai2017race)).",
  "585695": "The general approach has been to retrieve for each QA instance complete documents or sentences from a relevant in-domain corpus or Wikipedia articles, and then use a machine reading comprehension pipeline on the (context, question) pair to answer questions.",
  "585696": "See Figure (Ref fig:mrc_qa) for an overview.",
  "585697": "Conversely, there actually exists large-scale real-world question-answering data created by subject-matter experts, but rarely is it studied to improve machine reading comprehension.",
  "585698": "This paper aims to study whether the massive amount of subject-area QA data can improve machine reading comprehension.",
  "585699": "For many years it has been demonstrated that human readers' reading comprehension performance is affected by their prior knowledge about the topic of the given text Citation (johnston1984prior,laufer1985measuring,hirsch2003reading).",
  "585700": "Instead of retrieving and imparting topic-specific subject-area knowledge for a given text to a machine reader in an on-demand manner, we hypothesize that incorporating rich knowledge from all --- or as many as possible --- subjects into a machine reader may improve its ability to comprehend text on different topics.",
  "585701": "As most of the existing multi-subject question-answering datasets are relatively small-scale, we first collect a large-scale Question-Answering dataset from Exams covering a wide range of subjects (, sociology, education, and psychology), which contains 638k multiple-choice instances, called ExamQA.",
  "585702": "We then present a method to convert QA instances in ExamQA into training instances for a target MRC task, which may benefit from knowledge transfer Citation (ruder-2019-transfer).",
  "585703": "In contrast to previous studies that augment each QA instance with relevant sentences or documents retrieved from offline corpora, we are interested in another practical reading process to add context to QA instances: human readers type their questions on a web search engine and only read through the snippets returned by the search engine to seek potential answers.",
  "585704": "Imitating this process, we use relevant snippets retrieved by a web search engine as the context of each question-answering instance.",
  "585705": "We regard such an MRC instance as weakly-labeled as the context is a form of distant supervision: while it might contain the answer to the question as required for MRC, it is equally likely to be noisy, irrelevant, incomplete, and/or too informal to constitute a proper answer (Section (Ref sec:generation)).",
  "585706": "To better leverage the large-scale weakly labeled MRC data, we propose a self-teaching paradigm that iteratively uses a student model that outperforms its teacher model as the new teacher to generate soft labels for data.",
  "585707": "First, we train a multi-skilled teacher model using both the weakly labeled data and the data of a target MRC task.",
  "585708": "We then train a multi-skilled student model using the same data while replacing the hard labels of training data with the soft labels predicted by the multi-skilled teacher.",
  "585709": "Finally, we initialize an expert student model with the resulting multi-skilled student model and fine-tune it on the target MRC data, whose labels are set based on the soft labels generated by the multi-skilled student (Section (Ref sec:method)).",
  "585710": "We study the effect of the generated QA-based weakly labeled MRC data under the self-teaching paradigm on a multiple-choice MRC dataset,  Citation (sun2019investigating), in which most questions cannot be solved solely by matching or paraphrasing, and an extractive MRC dataset, CMRC 2018 Citation (cui-2019-span), in which all answers are spans in the given documents.",
  "585711": "Experimental results show that we can obtain an EQUATION_DEPTH_0_START +5.1\\% EQUATION_DEPTH_0_END in accuracy on  and EQUATION_DEPTH_0_START +3.8\\% EQUATION_DEPTH_0_END in exact match on CMRC 2018 over state-of-the-art baselines Citation (xu2020clue,cui-2020-revisiting).",
  "585712": "Furthermore, we present an easy way to adapt this paradigm to additionally leverage multiple types of weakly labeled MRC data wherein noise is introduced by different factors (, context retrieval, machine translation, and knowledge construction), again by using soft labels predicted by teacher models.",
  "585713": "Augmenting the MRC training data in this way leads to further improvements (up to EQUATION_DEPTH_0_START +2.5\\% EQUATION_DEPTH_0_END in accuracy on ).",
  "585714": "These results demonstrate the effectiveness and flexibility of our paradigm.",
  "585715": "The contributions of this paper are as follows.",
  "585716": "-0.1em     We offer the largest multi-subject QA dataset to date, ExamQA.",
  "585717": "ExamQA will be available at https://dataset.org/examqa/.",
  "585718": "Our study examines whether large-scale subject-area QA data can improve MRC.",
  "585719": "We explore an approach that adds noisy and incomplete context retrieved by a web search engine to QA instances to convert them into weakly-labeled MRC data.",
  "585720": "We propose a simple yet effective self-teaching paradigm to better leverage large-scale weakly-labeled data of one or multiple types (, in which the source of noise varies).",
  "585721": "Experimental results show that we can achieve up to EQUATION_DEPTH_0_START +7.6\\% EQUATION_DEPTH_0_END in accuracy on a challenging multiple-choice MRC dataset and EQUATION_DEPTH_0_START +3.8\\% EQUATION_DEPTH_0_END in exact match on a representative extractive MRC dataset.",
  "585722": "Weakly-Labeled Data Generation.",
  "585723": "(Label sec:generation)",
  "585724": "Question-Answering Data Collection.",
  "585725": "We collect large-scale question-answering instances from freely accessible exams (including mock exams) designed for a variety of subjects such as programming, journalism, and ecology.",
  "585726": "We only keep multiple-choice single-answer instances written in Chinese.",
  "585727": "After deduplication, we obtain 638,436 question-answering instances.",
  "585728": "To assess the subject coverage of ExamQA, we obtain a list of subjects from China national standard (GB/T 13745-2009) Citation (GBT137452009) and check for each subject in the list if the name of the subject appears in the title of any exam to estimate the lower bound of subject coverage.",
  "585729": "The estimation shows that ExamQA covers at least 48 out of 62 first-level subjects and 187 out of 676 second-level subjects.",
  "585730": "Note that the actual subject coverage of ExamQA may be greatly underestimated, as only EQUATION_DEPTH_0_START 24.2\\% EQUATION_DEPTH_0_END of titles contain a subject name.",
  "585731": "We do not annotate a small subset of questions for human performance, as most of the subject-area questions are from higher education exams that require advanced domain knowledge.",
  "585732": "Comparisons with Existing Subject-Area Question-Answering Datasets.",
  "585733": "Subject-area question answering is an increasingly popular direction in question answering, focusing on closing the performance gap between humans and machines in answering questions collected from real-world exams that are carefully designed by subject-matter experts.",
  "585734": "These tasks are mostly in multiple-choice forms.",
  "585735": "In Table (Ref tab:datasets), we list several representative subject-area multiple-choice question-answering datasets: NTCIR-11 QA-Lab Citation (shibuki2014overview), QS Citation (chengtaking2016), MCQA Citation (guo-etal-2017-ijcnlp), ARC Citation (clark2018think), GeoSQA Citation (huang2019geosqa), HEAD-QA Citation (vilares-gomez-rodriguez-2019-head), EXAMS Citation (hardalov-etal-2020-exams), JEC-QA Citation (zhong2020jec), and MEDQA Citation (jin2020disease).",
  "585736": "h!",
  "585737": "llllll  dataset   & \\# of subjects EQUATION_DEPTH_0_START ^\\circ EQUATION_DEPTH_0_END & subjects  & size",
  "585738": "QS                  & 1 &history               & 0.6K",
  "585739": "GeoSQA                 & 1 &geography                 & 4.1K",
  "585740": "JEC-QA                  & 1 &legal                & 26.4K",
  "585741": "ARC                        &  1 &science              & 7.8K",
  "585742": "QA-Lab                  & 1 &history    & 0.3K",
  "585743": "HEAD-QA                & 1 &healthcare         & 6.8K",
  "585744": "MEDQA                     & 1 &medical          & 61.1K",
  "585745": "MCQA        & 6 &multi-subject                 & 14.4K",
  "585746": "EXAMS       & 24 &multi-subject                & 24.1K",
  "585747": "ExamQA     & 48 &multi-subject             & 638.4K",
  "585748": "(Label tab:datasets) Representative subject-area QA datasets collected from exams ( EQUATION_DEPTH_0_START ^\\circ EQUATION_DEPTH_0_END : we simply report the number of subjects stated by previous studies and the number of first-level subjects in ExamQA).",
  "585749": "It is worth mentioning that some multiple-choice MRC datasets (, Citation (rodrigo2015overview,lai2017race)) are collected from language exams designed to test the reading comprehension ability of a human reader.",
  "585750": "These kinds of context-dependent problems are not included in ExamQA.",
  "585751": "Bringing Context to Question Answering.",
  "585752": "In this section, we present a method to convert QA instances into multiple-choice or extractive MRC instances to make the resulting data and target MRC task in a similar format, which may benefit from knowledge transfer Citation (ruder-2019-transfer).",
  "585753": "Previous studies attempt to convert a multiple-choice subject-area QA task to a multiple-choice MRC task by retrieving relevant sentences for each question from a clean corpus to form a document.",
  "585754": "Instead of relying on a relatively clean resource, we retrieve the top ranked snippets using a publicly available search engine.",
  "585755": "Specifically, we send each question to the search engine as the query and collect snippets from the first result page.",
  "585756": "Typically, we can collect ten snippets for each QA instance.",
  "585757": "Since all instances are freely accessible online, it is likely that a retrieved snippet merely contains the original QA instance rather than relevant context sufficient for answering the question.",
  "585758": "Therefore, we discard a snippet if more than one answer option appears as a substring in the snippet.",
  "585759": "We concatenate the remaining snippets into a document as the context of each QA instance.",
  "585760": "We show data statistics of ExamQA and retrieved context in Table (Ref tab:examqastat).",
  "585761": "Due to this construction method, it is very likely that a document is noisy, incomplete, informal, and/or irrelevant.",
  "585762": "We provide sample instances in Table (Ref tab:sample1b-en).",
  "585763": "To convert these multiple-choice MRC instances into extractive ones, we remove the wrong answer options of each multiple-choice instance and append the start/end offsets of the first mention of the correct answer in its associated snippet.",
  "585764": "h!",
  "585765": "ll  metric                & value",
  "585766": "average \\# of answer options &  4.0",
  "585767": "average question length (in characters) & 39.5",
  "585768": "average option length (in characters) & 6.7",
  "585769": "average context length (in characters) & 907.6",
  "585770": "character vocabulary size & 13,258",
  "585771": "non-extractive correct option (\\%) & 68.4",
  "585772": "(Label tab:examqastat) Data statistics of ExamQA with context.",
  "585773": "h!",
  "585774": "p0.2cmp6.5cm  C1: & 1.",
  "585775": "+ b / b is equivalent to ((int)",
  "585776": "a) + (b /",
  "585777": "b), which can be obtained according to the priority of the processor.",
  "585778": "(Int)",
  "585779": "This is a forced type conversion.",
  "585780": "After the forced conversion ((int)",
  "585781": "a) is generally the double conversion to the int type, most platforms round to zero.",
  "585782": "2./b, both sides of the division sign are doubletype , The result is also doubleType.",
  "585783": "That is 1.000000; integer.",
  "585784": "The first 5 is the int type, int.",
  "585785": "3.; a = 5.5; b = 2.5; c = (int) a + b / b; printf (\\\".",
  "585786": "Best answer: (int) a + b / b = 6, should be (int) a means round a, and round a is 5 (rounding cannot be used here, rounding is discarded, then b / b is 2.5 / 2.5, etc. 2019 July 25th, 2016-Analysis: The type of the value of the mixed expression is determined by the type with the highest precision in the expression, so it can be seen that option B can be excluded.",
  "585787": "Note that the result of b / b should be 1.00000, and (int) a is 5, and the result of the addition is still double.",
  "585788": "Q1: & Suppose a and b are double constants, a=5.5, and b=2.5, the value of the expression (int)a+b/b is ().",
  "585789": "2l7mmA.2mm5.500000.",
  "585790": "2l7mmB.2mm6.000000.",
  "585791": "EQUATION_DEPTH_0_START \\star EQUATION_DEPTH_0_END",
  "585792": "2l7mmC.2mm6.500000.",
  "585793": "2l7mmD.2mm6.",
  "585794": "C2: & November 22, 2016 It can be seen that it is not a white box test case design method, so the correct answer to question (31) is B. Black box testing is also called functional testing, which is to detect whether each function can be used normally.",
  "585795": "At the test site, treat the program as.",
  "585796": "November 18, 2016 Black box testing technology is also called functional testing, which tests the external characteristics of the software without considering the internal structure and characteristics of the software.",
  "585797": "The main purpose of black box testing is to discover the following types of errors: Are there any errors.",
  "585798": "Answer Analysis.",
  "585799": "Q2: &  Black box testing is also called functional testing, and black box testing cannot find ().",
  "585800": "2l7mmA.2mmterminal error.",
  "585801": "2l7mmB.2mmcommunication error.",
  "585802": "2l7mmC.2mminterface error.",
  "585803": "2l7mmD.2mmcode redundancy.",
  "585804": "EQUATION_DEPTH_0_START \\star EQUATION_DEPTH_0_END",
  "585805": "C3: & July 21, 2014-Friedman believes that the transmission variable of monetary policy should be ().",
  "585806": "Please help to give the correct answer and analysis, thank you!",
  "585807": "Reward: 0 answer bean Questioner: 00***42 Release time: 2014-07-21 View.",
  "585808": "Q3: & Friedman believes that the transmission variable of monetary policy should be ().",
  "585809": "2l7mmA.2mmexcess reserve.",
  "585810": "2l7mmB.2mminterest rate.",
  "585811": "2l7mmC.2mmcurrency supply.",
  "585812": "EQUATION_DEPTH_0_START \\star EQUATION_DEPTH_0_END",
  "585813": "2l7mmD.2mmbase currency.",
  "585814": "English translation of sample instances in ExamQA with retrieved context ( EQUATION_DEPTH_0_START \\star EQUATION_DEPTH_0_END : the correct answer option).",
  "585815": "(Label tab:sample1b-en)",
  "585816": "Self-Teaching Paradigm.",
  "585817": "(Label sec:method)",
  "585818": "In this section, we introduce a self-teaching paradigm to leverage large-scale QA-based weakly-labeled data to improve the performance of existing supervised methods on an MRC task of interest, which is relatively small-scale.",
  "585819": "Due to limited space, here we only discuss multiple-choice data and tasks and leave the reformulation (, soft labels and loss functions) for extractive MRC tasks in Appendix (Ref sec:appendix).",
  "585820": "Training a Multi-Skilled Teacher.",
  "585821": "(Label sec:method:multi-teacher)",
  "585822": "In previous teacher-student frameworks Citation (you2019teach,wang2019go,sun2020improving), multiple teacher models are trained using different data.",
  "585823": "However, it is difficult to divide the weakly-labeled data based on existing QA instances into sub-datasets by subjects or fine-grained types of knowledge required for answering questions.",
  "585824": "Instead, we train a multi-skilled teacher model using both the human-annotated target MRC data and the weakly-labeled data, which requires a diverse skill set and knowledge from a variety of domains.",
  "585825": "Let EQUATION_DEPTH_0_START V EQUATION_DEPTH_0_END denote a set of human-annotated instances and EQUATION_DEPTH_0_START W EQUATION_DEPTH_0_END denote a set of weakly-labeled instances.",
  "585826": "For each instance EQUATION_DEPTH_0_START t \\in V\\cup W EQUATION_DEPTH_0_END , we let EQUATION_DEPTH_0_START m_t EQUATION_DEPTH_0_END denote its total number of answer options, and EQUATION_DEPTH_0_START {\\bm h}^{(t)} EQUATION_DEPTH_0_END be a one-hot (hard-label) vector such that EQUATION_DEPTH_0_START {h}^{(t)}_j = 1 EQUATION_DEPTH_0_END if the EQUATION_DEPTH_0_START j EQUATION_DEPTH_0_END -th answer option is labeled as correct.",
  "585827": "We train a single multi-skilled teacher model, denoted by EQUATION_DEPTH_0_START \\mathcal{T} EQUATION_DEPTH_0_END , and optimize EQUATION_DEPTH_0_START \\mathcal{T} EQUATION_DEPTH_0_END by minimizing EQUATION_DEPTH_0_START \\sum_{t \\in V\\cup W} L_1(t, \\theta_{\\mathcal{T}}) EQUATION_DEPTH_0_END ; EQUATION_DEPTH_0_START L_1 EQUATION_DEPTH_0_END is defined as",
  "585828": "EQUATION_DEPTH_0_START L_1(t, \\theta) = - \\sum_{1 \\le k \\le m_t} {h}^{(t)}_k ~ \\log p_\\theta(k\\,|\\,t), \nEQUATION_DEPTH_0_END where EQUATION_DEPTH_0_START p_\\theta(k\\,|\\,t) EQUATION_DEPTH_0_END denotes the probability that the EQUATION_DEPTH_0_START k EQUATION_DEPTH_0_END -th answer option of instance EQUATION_DEPTH_0_START t EQUATION_DEPTH_0_END is correct, estimated by the  model with parameters EQUATION_DEPTH_0_START \\theta EQUATION_DEPTH_0_END.",
  "585829": "ht!",
  "585830": "width=0.96Figures/Examnet-examqa.pdf       Self-teaching framework using large-scale QA data to improve relatively small-scale MRC.",
  "585831": "(Label fig:overview_single)",
  "585832": "Training a Multi-Skilled Student.",
  "585833": "(Label sec:method:multi-student)",
  "585834": "We then train a multi-skilled student model EQUATION_DEPTH_0_START \\mathcal{S} EQUATION_DEPTH_0_END using the same data as the multi-skilled teacher model EQUATION_DEPTH_0_START \\mathcal{T} EQUATION_DEPTH_0_END while replacing the hard labels of answer options with the soft labels predicted by EQUATION_DEPTH_0_START \\mathcal{T} EQUATION_DEPTH_0_END.",
  "585835": "We define soft-label vector EQUATION_DEPTH_0_START {\\bm s}^{(t)} EQUATION_DEPTH_0_END for EQUATION_DEPTH_0_START t \\in V\\cup W EQUATION_DEPTH_0_END such that  EQUATION_DEPTH_0_START {s}^{(t)}_k =\n     \\lambda~{h}^{(t)}_k + (1 - \\lambda) p_{\\theta_{\\mathcal T}}(k\\,|\\,t)  \\\\,\nEQUATION_DEPTH_0_END where EQUATION_DEPTH_0_START \\lambda\\in [0, 1] EQUATION_DEPTH_0_END is a weight parameter, and EQUATION_DEPTH_0_START k = 1,\\dots, m_t EQUATION_DEPTH_0_END.",
  "585836": "We optimize multi-skilled student EQUATION_DEPTH_0_START \\mathcal{S} EQUATION_DEPTH_0_END",
  "585837": "Training an Expert Student.",
  "585838": "(Label sec:method:expert-student)",
  "585839": "Finally, we initialize an expert student EQUATION_DEPTH_0_START \\mathcal{E} EQUATION_DEPTH_0_END with the resulting multi-skilled student model EQUATION_DEPTH_0_START \\mathcal{S} EQUATION_DEPTH_0_END , and we fine-tune EQUATION_DEPTH_0_START \\mathcal{E} EQUATION_DEPTH_0_END on the target data EQUATION_DEPTH_0_START V EQUATION_DEPTH_0_END to help it achieve expertise in the task of interest, following most of the recent MRC methods Citation (radfordimproving,devlin-etal-2019-bert).",
  "585840": "This step differs from previous work in that we use the soft labels generated by the multi-skilled student model (Section (Ref sec:method:multi-student)) based on our assumption that a student model tends to learn better from a stronger teacher model.",
  "585841": "We will discuss more details in the experiment section and show that a student model tends to outperform its teacher model that provides soft labels to make itself a stronger teacher (Section (Ref sec:expriment)).",
  "585842": "We define new soft-label vector EQUATION_DEPTH_0_START {\\bm{\\tilde{s}}}^{(t)} EQUATION_DEPTH_0_END for EQUATION_DEPTH_0_START t \\in V EQUATION_DEPTH_0_END such that  EQUATION_DEPTH_0_START {\\tilde s}^{(t)}_k =\n     \\lambda~{h}^{(t)}_k + (1 - \\lambda) p_{\\theta_{\\mathcal S}}(k\\,|\\,t)  \\\\,\nEQUATION_DEPTH_0_END where EQUATION_DEPTH_0_START \\lambda\\in [0, 1] EQUATION_DEPTH_0_END is a weight parameter, and EQUATION_DEPTH_0_START k = 1,\\dots, m_t EQUATION_DEPTH_0_END.",
  "585843": "In this stage, we optimize EQUATION_DEPTH_0_START \\mathcal{E} EQUATION_DEPTH_0_END by minimizing EQUATION_DEPTH_0_START \\sum_{t\\in V} L_3(t, \\theta_{\\mathcal E}) EQUATION_DEPTH_0_END , where EQUATION_DEPTH_0_START L_3 EQUATION_DEPTH_0_END is defined as",
  "585844": "EQUATION_DEPTH_0_START L_3(t, \\theta) = -\\sum_{1\\le k\\le m_t} {\\tilde{s}}^{(t)}_k ~ \\log p_{\\theta}(k\\,|\\,t).\nEQUATION_DEPTH_0_END  Figure (Ref fig:overview_single) shows an overview of the proposed self-teaching paradigm.",
  "585845": "Integrating Different Types of Weakly-Labeled Data.",
  "585846": "(Label sec:method:integration)",
  "585847": "We study the integration of multiple types of weakly-labeled data during weakly-supervised training with soft labels to save time and effort in retraining models on EQUATION_DEPTH_0_START W EQUATION_DEPTH_0_END with hard labels.",
  "585848": "Take another weakly-labeled multiple-choice MRC data extracted automatically from television show and film scripts Citation (sun2020improving) as an example, denoted as EQUATION_DEPTH_0_START W_s EQUATION_DEPTH_0_END , besides the weakly-labeled data EQUATION_DEPTH_0_START W EQUATION_DEPTH_0_END we construct based on existing question-answering instances.",
  "585849": "Following the above three-step procedure, we first train a multi-skilled teacher EQUATION_DEPTH_0_START \\mathcal{T}_s EQUATION_DEPTH_0_END using EQUATION_DEPTH_0_START W_s EQUATION_DEPTH_0_END to generate soft labels of EQUATION_DEPTH_0_START W_s EQUATION_DEPTH_0_END and EQUATION_DEPTH_0_START V EQUATION_DEPTH_0_END.",
  "585850": "We then train a multi-skilled student EQUATION_DEPTH_0_START \\mathcal{S}_{\\ast} EQUATION_DEPTH_0_END upon the combination of soft-labeled EQUATION_DEPTH_0_START W_s EQUATION_DEPTH_0_END , EQUATION_DEPTH_0_START W EQUATION_DEPTH_0_END (Section (Ref sec:method:multi-student)), and EQUATION_DEPTH_0_START V EQUATION_DEPTH_0_END.",
  "585851": "Note that we simply use two versions of soft-labeled EQUATION_DEPTH_0_START V EQUATION_DEPTH_0_END generated by EQUATION_DEPTH_0_START \\mathcal{T} EQUATION_DEPTH_0_END and EQUATION_DEPTH_0_START \\mathcal{T}_s EQUATION_DEPTH_0_END , respectively.",
  "585852": "The resulting student EQUATION_DEPTH_0_START \\mathcal{S}_{\\ast} EQUATION_DEPTH_0_END is used to generate the final soft labels of EQUATION_DEPTH_0_START V EQUATION_DEPTH_0_END for training an expert student.",
  "585853": "We will discuss integration with other types of weakly-labeled data wherein noise is introduced by various factors in Section (Ref sec:exp:main).",
  "585854": "Experiments.",
  "585855": "(Label sec:expriment)",
  "585856": "Data Statistics.",
  "585857": "We show statistics of two relatively small-scale target MRC datasets and two kinds of large-scale weakly-labeled MRC data in Table (Ref tab:exp:statistics).",
  "585858": "Note that for CMRC 2018, we use its publicly available training and development sets.",
  "585859": "ht!",
  "585860": "p1.8cmll  data          & source            & \\# of instances",
  "585861": "3lhuman-annotated:",
  "585862": "& language exams & 19,577",
  "585863": "CMRC 2018   & Wikipedia                  & 19,071",
  "585864": "3lweakly-labeled:",
  "585865": "SCRIPT      & TV/movie scripts & 700,816",
  "585866": "ExamQA    & multi-subject exams  & 638,436",
  "585867": "(Label tab:exp:statistics) Human-annotated and weakly-labeled machine reading comprehension data statistics.",
  "585868": "ht!",
  "585869": "llllllcc  2*id & 2*model & 2*init.",
  "585870": "& 2*teacher  & 2ctraining data & 2*dev & 2*test",
  "585871": "5-6     &   &  &   & name       & label       &      &",
  "585872": "0       & RoBERTa-wwm-ext-large Citation (xu2020clue)     &    --    & -- &  EQUATION_DEPTH_0_START \\diamondsuit EQUATION_DEPTH_0_END    & hard  & --    & 73.8",
  "585873": "1     & baseline (our implementation of 0)           &    --     & -- &  EQUATION_DEPTH_0_START \\diamondsuit EQUATION_DEPTH_0_END    & hard  & 73.9  (0.5)    & 73.4 (0.5)",
  "585874": "2     & multi-skilled teacher  & -- & -- & EQUATION_DEPTH_0_START \\diamondsuit EQUATION_DEPTH_0_END + ExamQA  & hard  & 74.0 (0.8)    & 75.6 (0.5)",
  "585875": "3     & multi-skilled student  & -- & 2 &  EQUATION_DEPTH_0_START \\diamondsuit EQUATION_DEPTH_0_END + ExamQA  & soft   & 75.7 (0.5)    & 77.1 (0.4)",
  "585876": "4     & expert student (variant)       & 3  & 2 &  EQUATION_DEPTH_0_START \\diamondsuit EQUATION_DEPTH_0_END & soft   & 77.8 (0.4) &  78.0 (0.3)",
  "585877": "5     & expert student       & 3   & 3 &  EQUATION_DEPTH_0_START \\diamondsuit EQUATION_DEPTH_0_END & soft   & 78.2 (0.3)    & 78.5 (0.2)",
  "585878": "(Label tab:performance) Average accuracy and standard deviation ( EQUATION_DEPTH_0_START \\% EQUATION_DEPTH_0_END ) on the dev and test sets of the  dataset. EQUATION_DEPTH_0_START \\diamondsuit EQUATION_DEPTH_0_END is the training set of  for all experiments; init. means the starting point, and -- in this column means using the pre-trained language model for initialization.",
  "585879": "Implementation Details.",
  "585880": "We follow recent state-of-the-art MRC methods for the model architecture that consists of a pre-trained language model and a classification layer.",
  "585881": "We use the same architecture for baselines and all teacher or student models.",
  "585882": "We use RoBERTa-wwm-ext-large Citation (cui-2020-revisiting) as the pre-trained language model for Chinese, which achieves state-of-the-art performance on representative MRC tasks such as  and CMRC 2018 Citation (xu2020clue).",
  "585883": "We are aware of the emerging new pre-trained language models for Chinese and leave the exploration of them for future studies.",
  "585884": "We train a multi-skilled teacher or student model for one epoch as large-scale weakly-labeled data is used.",
  "585885": "We train baselines and expert students on  and CMRC 2018 for eight epochs and two epochs, respectively.",
  "585886": "In all experiments, we set EQUATION_DEPTH_0_START \\lambda EQUATION_DEPTH_0_END (defined in Section (Ref sec:method:multi-student)-(Ref sec:method:expert-student)) to EQUATION_DEPTH_0_START 0.5 EQUATION_DEPTH_0_END  to allow easy comparisons with previous work Citation (sun2020improving), and we report the average score of five runs with different random seeds and standard deviation in brackets.",
  "585887": "See more setting details in Appendix (Ref sec:appendix:settings).",
  "585888": "Main Results and Observations.",
  "585889": "(Label sec:exp:main)",
  "585890": "As shown in Table (Ref tab:performance), under the proposed self-teaching paradigm, expert student (5) improves the state-of-the-art baseline (1) based on the same model architecture by up to EQUATION_DEPTH_0_START 5.1\\% EQUATION_DEPTH_0_END in accuracy on the  dataset.",
  "585891": "We also compare (5), its variant (4), and the intermediate teacher models (2 and 3), and we have the following observations.",
  "585892": "EQUATION_DEPTH_0_START \\\\ EQUATION_DEPTH_0_END Student models tend to outperform their corresponding teacher models in self-teaching.",
  "585893": "Under the self-teaching paradigm, we notice that student models always achieve higher accuracy on the MRC task of interest than their teacher models that generate soft labels for them.",
  "585894": "For example, the accuracy of the multi-skilled student (3) is EQUATION_DEPTH_0_START 1.5\\% EQUATION_DEPTH_0_END higher than the result EQUATION_DEPTH_0_START 75.6\\% EQUATION_DEPTH_0_END achieved by the multi-skilled teacher (2).",
  "585895": "EQUATION_DEPTH_0_START \\\\ EQUATION_DEPTH_0_END Using a strong multi-skilled model to provide soft labels helps across settings.",
  "585896": "We consider a teacher model to be strong if it achieves good performance on the MRC task of interest.",
  "585897": "We already demonstrate that training a student model with soft labels generated by a multi-skilled teacher model instead of hard labels yields positive improvements (3 2).",
  "585898": "In addition, using the multi-skilled student (3), which is stronger than the multi-skilled teacher (2), to provide soft labels of  to train an expert student results in a EQUATION_DEPTH_0_START 0.5\\% EQUATION_DEPTH_0_END increase in accuracy (5 4).",
  "585899": "To explore whether this also applies to expert models, we experiment with a variant of expert student (5): still starting from the same multi-skilled student (3), we now put back expert student (5) as the teacher model to generate soft labels of  to train an expert student variant.",
  "585900": "However, this variant does not yield further gains ( EQUATION_DEPTH_0_START 78.2 EQUATION_DEPTH_0_END ( EQUATION_DEPTH_0_START \\scriptsize 0.4 EQUATION_DEPTH_0_END )) on the development set).",
  "585901": "Seeing more data than the expert student may make the more ``knowledgable'' multi-skilled student a better teacher to provide soft labels of the target MRC data.",
  "585902": "While it is possible to use the multi-skilled student itself to obtain a stronger multi-skilled student, it is much less efficient to retrain a model upon the large-scale weakly-labeled data than the above variant.",
  "585903": "We leave the exploration of iterative self-teaching over weakly-labeled data to future work.",
  "585904": "EQUATION_DEPTH_0_START \\\\ EQUATION_DEPTH_0_END Large-scale weakly-labeled data based on subject-area QA instances can be helpful for MRC.",
  "585905": "We do not see noticeable improvements by merely combining large-scale weakly-labeled data and small-scale data to train a multi-skilled teacher over the resulting data for one epoch (2 in Table (Ref tab:performance)).",
  "585906": "Nevertheless, helping train multi-skilled teacher/student models, especially the multi-skilled student that is further used as a good starting point of the expert student, reflect the usefulness of the large-scale weakly-labeled data.",
  "585907": "Though starting from the multi-skilled teacher slightly boosts ( EQUATION_DEPTH_0_START +0.3\\% EQUATION_DEPTH_0_END in accuracy) a multi-skilled student's performance (Table (Ref tab:ablation)), using the resulting multi-skilled student to initialize and teach the expert student actually hurts performance ( EQUATION_DEPTH_0_START -0.7\\% EQUATION_DEPTH_0_END in accuracy on the development set), perhaps due to the overuse of the same weakly-labeled data (both soft and hard labels) upon a single model or the convergence between the multi-skilled teacher and multi-skilled student.",
  "585908": "Therefore, we do not use the multi-skilled teacher to initialize the multi-skilled student in our main experiment (3 in Table (Ref tab:performance)).",
  "585909": "h!",
  "585910": "llc  model & initialization & dev",
  "585911": "expert student             & multi-skilled student        & 78.2 (0.3)",
  "585912": "expert student             & multi-skilled teacher         & 77.7 (0.3)",
  "585913": "expert student             & --         & 74.9 (0.3)",
  "585914": "multi-skilled student      & multi-skilled teacher       & 76.0 (0.2)",
  "585915": "multi-skilled student      & --         & 75.7 (0.5)",
  "585916": "(Label tab:ablation)",
  "585917": "Ablation study and variant comparisons on the development set of (-- means using the pre-trained language model for initialization).",
  "585918": "Introducing more weakly-labeled data can lead to further performance gains.",
  "585919": "Using the method mentioned in Section (Ref sec:method:integration), introducing additional weakly-labeled MRC instances generated based on verbal-nonverbal knowledge automatically extracted from scripts, we observe EQUATION_DEPTH_0_START +1.5\\% EQUATION_DEPTH_0_END in accuracy over the best-performing expert student (5 in Table (Ref tab:performance)), which already outperforms the expert student obtained when we only use one-third of weakly-labeled data constructed based on ExamQA by EQUATION_DEPTH_0_START 0.8\\% EQUATION_DEPTH_0_END in accuracy (Table (Ref tab:more_data)).",
  "585920": "Furthermore, we show it is possible to use the same procedure to adapt self-teaching to incorporate extra noisy human-labeled multiple-choice MRC instances as there is a growing trend in constructing MRC benchmarks, especially in resource-rich languages such as English.",
  "585921": "We automatically translate instances from 's English counterparts RACE Citation (lai2017race) and DREAM Citation (sundream2018) that are also collected from language exams into Chinese (referred to as EQUATION_DEPTH_0_START \\text{MRC}_\\text{MT} EQUATION_DEPTH_0_END in Table (Ref tab:more_data)), and we apply self-teaching to additionally incorporate the data, leading to EQUATION_DEPTH_0_START +2.5\\% EQUATION_DEPTH_0_END in accuracy. We do not study how to further improve machine reading comprehension by just using extra clean human-annotated MRC data, which is not the main focus of this paper. These results suggest the flexibility and scalability of self-teaching, and increasing the amount of weakly-labeled data of different types may yield further improvements.",
  "585922": "h!   lccc  weakly-labeled data  & size & dev & test",
  "585923": "--                            & --   &  73.9  (0.5)    & 73.4 (0.5)",
  "585924": "subset of ExamQA              & 0.2M &  77.8 (0.2)      & 77.7 (0.1)",
  "585925": "ExamQA                        & 0.6M &  78.2 (0.3)      & 78.5 (0.2)",
  "585926": "ExamQA + SCRIPT              & 1.3M &   79.5 (0.2)      & 80.0 (0.2)",
  "585927": "mixed-labeled data  &   &   &",
  "585928": "ExamQA + EQUATION_DEPTH_0_START \\text{MRC}_\\text{MT} EQUATION_DEPTH_0_END & 0.7M   &  80.4 (0.1)       &   81.0 (0.2)",
  "585929": "(Label tab:more_data) Accuracy comparison of expert students, which are obtained when different size of weakly-labeled MRC data is used during self-teaching, on the dev and test sets of the  dataset (size: number of instances).",
  "585930": "The Usefulness of ExamQA for Extractive MRC Tasks.",
  "585931": "We mostly follow the self-teaching paradigm introduced in Section (Ref sec:method) and introduce how to apply self-teaching to extractive tasks by redefining hard and soft labels for probability distributions of being answer start and end tokens, changing the loss function for multi-skilled student and expert student from the original maximum likelihood to Kullback-Leibler divergence, etc., in Appendix (Ref sec:appendix). As there are major differences (, type of questions/answers and required prior knowledge) between extractive and multiple-choice MRC tasks, we do not see positive results by adapting the resulting best-performing multiple-choice expert student to initialize an extractive model.",
  "585932": "As shown in Table (Ref tab:cmrc), similar to our observations on , the expert student achieves the best performance, outperforming the baseline model we implemented based on the same pre-trained language model as previous work Citation (cui-2020-revisiting) by EQUATION_DEPTH_0_START 3.8\\% EQUATION_DEPTH_0_END in exact match and EQUATION_DEPTH_0_START 2.0\\% EQUATION_DEPTH_0_END in F EQUATION_DEPTH_0_START 1 EQUATION_DEPTH_0_END. As each (question, document) corresponds to two probability distributions in a much larger dimension compared to that of soft labels for multiple-choice tasks, due to memory limitations, we only use the weakly-labeled extractive MRC data based on one-third of ExamQA instances (same as the subset of ExamQA in Table (Ref tab:more_data)). It is likely that extractive MRC tasks will benefit from more weakly-labeled data.",
  "585933": "h!   llll  method                 &  extra data      & EM & F1",
  "585934": "Citation (cui-2020-revisiting) & N/A &  67.6  &  87.9",
  "585935": "baseline                      & N/A &  70.3 (1.4)  &  89.2  (0.2)",
  "585936": "multi-skilled teacher        & EQUATION_DEPTH_0_START \\diamond EQUATION_DEPTH_0_END &  71.8 (0.6)  &   89.8 (0.4)",
  "585937": "multi-skilled student        & EQUATION_DEPTH_0_START \\diamond EQUATION_DEPTH_0_END & 72.5 (0.6)      & 90.1    (0.5)",
  "585938": "expert student                                   & N/A &  74.1 (0.7)   &   91.2 (0.3)",
  "585939": "(Label tab:cmrc) EM and F1 (\\%) on the publicly available development set of CMRC 2018 ( EQUATION_DEPTH_0_START \\diamond EQUATION_DEPTH_0_END : subset of ExamQA used for training multi-skilled teacher and multi-skilled student under self-teaching).",
  "585940": "We also explore the robustness of the resulting extractive MRC expert model in a relatively noisy setting: we consider an extractive MRC dataset DRCD Citation (shao2018drcd) originally written in traditional Chinese wherein noise is caused by converting traditional characters into simplified ones, and we fine-tune the extractive expert model (Table (Ref tab:cmrc)) on this converted dataset. We still see positive results in this noisy setting (Table (Ref tab:drcd:dev) and (Ref tab:drcd:test)).",
  "585941": "h!   lll  model initialization                       & EM & F1",
  "585942": "Citation (cui-2020-revisiting)                     &  89.1  &  94.4",
  "585943": "RoBERTa-wwm-ext-large                          &  90.5 (0.4)  &  95.3  (0.1)",
  "585944": "baseline in Table (Ref tab:cmrc)               &  90.4 (0.1)  &  95.2 (0.2)",
  "585945": "expert student in Table (Ref tab:cmrc)         &  91.1 (0.2)  &  95.7 (0.2)",
  "585946": "(Label tab:drcd:dev) EM and F1 (\\%) on the dev set of DRCD.",
  "585947": "h!   lll  model initialization                       & EM & F1",
  "585948": "Citation (cui-2020-revisiting)                     &  88.9  &  94.1",
  "585949": "RoBERTa-wwm-ext-large                          &  90.2 (0.4)  &  94.9  (0.2)",
  "585950": "baseline in Table (Ref tab:cmrc)               &  90.5 (0.6)  &  94.9 (0.4)",
  "585951": "expert student in Table (Ref tab:cmrc)         &  90.9 (0.1)  &  95.2 (0.1)",
  "585952": "(Label tab:drcd:test) EM and F1 (\\%) on the test set of DRCD.",
  "585953": "Comparing Self-Teaching and Multi-Teacher Paradigms.",
  "585954": "h!   lllccc  paradigm    & weakly-labeled data  & data segmentation criteria & \\# of multi-skilled teachers  & dev & test",
  "585955": "self-teaching   & ExamQA & -- & 1 & 78.2 (0.3)    & 78.5 (0.2)",
  "585956": "multi-teacher   & ExamQA & random & 2  &  77.3 (0.5)   & 78.1 (0.2)",
  "585957": "multi-teacher   & ExamQA & random & 4  &  77.5 (0.5)   & 77.9 (0.2)",
  "585958": "self-teaching   & SCRIPT & -- & 1  & 77.9 (0.4)    & 77.9 (0.4)",
  "585959": "multi-teacher   & SCRIPT & random & 4  &  77.7 (0.2)    & 77.5 (0.3)",
  "585960": "multi-teacher   & SCRIPT & knowledge type & 4  &  77.7 (0.4)    & 77.9 (0.3)",
  "585961": "(Label tab:self-multi-teacher)",
  "585962": "Comparison of self-teaching and multi-teacher using different types of weakly-labeled data in accuracy (\\%) on the dev and test sets of the  dataset.",
  "585963": "For simplicity, we concentrate on multiple-choice tasks hereafter. Previous work shows that it is better to train multiple teacher models upon different types of weakly-labeled data with hard labels and then use these teachers to generate soft labels for both the weakly-labeled data and the small-scale MRC data, compared against training one model over the entire weakly-labeled data with hard labels and then fine-tuning it on the small-scale MRC dataset with hard labels Citation (sun2020improving). However, herein lies an unanswered question: whether teacher models' data diversity or number matters to the resulting expert student's performance.",
  "585964": "As it is difficult to divide ExamQA into subsets by subjects, which can result in hundreds of teachers, we shuffle ExamQA and divide it into two subsets of similar size and follow the multi-teacher paradigm mentioned above.",
  "585965": "We compare it with the self-teaching paradigm and find that self-teaching provides larger accuracy gains compared against multi-teacher when knowledge-based data segmentation is tricky (Table (Ref tab:self-multi-teacher)).",
  "585966": "We also consider the case when it is easy to split data into subsets by the type of knowledge: we compare self-teaching with multi-teacher given the weakly-labeled data based on four types of verbal-nonverbal knowledge extracted from scripts.",
  "585967": "Results reveal that the two paradigms have similar performance, indicating that the impact of the number of teacher models may be limited.",
  "585968": "To study the impact of data diversity of teachers, we shuffle SCRIPT and divide it into four subsets of similar size to train four teacher models.",
  "585969": "Using the same multi-teacher paradigm, we experimentally demonstrate that there is a weak correlation between the data diversity of teachers and the final performance of the expert student.",
  "585970": "The Impact of Cleanness and Source of Context in Weakly-Labeled Data.",
  "585971": "h!",
  "585972": "lccc  source of context  & denoise & dev & test",
  "585973": "search engine             & EQUATION_DEPTH_0_START \\times EQUATION_DEPTH_0_END &   78.2 (0.3)     & 78.5 (0.2)",
  "585974": "search engine             & EQUATION_DEPTH_0_START \\checkmark EQUATION_DEPTH_0_END &  77.0 (0.3)       & 77.5 (0.3)",
  "585975": "Wikipedia                 & EQUATION_DEPTH_0_START \\times EQUATION_DEPTH_0_END &  77.1  (0.3)      &  77.4 (0.2)",
  "585976": "(Label tab:context) Accuracy comparison of expert students on the dev and test sets of the  dataset, which are obtained when different types of sources are used to form context of weakly-labeled data.",
  "585977": "As mentioned previously, context returned by a web search engine tends to be noisy.",
  "585978": "For example, given a question as the search query, the question and its wrong answer options are included in context.",
  "585979": "We conduct a preliminary experiment to evaluate the impact of context cleanliness by removing wrong answer options from the context of each weakly-labeled MRC instance.",
  "585980": "However, context cleaning hurts accuracy by EQUATION_DEPTH_0_START 1.2\\% EQUATION_DEPTH_0_END on the development set of.",
  "585981": "It is possible that noisy context helps improve the generalization ability of both teacher and student models, similar to the role of noise (, dropout and data augmentation) that is intentionally injected to the student models during self-training in previous studies (, Citation (he2019revisiting,xie2020self)).",
  "585982": "Besides using snippets retrieved from a search engine to form context, we use the default search engine in Wikipedia to collect relevant snippets from Wikipedia for each question, leading to decreased accuracy ( EQUATION_DEPTH_0_START -1.1\\% EQUATION_DEPTH_0_END on ), perhaps due to answering questions in ExamQA requires fine-grained subject-specific knowledge that is seldom covered in Wikipedia.",
  "585983": "In the future, we are interested in using different types of texts (, news reports and dialogues) as context of structured knowledge such as question-answering instances to study the impact of relevance, completeness, and style on performance on downstream tasks.",
  "585984": "Related Work.",
  "585985": "From Question Answering to Machine Reading Comprehension.",
  "585986": "Here we do not compare with one direction in transfer learning in MRC when the source and target tasks are all clean MRC tasks Citation (chung-etal-2018-supervised,wang-2018-yuanfudao,shakeri-etal-2020-end,nishida-etal-2020-unsupervised), as it is expensive and time-consuming to construct high-quality large-scale MRC datasets considering factors such as ensuring the high relevance between questions and documents and the degree of difficulty of questions.",
  "585987": "This work is related to data augmentation in semi-supervised MRC studies, which partially or fully rely on the document-question-answer triples Citation (yang-etal-2017-semi,yuan2017machine,yu2018qanet,zhang-bansal-2019-addressing,zhu-2019-learning,dong2019unified,sun2018improving,alberti-etal-2019-synthetic,asai-hajishirzi-2020-logic,rennie-2020-unsupervised) of target MRC tasks or at least similar domain corpora Citation (dhingra-etal-2018-simple).",
  "585988": "We mainly focus on studying leveraging multi-domain question-answering data to improve different types of general-domain MRC tasks; though, at first glance, subject-area knowledge is seldom required for these MRC tasks.",
  "585989": "To the best of our knowledge, ExamQA is the largest multi-subject QA dataset collected from exams to date.",
  "585990": "We offer ExamQA mainly for the purpose of using large-scale subject-area QA data to improve other tasks such as MRC, rather than focusing on improving single-subject or multi-subject question answering.",
  "585991": "Teacher-Student Paradigms.",
  "585992": "Teacher-student paradigms are widely used for knowledge distillation Citation (ba2014deep,li2014learning,hinton2015distilling).",
  "585993": "We aim to let the student model outperform its teacher model to improve existing competitive supervised methods and simply use the same architecture for all teacher and student models.",
  "585994": "Our work is related to self-training Citation (yarowsky-1995-unsupervised,riloff1996automatically), as we also leverage unstructured texts for context generation.",
  "585995": "The main difference is that we generate weakly-labeled data based on existing large-scale QA data covering a wide range of domains, instead of the same domain Citation (he2019revisiting,xie2020unsupervised,zhao-etal-2020-robust,chen2020improved) or at least approximately in-domain Citation (du2020self) as the target task.",
  "585996": "Different from previous studies that iteratively use new teacher models to generate new pseudo data from unlabeled data (,  Citation (wang2020combining)), we use the new teacher model to generate new soft labels for fixed weakly-labeled and target data.",
  "585997": "Furthermore, we use a search engine to retrieve noisy and incomplete snippets instead of full sentences or even documents, which are seldom studied and used as context for structured knowledge via distant supervision for downstream natural language understanding tasks Citation (ye2019align).",
  "585998": "Compared with previous multi-teacher student paradigms Citation (you2019teach,wang2019go,yang2020model), we conduct iterative training and leverage large-scale weakly-labeled data to train models to be teachers, instead of using human-labeled clean data of similar tasks.",
  "585999": "Conclusions.",
  "586000": "It is still unclear how to improve MRC using large-scale subject-area QA data.",
  "586001": "In this paper, we collect a large-scale multi-subject multiple-choice QA dataset ExamQA.",
  "586002": "We use incomplete and noisy snippets returned by a web search engine as relevant context of each QA instance to convert it into a weakly-labeled MRC instance.",
  "586003": "Furthermore, we propose a self-teaching paradigm to better use these weakly-labeled MRC instances to improve an MRC task of interest.",
  "586004": "Experimental results show that we can obtain EQUATION_DEPTH_0_START +5.1\\% EQUATION_DEPTH_0_END in accuracy on a multiple-choice MRC dataset  and EQUATION_DEPTH_0_START +3.8\\% EQUATION_DEPTH_0_END in exact match on an extractive MRC dataset CMRC 2018, demonstrating the effectiveness of our framework and the usefulness of large-scale subject-area QA data for MRC.",
  "586005": "tacl2018 acl_natbib",
  "586006": "Appendix.",
  "586007": "(Label sec:appendix)",
  "586008": "Training a Multi-Skilled Teacher.",
  "586009": "(Label appendx:method:multi-teacher)",
  "586010": "Let EQUATION_DEPTH_0_START V EQUATION_DEPTH_0_END denote a set of human-labeled instances and EQUATION_DEPTH_0_START W EQUATION_DEPTH_0_END denote a set of weakly-labeled instances.",
  "586011": "Each instance contains a document EQUATION_DEPTH_0_START d EQUATION_DEPTH_0_END , a question EQUATION_DEPTH_0_START q EQUATION_DEPTH_0_END , and an answer span EQUATION_DEPTH_0_START a EQUATION_DEPTH_0_END in EQUATION_DEPTH_0_START d EQUATION_DEPTH_0_END.",
  "586012": "Let EQUATION_DEPTH_0_START a_{\\text{start}} EQUATION_DEPTH_0_END and EQUATION_DEPTH_0_START a_{\\text{end}} EQUATION_DEPTH_0_END denote, respectively, the start offset and end offset of EQUATION_DEPTH_0_START a EQUATION_DEPTH_0_END , which appears in EQUATION_DEPTH_0_START d EQUATION_DEPTH_0_END.",
  "586013": "For each instance EQUATION_DEPTH_0_START t EQUATION_DEPTH_0_END = ( EQUATION_DEPTH_0_START d EQUATION_DEPTH_0_END , EQUATION_DEPTH_0_START q EQUATION_DEPTH_0_END , EQUATION_DEPTH_0_START a EQUATION_DEPTH_0_END ), let EQUATION_DEPTH_0_START l_t EQUATION_DEPTH_0_END denote the length of the concatenated ( EQUATION_DEPTH_0_START q EQUATION_DEPTH_0_END , EQUATION_DEPTH_0_START d EQUATION_DEPTH_0_END ) taken as the input to an MRC model.",
  "586014": "We train a multi-skilled teacher model, denoted by EQUATION_DEPTH_0_START \\mathcal{T} EQUATION_DEPTH_0_END , which learns to predict the probability of each token in the input to be the start or end token of the correct answer.",
  "586015": "Let EQUATION_DEPTH_0_START p_{\\text{start}, \\theta}(k\\,|\\,t) EQUATION_DEPTH_0_END and EQUATION_DEPTH_0_START p_{\\text{end}, \\theta}(k\\,|\\,t) EQUATION_DEPTH_0_END denote the probabilities that the EQUATION_DEPTH_0_START k EQUATION_DEPTH_0_END -th token in ( EQUATION_DEPTH_0_START q EQUATION_DEPTH_0_END , EQUATION_DEPTH_0_START d EQUATION_DEPTH_0_END ) to be the start and end token respectively, estimated by a model with parameters EQUATION_DEPTH_0_START \\theta EQUATION_DEPTH_0_END.",
  "586016": "We optimize EQUATION_DEPTH_0_START \\mathcal{T} EQUATION_DEPTH_0_END by minimizing EQUATION_DEPTH_0_START \\sum_{t \\in V\\cup W} L_1(t, \\theta_{\\mathcal{T}}) EQUATION_DEPTH_0_END , where EQUATION_DEPTH_0_START L_1 EQUATION_DEPTH_0_END is defined as",
  "586017": "EQUATION_DEPTH_0_START L_{1}(t, \\theta) = - \\log p_{\\text{start}, \\theta}(a_{\\text{start}}\\,|\\,t)  - \\log p_{\\text{end}, \\theta}(a_{\\text{end}}\\,|\\,t). \nEQUATION_DEPTH_0_END",
  "586018": "Training a Multi-Skilled Student.",
  "586019": "(Label appendix:method:multi-student)",
  "586020": "We then train a multi-skilled student model EQUATION_DEPTH_0_START \\mathcal{S} EQUATION_DEPTH_0_END using the same data as the multi-skilled teacher model EQUATION_DEPTH_0_START \\mathcal{T} EQUATION_DEPTH_0_END while replacing the hard labels with the soft labels predicted by EQUATION_DEPTH_0_START \\mathcal{T} EQUATION_DEPTH_0_END.",
  "586021": "We define EQUATION_DEPTH_0_START {\\bm h}^{(t)}_{\\text{start}} EQUATION_DEPTH_0_END and EQUATION_DEPTH_0_START {\\bm h}^{(t)}_{\n\\text{end}} EQUATION_DEPTH_0_END to be one-hot hard-label vectors such that EQUATION_DEPTH_0_START {\\bm h}^{(t)}_{\\text{start},i} = 1 EQUATION_DEPTH_0_END and EQUATION_DEPTH_0_START {\\bm h}^{(t)}_{\\text{end},j}=1 EQUATION_DEPTH_0_END if the EQUATION_DEPTH_0_START i EQUATION_DEPTH_0_END -th and EQUATION_DEPTH_0_START j EQUATION_DEPTH_0_END -th tokens in ( EQUATION_DEPTH_0_START q EQUATION_DEPTH_0_END , EQUATION_DEPTH_0_START d EQUATION_DEPTH_0_END ) are the start and end token of the correct answer respectively.",
  "586022": "We define soft-label vectors EQUATION_DEPTH_0_START {\\bm s_\\text{start}}^{(t)} EQUATION_DEPTH_0_END and EQUATION_DEPTH_0_START {\\bm s_\\text{end}}^{(t)} EQUATION_DEPTH_0_END for EQUATION_DEPTH_0_START t \\in V\\cup W EQUATION_DEPTH_0_END such that  EQUATION_DEPTH_0_START {\\bm s}^{(t)}_{\\text{start},k}  =\n     \\lambda~{\\bm h}^{(t)}_{\\text{start}, k} + (1 - \\lambda) p_{\\text{start}, \\theta_{\\mathcal T}}(k\\,|\\,t)  \\\\\nEQUATION_DEPTH_0_END and   EQUATION_DEPTH_0_START {\\bm s}^{(t)}_{\\text{end},k}  =\n     \\lambda~{\\bm h}^{(t)}_{\\text{end},k} + (1 - \\lambda) p_{\\text{end}, \\theta_{\\mathcal T}}(k\\,|\\,t)  \\\\,\nEQUATION_DEPTH_0_END where EQUATION_DEPTH_0_START \\lambda\\in [0, 1] EQUATION_DEPTH_0_END is a weight parameter, and EQUATION_DEPTH_0_START k = 1,\\dots, l_t EQUATION_DEPTH_0_END.",
  "586023": "We optimize multi-skilled student EQUATION_DEPTH_0_START \\mathcal{S} EQUATION_DEPTH_0_END by minimizing EQUATION_DEPTH_0_START \\sum_{t \\in V\\cup W}L_2(t, \\theta_{\\mathcal S}) EQUATION_DEPTH_0_END , where EQUATION_DEPTH_0_START L_2 EQUATION_DEPTH_0_END is defined as  EQUATION_DEPTH_0_START L_{\\text{start}, 2}(t, \\theta) & = -\\sum_{1\\le k\\le l_t} {\\bm{s}}^{(t)}_{\\text{start}, k} ~ \\log p_{\\text{start}, \\theta}(k\\,|\\,t) \\\\\n     L_{\\text{end}, 2}(t, \\theta) & = -\\sum_{1\\le k\\le l_t} {\\bm{s}}^{(t)}_{\\text{end},k} ~ \\log p_{\\text{end}, \\theta}(k\\,|\\,t)  \\\\\n     L_{2}(t, \\theta) & = \\frac{1}{2} (L_{\\text{start}, 2}(t, \\theta) + L_{\\text{end}, 2}(t, \\theta) ).\nEQUATION_DEPTH_0_END",
  "586024": "Training an Expert Student.",
  "586025": "(Label appendix:method:expert-student)",
  "586026": "We now introduce the formulation of training expert student EQUATION_DEPTH_0_START \\mathcal{E} EQUATION_DEPTH_0_END.",
  "586027": "For instance EQUATION_DEPTH_0_START t\\in V EQUATION_DEPTH_0_END , we define new soft-label vectors EQUATION_DEPTH_0_START {\\bm{\\tilde{s}_\\text{start}}}^{(t)} EQUATION_DEPTH_0_END and EQUATION_DEPTH_0_START {\\bm{\\tilde{s}_\\text{end}}}^{(t)} EQUATION_DEPTH_0_END such that   EQUATION_DEPTH_0_START { \\bm{\\tilde s}}^{(t)}_{\\text{start},k} =\n     \\lambda~{\\bm h}^{(t)}_{\\text{start},k} + (1 - \\lambda) p_{\\text{start}, \\theta_{\\mathcal S}}(k\\,|\\,t)  \\\\ \nEQUATION_DEPTH_0_END and  EQUATION_DEPTH_0_START { \\bm{\\tilde s}}^{(t)}_{\\text{end},k} =\n     \\lambda~{\\bm h}^{(t)}_{\\text{end},k} + (1 - \\lambda) p_{\\text{end}, \\theta_{\\mathcal S}}(k\\,|\\,t)  \\\\,\nEQUATION_DEPTH_0_END where EQUATION_DEPTH_0_START \\lambda\\in [0, 1] EQUATION_DEPTH_0_END is a weight parameter, and EQUATION_DEPTH_0_START k = 1,\\dots, l_t EQUATION_DEPTH_0_END.",
  "586028": "We optimize EQUATION_DEPTH_0_START \\mathcal{E} EQUATION_DEPTH_0_END by minimizing EQUATION_DEPTH_0_START \\sum_{t\\in V} L_3(t, \\theta_{\\mathcal E}) EQUATION_DEPTH_0_END , where EQUATION_DEPTH_0_START L_3 EQUATION_DEPTH_0_END is defined as  EQUATION_DEPTH_0_START L_{\\text{start}, 3}(t, \\theta) & = -\\sum_{1\\le k\\le l_t} {\\bm{\\tilde{s}}}^{(t)}_{\\text{start},k} ~ \\log p_{\\text{start}, \\theta}(k\\,|\\,t) \\\\\n     L_{\\text{end}, 3}(t, \\theta) & = -\\sum_{1\\le k\\le l_t} {\\bm{\\tilde{s}}}^{(t)}_{\\text{end},k} ~ \\log p_{\\text{end}, \\theta}(k\\,|\\,t)  \\\\\n     L_{3}(t, \\theta) & = \\frac{1}{2} (L_{\\text{start}}(t, \\theta) + L_{\\text{end}}(t, \\theta) ).\nEQUATION_DEPTH_0_END",
  "586029": "Settings.",
  "586030": "(Label sec:appendix:settings)",
  "586031": "h   lll                        & mst/mss & es/baseline",
  "586032": "training data         &  ExamQA + &",
  "586033": "initial learning rate &   2e-5                            & 2e-5",
  "586034": "batch size            &   24                               &   24",
  "586035": "\\# of training epochs       &    1                       &      8",
  "586036": "max sequence length         &  512                         & 512",
  "586037": "training labels                    & hard/soft          & soft/hard",
  "586038": "(Label tab:hyper) Hyper-parameter settings for training multiple-choice machine reading comprehension models (mst: multi-skilled teacher; mss: multi-skilled student; es: expert student).",
  "586039": "h   p2.4cmp1.9cmp2.1cm                        & mst/mss  & es/baseline",
  "586040": "training data                           & EQUATION_DEPTH_0_START \\diamond EQUATION_DEPTH_0_END  + CMRC 2018   & CMRC 2018 / DRCD",
  "586041": "initial learning rate                   &  3e-5          & 3e-5",
  "586042": "batch size                                &  32     &  32",
  "586043": "\\# of training epochs                      &  1     &     2",
  "586044": "max sequence length                         & 512   & 512",
  "586045": "training labels                                  & hard/soft          & soft/hard",
  "586046": "(Label tab:hyper2) Hyper-parameter settings for training extractive machine reading comprehension models ( EQUATION_DEPTH_0_START \\diamond EQUATION_DEPTH_0_END : subset of ExamQA; mst: multi-skilled teacher; mss: multi-skilled student; es: expert student)."
}
