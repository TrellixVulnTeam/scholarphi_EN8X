{"uist-8": [{"text": "Text entry is expected to be a common task for smart glass users, which is generally performed using a touchpad on the temple or by a promising approach using eye tracking.", "label": "Background", "bboxes": [{"left": 0.08761437908496732, "top": 0.2427209595959596, "width": 0.39767156862745096, "height": 0.012579545454545454, "page": 0}, {"left": 0.08811928104575163, "top": 0.2565580808080808, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.270395202020202, "width": 0.35780065359477126, "height": 0.012579545454545482, "page": 0}], "section": "Abstract", "prob": 0.8098876476287842, "is_author_statement": false, "is_in_expected_section": false, "id": "5724"}, {"text": "Text entry; Smart glasses; Gaze and Touch, Multi-modal input", "label": "Background", "bboxes": [{"left": 0.08811928104575163, "top": 0.549324494949495, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 0}], "section": "Abstract", "prob": 0.5530502796173096, "is_author_statement": false, "is_in_expected_section": false, "id": "5725"}, {"text": "For more efcient text entry, we present the concept of gaze-assisted typing (GAT), which uses both a touchpad and eye tracking.", "label": "Background", "bboxes": [{"left": 0.3706993464052287, "top": 0.28423106060606057, "width": 0.11459313725490206, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.2980681818181818, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.08758823529411765, "top": 0.311905303030303, "width": 0.3658578431372549, "height": 0.012579545454545482, "page": 0}], "section": "Abstract", "prob": 0.4802558720111847, "is_author_statement": true, "is_in_expected_section": false, "id": "5726"}, {"text": "However, each approach has its own limitations.", "label": "Background", "bboxes": [{"left": 0.45096732026143793, "top": 0.270395202020202, "width": 0.03702614379084962, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.28423106060606057, "width": 0.27755555555555556, "height": 0.012579545454545482, "page": 0}], "section": "Abstract", "prob": 0.43163543939590454, "is_author_statement": false, "is_in_expected_section": false, "id": "5727"}, {"text": "We initially examined GAT with a minimal eye input load, and demonstrated that the GAT technology was 51% faster than a two-step touch input typing method ( i.e. ,M-SwipeBoard: 5.85 words per minute (wpm) and GAT: 8.87 wpm).", "label": "Result", "bboxes": [{"left": 0.46355718954248365, "top": 0.311905303030303, "width": 0.021735294117647075, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.3257424242424242, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.33957954545454544, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.3531275252525253, "width": 0.39943137254901956, "height": 0.012867424242424208, "page": 0}, {"left": 0.08811928104575163, "top": 0.3672537878787879, "width": 0.34039869281045754, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.40916359424591064, "is_author_statement": true, "is_in_expected_section": false, "id": "5728"}, {"text": "Finally, we compared GAT with touch-only typing (SwipeZone) and eye-only typing (adjustable dwell) using an eye-trackable head-worn display.", "label": "Method", "bboxes": [{"left": 0.2998725490196078, "top": 0.42260227272727274, "width": 0.18591830065359477, "height": 0.012579545454545427, "page": 0}, {"left": 0.08753267973856209, "top": 0.4364381313131313, "width": 0.4004640522875817, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.4502752525252525, "width": 0.3672761437908497, "height": 0.012579545454545482, "page": 0}], "section": "Abstract", "prob": 0.41956189274787903, "is_author_statement": true, "is_in_expected_section": true, "id": "5729"}, {"text": "The results showed that a GAT requiring ve different touch gestures was the most preferred, although all GAT techniques were equally efcient.", "label": "Result", "bboxes": [{"left": 0.1293218954248366, "top": 0.3949280303030303, "width": 0.35597385620915023, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.4087651515151515, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.42260227272727274, "width": 0.20077777777777778, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.8001009225845337, "is_author_statement": false, "is_in_expected_section": false, "id": "5730"}, {"text": "We also compared GAT methods with varying numbers of touch gestures.", "label": "Result", "bboxes": [{"left": 0.4335620915032679, "top": 0.3672537878787879, "width": 0.05172712418300662, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.3810909090909091, "width": 0.39987418300653593, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.3949280303030303, "width": 0.036143790849673216, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.623153030872345, "is_author_statement": true, "is_in_expected_section": false, "id": "5731"}, {"text": "The results demonstrate that the most preferred technique, GAT, was 25.4% faster than the eye-only typing and 29.4% faster than the touch-only typing (GAT: 11.04 wpm, eye-only typing: 8.81 wpm, and touch-only typing: 8.53 wpm).", "label": "Result", "bboxes": [{"left": 0.46048529411764705, "top": 0.4502752525252525, "width": 0.024807189542483676, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.46411237373737374, "width": 0.39919934640522875, "height": 0.012579545454545482, "page": 0}, {"left": 0.0875375816993464, "top": 0.47794949494949496, "width": 0.3980375816993465, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.4917866161616162, "width": 0.399437908496732, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.5056237373737373, "width": 0.3012222222222222, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.8713722825050354, "is_author_statement": false, "is_in_expected_section": false, "id": "5732"}], "uist-4": [{"text": "Lip-Interact repurposes the front camera to capture the users mouth movements and recognize the issued commands with an end-to-end deep learning model.", "label": "Background", "bboxes": [{"left": 0.14036764705882354, "top": 0.3320328282828283, "width": 0.34493790849673206, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.3458686868686869, "width": 0.39719117647058827, "height": 0.012579545454545482, "page": 0}, {"left": 0.08753267973856209, "top": 0.3597058080808081, "width": 0.26062745098039214, "height": 0.012579545454545482, "page": 0}], "section": "Abstract", "prob": 0.74391770362854, "is_author_statement": false, "is_in_expected_section": false, "id": "5733"}, {"text": "We present Lip-Interact, an interaction technique that allows users to issue commands on their smartphone through silent speech.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.30435858585858583, "width": 0.3979477124183007, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.31819570707070705, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.3320328282828283, "width": 0.04739869281045751, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.5667060017585754, "is_author_statement": true, "is_in_expected_section": false, "id": "5734"}, {"text": "Our system supports 44 commands for accessing both system-level functionalities (launching apps, changing system settings, and handling pop-up windows) and application-level functionalities (integrated operations for two apps).", "label": "Background", "bboxes": [{"left": 0.35321732026143793, "top": 0.3597058080808081, "width": 0.13208823529411762, "height": 0.012579545454545482, "page": 0}, {"left": 0.08754901960784313, "top": 0.3735429292929293, "width": 0.40046241830065366, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.38738005050505053, "width": 0.3971960784313725, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.40121717171717175, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.41505429292929297, "width": 0.21226143790849678, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.4839771091938019, "is_author_statement": true, "is_in_expected_section": false, "id": "5735"}, {"text": "We verify the feasibility of Lip-Interact with three user experiments: evaluating the recognition accuracy, comparing with touch on input efciency, and comparing with voiced commands with regards to personal privacy and social norms.", "label": "Method", "bboxes": [{"left": 0.30624183006535943, "top": 0.41505429292929297, "width": 0.17906862745098046, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.4288914141414142, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.4427285353535354, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.4565656565656566, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.47040277777777784, "width": 0.17048366013071897, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.5588720440864563, "is_author_statement": true, "is_in_expected_section": false, "id": "5736"}, {"text": "We demonstrate that Lip-Interact can help users access functionality efciently in one step, enable one-handed input when the other hand is occupied, and assist touch to make interactions more uent.", "label": "Result", "bboxes": [{"left": 0.2648137254901961, "top": 0.47040277777777784, "width": 0.22049673202614373, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.48423863636363634, "width": 0.39988562091503266, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.49807575757575756, "width": 0.39718300653594774, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.5119128787878788, "width": 0.29420098039215686, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.6878758072853088, "is_author_statement": true, "is_in_expected_section": false, "id": "5737"}], "uist-5": [{"text": "Navigating stairs is a dangerous mobility challenge for people with low vision, who have a visual impairment that falls short of blindness.", "label": "Background", "bboxes": [{"left": 0.08823529411764706, "top": 0.2572941919191919, "width": 0.3942238562091503, "height": 0.012727272727272754, "page": 0}, {"left": 0.08821895424836601, "top": 0.27184217171717173, "width": 0.39417320261437905, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.28638888888888886, "width": 0.1263300653594771, "height": 0.012727272727272754, "page": 0}], "section": "Abstract", "prob": 0.9330760836601257, "is_author_statement": false, "is_in_expected_section": false, "id": "5738"}, {"text": "Prior research contributed systems for stair navigation that provide audio or tactile feedback, but people with low vision have usable vision and dont typically use nonvisual aids.", "label": "Background", "bboxes": [{"left": 0.22163071895424838, "top": 0.28638888888888886, "width": 0.2607728758169935, "height": 0.012727272727272754, "page": 0}, {"left": 0.08821895424836601, "top": 0.3009368686868687, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.3154835858585859, "width": 0.3941437908496732, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.33003030303030306, "width": 0.12932679738562092, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.8925529718399048, "is_author_statement": false, "is_in_expected_section": false, "id": "5739"}, {"text": "We designed visualizations for a projection-based AR platform and smartglasses, considering the different characteristics of these platforms.", "label": "Background", "bboxes": [{"left": 0.3269934640522876, "top": 0.359125, "width": 0.15543300653594772, "height": 0.012727272727272698, "page": 0}, {"left": 0.08820261437908496, "top": 0.3736729797979798, "width": 0.3941405228758171, "height": 0.012727272727272698, "page": 0}, {"left": 0.08820261437908496, "top": 0.388219696969697, "width": 0.394124183006536, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.4292498230934143, "is_author_statement": true, "is_in_expected_section": false, "id": "5740"}, {"text": "We conducted the first exploration of augmented reality (AR) visualizations to facilitate stair navigation for people with low vision.", "label": "Background", "bboxes": [{"left": 0.22429901960784313, "top": 0.33003030303030306, "width": 0.25807679738562095, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.34457828282828284, "width": 0.39422385620915035, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.359125, "width": 0.23327287581699346, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.37689483165740967, "is_author_statement": true, "is_in_expected_section": false, "id": "5741"}, {"text": "For projection-based AR, we designed visual highlights that are projected directly on the stairs.", "label": "Method", "bboxes": [{"left": 0.08820261437908496, "top": 0.4027676767676768, "width": 0.3941732026143791, "height": 0.012727272727272698, "page": 0}, {"left": 0.08818627450980392, "top": 0.4170088383838384, "width": 0.2379673202614379, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.38944220542907715, "is_author_statement": true, "is_in_expected_section": true, "id": "5742"}, {"text": "In contrast, for smartglasses that have a limited vertical field of view, we designed visualizations that indicate the users position on the stairs, without directly augmenting the stairs themselves.", "label": "Method", "bboxes": [{"left": 0.33262581699346405, "top": 0.4170088383838384, "width": 0.14974183006535946, "height": 0.012727272727272698, "page": 0}, {"left": 0.08820261437908496, "top": 0.4315568181818182, "width": 0.3941732026143791, "height": 0.012727272727272698, "page": 0}, {"left": 0.08820261437908496, "top": 0.44610353535353536, "width": 0.3941503267973856, "height": 0.012727272727272698, "page": 0}, {"left": 0.08820261437908496, "top": 0.46065151515151515, "width": 0.3300539215686275, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.3234885036945343, "is_author_statement": true, "is_in_expected_section": true, "id": "5743"}, {"text": "Our designs on both platforms largely increased participants self-reported psychological security.", "label": "Result", "bboxes": [{"left": 0.4568970588235294, "top": 0.5042929292929292, "width": 0.025446078431372587, "height": 0.01272727272727281, "page": 0}, {"left": 0.08818627450980392, "top": 0.5188396464646464, "width": 0.3941568627450981, "height": 0.012727272727272698, "page": 0}, {"left": 0.08818627450980392, "top": 0.5333876262626263, "width": 0.23823039215686276, "height": 0.01272727272727281, "page": 0}], "section": "Abstract", "prob": 0.6490868926048279, "is_author_statement": true, "is_in_expected_section": false, "id": "5744"}, {"text": "Accessibility; augmented reality; low vision; visualization.", "label": "Other", "bboxes": [{"left": 0.08823529411764706, "top": 0.5685063131313132, "width": 0.3848676470588235, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.6437925696372986, "is_author_statement": false, "is_in_expected_section": false, "id": "5745"}, {"text": "We evaluated our visualizations on each platform with 12 people with low vision, finding that the visualizations for projection-based AR increased participants walking speed.", "label": "Result", "bboxes": [{"left": 0.4226045751633987, "top": 0.46065151515151515, "width": 0.059771241830065325, "height": 0.012727272727272698, "page": 0}, {"left": 0.08820261437908496, "top": 0.4751982323232324, "width": 0.3941732026143791, "height": 0.012727272727272698, "page": 0}, {"left": 0.08820261437908496, "top": 0.48974494949494946, "width": 0.39422385620915035, "height": 0.012727272727272754, "page": 0}, {"left": 0.08821895424836601, "top": 0.5042929292929292, "width": 0.36208496732026146, "height": 0.01272727272727281, "page": 0}], "section": "Abstract", "prob": 0.6766931414604187, "is_author_statement": true, "is_in_expected_section": false, "id": "5746"}], "uist-2": [{"text": "However, where handheld phones present challenges, head-worn displays (HWDs) could further communication through privately transcribed text, handsfree use, improved mobility, and socially acceptable interactions.", "label": "Background", "bboxes": [{"left": 0.20035130718954247, "top": 0.4821439393939394, "width": 0.28212418300653597, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.4966919191919192, "width": 0.39442156862745104, "height": 0.012727272727272754, "page": 0}, {"left": 0.08823529411764706, "top": 0.5112386363636364, "width": 0.39443790849673205, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.5257866161616161, "width": 0.3943545751633987, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.5403333333333333, "width": 0.08058986928104574, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.8965640068054199, "is_author_statement": false, "is_in_expected_section": false, "id": "5747"}, {"text": "Mobile solutions can help transform speech and sound into visual representations for people who are deaf or hard-ofhearing (DHH).", "label": "Background", "bboxes": [{"left": 0.08823529411764706, "top": 0.453354797979798, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.4679027777777778, "width": 0.3944052287581699, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.4821439393939394, "width": 0.1056421568627451, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.8370268940925598, "is_author_statement": false, "is_in_expected_section": false, "id": "5748"}, {"text": "Wearable Subtitles is a lightweight 3D-printed proof-ofconcept HWD that explores augmenting communication through sound transcription for a full workday.", "label": "Background", "bboxes": [{"left": 0.08823529411764706, "top": 0.5624532828282829, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.577, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 0}, {"left": 0.0882516339869281, "top": 0.5915479797979798, "width": 0.30643464052287583, "height": 0.01272727272727281, "page": 0}], "section": "Abstract", "prob": 0.7926464080810547, "is_author_statement": false, "is_in_expected_section": false, "id": "5749"}, {"text": "Our studies and prior research identify critical challenges for the adoption of HWDs which we address through extended battery life, lightweight and balanced mechanical design (54 g), fitting options, and form factors that are compatible with current social norms.", "label": "Background", "bboxes": [{"left": 0.1678545751633987, "top": 0.6639785353535353, "width": 0.3147352941176471, "height": 0.01272727272727281, "page": 0}, {"left": 0.0882516339869281, "top": 0.6785265151515152, "width": 0.39417483660130725, "height": 0.012727272727272587, "page": 0}, {"left": 0.0882516339869281, "top": 0.6930732323232323, "width": 0.3944542483660131, "height": 0.01272727272727281, "page": 0}, {"left": 0.0882516339869281, "top": 0.7076212121212122, "width": 0.3945866013071896, "height": 0.012727272727272587, "page": 0}, {"left": 0.0882516339869281, "top": 0.7221679292929293, "width": 0.3001764705882353, "height": 0.01272727272727281, "page": 0}], "section": "Abstract", "prob": 0.4517780840396881, "is_author_statement": true, "is_in_expected_section": false, "id": "5750"}, {"text": "Using a lowpower microcontroller architecture, we enable up to 15 hours of continuous use.", "label": "Background", "bboxes": [{"left": 0.3988039215686275, "top": 0.5915479797979798, "width": 0.08391830065359468, "height": 0.01272727272727281, "page": 0}, {"left": 0.0882516339869281, "top": 0.606094696969697, "width": 0.3945702614379086, "height": 0.01272727272727281, "page": 0}, {"left": 0.0882516339869281, "top": 0.6206426767676767, "width": 0.12174999999999998, "height": 0.01272727272727281, "page": 0}], "section": "Abstract", "prob": 0.32815003395080566, "is_author_statement": true, "is_in_expected_section": false, "id": "5751"}, {"text": "technologies ; HCI; Mobile Computing; User Studies", "label": "Other", "bboxes": [{"left": 0.5176470588235295, "top": 0.4679027777777778, "width": 0.34757843137254896, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.6247522830963135, "is_author_statement": false, "is_in_expected_section": false, "id": "5752"}, {"text": "We describe a large survey (n=501) and three user studies with 24 deaf/hard-of-hearing participants which inform our development and help us refine our prototypes.", "label": "Result", "bboxes": [{"left": 0.21491013071895426, "top": 0.6206426767676767, "width": 0.2677794117647059, "height": 0.01272727272727281, "page": 0}, {"left": 0.0882516339869281, "top": 0.6351893939393939, "width": 0.39455228758169936, "height": 0.012727272727272698, "page": 0}, {"left": 0.0882516339869281, "top": 0.6494318181818182, "width": 0.39443790849673205, "height": 0.012727272727272587, "page": 0}, {"left": 0.0882516339869281, "top": 0.6639785353535353, "width": 0.07312908496732028, "height": 0.01272727272727281, "page": 0}], "section": "Abstract", "prob": 0.7320784330368042, "is_author_statement": true, "is_in_expected_section": false, "id": "5753"}, {"text": "Human-centered computing~Accessibility", "label": "Other", "bboxes": [{"left": 0.5176470588235295, "top": 0.453354797979798, "width": 0.3023480392156862, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.8462778925895691, "is_author_statement": false, "is_in_expected_section": false, "id": "5754"}, {"text": "CSS Concepts", "label": "Other", "bboxes": [{"left": 0.5176470588235295, "top": 0.4400164141414141, "width": 0.101609477124183, "height": 0.011515151515151534, "page": 0}], "section": "Abstract", "prob": 0.7828208208084106, "is_author_statement": false, "is_in_expected_section": false, "id": "5755"}], "uist-3": [{"text": "Augmented reality requires precise and instant overlay of digital information onto everyday objects.", "label": "Background", "bboxes": [{"left": 0.08823529411764706, "top": 0.2139608585858586, "width": 0.39448692810457514, "height": 0.012727272727272754, "page": 0}, {"left": 0.08823529411764706, "top": 0.2285088383838384, "width": 0.2837222222222222, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.9238960146903992, "is_author_statement": false, "is_in_expected_section": false, "id": "5756"}, {"text": "We take advantage of pervasive point lights  such as LEDs and light bulbs  for both in-view anchoring and data transmission.", "label": "Background", "bboxes": [{"left": 0.22935457516339872, "top": 0.2572979797979798, "width": 0.2531862745098039, "height": 0.012727272727272754, "page": 0}, {"left": 0.08823529411764706, "top": 0.27184469696969693, "width": 0.39448692810457514, "height": 0.012727272727272754, "page": 0}, {"left": 0.08823529411764706, "top": 0.28639267676767677, "width": 0.20644281045751628, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.7427546977996826, "is_author_statement": true, "is_in_expected_section": false, "id": "5757"}, {"text": "These lights are blinked at high speed to encode data.", "label": "Background", "bboxes": [{"left": 0.3011503267973856, "top": 0.28639267676767677, "width": 0.18140686274509799, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.30093939393939395, "width": 0.1770424836601307, "height": 0.012727272727272754, "page": 0}], "section": "Abstract", "prob": 0.6841810345649719, "is_author_statement": false, "is_in_expected_section": false, "id": "5758"}, {"text": "Augmented Reality; Smartphones, Tags, Markers, Visible Light Communication; Mobile Interaction.", "label": "Background", "bboxes": [{"left": 0.08823529411764706, "top": 0.3942638888888889, "width": 0.3943725490196078, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.4088118686868687, "width": 0.2796862745098039, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.49970096349716187, "is_author_statement": false, "is_in_expected_section": false, "id": "5759"}, {"text": "We built a proof-of-concept application that runs on iOS without any hardware or software modifications.", "label": "Background", "bboxes": [{"left": 0.2705653594771242, "top": 0.30093939393939395, "width": 0.21212418300653596, "height": 0.012727272727272754, "page": 0}, {"left": 0.08823529411764706, "top": 0.31548611111111113, "width": 0.3943218954248366, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.3300340909090909, "width": 0.0943267973856209, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.48021939396858215, "is_author_statement": true, "is_in_expected_section": false, "id": "5760"}, {"text": "We also ran a study to characterize the performance of LightAnchors and built eleven example demos to highlight the potential of our approach.", "label": "Result", "bboxes": [{"left": 0.18588888888888888, "top": 0.3300340909090909, "width": 0.29681699346405227, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.3445808080808081, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.3591287878787879, "width": 0.25625, "height": 0.012727272727272698, "page": 0}], "section": "Abstract", "prob": 0.45807626843452454, "is_author_statement": true, "is_in_expected_section": false, "id": "5761"}, {"text": "We present our work on LightAnchors, a new method for displaying spatially-anchored data.", "label": "Method", "bboxes": [{"left": 0.37803594771241833, "top": 0.2285088383838384, "width": 0.10466993464052288, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.24275000000000002, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 0}, {"left": 0.08823529411764706, "top": 0.2572979797979798, "width": 0.1358333333333333, "height": 0.012727272727272754, "page": 0}], "section": "Abstract", "prob": 0.734537661075592, "is_author_statement": true, "is_in_expected_section": true, "id": "5762"}], "2104.03820": [{"text": "Generative models have become adept at producing artifacts such as images, videos, and prose at human-like levels of proficiency.", "label": "Background", "bboxes": [{"left": 0.1200016339869281, "top": 0.33047601010101013, "width": 0.6734689542483661, "height": 0.010063131313131302, "page": 0}], "section": "Abstract", "prob": 0.9499941468238831, "is_author_statement": false, "is_in_expected_section": false, "id": "5763"}, {"text": "The artifacts produced in this way may contain imperfections, such as compilation or logical errors.", "label": "Background", "bboxes": [{"left": 0.4949673202614379, "top": 0.3619242424242424, "width": 0.3264656862745097, "height": 0.010063131313131302, "page": 0}, {"left": 0.1200016339869281, "top": 0.3776477272727273, "width": 0.201218954248366, "height": 0.010063131313131357, "page": 0}], "section": "Abstract", "prob": 0.9396002292633057, "is_author_statement": false, "is_in_expected_section": false, "id": "5764"}, {"text": "New generative techniques, such as unsupervised neural machine translation (NMT), have recently been applied to the task of generating source code, translating it from one programming language to another.", "label": "Background", "bboxes": [{"left": 0.7964052287581699, "top": 0.33047601010101013, "width": 0.02407026143790847, "height": 0.010063131313131302, "page": 0}, {"left": 0.1200016339869281, "top": 0.3462007575757575, "width": 0.6999950980392158, "height": 0.010063131313131357, "page": 0}, {"left": 0.1200016339869281, "top": 0.3619242424242424, "width": 0.3717205882352942, "height": 0.010063131313131302, "page": 0}], "section": "Abstract", "prob": 0.8557249903678894, "is_author_statement": false, "is_in_expected_section": false, "id": "5765"}, {"text": "Our three-stage scenario sparked discussions about the utility and desirability working with an imperfect AI system, how acceptance of that systems outputs would be established, and future opportunities generative AI in application modernization.", "label": "Background", "bboxes": [{"left": 0.41613235294117645, "top": 0.4248194444444444, "width": 0.4042026143790851, "height": 0.010063131313131357, "page": 0}, {"left": 0.13416176470588234, "top": 0.4405429292929293, "width": 0.6858398692810457, "height": 0.010063131313131302, "page": 0}, {"left": 0.13876470588235296, "top": 0.4562676767676767, "width": 0.23288071895424833, "height": 0.010063131313131357, "page": 0}], "section": "Abstract", "prob": 0.7097061276435852, "is_author_statement": true, "is_in_expected_section": false, "id": "5766"}, {"text": "We examine the extent to which software engineers would tolerate such imperfections and explore ways to aid the detection and correction of those errors.", "label": "Background", "bboxes": [{"left": 0.3247140522875817, "top": 0.3776477272727273, "width": 0.49528758169934645, "height": 0.010063131313131357, "page": 0}, {"left": 0.1200016339869281, "top": 0.3933712121212121, "width": 0.35347385620915034, "height": 0.010063131313131302, "page": 0}], "section": "Abstract", "prob": 0.49545973539352417, "is_author_statement": true, "is_in_expected_section": false, "id": "5767"}, {"text": "Our study highlights how UI features such as confidence highlighting and alternate translations help software engineers work with and better understand generative NMT models.", "label": "Result", "bboxes": [{"left": 0.3749019607843137, "top": 0.4562676767676767, "width": 0.44508986928104577, "height": 0.010063131313131357, "page": 0}, {"left": 0.1200016339869281, "top": 0.4719911616161616, "width": 0.5066486928104575, "height": 0.010063131313131302, "page": 0}], "section": "Abstract", "prob": 0.36043983697891235, "is_author_statement": true, "is_in_expected_section": false, "id": "5768"}, {"text": "Using a design scenario approach, we interviewed 11 software engineers to understand their reactions to the use of an NMT model in the context of application modernization, focusing on the task translating source code from one language to another.", "label": "Method", "bboxes": [{"left": 0.477421568627451, "top": 0.3933712121212121, "width": 0.34258006535947716, "height": 0.010063131313131302, "page": 0}, {"left": 0.1200016339869281, "top": 0.4090959595959596, "width": 0.7003333333333334, "height": 0.010063131313131302, "page": 0}, {"left": 0.1333513071895425, "top": 0.4248194444444444, "width": 0.2798071895424836, "height": 0.010063131313131357, "page": 0}], "section": "Abstract", "prob": 0.4432009160518646, "is_author_statement": true, "is_in_expected_section": true, "id": "5769"}], "uist-0": [{"text": "However in the moment, photographers have so many things to simultaneously consider, it can be hard to catch every detail.", "label": "Background", "bboxes": [{"left": 0.4274133986928105, "top": 0.6021843434343435, "width": 0.05330882352941174, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.6160214646464647, "width": 0.3950245098039216, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.6298573232323232, "width": 0.2986846405228758, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.9471307396888733, "is_author_statement": false, "is_in_expected_section": false, "id": "5770"}, {"text": "Designers have long known the benefts of abstraction for seeing a more holistic view of their design.", "label": "Background", "bboxes": [{"left": 0.3902598039215686, "top": 0.6298573232323232, "width": 0.0901960784313725, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.6436944444444445, "width": 0.3925522875816994, "height": 0.011320707070707203, "page": 0}, {"left": 0.08753921568627451, "top": 0.6575315656565657, "width": 0.11857679738562091, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.9264216423034668, "is_author_statement": false, "is_in_expected_section": false, "id": "5771"}, {"text": "Unwanted clutter in a photo can be incredibly distracting.", "label": "Background", "bboxes": [{"left": 0.08790522875816993, "top": 0.6021843434343435, "width": 0.33627777777777784, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.9043946862220764, "is_author_statement": false, "is_in_expected_section": false, "id": "5772"}, {"text": "Specifcally, we wondered if such abstraction might draw the photographers attention away from details in the subject to noticing objects in the background, such as unwanted clutter.", "label": "Background", "bboxes": [{"left": 0.41012745098039216, "top": 0.6852032828282829, "width": 0.07193790849673204, "height": 0.011320707070707092, "page": 0}, {"left": 0.08736764705882354, "top": 0.699036616161616, "width": 0.39308333333333334, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.7128699494949494, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.7267171717171717, "width": 0.25043137254901965, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.8328042030334473, "is_author_statement": true, "is_in_expected_section": false, "id": "5773"}, {"text": "We wondered if, similarly, some form of image abstraction might be helpful for photographers as an alternative perspective or lens with which to see their image.", "label": "Background", "bboxes": [{"left": 0.20933333333333332, "top": 0.6575315656565657, "width": 0.27110947712418304, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.6713686868686869, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.6852032828282829, "width": 0.3180539215686275, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.8044585585594177, "is_author_statement": true, "is_in_expected_section": false, "id": "5774"}, {"text": "photography, camera interfaces, declutter, composition", "label": "Other", "bboxes": [{"left": 0.5195343137254902, "top": 0.7940719696969697, "width": 0.32867973856209143, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.6247957348823547, "is_author_statement": false, "is_in_expected_section": false, "id": "5775"}, {"text": "We present our process for designing such a camera overlay, based on the idea of using", "label": "Method", "bboxes": [{"left": 0.3420032679738562, "top": 0.7267171717171717, "width": 0.13844444444444443, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.740554292929293, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.34285447001457214, "is_author_statement": true, "is_in_expected_section": true, "id": "5776"}], "2102.09039": [{"text": "User interface design is a complex task that involves designers examining a wide range of options.", "label": "Background", "bboxes": [{"left": 0.08790522875816993, "top": 0.2585669191919192, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.2724040404040404, "width": 0.22149999999999997, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.8719049692153931, "is_author_statement": false, "is_in_expected_section": false, "id": "5777"}, {"text": "Based on our experiments, Spacewalker allows designers to effectively search a large design space of a UI, using the language they are familiar with, and improve their design rapidly at a minimal cost.", "label": "Result", "bboxes": [{"left": 0.44607843137254904, "top": 0.3969381313131313, "width": 0.03438725490196082, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.41077398989898994, "width": 0.39293790849673205, "height": 0.011320707070706981, "page": 0}, {"left": 0.08790522875816993, "top": 0.42461111111111105, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.43844823232323227, "width": 0.3903464052287582, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.7685990333557129, "is_author_statement": true, "is_in_expected_section": false, "id": "5778"}, {"text": "Markup language, crowdsourcing, design search, tools, genetic programming", "label": "Background", "bboxes": [{"left": 0.08790522875816993, "top": 0.5282512626262627, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.5420883838383839, "width": 0.08217810457516338, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.5442944765090942, "is_author_statement": false, "is_in_expected_section": false, "id": "5779"}, {"text": "We enhanced a genetic algorithm to accommodate crowd worker responses from pairwise comparison of UI designs, which is crucial for obtaining reliable feedback.", "label": "Method", "bboxes": [{"left": 0.26858333333333334, "top": 0.3692638888888889, "width": 0.21188071895424831, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.3831010101010101, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.3969381313131313, "width": 0.3552156862745098, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.44135934114456177, "is_author_statement": true, "is_in_expected_section": false, "id": "5780"}, {"text": "We present Spacewalker, a tool that allows designers to rapidly search a large design space for an optimal web UI with integrated support.", "label": "Method", "bboxes": [{"left": 0.3144705882352941, "top": 0.2724040404040404, "width": 0.16599836601307194, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.28624116161616164, "width": 0.3925637254901962, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.30007828282828286, "width": 0.29678758169934644, "height": 0.011320707070707037, "page": 0}], "section": "Abstract", "prob": 0.4108668565750122, "is_author_statement": true, "is_in_expected_section": false, "id": "5781"}, {"text": "Spacewalker then parses the annotated HTML specification, and intelligently generates and distributes various configurations of the web UI to crowd workers for evaluation.", "label": "Background", "bboxes": [{"left": 0.4055326797385621, "top": 0.32775252525252524, "width": 0.07520588235294112, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.34158964646464646, "width": 0.39293790849673205, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.3554267676767677, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.3692638888888889, "width": 0.17702777777777778, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.5004971027374268, "is_author_statement": false, "is_in_expected_section": true, "id": "5782"}, {"text": "Human-centeredcomputing  Interactivesystemsandtools .", "label": "Other", "bboxes": [{"left": 0.08790522875816993, "top": 0.4824318181818182, "width": 0.40661437908496734, "height": 0.012238636363636368, "page": 0}], "section": "Abstract", "prob": 0.5495071411132812, "is_author_statement": false, "is_in_expected_section": false, "id": "5783"}, {"text": "Designers first annotate each attribute they want to explore in a typical HTML page, using a simple markup extension we designed.", "label": "Background", "bboxes": [{"left": 0.39001307189542483, "top": 0.30007828282828286, "width": 0.09045588235294122, "height": 0.011320707070707037, "page": 0}, {"left": 0.08790522875816993, "top": 0.3139154040404041, "width": 0.39293790849673205, "height": 0.011320707070707037, "page": 0}, {"left": 0.08790522875816993, "top": 0.32775252525252524, "width": 0.31394771241830066, "height": 0.011320707070707092, "page": 0}], "section": "Abstract", "prob": 0.620433509349823, "is_author_statement": true, "is_in_expected_section": true, "id": "5784"}], "uist-1": [{"text": "Blind people frequently encounter inaccessible dynamic touch screens in their everyday lives that are difcult, frustrating, and often impossible to use independently.", "label": "Background", "bboxes": [{"left": 0.08811928104575163, "top": 0.24397853535353534, "width": 0.3998839869281046, "height": 0.012579545454545454, "page": 0}, {"left": 0.08811928104575163, "top": 0.2578156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.2716527777777778, "width": 0.24525000000000002, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.9628199338912964, "is_author_statement": false, "is_in_expected_section": false, "id": "5785"}, {"text": "Touchscreens are often the only way to control everything from coffee machines and payment terminals, to subway ticket machines and in-ight en tertainment systems.", "label": "Background", "bboxes": [{"left": 0.33845751633986926, "top": 0.2716527777777778, "width": 0.14683496732026152, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.285489898989899, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.29932702020202023, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.31316414141414145, "width": 0.13108823529411767, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.9404858350753784, "is_author_statement": false, "is_in_expected_section": false, "id": "5786"}, {"text": "Interacting with dynamic touchscreens is difcult non-visually because the visual user interfaces change, interactions often occur over multiple different screens, and it is easy to accidentally trigger interface actions while exploring the screen.", "label": "Background", "bboxes": [{"left": 0.22420098039215688, "top": 0.31316414141414145, "width": 0.2610915032679739, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.32700126262626267, "width": 0.3992140522875817, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.3408371212121212, "width": 0.3971797385620915, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.3546742424242424, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.3685113636363636, "width": 0.07058496732026144, "height": 0.012579545454545482, "page": 0}], "section": "Abstract", "prob": 0.9308024048805237, "is_author_statement": false, "is_in_expected_section": false, "id": "5787"}, {"text": "Reverse engineering; dynamic interfaces; touchscreen appliances; accessibility; crowdsourcing; computer vision; conversational agents.", "label": "Background", "bboxes": [{"left": 0.08811928104575163, "top": 0.6437171717171717, "width": 0.3508888888888889, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6575542929292929, "width": 0.3811993464052288, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6713914141414141, "width": 0.1437745098039216, "height": 0.012579545454545538, "page": 0}], "section": "Abstract", "prob": 0.6283090114593506, "is_author_statement": false, "is_in_expected_section": false, "id": "5788"}, {"text": "Finally, a set of 3D printed accessories enable blind people to explore capacitive touchscreens without the risk of triggering accidental touches on the interface.", "label": "Background", "bboxes": [{"left": 0.3629624183006536, "top": 0.5068813131313131, "width": 0.12504084967320261, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.5207184343434343, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5345555555555556, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.5483926767676768, "width": 0.10880718954248365, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.5152112245559692, "is_author_statement": false, "is_in_expected_section": false, "id": "5789"}, {"text": "First, StateLens reverse engineers the underlying state diagrams of existing interfaces using point-of-view videos found online or taken by users using a hybrid crowd-computer vision pipeline.", "label": "Background", "bboxes": [{"left": 0.33411437908496733, "top": 0.39618560606060604, "width": 0.15118464052287578, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.41002272727272726, "width": 0.39716666666666667, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.4238598484848485, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.4376969696969697, "width": 0.30261764705882355, "height": 0.012579545454545482, "page": 0}], "section": "Abstract", "prob": 0.3878459632396698, "is_author_statement": false, "is_in_expected_section": false, "id": "5790"}, {"text": "To solve these problems, we introduce StateLens  a three-part reverse engineering solution that makes exist ing dynamic touchscreens accessible.", "label": "Method", "bboxes": [{"left": 0.1637450980392157, "top": 0.36822222222222223, "width": 0.3215506535947712, "height": 0.012868686868686863, "page": 0}, {"left": 0.08486274509803922, "top": 0.3823484848484848, "width": 0.40312745098039215, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.39618560606060604, "width": 0.24093464052287583, "height": 0.012579545454545482, "page": 0}], "section": "Abstract", "prob": 0.3243297338485718, "is_author_statement": true, "is_in_expected_section": false, "id": "5791"}, {"text": "Second, using the state diagrams, StateLens automatically generates conver sational agents to guide blind users through specifying the tasks that the interface can perform, allowing the StateLens iOS application to provide interactive guidance and feedback so that blind users can access the interface.", "label": "Background", "bboxes": [{"left": 0.3957614379084967, "top": 0.4376969696969697, "width": 0.08953104575163406, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.4515340909090909, "width": 0.39988562091503266, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.46537121212121213, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.47920833333333335, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.4930441919191919, "width": 0.39758333333333334, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5068813131313131, "width": 0.2699934640522876, "height": 0.012579545454545538, "page": 0}], "section": "Abstract", "prob": 0.6354931592941284, "is_author_statement": false, "is_in_expected_section": true, "id": "5792"}, {"text": "Our technical evaluation shows that StateLens can accurately reconstruct interfaces from stationary, hand-held, and web videos; and, a user study of the complete system demonstrates that StateLens successfully enables blind users to access otherwise inaccessible dynamic touchscreens.", "label": "Result", "bboxes": [{"left": 0.204531045751634, "top": 0.5483926767676768, "width": 0.283468954248366, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.562229797979798, "width": 0.39919934640522875, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.5760669191919192, "width": 0.39717156862745095, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5899040404040404, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6037411616161616, "width": 0.3982761437908497, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.641381025314331, "is_author_statement": true, "is_in_expected_section": false, "id": "5793"}], "1602.06979": [{"text": "Empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern ction.", "label": "Background", "bboxes": [{"left": 0.19675, "top": 0.6195530303030303, "width": 0.2885408496732026, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6333901515151515, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.6472272727272728, "width": 0.2631290849673203, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.8841196894645691, "is_author_statement": false, "is_in_expected_section": false, "id": "5794"}, {"text": "Human language is colored by a broad range of topics, but existing text analysis tools only focus on a small number of them.", "label": "Background", "bboxes": [{"left": 0.08811928104575163, "top": 0.5503686868686869, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.5642058080808081, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5780429292929293, "width": 0.03662745098039215, "height": 0.012579545454545538, "page": 0}], "section": "Abstract", "prob": 0.8725820779800415, "is_author_statement": false, "is_in_expected_section": false, "id": "5795"}, {"text": "Empath also analyzes text across 200 built-in, pre-validated categories we have generated from common topics in our web dataset, like neglect , government , and social media .", "label": "Background", "bboxes": [{"left": 0.37573202614379086, "top": 0.6887386363636363, "width": 0.10955718954248367, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.7025757575757576, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.7164128787878787, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.729959595959596, "width": 0.2616486928104575, "height": 0.012868686868686807, "page": 0}], "section": "Abstract", "prob": 0.5809584856033325, "is_author_statement": true, "is_in_expected_section": false, "id": "5796"}, {"text": "We present Empath , a tool that can generate and validate new lexical categories on demand from a small set of seed terms (like bleed and punch to generate the category violence ).", "label": "Background", "bboxes": [{"left": 0.13291830065359478, "top": 0.577753787878788, "width": 0.35237581699346404, "height": 0.012868686868686918, "page": 0}, {"left": 0.08811928104575163, "top": 0.5918787878787879, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6057159090909091, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.6192638888888888, "width": 0.09912418300653596, "height": 0.012868686868686918, "page": 0}], "section": "Abstract", "prob": 0.49230965971946716, "is_author_statement": true, "is_in_expected_section": false, "id": "5797"}, {"text": "Given a small set of seed words that characterize a category, Empath uses its neural embedding to discover new related terms, then validates the category with a crowd-powered lter.", "label": "Method", "bboxes": [{"left": 0.35624673202614376, "top": 0.6472272727272728, "width": 0.12904248366013077, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.661064393939394, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6749015151515152, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6887386363636363, "width": 0.2769019607843137, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.36605095863342285, "is_author_statement": false, "is_in_expected_section": true, "id": "5798"}, {"text": "We show that Empaths data-driven, human validated categories are highly correlated (r=0.906) with similar categories in LIWC.", "label": "Result", "bboxes": [{"left": 0.36010457516339867, "top": 0.7302487373737373, "width": 0.12518300653594772, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.7440858585858586, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.7579229797979797, "width": 0.3290261437908497, "height": 0.012579545454545538, "page": 0}], "section": "Abstract", "prob": 0.7136130928993225, "is_author_statement": true, "is_in_expected_section": false, "id": "5799"}, {"text": "social computing, computational social science, ction", "label": "Other", "bboxes": [{"left": 0.5246633986928104, "top": 0.5503686868686869, "width": 0.3567483660130719, "height": 0.012579545454545538, "page": 0}], "section": "Abstract", "prob": 0.7817838788032532, "is_author_statement": false, "is_in_expected_section": false, "id": "5800"}], "uist-6": [{"text": "Skin-on, Articial Skin, Malleable, Deformable, Sensing, Interaction Techniques.", "label": "Background", "bboxes": [{"left": 0.08811928104575163, "top": 0.6698446969696971, "width": 0.37530555555555556, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6836818181818182, "width": 0.15257026143790853, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.8163875937461853, "is_author_statement": false, "is_in_expected_section": false, "id": "5801"}, {"text": "We propose a paradigm called Skin-On interfaces, in which interactive devices have their own (articial) skin, thus enabling new forms of input gestures for end-users (e.g. twist, scratch).", "label": "Background", "bboxes": [{"left": 0.08735457516339869, "top": 0.41325757575757577, "width": 0.4006633986928105, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.42709343434343433, "width": 0.3971846405228758, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.44093055555555555, "width": 0.40004084967320264, "height": 0.012579545454545482, "page": 0}], "section": "Abstract", "prob": 0.5416725277900696, "is_author_statement": true, "is_in_expected_section": false, "id": "5802"}, {"text": "Our work explores the design space of Skin-On interfaces by following a bio-driven approach: (1) From a sensory point of view, we study how to reproduce the look and feel of the human skin through three user studies; (2) From a gestural point of view, we explore how gestures naturally performed on skin can be transposed to Skin-On interfaces; (3) From a technical point of view, we explore and discuss different ways of fabricating interfaces that mimic human skin sensitivity and can recognize the gestures observed in the previous study; (4) We assemble the insights of our three exploratory facets to implement a series of Skin-On interfaces and we also contribute by providing a toolkit that enables easy reproduction and fabrication.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.45476767676767677, "width": 0.39775653594771243, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.468604797979798, "width": 0.3971928104575164, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.4824419191919192, "width": 0.39719281045751637, "height": 0.012579545454545482, "page": 0}, {"left": 0.08811928104575163, "top": 0.496280303030303, "width": 0.39719281045751637, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.5101174242424242, "width": 0.39719281045751637, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5239545454545455, "width": 0.39719281045751637, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.537790404040404, "width": 0.39719934640522875, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5516275252525252, "width": 0.3971846405228758, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5654633838383838, "width": 0.3982875816993464, "height": 0.012579545454545427, "page": 0}, {"left": 0.08735457516339869, "top": 0.579300505050505, "width": 0.3979575163398693, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811764705882352, "top": 0.5931376262626262, "width": 0.3999003267973856, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811764705882352, "top": 0.6069760101010101, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6208118686868687, "width": 0.1020065359477124, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.5687056183815002, "is_author_statement": true, "is_in_expected_section": true, "id": "5803"}], "uist-7": [{"text": "that hand-tracking has the potential to enable rapid text entry in mobile environments.", "label": "Background", "bboxes": [{"left": 0.5246633986928104, "top": 0.4925138888888889, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.50635101010101, "width": 0.15806699346405229, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.8788800835609436, "is_author_statement": false, "is_in_expected_section": false, "id": "5804"}, {"text": "To enable touch typing without the haptic feedback of a physical keyboard, we had to address more erratic typing motion due to drift of the ngers.", "label": "Background", "bboxes": [{"left": 0.37852777777777774, "top": 0.6038383838383838, "width": 0.10676470588235298, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6176755050505051, "width": 0.3991993464052287, "height": 0.012579545454545427, "page": 0}, {"left": 0.0875375816993464, "top": 0.6315126262626263, "width": 0.39775490196078434, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6453497474747475, "width": 0.07349019607843137, "height": 0.012579545454545538, "page": 0}], "section": "Abstract", "prob": 0.7481642961502075, "is_author_statement": true, "is_in_expected_section": false, "id": "5805"}, {"text": "Thus, we incorporate a language model as a text prior and use beam search to efciently combine our motion and language models to decode text from erratic or ambiguous hand motion.", "label": "Background", "bboxes": [{"left": 0.16665849673202615, "top": 0.6453497474747475, "width": 0.31862908496732023, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.6591868686868687, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6730239898989898, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.6868611111111111, "width": 0.08773692810457516, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.46929842233657837, "is_author_statement": true, "is_in_expected_section": false, "id": "5806"}, {"text": "We collected a dataset of 20 touch typists and evaluated our model on several baselines, including contactbased text decoding and typing on a physical keyboard.", "label": "Background", "bboxes": [{"left": 0.18145098039215687, "top": 0.6868611111111111, "width": 0.3038415032679739, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.7006982323232323, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.7145353535353536, "width": 0.3666127450980392, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.3713776171207428, "is_author_statement": true, "is_in_expected_section": false, "id": "5807"}, {"text": "We use a temporal convolutional network to represent a motion model that maps the hand motion, represented as a sequence of hand pose features, into text characters.", "label": "Method", "bboxes": [{"left": 0.4640686274509804, "top": 0.5623282828282828, "width": 0.02122875816993469, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.5761654040404041, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5900025252525253, "width": 0.3971764705882353, "height": 0.012579545454545538, "page": 0}, {"left": 0.08811928104575163, "top": 0.6038383838383838, "width": 0.28350163398692807, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.4697747528553009, "is_author_statement": true, "is_in_expected_section": true, "id": "5808"}, {"text": "Human-centered computing  Text input; Virtual reality;  Computing methodologies  Natural language generation;", "label": "Other", "bboxes": [{"left": 0.5238006535947712, "top": 0.5829204545454546, "width": 0.3993872549019608, "height": 0.012856060606060482, "page": 0}, {"left": 0.5238006535947712, "top": 0.5967575757575757, "width": 0.39932189542483665, "height": 0.012856060606060704, "page": 0}], "section": "Abstract", "prob": 0.5272074937820435, "is_author_statement": false, "is_in_expected_section": false, "id": "5809"}, {"text": "text input; hand-tracking; augmented reality; virtual reality", "label": "Other", "bboxes": [{"left": 0.5246633986928104, "top": 0.54477398989899, "width": 0.38316993464052296, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.5843514800071716, "is_author_statement": false, "is_in_expected_section": false, "id": "5810"}, {"text": "We propose a novel text decoding method that enables touch typing on an uninstrumented at surface.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.506979797979798, "width": 0.3979330065359477, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5208169191919192, "width": 0.2770081699346405, "height": 0.012579545454545427, "page": 0}], "section": "Abstract", "prob": 0.6415295004844666, "is_author_statement": true, "is_in_expected_section": true, "id": "5811"}, {"text": "Our proposed method is able to leverage continuous hand pose information to decode text more accurately than contact-based methods and an ofine study shows parity (73 WPM, 2.38% UER) with typing on a physical keyboard.", "label": "Result", "bboxes": [{"left": 0.4597745098039216, "top": 0.7145353535353536, "width": 0.025795751633986896, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.7283724747474747, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.742209595959596, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.7560454545454546, "width": 0.398531045751634, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.7698825757575757, "width": 0.2803627450980392, "height": 0.012579545454545538, "page": 0}], "section": "Abstract", "prob": 0.7168692946434021, "is_author_statement": true, "is_in_expected_section": false, "id": "5812"}, {"text": "Rather than relying on physical keyboards or capacitive touch, our method takes as input hand motion of the typist, obtained through hand-tracking, and decodes this motion directly into text.", "label": "Method", "bboxes": [{"left": 0.37344607843137256, "top": 0.5208169191919192, "width": 0.11455392156862743, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5346540404040404, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5484911616161616, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.5623282828282828, "width": 0.3708856209150327, "height": 0.012579545454545538, "page": 0}], "section": "Abstract", "prob": 0.823117196559906, "is_author_statement": true, "is_in_expected_section": true, "id": "5813"}, {"text": "Our results show", "label": "Result", "bboxes": [{"left": 0.3737614379084967, "top": 0.7698825757575757, "width": 0.11211274509803926, "height": 0.012579545454545538, "page": 0}], "section": "Abstract", "prob": 0.7990118265151978, "is_author_statement": true, "is_in_expected_section": false, "id": "5814"}]}