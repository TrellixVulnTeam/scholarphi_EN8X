{"uist-8": [{"text": "To overcome the problems of the touchpadand eye-gazebased methods, we propose the gaze-assisted typing (GAT).", "label": "Author", "bboxes": [{"left": 0.5241601307189543, "top": 0.6873914141414141, "width": 0.4003774509803921, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246584967320261, "top": 0.7012285353535355, "width": 0.40002941176470586, "height": 0.012579545454545427, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "449"}, {"text": "With this combination, we can use larger targets for the eye gaze and a set of simpler touchpad operations.", "label": "Author", "bboxes": [{"left": 0.7180816993464052, "top": 0.7289027777777779, "width": 0.20375, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.742739898989899, "width": 0.399874183006536, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.7565770202020201, "width": 0.10193300653594772, "height": 0.012579545454545538, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "450"}, {"text": "In the subsequent section, we review the literature concerning possible text entry methods for smart glasses that can be used in a general mobile environment.", "label": "Author", "bboxes": [{"left": 0.16039869281045752, "top": 0.4037411616161616, "width": 0.3248921568627451, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4175782828282828, "width": 0.39717647058823524, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.431415404040404, "width": 0.29501960784313724, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "451"}, {"text": "In this study, we attempted to achieve a more usable on-glass text entry, primarily in terms of speed.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.34680808080808084, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.3606439393939394, "width": 0.25590522875817, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "452"}, {"text": "To achieve this goal, we focused on the gaze input modality.", "label": "Author", "bboxes": [{"left": 0.7866617647058823, "top": 0.3606439393939394, "width": 0.1372009803921569, "height": 0.012579545454545482, "page": 1}, {"left": 0.5240816993464052, "top": 0.3744810606060606, "width": 0.27235947712418307, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "453"}, {"text": "We rst conducted a preliminary study to validate the effect of introducing eye input modality to a two-step touch-only text entry with a minimal load on eye input.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.11667803030303031, "width": 0.39793790849673205, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.1305151515151515, "width": 0.39717647058823524, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.14435227272727272, "width": 0.24770588235294122, "height": 0.012579545454545454, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "454"}, {"text": "After we validated that GAT can improve touchpad typing, we attempted to optimize the GAT.", "label": "Author", "bboxes": [{"left": 0.3406601307189543, "top": 0.14435227272727272, "width": 0.1446323529411765, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.15818939393939394, "width": 0.3971666666666667, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.17202651515151515, "width": 0.05816503267973856, "height": 0.012579545454545454, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "455"}, {"text": "To determine the best choice, we conducted Experiment 1, where we varied the precision requirement of eye tracking and the touch gesture set size.", "label": "Author", "bboxes": [{"left": 0.2873366013071895, "top": 0.18586363636363637, "width": 0.19998202614379085, "height": 0.012579545454545454, "page": 1}, {"left": 0.0875375816993464, "top": 0.1997007575757576, "width": 0.3977549019607844, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.2135378787878788, "width": 0.4000130718954249, "height": 0.012579545454545454, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "456"}, {"text": "Finally, to demonstrate that the GAT method with larger keys for eye selection and simpler touchpad gestures would lead to improved performance and usability benets over eye-only and touch-only typing methods, we conducted Experiment 2.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.3971764705882353, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.24121085858585856, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.3977385620915033, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.268885101010101, "width": 0.3921372549019607, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "457"}, {"text": "For selection, we provided a cross-hair-shaped eye cursor on the keyboard.", "label": "Author", "bboxes": [{"left": 0.18645915032679738, "top": 0.4969128787878788, "width": 0.2988382352941176, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.51075, "width": 0.18236111111111114, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "458"}, {"text": "Our proposed approach is a two-step method using a combination of gaze and touch gesture for text entry.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.3722070707070707, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3860441919191919, "width": 0.3324003267973856, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "459"}, {"text": "In our design, the left and right directions on the screen were mapped in the backward and forward directions on the touchpad, respectively.", "label": "Author", "bboxes": [{"left": 0.47146078431372546, "top": 0.7046414141414141, "width": 0.013831699346405268, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39988562091503266, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.11667810457516341, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "460"}, {"text": "To reduce the typing error by confusing the swipe direction, we provided a visual feedback (touch cursor in Figure 2(b)) for the touch position to permit adjustments in the swipe direction before the swipe gesture is nalized.", "label": "Author", "bboxes": [{"left": 0.6214330065359477, "top": 0.5508661616161616, "width": 0.3004035947712418, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5647032828282829, "width": 0.3974460784313726, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.5785404040404041, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5923775252525253, "width": 0.4000130718954249, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "461"}, {"text": "We also provided an audio feedback from the glasses for the touch input.", "label": "Author", "bboxes": [{"left": 0.7872614379084967, "top": 0.633888888888889, "width": 0.1345784313725491, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6477260101010102, "width": 0.33771895424836607, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "462"}, {"text": "An intentional eye movement of the user may occur before starting the touch input; therefore, we xed the eye cursor once a user started the touch input.", "label": "Author", "bboxes": [{"left": 0.6186601307189542, "top": 0.46029671717171716, "width": 0.3037418300653595, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.4741338383838384, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.4879709595959596, "width": 0.29561437908496735, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "463"}, {"text": "To distinguish the touch gestures, we used a vector from the placement to release positions.", "label": "Author", "bboxes": [{"left": 0.7686209150326797, "top": 0.6967840909090909, "width": 0.15321568627450988, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.7106212121212121, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7244583333333333, "width": 0.04752124183006545, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "464"}, {"text": "As far as text entry methods are considered, however, gazeassisted typing (GAT) is distinguished from the prior works as we consider a taxonomy for Gaze + Gesture combinations that was presented by Chatterjee et al.", "label": "Author", "bboxes": [{"left": 0.08752124183006536, "top": 0.12296843434343435, "width": 0.40046078431372545, "height": 0.012579545454545454, "page": 2}, {"left": 0.08810294117647058, "top": 0.13680555555555554, "width": 0.39717320261437905, "height": 0.012578282828282827, "page": 2}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.16418939393939394, "width": 0.2635751633986928, "height": 0.012868686868686863, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "465"}, {"text": "Moreover, in our pilot study, we also observed that users were inclined to not blink until a", "label": "Author", "bboxes": [{"left": 0.30829084967320264, "top": 0.8782335858585859, "width": 0.17904248366013065, "height": 0.012579545454545427, "page": 2}, {"left": 0.08753267973856209, "top": 0.892070707070707, "width": 0.3977549019607843, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "466"}, {"text": "We recruited 12 participants (4 females and 8 males, mean age: 22.58, from 19 to 30 years) from our university.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.8090479797979797, "width": 0.39793790849673216, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246617647058823, "top": 0.822885101010101, "width": 0.35187745098039225, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "467"}, {"text": "To implement a complete mobile gaze tracking system on smart glasses, we attempted to use an eye tracker from Pupil Labs and Tobii Pro Glasses", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.614534090909091, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6283712121212122, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6422083333333334, "width": 0.18021568627450985, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "468"}, {"text": "To avoid the issues, we simulated the environment instead of using a mobile eye tracker.", "label": "Author", "bboxes": [{"left": 0.14512091503267974, "top": 0.7113926767676768, "width": 0.340171568627451, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7252297979797979, "width": 0.2418741830065359, "height": 0.012579545454545538, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "469"}, {"text": "Thus, we used the GP3 eye tracker 3 with a laptop as shown in gure", "label": "Author", "bboxes": [{"left": 0.3350457516339869, "top": 0.7252297979797979, "width": 0.15024509803921565, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.7370656565656565, "width": 0.2918366013071895, "height": 0.014580808080808194, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "470"}, {"text": "We used an average lter to reduce jitter.", "label": "Author", "bboxes": [{"left": 0.40984313725490196, "top": 0.7529040404040404, "width": 0.0754526143790849, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.7667411616161617, "width": 0.19355555555555554, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "471"}, {"text": "We also used a Google Glass Enterprise Edition to use the built-in touchpad on the glasses.", "label": "Author", "bboxes": [{"left": 0.28903104575163396, "top": 0.7667411616161617, "width": 0.19626143790849676, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7805782828282828, "width": 0.39938235294117647, "height": 0.012579545454545538, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "472"}, {"text": "We used a within-subject design with one factor, i.e. , technique: M-SwipeBoard and GAT.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.5172588383838383, "width": 0.4002042483660132, "height": 0.012868686868686918, "page": 3}, {"left": 0.5246633986928104, "top": 0.5313851010101011, "width": 0.1715882352941177, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "473"}, {"text": "At the start of the session, we instructed a user on the process of entering text using a given technique and we calibrated the eye tracker with a ve-point calibration.", "label": "Author", "bboxes": [{"left": 0.9052434640522876, "top": 0.5590593434343434, "width": 0.01658823529411757, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5728964646464646, "width": 0.39717647058823546, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.5867335858585858, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.600570707070707, "width": 0.23665359477124182, "height": 0.012579545454545538, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "474"}, {"text": "In the GAT session, if a user identied a mismatch in the position of the eye cursor, we recalibrated the eye tracker and discarded the task in progress.", "label": "Author", "bboxes": [{"left": 0.7663627450980393, "top": 0.600570707070707, "width": 0.15547058823529414, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.6144078282828283, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6282449494949495, "width": 0.40002450980392157, "height": 0.012579545454545538, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "475"}, {"text": "After completing the experiment, we asked the user to compare the eye fatigue for both techniques.", "label": "Author", "bboxes": [{"left": 0.6880081699346405, "top": 0.7112664141414141, "width": 0.23382843137254894, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.7251035353535354, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "476"}, {"text": "Finally, we gathered 12 participants  2 techniques  3 blocks  5 tasks = 360 phrases.", "label": "Author", "bboxes": [{"left": 0.5246699346405229, "top": 0.7525012626262626, "width": 0.39717483660130715, "height": 0.012856060606060704, "page": 3}, {"left": 0.5246633986928104, "top": 0.7663383838383839, "width": 0.16365032679738567, "height": 0.012856060606060482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "477"}, {"text": "To verify GAT, initially, we demonstrate the net effect of introducing gaze modality with a minimum expression of the eye input.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.2108838383838384, "width": 0.4003774509803922, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811274509803921, "top": 0.2247209595959596, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811274509803921, "top": 0.2385580808080808, "width": 0.06527124183006536, "height": 0.012579545454545454, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "478"}, {"text": "For the minimum expression of the eye input, we used the nine touch gestures for key selection.", "label": "Author", "bboxes": [{"left": 0.15876307189542482, "top": 0.2385580808080808, "width": 0.3265228758169935, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811274509803921, "top": 0.252395202020202, "width": 0.30543464052287583, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "479"}, {"text": "Thus, we choose SwipeBoard as a baseline technique in this study, to demonstrate the net effect of replacing an input step with gaze input.", "label": "Author", "bboxes": [{"left": 0.37100653594771243, "top": 0.29390530303030304, "width": 0.11428594771241835, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.30774242424242426, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3215795454545455, "width": 0.4000261437908497, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "480"}, {"text": "We instructed the participants to memorize a target phrase for 15 s before starting the task, but, the target phrase was not erased while performing the task.", "label": "Author", "bboxes": [{"left": 0.8298447712418301, "top": 0.4059292929292929, "width": 0.0919869281045751, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246584967320261, "top": 0.41976641414141413, "width": 0.39717320261437894, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246584967320261, "top": 0.43360353535353535, "width": 0.39717320261437905, "height": 0.012578282828282827, "page": 3}, {"left": 0.5246633986928104, "top": 0.44744065656565657, "width": 0.1309558823529411, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "481"}, {"text": "We also asked the participants to correct errors only found within 3 or 4 letters.", "label": "Author", "bboxes": [{"left": 0.8038741830065359, "top": 0.4612777777777778, "width": 0.11795751633986928, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.475114898989899, "width": 0.39469444444444457, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "482"}, {"text": "Thus, we decided to use 21 as the length of the square bound, tted in the  15 round.", "label": "Author", "bboxes": [{"left": 0.32385130718954247, "top": 0.8434734848484848, "width": 0.16144281045751635, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.8570340909090909, "width": 0.4000245098039216, "height": 0.012856060606060593, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "483"}, {"text": "We also followed the recommendation of Kito et al.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.8708585858585859, "width": 0.34232026143790856, "height": 0.012868686868686918, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "484"}, {"text": "To demonstrate the net effect, we modied the SwipeBoard design to be similar to our GAT design; we called this MSwipeBoard.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.3429633838383839, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811274509803921, "top": 0.3568005050505051, "width": 0.3998790849673203, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3706376262626263, "width": 0.0851437908496732, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "485"}, {"text": "We presented a touch cursor for M-SwipeBoard to guide the nger movements of the user to reduce the input error for swipe gestures, same as GAT (Figure 2(b)).", "label": "Author", "bboxes": [{"left": 0.12568954248366013, "top": 0.4398232323232323, "width": 0.3596029411764706, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.4536590909090909, "width": 0.3974542483660131, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.4674962121212121, "width": 0.2830065359477124, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "486"}, {"text": "We thought that GAT with a lesser number of touch gestures may require less effort of touch input by eliminating difcult gestures, i.e. , diagonal swipe gestures [11] and vertical swipe gestures.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.11353282828282829, "width": 0.39793464052287597, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.12736994949494948, "width": 0.39717483660130715, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.1409179292929293, "width": 0.39717810457516345, "height": 0.012868686868686863, "page": 4}, {"left": 0.5246633986928104, "top": 0.15504419191919191, "width": 0.05856372549019606, "height": 0.012579545454545454, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "487"}, {"text": "To examine our expectation, we designed and evaluated three variations of GAT with varying number of touch gestures.", "label": "Author", "bboxes": [{"left": 0.5241601307189543, "top": 0.21039267676767676, "width": 0.3976699346405228, "height": 0.012579545454545454, "page": 4}, {"left": 0.5242565359477125, "top": 0.22422979797979797, "width": 0.3767075163398692, "height": 0.012579545454545454, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "488"}, {"text": "We designed three variations of GAT: GAT3, GAT6, and GAT9.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.27064015151515153, "width": 0.40079411764705897, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "489"}, {"text": "We analyzed the text entry speed, corrected error rate (CER), and uncorrected error rate (UER) statistically using a two-way (2 techniques  3 blocks) repeated measure ANOVA (RMANOVA) on each.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.39248737373737375, "width": 0.39996405228758164, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.40632449494949496, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 4}, {"left": 0.08758169934640524, "top": 0.41988510101010096, "width": 0.4004101307189542, "height": 0.012856060606060649, "page": 4}, {"left": 0.08753267973856209, "top": 0.43399873737373734, "width": 0.12134803921568628, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "490"}, {"text": "The UER violated normality assumption; therefore, we performed an aligned rank transform (ART) [42] on the UER before conducting RM-ANOVA.", "label": "Author", "bboxes": [{"left": 0.2139591503267974, "top": 0.43399873737373734, "width": 0.2726813725490196, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.44783585858585856, "width": 0.39826960784313725, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4616729797979798, "width": 0.33811274509803924, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "491"}, {"text": "We also conducted post-hoc tests to compare both techniques in each block; paired sample t-tests on WPM and CER with a Bonferroni correction.", "label": "Author", "bboxes": [{"left": 0.4315114379084967, "top": 0.4616729797979798, "width": 0.05378104575163406, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.475510101010101, "width": 0.3971797385620915, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4893459595959596, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.5031830808080808, "width": 0.10035784313725492, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "492"}, {"text": "Because we varied the number of touch gestures, the keyboard layouts were correspondingly varied.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.40272095959595955, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.41655808080808077, "width": 0.25012581699346403, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "493"}, {"text": "Thus, we placed the keys in an alphabetical order to minimize the memorability effect of the key placement.", "label": "Author", "bboxes": [{"left": 0.7843709150326797, "top": 0.41655808080808077, "width": 0.13746568627450972, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.430395202020202, "width": 0.39773856209150327, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4442323232323232, "width": 0.17660294117647068, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "494"}, {"text": "Thus, we removed the boundary of sub-keyboards, and displayed the visualization to notify the sub-keyboard activation as shown in Figure 6.", "label": "Author", "bboxes": [{"left": 0.730985294117647, "top": 0.47190656565656564, "width": 0.1935588235294119, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.48574242424242425, "width": 0.39774673202614375, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.4995795454545454, "width": 0.32300326797385637, "height": 0.012579545454545538, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "495"}, {"text": "We recruited 18 participants (8 females, 10 males, mean age: 20.72, from 17 to 24 years) from our university.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.6326792929292929, "width": 0.400202614379085, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6465151515151515, "width": 0.31986437908496734, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "496"}, {"text": "We analyzed the results statistically for both NC and VGC, separately.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.5751502525252525, "width": 0.3999803921568629, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.5889873737373738, "width": 0.06900490196078435, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "497"}, {"text": "For VGC, we used one-way RM-ANOVA with a layout factor, and also conducted three post-hoc tests, similar to the NC.", "label": "Author", "bboxes": [{"left": 0.7426683006535947, "top": 0.602824494949495, "width": 0.1797369281045752, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.6166603535353535, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.6304974747474748, "width": 0.2138872549019608, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "498"}, {"text": "We used the within-subject design, with the layout as a factor, i.e. , GAT3, GAT6, and GAT9.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.3274785353535353, "width": 0.3999673202614379, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.34102651515151516, "width": 0.2010767973856209, "height": 0.012868686868686863, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "499"}, {"text": "At the start of each session, we calibrated the eye tracker as same to preliminary study.", "label": "Author", "bboxes": [{"left": 0.1374673202614379, "top": 0.4243371212121212, "width": 0.34782516339869285, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4381742424242424, "width": 0.25389542483660127, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "500"}, {"text": "We controlled the interval of sessions to approximately 24 h as much as possible (from 15 to 41 h; mean = 23.58,  = 4.12).", "label": "Author", "bboxes": [{"left": 0.28951143790849676, "top": 0.47968560606060606, "width": 0.19578104575163396, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4935227272727273, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.08689052287581699, "top": 0.5073598484848485, "width": 0.26542320261437913, "height": 0.012843434343434268, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "501"}, {"text": "At the end of the last session, we surveyed a questionnaire with four questions  1) Easy to learn ; 2) Easy to use ; 3) Prefer to use ; and", "label": "Author", "bboxes": [{"left": 0.36450000000000005, "top": 0.5073598484848485, "width": 0.12079738562091497, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5211969696969697, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5347563131313131, "width": 0.3799019607843137, "height": 0.012856060606060593, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "502"}, {"text": "We gathered 18 participants  3 layouts  (3 blocks of NC + 1 block of VGC)  5 tasks = 1,080 phrases of text entry results.", "label": "Author", "bboxes": [{"left": 0.3790375816993464, "top": 0.5627070707070707, "width": 0.10624999999999996, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5762676767676768, "width": 0.39827124183006535, "height": 0.012856060606060704, "page": 5}, {"left": 0.08811928104575163, "top": 0.590104797979798, "width": 0.3023611111111111, "height": 0.012856060606060593, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "503"}, {"text": "We were also uncertain of how fast a user will get used to the layout.", "label": "Author", "bboxes": [{"left": 0.392359477124183, "top": 0.8367222222222221, "width": 0.09293300653594777, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3626160130718955, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "504"}, {"text": "we conducted one additional block with VGC to simulate users who are familiar with each keyboard layout.", "label": "Author", "bboxes": [{"left": 0.5240784313725491, "top": 0.4096893939393939, "width": 0.39775490196078434, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.4235265151515151, "width": 0.32071078431372557, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "505"}, {"text": "At the start of VGC, we explained the reason to the participants.", "label": "Author", "bboxes": [{"left": 0.8504150326797385, "top": 0.4235265151515151, "width": 0.07142647058823537, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.43736363636363634, "width": 0.35091339869281046, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "506"}, {"text": "Owing to the differences in remote and wearable eye tracking environments, and the slippage and HWD wobbling problems of wearable eye trackers, we conducted Experiment 2 utilizing an eye-trackable HWD, FOVE VR 4 .", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.6991464646464647, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7129835858585859, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7268207070707071, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.738655303030303, "width": 0.23382679738562107, "height": 0.014580808080808083, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "507"}, {"text": "We analyzed the key input time to better understand the typing skills of users utilizing GAT, with only characters that were not erased and typed correctly (27,383 key entries).", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.3511691919191919, "width": 0.3979379084967321, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3650063131313131, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3788434343434343, "width": 0.3345767973856209, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "508"}, {"text": "Thus, we argue that further reduction of T1 after long-term utilization and/or incorporation of an optimized key placement that requires less SWITCH , can make GAT better.", "label": "Author", "bboxes": [{"left": 0.8252630718954248, "top": 0.5346515151515152, "width": 0.09658006535947716, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5484015151515151, "width": 0.39745915032679746, "height": 0.012666666666666715, "page": 6}, {"left": 0.5246633986928104, "top": 0.5623257575757576, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5760757575757576, "width": 0.21686601307189535, "height": 0.012666666666666715, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "509"}, {"text": "To understand the observation on T1 , we calculated the average number of sub-keyboard switches per letter inputted ( SWITCH , Figure 11(e)).", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.6429166666666667, "width": 0.4003790849673202, "height": 0.012666666666666604, "page": 6}, {"left": 0.08811437908496732, "top": 0.6568409090909091, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 6}, {"left": 0.08758169934640524, "top": 0.670590909090909, "width": 0.17174836601307192, "height": 0.012666666666666715, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "510"}, {"text": "Thus, we concluded that T1 increased with more sub-keyboards because with the larger number of sub-keyboards, a user might have been exploring other sub-keyboards to nd a target key.", "label": "Author", "bboxes": [{"left": 0.4225702614379085, "top": 0.712189393939394, "width": 0.0627140522875817, "height": 0.012579545454545538, "page": 6}, {"left": 0.08812745098039215, "top": 0.7259393939393939, "width": 0.3998627450980392, "height": 0.012666666666666715, "page": 6}, {"left": 0.08811928104575163, "top": 0.7398636363636364, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.7537007575757576, "width": 0.40002614379084966, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "511"}, {"text": "Subsequently, we simulated an optimal SWITCH for each layout with a used phrase set [21].", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.767449494949495, "width": 0.3971699346405229, "height": 0.012666666666666604, "page": 6}, {"left": 0.08811601307189543, "top": 0.7813737373737373, "width": 0.2283071895424837, "height": 0.012579545454545538, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "512"}, {"text": "We decomposed the key input time into four parts considering eyeand touch input phase; Eye movement data was logged every 30 ms. T1 is the time from the moment the previous key was entered to the moment the eye cursor entered a target sub-keyboard containing a target key.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.40022727272727276, "width": 0.39794281045751634, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.414064393939394, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.42781439393939397, "width": 0.39717810457516345, "height": 0.01266666666666666, "page": 6}, {"left": 0.08811928104575163, "top": 0.4417386363636364, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.4555757575757575, "width": 0.24215032679738563, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "513"}, {"text": "To validate that GAT has better usability than the monomodal text entry methods, we compared GAT to eye-only typing (adjustable dwell time [22]) and touch-only typing (SwipeZone [11]) utilizing eye-trackable HWD.", "label": "Author", "bboxes": [{"left": 0.5241601307189543, "top": 0.61689898989899, "width": 0.3976781045751634, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6307361111111112, "width": 0.3998725490196079, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6445732323232324, "width": 0.39716830065359465, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6584103535353536, "width": 0.22940032679738565, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "514"}, {"text": "We recruited 12 participants (3 females, 9 males, mean age: 22.33, from 18 to 28 years) from our university.", "label": "Author", "bboxes": [{"left": 0.8408480392156864, "top": 0.7385681818181818, "width": 0.08099509803921556, "height": 0.012579545454545427, "page": 7}, {"left": 0.5234428104575163, "top": 0.7524040404040404, "width": 0.3983970588235296, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7662411616161617, "width": 0.21573856209150322, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "515"}, {"text": "We used plain QWERTY layout for the eye only typing as shown in gure 13 (d).", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.5712272727272727, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5850643939393939, "width": 0.14745588235294116, "height": 0.012579545454545538, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "516"}, {"text": "We used dwell time feedback as similar to in the implementation of Dynamic cascading [28].", "label": "Author", "bboxes": [{"left": 0.6708692810457516, "top": 0.6127386363636363, "width": 0.251248366013072, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.6265757575757576, "width": 0.3554133986928104, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "517"}, {"text": "We also provided audio feedback for key entered, it is same to the other techniques.", "label": "Author", "bboxes": [{"left": 0.6754624183006536, "top": 0.6819242424242423, "width": 0.24665686274509813, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.6957613636363637, "width": 0.30070098039215687, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "518"}, {"text": "We used the within-subject design with the technique as a factor; touch-only (SwipeZone), eye-only (A-Dwell), and GAT.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.8782335858585859, "width": 0.4006339869281046, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.39323039215686273, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "519"}, {"text": "Thus, we used the multinger swipe gestures for the keys, which are same as GAT.", "label": "Author", "bboxes": [{"left": 0.7968692810457517, "top": 0.3600833333333333, "width": 0.12496405228758167, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246601307189542, "top": 0.37392045454545453, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.38775757575757575, "width": 0.034477124183006524, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "520"}, {"text": "We constructed a VR environment that was similar to the smart glass-based environment.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.5704444444444444, "width": 0.3979379084967321, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.5842815656565656, "width": 0.16820098039215686, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "521"}, {"text": "We set the FoV of the smart glass display to 100, 85  85 cm sized display placed at a distance of 50 cm from the eyes.", "label": "Author", "bboxes": [{"left": 0.26179901960784313, "top": 0.5842815656565656, "width": 0.22349346405228765, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.5978421717171717, "width": 0.39718300653594774, "height": 0.012856060606060704, "page": 7}, {"left": 0.08811928104575163, "top": 0.6119558080808081, "width": 0.16604248366013075, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "522"}, {"text": "To calculate a gaze position, we used gaze direction vectors of both eyes, provided by FOVE software development kit.", "label": "Author", "bboxes": [{"left": 0.2646062091503268, "top": 0.6119558080808081, "width": 0.22068627450980394, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6257929292929293, "width": 0.39717647058823524, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6396300505050505, "width": 0.16856045751633986, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "523"}, {"text": "Then, we used a center position of the ray-casted positions of both eyes as the gaze position on the plane.", "label": "Author", "bboxes": [{"left": 0.2617205882352941, "top": 0.6396300505050505, "width": 0.22357516339869277, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6534671717171717, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6673030303030303, "width": 0.06329248366013072, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "524"}, {"text": "To reduce the jitter, we also used an average lter.", "label": "Author", "bboxes": [{"left": 0.15645751633986926, "top": 0.6673030303030303, "width": 0.32523366013071897, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "525"}, {"text": "To help a user distinguish between the regions, we attached two strips of tape on the backward and forward regions of the touchpad to provide a tactile feedback, similar to the original Swipezone.", "label": "Author", "bboxes": [{"left": 0.8573627450980392, "top": 0.2695138888888889, "width": 0.06447385620915025, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.2833510101010101, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.2971881313131313, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.31102525252525254, "width": 0.39580228758169944, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "526"}, {"text": "We increased the keyboard size to more than that of Experiment 1 considering the mean accuracy of FOVE.", "label": "Author", "bboxes": [{"left": 0.4010588235294118, "top": 0.7813737373737373, "width": 0.08423366013071892, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.39987091503267974, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.2191764705882353, "height": 0.012579545454545538, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "527"}, {"text": "We chose the Adjustable dwell time selection method [22] as the eye only typing baseline, i.e. , A-Dwell.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.4668219696969697, "width": 0.39793790849673216, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246617647058823, "top": 0.4803699494949495, "width": 0.31614542483660124, "height": 0.012868686868686863, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "528"}, {"text": "We follow dwell time adjust fomula used in the original [22], Dwell = 300  EXP ( X / 12 )  150, where as X, from 1 to 20, can be adjust in a digit (gure 13 (e)).", "label": "Author", "bboxes": [{"left": 0.6009771241830065, "top": 0.5083320707070706, "width": 0.32085947712418295, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5218800505050505, "width": 0.39921568627450976, "height": 0.012868686868686918, "page": 7}, {"left": 0.5246633986928104, "top": 0.5360063131313131, "width": 0.33710947712418304, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "529"}, {"text": "We set a X to 11 ( Dwell = 600 ms ) as a default value.", "label": "Author", "bboxes": [{"left": 0.8668169934640523, "top": 0.5360063131313131, "width": 0.055024509803921595, "height": 0.012579545454545427, "page": 7}, {"left": 0.5240784313725491, "top": 0.5495542929292929, "width": 0.28771078431372543, "height": 0.012868686868686918, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "530"}, {"text": "We chose the fastest letter-wise text entry method, i.e. , SwipeZone [11].", "label": "Author", "bboxes": [{"left": 0.646217320261438, "top": 0.10976010101010102, "width": 0.27617973856209144, "height": 0.01257954545454544, "page": 7}, {"left": 0.5246633986928104, "top": 0.1233080808080808, "width": 0.1941732026143791, "height": 0.012868686868686877, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "531"}, {"text": "Regardless, we chose GAT9 to compare the other variations because GAT6 cannot use a QWERTY-like layout, which may be more familiar to users, and GAT3 requires complicated touch gestures.", "label": "Author", "bboxes": [{"left": 0.3864003267973856, "top": 0.7184785353535353, "width": 0.09889705882352945, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.3998839869281046, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.759989898989899, "width": 0.3888022875816993, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "532"}, {"text": "To conduct statistical tests, we simulated the missing data.", "label": "Author", "bboxes": [{"left": 0.18784477124183005, "top": 0.554239898989899, "width": 0.2974477124183007, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5680770202020202, "width": 0.08772058823529415, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "533"}, {"text": "At the end of the last session, we surveyed a questionnaire with ve questions using a ve-point Likert scale; four of the questions were same to Experiment 1; and, the fth option was Eye fatigue .", "label": "Author", "bboxes": [{"left": 0.2785049019607843, "top": 0.2625959595959596, "width": 0.2067875816993464, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.2764330808080808, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.29027020202020204, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08689052287581699, "top": 0.30401893939393937, "width": 0.2804084967320261, "height": 0.012667929292929314, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "534"}, {"text": "The analysis methods were nearly the same as those for Experiment 1; however, we conducted three post-hoc tests only in the last block, and we used a one-tailed assumption for the tests because we assumed that GAT outperforms the others.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.47750757575757574, "width": 0.4003774509803921, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.49134343434343436, "width": 0.39773692810457517, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.5051805555555555, "width": 0.397171568627451, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5190176767676767, "width": 0.38745098039215686, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "535"}, {"text": "Finally, we gathered 12 participants  2 days  3 sessions (techniques)  3 blocks  4 tasks = 864 phrases.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.3805631313131313, "width": 0.39717156862745095, "height": 0.012856060606060649, "page": 8}, {"left": 0.08758660130718955, "top": 0.39439898989898986, "width": 0.3307173202614379, "height": 0.012857323232323303, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "536"}, {"text": "At the start of each session, we calibrated the eye tracker with the default calibration method of FOVE.", "label": "Author", "bboxes": [{"left": 0.08753267973856209, "top": 0.14435227272727272, "width": 0.3977549019607843, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.15818939393939394, "width": 0.26888888888888896, "height": 0.012579545454545454, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "537"}, {"text": "If a user experienced a mismatch in the eye cursor position, we recalibrated the eye tracker and discarded the task in progress.", "label": "Author", "bboxes": [{"left": 0.4743071895424837, "top": 0.17202651515151515, "width": 0.010982026143790846, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.18586363636363637, "width": 0.39717810457516345, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.1997007575757576, "width": 0.39983169934640517, "height": 0.012579545454545454, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "538"}, {"text": "Therefore, we conducted post-hoc Wilcoxon signed-rank tests with Bonferroni correction.", "label": "Author", "bboxes": [{"left": 0.5241601307189543, "top": 0.49995454545454543, "width": 0.3976813725490196, "height": 0.012579545454545538, "page": 8}, {"left": 0.5240784313725491, "top": 0.5137916666666666, "width": 0.17857843137254903, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "539"}, {"text": "In our study, we considered rst the input modalities for the conventional gesture input and nalized on a touchpad.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.5162007575757576, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5300378787878788, "width": 0.3582450980392158, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "540"}, {"text": "We presented the concept of GAT and the rst adaptation of a complementary combination of the gaze and touch input modalities for wearable text entry.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.6706780303030303, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246617647058823, "top": 0.6845151515151515, "width": 0.39717483660130715, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.6983522727272727, "width": 0.21854248366013063, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "541"}, {"text": "We demonstrated that GAT can ensure faster on-glass text entry when compared to the touch-only text entry.", "label": "Author", "bboxes": [{"left": 0.7482516339869281, "top": 0.6983522727272727, "width": 0.17409803921568623, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.712189393939394, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.7260265151515152, "width": 0.14233169934640522, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "542"}, {"text": "Further, we explored the variations in the number of touch gestures.", "label": "Author", "bboxes": [{"left": 0.672607843137255, "top": 0.7260265151515152, "width": 0.2492287581699345, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.7398636363636364, "width": 0.1879297385620916, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "543"}, {"text": "According to our micro analysis results, the gesture completion time ( T4 ) increased with the number of touch gestures.", "label": "Author", "bboxes": [{"left": 0.7175065359477124, "top": 0.7398636363636364, "width": 0.20433006535947718, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.7536136363636363, "width": 0.39716666666666667, "height": 0.012666666666666715, "page": 9}, {"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.1646372549019608, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "544"}, {"text": "Finally, we demonstrated that GAT outperformed the single input methods in the HWD environment as GAT (11.04 wpm) was 25.4% faster than the eye-only typing technique (8.81 wpm) and 29.4% faster than the touch-only typing method (8.53 wpm); moreover, it was the most preferred technique.", "label": "Author", "bboxes": [{"left": 0.8472009803921569, "top": 0.822885101010101, "width": 0.07463562091503273, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.39851797385620913, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39852124183006543, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3024918300653595, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "545"}, {"text": "For the \"Prefer to use\" question, we asked the reason for the highest/lowest rating.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.0959229797979798, "width": 0.3971748366013072, "height": 0.01257954545454544, "page": 9}, {"left": 0.08811928104575163, "top": 0.10976010101010102, "width": 0.14101143790849674, "height": 0.01257954545454544, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "546"}, {"text": "If the highest/lowest score was not the only one, we asked the participants to choose the best/worst technique along with the reason for this choice.", "label": "Author", "bboxes": [{"left": 0.23420588235294118, "top": 0.10976010101010102, "width": 0.2510947712418301, "height": 0.01257954545454544, "page": 9}, {"left": 0.08811928104575163, "top": 0.12359722222222222, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.13743434343434344, "width": 0.3083366013071896, "height": 0.012579545454545454, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "547"}, {"text": "Our future work will handle such environmental validity issues using an eyetracking AR glasses which can meet the requirements of GAT.", "label": "Author", "bboxes": [{"left": 0.8111960784313725, "top": 0.10912626262626263, "width": 0.11103267973856212, "height": 0.012579545454545468, "page": 9}, {"left": 0.5240751633986928, "top": 0.12296338383838384, "width": 0.40046078431372556, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.13680429292929294, "width": 0.40003267973856205, "height": 0.012579545454545454, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "548"}, {"text": "If we consider that the eye-tracking accuracy may affect the performance of GAT variations, the results of Exp 1 may differ in the wearable context, because a wearable eye tracker may be less accurate than a stationary one.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.3974591503267974, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.22814215686274514, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "549"}, {"text": "However, we determined that the performances of GAT9 in the stationary setup (Exp 1, ve tasks/block) and in the wearable setup (Exp 2, four tasks/block) were consistent in terms of both speed (9.3 wpm vs. 9.0 wpm, respectively, at the 3rd block) and accuracy (CER: 6.3% vs. 7.4% and UER: 0.3% vs. 0.9%).", "label": "Author", "bboxes": [{"left": 0.3213251633986928, "top": 0.7461527777777778, "width": 0.16397222222222219, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.759989898989899, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 9}, {"left": 0.08689052287581699, "top": 0.7738270202020202, "width": 0.3986683006535947, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7876641414141414, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 9}, {"left": 0.0877124183006536, "top": 0.8015012626262626, "width": 0.3981535947712419, "height": 0.012579545454545427, "page": 9}, {"left": 0.08758006535947711, "top": 0.8153383838383839, "width": 0.32517647058823534, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "550"}, {"text": "We conducted the experiments in simulated environments.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.6417462121212121, "width": 0.4007941176470589, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "551"}, {"text": "We demonstrated that GAT (11.04 wpm, after 40 min of usage) is faster than SwipeZone (8.53 wpm, after 50 min of usage) in Experiment 2.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.1805858585858586, "width": 0.39903921568627454, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246650326797386, "top": 0.19442297979797982, "width": 0.39717156862745084, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.208260101010101, "width": 0.09251307189542479, "height": 0.012579545454545454, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "552"}, {"text": "This research was supported by Basic Science ResearchProgram through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF2015R1D1A1A01058992).", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.0959229797979798, "width": 0.4003774509803922, "height": 0.01257954545454544, "page": 10}, {"left": 0.08811274509803921, "top": 0.10976010101010102, "width": 0.3998790849673203, "height": 0.01257954545454544, "page": 10}, {"left": 0.08811274509803921, "top": 0.12359722222222222, "width": 0.3998790849673203, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.13743434343434344, "width": 0.17769934640522878, "height": 0.012579545454545454, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "553"}, {"text": "However, text entry on the on-frame touchpad", "label": "Novelty", "bboxes": [{"left": 0.1731830065359477, "top": 0.775094696969697, "width": 0.3121094771241831, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "554"}, {"text": "However, gaze typing also has limitations.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.49996338383838385, "width": 0.2844967320261438, "height": 0.012579545454545482, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "555"}, {"text": "Its performance depends signicantly on eye-tracking accuracy, which varies for different users [33].", "label": "Novelty", "bboxes": [{"left": 0.8165, "top": 0.49996338383838385, "width": 0.10533660130718947, "height": 0.012579545454545482, "page": 0}, {"left": 0.5246633986928104, "top": 0.513800505050505, "width": 0.39717810457516356, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.5276376262626262, "width": 0.15521732026143797, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "556"}, {"text": "Further, it may be accurate initially, but becomes inaccurate over time owing to the slippage of the HWD.", "label": "Novelty", "bboxes": [{"left": 0.685890522875817, "top": 0.5276376262626262, "width": 0.23797222222222225, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.5414747474747474, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.555310606060606, "width": 0.06615032679738564, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "557"}, {"text": "However, according to a previous study [33], the inaccuracy of a mobile eye tracker is still problematic for text entry.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.6106590909090909, "width": 0.39774836601307184, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.6244962121212121, "width": 0.3771486928104575, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "558"}, {"text": "In addition, the Midas touch [15] problem in smart glasses should be considered more seriously because a user always has to see the display while s/he sees the \"real world\" over the display.", "label": "Novelty", "bboxes": [{"left": 0.9080049019607842, "top": 0.6244962121212121, "width": 0.013831699346405268, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.6383333333333333, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.6521704545454545, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.6660075757575757, "width": 0.39090196078431383, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "559"}, {"text": "However, a hand-held device may constitute an additional input device rather than being a primary input device for the glasses users due to carriage load.", "label": "Novelty", "bboxes": [{"left": 0.2872042483660131, "top": 0.7154570707070708, "width": 0.19808823529411768, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7292941919191919, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7431313131313131, "width": 0.3999542483660131, "height": 0.012579545454545538, "page": 1}], "section": "On glasses frame input", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "560"}, {"text": "Nevertheless, mimicking of standard keyboard typing (ATK [45]: 29.2 wpm, an hour of usage, and VISAR [8]: 17.8 wpm, 8 days of usage) and word-stroke gestures (Vulture [25]: 20.6 wpm, 10 days of usage, and RotoSwipe [12]: 14.8 wpm, 5 days of usage) demonstrated high input speeds; however, according to Yeo et al.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.21791287878787877, "width": 0.39775490196078434, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.23174999999999998, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 1}, {"left": 0.5240784313725491, "top": 0.2455871212121212, "width": 0.40002777777777776, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.25942424242424245, "width": 0.3992075163398693, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.273260101010101, "width": 0.3991993464052288, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.2868080808080808, "width": 0.1511650326797387, "height": 0.012868686868686863, "page": 1}], "section": "Mid-air input", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "561"}, {"text": "To enter text, voice input is a natural and fast technique [3]; however, its use is limited because", "label": "Novelty", "bboxes": [{"left": 0.08761437908496732, "top": 0.33455681818181815, "width": 0.39903267973856205, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.34839393939393937, "width": 0.22026633986928107, "height": 0.012579545454545482, "page": 1}], "section": "Possible Text Entry Methods for Smart Glasses", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "562"}, {"text": "These techniques improved the dwell-based eye typing; however, a user can practice to create an optimal dwell time.", "label": "Novelty", "bboxes": [{"left": 0.565843137254902, "top": 0.6694204545454546, "width": 0.35734967320261446, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6832575757575757, "width": 0.3933921568627451, "height": 0.012579545454545427, "page": 1}], "section": "Gaze-based Text entry methods", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "563"}, {"text": "The eye cursor can provide a sense of control for the eye input; however, it may distract a user or occlude a key.", "label": "Novelty", "bboxes": [{"left": 0.23546568627450978, "top": 0.5522613636363637, "width": 0.24982679738562094, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5660984848484849, "width": 0.3974558823529412, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5799356060606061, "width": 0.09037581699346407, "height": 0.012579545454545427, "page": 2}], "section": "Sub-keyboard Selection", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "564"}, {"text": "Owing to the touch cursor, a novice user can enter a key in a touch-adjust-release manner; however, an expert user can enter a key with a swipe or tap gesture.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.6062146464646465, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6200517676767677, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.633888888888889, "width": 0.2575261437908497, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "565"}, {"text": "In fact, Gaze + Gesture interaction have been used not only for text entry but also for many other interactions [30, 31, 47], e.g., pointing, object manipulation, menu selection, etc .", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.0814570707070707, "width": 0.3977385620915033, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.39919934640522875, "height": 0.012579545454545468, "page": 2}, {"left": 0.08755392156862744, "top": 0.10884217171717173, "width": 0.40057843137254906, "height": 0.012868686868686863, "page": 2}], "section": "Gaze-based Text entry methods", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "566"}, {"text": "As far as text entry methods are considered, however, gazeassisted typing (GAT) is distinguished from the prior works as we consider a taxonomy for Gaze + Gesture combinations that was presented by Chatterjee et al.", "label": "Novelty", "bboxes": [{"left": 0.08752124183006536, "top": 0.12296843434343435, "width": 0.40046078431372545, "height": 0.012579545454545454, "page": 2}, {"left": 0.08810294117647058, "top": 0.13680555555555554, "width": 0.39717320261437905, "height": 0.012578282828282827, "page": 2}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.16418939393939394, "width": 0.2635751633986928, "height": 0.012868686868686863, "page": 2}], "section": "Gaze-based Text entry methods", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "567"}, {"text": "While selecting a key with a touch input, owing to eye jitter and target key searching behavior, the eyes of the user are not stable during a keystroke.", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.822885101010101, "width": 0.398218954248366, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811764705882352, "top": 0.8367222222222221, "width": 0.39717483660130726, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.191359477124183, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "568"}, {"text": "Three participants had previous experience in using a smart glass and/or an eye tracker; however, none of them were regular users.", "label": "Novelty", "bboxes": [{"left": 0.8846323529411765, "top": 0.8367222222222221, "width": 0.037204248366013104, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39745588235294116, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.0378235294117647, "height": 0.012579545454545427, "page": 3}], "section": "Participants", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "569"}, {"text": "2. However, there were slippage problems (Pupil Labs) and latency issues (Tobii, over 70 ms).", "label": "Novelty", "bboxes": [{"left": 0.27241503267973854, "top": 0.6422083333333334, "width": 0.21288071895424832, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6560454545454545, "width": 0.4000179738562092, "height": 0.012579545454545538, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "570"}, {"text": "Different wearable eye trackers have different problems; consequently, for studies conducted with a particular tracker, the results would be dependent on the specic problems of that tracker.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.6698825757575758, "width": 0.39987091503267974, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6837184343434344, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6975555555555556, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7113926767676768, "width": 0.04918137254901962, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "571"}, {"text": "The experiment consisted of two sessions with a different technique for each session.", "label": "Novelty", "bboxes": [{"left": 0.7012336601307189, "top": 0.5313851010101011, "width": 0.22060294117647072, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5452222222222222, "width": 0.32999346405228747, "height": 0.012579545454545538, "page": 3}], "section": "Design and Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "572"}, {"text": "We instructed the participants to memorize a target phrase for 15 s before starting the task, but, the target phrase was not erased while performing the task.", "label": "Novelty", "bboxes": [{"left": 0.8298447712418301, "top": 0.4059292929292929, "width": 0.0919869281045751, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246584967320261, "top": 0.41976641414141413, "width": 0.39717320261437894, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246584967320261, "top": 0.43360353535353535, "width": 0.39717320261437905, "height": 0.012578282828282827, "page": 3}, {"left": 0.5246633986928104, "top": 0.44744065656565657, "width": 0.1309558823529411, "height": 0.012579545454545482, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "573"}, {"text": "The only difference between GAT and M-SwipeBoard is the sub-keyboard selection method.", "label": "Novelty", "bboxes": [{"left": 0.08761437908496732, "top": 0.48888131313131317, "width": 0.39768137254901953, "height": 0.012579545454545371, "page": 3}, {"left": 0.08811928104575163, "top": 0.5027184343434343, "width": 0.2072892156862745, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "574"}, {"text": "However, M-SwipeBoard consists of three sub-keyboards and the key selection utilized nine touch gestures, similar to GAT.", "label": "Novelty", "bboxes": [{"left": 0.12168464052287582, "top": 0.4121489898989899, "width": 0.3636078431372549, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.42598611111111107, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.4398232323232323, "width": 0.0337565359477124, "height": 0.012579545454545482, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "575"}, {"text": "Five of twelve experienced more eye fatigue while using GAT because they had to pay more attention to the eye movement; however, another one said that M-SwipeBoard was more tiring owing to the long usage time.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.39768137254901964, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3985163398692811, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.19015359477124183, "height": 0.012579545454545538, "page": 4}], "section": "Eye fatigue", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "576"}, {"text": "The others said that they had to observe the keyboard in the same manner for both techniques; therefore, they could not feel any difference in the eye fatigue.", "label": "Novelty", "bboxes": [{"left": 0.2833218954248366, "top": 0.8643964646464646, "width": 0.20197549019607847, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3985245098039216, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.39232352941176474, "height": 0.012579545454545427, "page": 4}], "section": "Eye fatigue", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "577"}, {"text": "RM-ANOVA demonstrated signicant effects on the technique (F ( 1 , 11 ) = 42.763, p < 0.01) and the block (F ( 2 , 22 ) = 53.962, p < 0.01); however, the interaction was not signicant.", "label": "Novelty", "bboxes": [{"left": 0.1808545751633987, "top": 0.5544027777777778, "width": 0.3044379084967321, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.5679633838383839, "width": 0.39758006535947715, "height": 0.014803030303030318, "page": 4}, {"left": 0.08757843137254902, "top": 0.5818005050505051, "width": 0.39771241830065357, "height": 0.014803030303030318, "page": 4}, {"left": 0.08811928104575163, "top": 0.5959141414141415, "width": 0.0715441176470588, "height": 0.012579545454545538, "page": 4}], "section": "Text entry speed", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "578"}, {"text": "All participants were different from those at the preliminary study, were right-handed, non-native English speakers, with no experience in using a smart glass or an eye tracker.", "label": "Novelty", "bboxes": [{"left": 0.41432679738562095, "top": 0.6465151515151515, "width": 0.07367320261437904, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6603522727272727, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6741893939393939, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6880265151515151, "width": 0.2509771241830065, "height": 0.012579545454545427, "page": 5}], "section": "Design and Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "579"}, {"text": "The sub-keyboard size for all variations was 6.85  6.85; however, if the eye cursor out of boundary of the keyboard, a closest sub-keyboard will be activated.", "label": "Novelty", "bboxes": [{"left": 0.08761437908496732, "top": 0.22955681818181817, "width": 0.39903104575163395, "height": 0.012856060606060649, "page": 5}, {"left": 0.08812745098039215, "top": 0.24367045454545455, "width": 0.39919934640522886, "height": 0.012579545454545427, "page": 5}, {"left": 0.08812745098039215, "top": 0.25750757575757577, "width": 0.2793496732026144, "height": 0.012579545454545427, "page": 5}], "section": "GAT Variations: Different Number of Gestures", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "580"}, {"text": "4) Feel natural eye movements while entering text ( Eye feels natural )  using a ve-point Likert scale.", "label": "Novelty", "bboxes": [{"left": 0.4725539215686274, "top": 0.5350328282828283, "width": 0.013831699346405268, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811601307189543, "top": 0.5487828282828283, "width": 0.39717973856209143, "height": 0.012666666666666715, "page": 5}, {"left": 0.08812745098039215, "top": 0.5624305555555555, "width": 0.2818267973856209, "height": 0.012856060606060593, "page": 5}], "section": "Design and Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "581"}, {"text": "A user completed tasks in two different conditions: normal condition (NC) and visual guidance condition (VGC).", "label": "Novelty", "bboxes": [{"left": 0.08753104575163399, "top": 0.7579987373737375, "width": 0.3977549019607843, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7718345959595959, "width": 0.35586928104575166, "height": 0.012579545454545427, "page": 5}], "section": "Apparatus and Task", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "582"}, {"text": "For NC, two-way (the technique and the block) RM-ANOVA analysis on text entry speed and on CER showed signicant effects of the block (Speed: F ( 2 , 34 ) = 156.903, p < 0.01, CER: F ( 2 , 34 ) = 20.775, p < 0.01); however, the effects of layout and interaction were not signicant.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.6709911616161616, "width": 0.39775653594771243, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.6848282828282829, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.698388888888889, "width": 0.3994395424836602, "height": 0.014803030303030207, "page": 5}, {"left": 0.5246633986928104, "top": 0.71222601010101, "width": 0.39716993464052297, "height": 0.014803030303030318, "page": 5}, {"left": 0.5246633986928104, "top": 0.7263396464646464, "width": 0.20920424836601315, "height": 0.012579545454545538, "page": 5}], "section": "Text entry speed and corrected error rate", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "583"}, {"text": "However, four out of these six participants mentioned that the vertical swipe was easy to use in GAT6 owing to zero conict between diagonal and vertical swipe gestures.", "label": "Novelty", "bboxes": [{"left": 0.36851633986928106, "top": 0.24228409090909092, "width": 0.11678104575163395, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.25612121212121214, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.2699583333333333, "width": 0.3971797385620915, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.2837954545454545, "width": 0.17593300653594773, "height": 0.012579545454545482, "page": 6}], "section": "Comments from the participants", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "584"}, {"text": "In fact, the virtual reality environment and the smart glass-based environment may differ from each other.", "label": "Novelty", "bboxes": [{"left": 0.7625751633986928, "top": 0.7406565656565657, "width": 0.15983006535947708, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7544936868686868, "width": 0.39745915032679746, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.7683308080808081, "width": 0.10950490196078433, "height": 0.012579545454545427, "page": 6}], "section": "Implementation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "585"}, {"text": "Even though it is not a smart glass, it was effective for simulating the wearable context of the smart glass.", "label": "Novelty", "bboxes": [{"left": 0.6411584967320262, "top": 0.7683308080808081, "width": 0.2806781045751634, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7821679292929292, "width": 0.40002777777777765, "height": 0.012579545454545538, "page": 6}], "section": "Implementation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "586"}, {"text": "For example, for a word, \"the\", while using GAT3 (Figure 6(a)), \"t\" and \"h\" are on different sub-keyboards; therefore, one sub-keyboard switch is required; however, \"h\" and \"e\" are on the same sub-keyboard; hence, no sub-keyboard switch is required.", "label": "Novelty", "bboxes": [{"left": 0.3221846405228758, "top": 0.7813737373737373, "width": 0.1651307189542484, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.39987091503267974, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3985294117647059, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.27482189542483665, "height": 0.012579545454545538, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "587"}, {"text": "On the other hand, T1 increased and T4 decreased as the number of sub-keyboards increased.", "label": "Novelty", "bboxes": [{"left": 0.4401274509803922, "top": 0.5385972222222223, "width": 0.04516503267973854, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811764705882352, "top": 0.5523472222222222, "width": 0.3974493464052287, "height": 0.012666666666666715, "page": 6}, {"left": 0.08812745098039215, "top": 0.5662714646464647, "width": 0.18526960784313723, "height": 0.012579545454545427, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "588"}, {"text": "However, according to a previous study that used FOVE to evaluate dwell-based eye typing [33], its mean accuracy was approximately 4.", "label": "Novelty", "bboxes": [{"left": 0.8119771241830065, "top": 0.8450631313131313, "width": 0.1125669934640523, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246584967320261, "top": 0.8589002525252526, "width": 0.39717320261437894, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.8727373737373737, "width": 0.3738578431372549, "height": 0.012579545454545538, "page": 6}], "section": "Implementation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "589"}, {"text": "Wilcoxon-signed rank tests with Bonferroni correction showed that GAT6 got signicantly higher ratings than GAT3 for the different options; i.e. , Easy to learn (Z = -2.449, p < 0.05), Easy to use (Z = -3.372, p < 0.01), Prefer to use (Z = -2.864, p < 0.05).", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.3979379084967321, "height": 0.012579545454545468, "page": 6}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.39717810457516345, "height": 0.012579545454545468, "page": 6}, {"left": 0.08811928104575163, "top": 0.10884217171717173, "width": 0.3992091503267974, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.12269065656565657, "width": 0.3992042483660131, "height": 0.01285606060606058, "page": 6}, {"left": 0.08811928104575163, "top": 0.13652777777777778, "width": 0.0648545751633987, "height": 0.01285606060606062, "page": 6}], "section": "Questionnaire", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "590"}, {"text": "GAT9 also got a signicantly higher rating than GAT3 for the Easy to use option (Z = -2.967, p < 0.01); however, for the option of Eye feels natural , GAT9 got a lower rating than GAT3 (Z = -2.423, p < 0.05).", "label": "Novelty", "bboxes": [{"left": 0.16131045751633985, "top": 0.13680429292929294, "width": 0.3239820261437909, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.150364898989899, "width": 0.3998709150326797, "height": 0.012856060606060593, "page": 6}, {"left": 0.08811928104575163, "top": 0.16439141414141414, "width": 0.3974575163398693, "height": 0.01266666666666666, "page": 6}, {"left": 0.08811928104575163, "top": 0.17803914141414143, "width": 0.2637156862745098, "height": 0.012856060606060593, "page": 6}], "section": "Questionnaire", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "591"}, {"text": "All of them were different from the previous studies, right-handed, non-native English speakers, with no experience in using a smart glasses or eye tracker.", "label": "Novelty", "bboxes": [{"left": 0.7466617647058823, "top": 0.7662411616161617, "width": 0.17517483660130728, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7800782828282828, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.793915404040404, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.8077525252525253, "width": 0.047462418300653675, "height": 0.012579545454545427, "page": 7}], "section": "Task and Participants", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "592"}, {"text": "However, the original SwipeZone did not have the delete, space, and enter keys.", "label": "Novelty", "bboxes": [{"left": 0.6377173202614379, "top": 0.3462462121212121, "width": 0.2841160130718955, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246601307189542, "top": 0.3600833333333333, "width": 0.2622794117647059, "height": 0.012579545454545482, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "593"}, {"text": "SwipeZone is a two-step key selection method, similar to SwipeBoard, but using different sub-keyboard selection gestures.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.1449810606060606, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 7}, {"left": 0.5246633986928104, "top": 0.15881818181818183, "width": 0.39988235294117647, "height": 0.012579545454545454, "page": 7}, {"left": 0.5246633986928104, "top": 0.17265530303030302, "width": 0.03642973856209153, "height": 0.012579545454545454, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "594"}, {"text": "Thus, the dimensions of the GAT keyboard are 30  21; however, the required minimum FoV is 23  16 considering blank space as shown in Figure 13", "label": "Novelty", "bboxes": [{"left": 0.29894607843137255, "top": 0.822885101010101, "width": 0.18634967320261436, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.8364457070707071, "width": 0.39718137254901964, "height": 0.012856060606060593, "page": 7}, {"left": 0.08811928104575163, "top": 0.8502828282828283, "width": 0.3998709150326797, "height": 0.012856060606060593, "page": 7}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.020954248366013076, "height": 0.012579545454545538, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "595"}, {"text": "In Experiment 1, GAT3, GAT6, and GAT9 were equally effective; however, GAT6 was the most preferred.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.2932320261437909, "height": 0.012579545454545538, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "596"}, {"text": "The analysis methods were nearly the same as those for Experiment 1; however, we conducted three post-hoc tests only in the last block, and we used a one-tailed assumption for the tests because we assumed that GAT outperforms the others.", "label": "Novelty", "bboxes": [{"left": 0.08761437908496732, "top": 0.47750757575757574, "width": 0.4003774509803921, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.49134343434343436, "width": 0.39773692810457517, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.5051805555555555, "width": 0.397171568627451, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5190176767676767, "width": 0.38745098039215686, "height": 0.012579545454545427, "page": 8}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "597"}, {"text": "However, four participants could not complete all tasks for the eye-only technique owing to a time constraint.", "label": "Novelty", "bboxes": [{"left": 0.42403267973856207, "top": 0.3946755050505051, "width": 0.06329575163398699, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.40851262626262624, "width": 0.3977385620915033, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.42234974747474746, "width": 0.24625816993464056, "height": 0.012579545454545482, "page": 8}], "section": "Design and Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "598"}, {"text": "However, three out of six Wilcoxon signedrank tests represent medium effect sizes (r > 0.3) as following: GAT received more positive ratings on Easy to use than both touch-only (Z= -2.280, r= 0.33) and eye-only (Z= -2.095, r= 0.30).", "label": "Novelty", "bboxes": [{"left": 0.6293366013071895, "top": 0.6395833333333334, "width": 0.2952075163398693, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6531426767676768, "width": 0.3994444444444445, "height": 0.012856060606060593, "page": 8}, {"left": 0.5246633986928104, "top": 0.667169191919192, "width": 0.39717973856209154, "height": 0.012666666666666493, "page": 8}, {"left": 0.5246633986928104, "top": 0.6810934343434344, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6949305555555555, "width": 0.03733333333333333, "height": 0.012579545454545427, "page": 8}], "section": "Questionnaire", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "599"}, {"text": "The RM-ANOVA analysis on CER showed signicant effects of block (F ( 5 , 55 ) = 3.260, p < 0.05); however, the effects of technique and interaction were not signicant.", "label": "Novelty", "bboxes": [{"left": 0.08761437908496732, "top": 0.8291742424242424, "width": 0.39767320261437905, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8427348484848485, "width": 0.3971650326797386, "height": 0.014803030303030318, "page": 8}, {"left": 0.08811928104575163, "top": 0.8568484848484849, "width": 0.3011078431372549, "height": 0.012579545454545427, "page": 8}], "section": "Error rate", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "600"}, {"text": "However, the gesture input for GAT can be performed by not only other touch input devices, such as smartwatch, handheld touchpad, or on-body touch input techniques, but also using freehand gestures, such as mid-air nger gesture [19] or wrist tilting gestures [10]; this incorporates more subtle gestures, considering the social context and/or utilizing more comfortable postures rather than raising a hand to the glasses.", "label": "Novelty", "bboxes": [{"left": 0.8879624183006536, "top": 0.5300378787878788, "width": 0.03658660130718949, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.543875, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5577121212121212, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5715492424242424, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.5853863636363636, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5992234848484849, "width": 0.39717320261437916, "height": 0.012578282828282883, "page": 9}, {"left": 0.5246633986928104, "top": 0.6130606060606061, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.6268977272727273, "width": 0.4000049019607844, "height": 0.012579545454545538, "page": 9}], "section": "Scalability of GAT", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "601"}, {"text": "However, three out of twelve chose touch-only as the worst.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.4050643939393939, "width": 0.40001307189542484, "height": 0.012579545454545482, "page": 9}], "section": "Touch-only: SwipeZone", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "602"}, {"text": "This is because it was not only slow, but also difcult to perform the touch gestures.", "label": "Novelty", "bboxes": [{"left": 0.08761437908496732, "top": 0.41890151515151514, "width": 0.40038562091503266, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.43273863636363635, "width": 0.1625392156862745, "height": 0.012579545454545482, "page": 9}], "section": "Touch-only: SwipeZone", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "603"}, {"text": "They also experienced more wrist and/or nger fatigue than while using GAT.", "label": "Novelty", "bboxes": [{"left": 0.2580147058823529, "top": 0.43273863636363635, "width": 0.22727777777777786, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.44657575757575757, "width": 0.2830228758169935, "height": 0.012579545454545482, "page": 9}], "section": "Touch-only: SwipeZone", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "604"}, {"text": "Consequently, the text entry efciencies were not different from each other.", "label": "Novelty", "bboxes": [{"left": 0.7298415032679739, "top": 0.8090479797979797, "width": 0.19256045751633977, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.3149656862745098, "height": 0.012579545454545427, "page": 9}], "section": "CONCLUSION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "605"}, {"text": "However, when compared to eye-only typing, the typing speed may be questionable as the previous studies were conducted in a desk-based environment.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.2434810606060606, "width": 0.39717320261437905, "height": 0.01257954545454551, "page": 9}, {"left": 0.5246633986928104, "top": 0.25731818181818183, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.27115530303030305, "width": 0.19403758169934648, "height": 0.012579545454545427, "page": 9}], "section": "Performance Comparison with Literature", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "606"}, {"text": "However, in a previous study [22], A-Dwell achieved 20 wpm over 10 days of training.", "label": "Novelty", "bboxes": [{"left": 0.5733186274509804, "top": 0.31266666666666665, "width": 0.3485228758169935, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.3265025252525252, "width": 0.2243725490196079, "height": 0.012579545454545482, "page": 9}], "section": "Performance Comparison with Literature", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "607"}, {"text": "However, nine out of twelve chose eye-only as the worst.", "label": "Novelty", "bboxes": [{"left": 0.25864215686274505, "top": 0.5112058080808081, "width": 0.2266568627450981, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.5250429292929293, "width": 0.13828267973856212, "height": 0.012579545454545427, "page": 9}], "section": "Touch-only: SwipeZone", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "608"}, {"text": "Even though the learning curve of GAT was steeper than A-Dwell, a longitudinal study was required to compare both techniques after long-term training.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.3893989898989899, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.5240784313725491, "top": 0.40323611111111113, "width": 0.3977630718954248, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.41707196969696975, "width": 0.22383986928104582, "height": 0.012579545454545427, "page": 9}], "section": "Performance Comparison with Literature", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "609"}, {"text": "If we consider that the eye-tracking accuracy may affect the performance of GAT variations, the results of Exp 1 may differ in the wearable context, because a wearable eye tracker may be less accurate than a stationary one.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.3974591503267974, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.22814215686274514, "height": 0.012579545454545538, "page": 9}], "section": "Validity of the Simulated Environment", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "610"}, {"text": "However, we determined that the performances of GAT9 in the stationary setup (Exp 1, ve tasks/block) and in the wearable setup (Exp 2, four tasks/block) were consistent in terms of both speed (9.3 wpm vs. 9.0 wpm, respectively, at the 3rd block) and accuracy (CER: 6.3% vs. 7.4% and UER: 0.3% vs. 0.9%).", "label": "Novelty", "bboxes": [{"left": 0.3213251633986928, "top": 0.7461527777777778, "width": 0.16397222222222219, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.759989898989899, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 9}, {"left": 0.08689052287581699, "top": 0.7738270202020202, "width": 0.3986683006535947, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7876641414141414, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 9}, {"left": 0.0877124183006536, "top": 0.8015012626262626, "width": 0.3981535947712419, "height": 0.012579545454545427, "page": 9}, {"left": 0.08758006535947711, "top": 0.8153383838383839, "width": 0.32517647058823534, "height": 0.012579545454545427, "page": 9}], "section": "Validity of the Simulated Environment", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "611"}, {"text": "Although Exp 2 was conducted with a VR HWD rather than with an augmented reality (AR) glasses, it was effective", "label": "Novelty", "bboxes": [{"left": 0.08753267973856209, "top": 0.8782335858585859, "width": 0.39776797385620916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08753267973856209, "top": 0.892070707070707, "width": 0.3977549019607843, "height": 0.012579545454545427, "page": 9}], "section": "Validity of the Simulated Environment", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "612"}, {"text": "To achieve this goal, we focused on the gaze input modality.", "label": "Objective", "bboxes": [{"left": 0.7866617647058823, "top": 0.3606439393939394, "width": 0.1372009803921569, "height": 0.012579545454545482, "page": 1}, {"left": 0.5240816993464052, "top": 0.3744810606060606, "width": 0.27235947712418307, "height": 0.012579545454545482, "page": 1}], "section": "Gaze-based Interaction for Head-worn Display (HWD)", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "613"}, {"text": "At the end of the last session, we surveyed a questionnaire with four questions  1) Easy to learn ; 2) Easy to use ; 3) Prefer to use ; and", "label": "Objective", "bboxes": [{"left": 0.36450000000000005, "top": 0.5073598484848485, "width": 0.12079738562091497, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5211969696969697, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5347563131313131, "width": 0.3799019607843137, "height": 0.012856060606060593, "page": 5}], "section": "Design and Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "614"}, {"text": "Friedman tests showed signicant effects of the layouts on all of the four questions: Easy to learn , Easy to use , Prefer to use , and Eye feels natural : p < 0.05, 0.01, 0.01, and 0.01; and,  ( 22 ) = 8.561, 15.524, 9.660, and 13.661, respectively.", "label": "Objective", "bboxes": [{"left": 0.5246633986928104, "top": 0.8488358585858586, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8625858585858586, "width": 0.3975343137254903, "height": 0.012666666666666715, "page": 5}, {"left": 0.5246699346405229, "top": 0.8762335858585858, "width": 0.39851470588235294, "height": 0.012856060606060704, "page": 5}, {"left": 0.5246633986928104, "top": 0.8883472222222223, "width": 0.38145098039215686, "height": 0.018300505050505067, "page": 5}], "section": "Questionnaire", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "615"}, {"text": "At the end of the last session, we surveyed a questionnaire with ve questions using a ve-point Likert scale; four of the questions were same to Experiment 1; and, the fth option was Eye fatigue .", "label": "Objective", "bboxes": [{"left": 0.2785049019607843, "top": 0.2625959595959596, "width": 0.2067875816993464, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.2764330808080808, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.29027020202020204, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08689052287581699, "top": 0.30401893939393937, "width": 0.2804084967320261, "height": 0.012667929292929314, "page": 8}], "section": "Design and Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "616"}, {"text": "Friedman tests did not show signicant effects on the other two questions.", "label": "Objective", "bboxes": [{"left": 0.5246633986928104, "top": 0.6257462121212122, "width": 0.39745588235294116, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6395833333333334, "width": 0.09668627450980405, "height": 0.012579545454545427, "page": 8}], "section": "Questionnaire", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "617"}, {"text": "For the \"Prefer to use\" question, we asked the reason for the highest/lowest rating.", "label": "Objective", "bboxes": [{"left": 0.08811928104575163, "top": 0.0959229797979798, "width": 0.3971748366013072, "height": 0.01257954545454544, "page": 9}, {"left": 0.08811928104575163, "top": 0.10976010101010102, "width": 0.14101143790849674, "height": 0.01257954545454544, "page": 9}], "section": "Questionnaire", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "618"}, {"text": "To overcome the problems of the touchpadand eye-gazebased methods, we propose the gaze-assisted typing (GAT).", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.6873914141414141, "width": 0.4003774509803921, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246584967320261, "top": 0.7012285353535355, "width": 0.40002941176470586, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "619"}, {"text": "In the subsequent section, we review the literature concerning possible text entry methods for smart glasses that can be used in a general mobile environment.", "label": "Method", "bboxes": [{"left": 0.16039869281045752, "top": 0.4037411616161616, "width": 0.3248921568627451, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4175782828282828, "width": 0.39717647058823524, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.431415404040404, "width": 0.29501960784313724, "height": 0.012579545454545482, "page": 1}], "section": "Possible Text Entry Methods for Smart Glasses", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "620"}, {"text": "In this study, we attempted to achieve a more usable on-glass text entry, primarily in terms of speed.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.34680808080808084, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.3606439393939394, "width": 0.25590522875817, "height": 0.012579545454545482, "page": 1}], "section": "Gaze-based Interaction for Head-worn Display (HWD)", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "621"}, {"text": "We rst conducted a preliminary study to validate the effect of introducing eye input modality to a two-step touch-only text entry with a minimal load on eye input.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.11667803030303031, "width": 0.39793790849673205, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.1305151515151515, "width": 0.39717647058823524, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.14435227272727272, "width": 0.24770588235294122, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "622"}, {"text": "For selection, we provided a cross-hair-shaped eye cursor on the keyboard.", "label": "Method", "bboxes": [{"left": 0.18645915032679738, "top": 0.4969128787878788, "width": 0.2988382352941176, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.51075, "width": 0.18236111111111114, "height": 0.012579545454545427, "page": 2}], "section": "Sub-keyboard Selection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "623"}, {"text": "Our proposed approach is a two-step method using a combination of gaze and touch gesture for text entry.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.3722070707070707, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3860441919191919, "width": 0.3324003267973856, "height": 0.012579545454545427, "page": 2}], "section": "Gaze-based Text entry methods", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "624"}, {"text": "In our design, the left and right directions on the screen were mapped in the backward and forward directions on the touchpad, respectively.", "label": "Method", "bboxes": [{"left": 0.47146078431372546, "top": 0.7046414141414141, "width": 0.013831699346405268, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39988562091503266, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.11667810457516341, "height": 0.012579545454545538, "page": 2}], "section": "Key Selection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "625"}, {"text": "To reduce the typing error by confusing the swipe direction, we provided a visual feedback (touch cursor in Figure 2(b)) for the touch position to permit adjustments in the swipe direction before the swipe gesture is nalized.", "label": "Method", "bboxes": [{"left": 0.6214330065359477, "top": 0.5508661616161616, "width": 0.3004035947712418, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5647032828282829, "width": 0.3974460784313726, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.5785404040404041, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5923775252525253, "width": 0.4000130718954249, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "626"}, {"text": "An intentional eye movement of the user may occur before starting the touch input; therefore, we xed the eye cursor once a user started the touch input.", "label": "Method", "bboxes": [{"left": 0.6186601307189542, "top": 0.46029671717171716, "width": 0.3037418300653595, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.4741338383838384, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.4879709595959596, "width": 0.29561437908496735, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "627"}, {"text": "To distinguish the touch gestures, we used a vector from the placement to release positions.", "label": "Method", "bboxes": [{"left": 0.7686209150326797, "top": 0.6967840909090909, "width": 0.15321568627450988, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.7106212121212121, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7244583333333333, "width": 0.04752124183006545, "height": 0.012579545454545538, "page": 2}], "section": "Key Selection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "628"}, {"text": "As far as text entry methods are considered, however, gazeassisted typing (GAT) is distinguished from the prior works as we consider a taxonomy for Gaze + Gesture combinations that was presented by Chatterjee et al.", "label": "Method", "bboxes": [{"left": 0.08752124183006536, "top": 0.12296843434343435, "width": 0.40046078431372545, "height": 0.012579545454545454, "page": 2}, {"left": 0.08810294117647058, "top": 0.13680555555555554, "width": 0.39717320261437905, "height": 0.012578282828282827, "page": 2}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.16418939393939394, "width": 0.2635751633986928, "height": 0.012868686868686863, "page": 2}], "section": "Gaze-based Text entry methods", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "629"}, {"text": "Moreover, in our pilot study, we also observed that users were inclined to not blink until a", "label": "Method", "bboxes": [{"left": 0.30829084967320264, "top": 0.8782335858585859, "width": 0.17904248366013065, "height": 0.012579545454545427, "page": 2}, {"left": 0.08753267973856209, "top": 0.892070707070707, "width": 0.3977549019607843, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "630"}, {"text": "We recruited 12 participants (4 females and 8 males, mean age: 22.58, from 19 to 30 years) from our university.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.8090479797979797, "width": 0.39793790849673216, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246617647058823, "top": 0.822885101010101, "width": 0.35187745098039225, "height": 0.012579545454545427, "page": 3}], "section": "Participants", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "631"}, {"text": "To implement a complete mobile gaze tracking system on smart glasses, we attempted to use an eye tracker from Pupil Labs and Tobii Pro Glasses", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.614534090909091, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6283712121212122, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6422083333333334, "width": 0.18021568627450985, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "632"}, {"text": "We used a within-subject design with one factor, i.e. , technique: M-SwipeBoard and GAT.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.5172588383838383, "width": 0.4002042483660132, "height": 0.012868686868686918, "page": 3}, {"left": 0.5246633986928104, "top": 0.5313851010101011, "width": 0.1715882352941177, "height": 0.012579545454545427, "page": 3}], "section": "Design and Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "633"}, {"text": "To verify GAT, initially, we demonstrate the net effect of introducing gaze modality with a minimum expression of the eye input.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.2108838383838384, "width": 0.4003774509803922, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811274509803921, "top": 0.2247209595959596, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811274509803921, "top": 0.2385580808080808, "width": 0.06527124183006536, "height": 0.012579545454545454, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "634"}, {"text": "We instructed the participants to memorize a target phrase for 15 s before starting the task, but, the target phrase was not erased while performing the task.", "label": "Method", "bboxes": [{"left": 0.8298447712418301, "top": 0.4059292929292929, "width": 0.0919869281045751, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246584967320261, "top": 0.41976641414141413, "width": 0.39717320261437894, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246584967320261, "top": 0.43360353535353535, "width": 0.39717320261437905, "height": 0.012578282828282827, "page": 3}, {"left": 0.5246633986928104, "top": 0.44744065656565657, "width": 0.1309558823529411, "height": 0.012579545454545482, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "635"}, {"text": "Thus, we decided to use 21 as the length of the square bound, tted in the  15 round.", "label": "Method", "bboxes": [{"left": 0.32385130718954247, "top": 0.8434734848484848, "width": 0.16144281045751635, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.8570340909090909, "width": 0.4000245098039216, "height": 0.012856060606060593, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "636"}, {"text": "To demonstrate the net effect, we modied the SwipeBoard design to be similar to our GAT design; we called this MSwipeBoard.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.3429633838383839, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811274509803921, "top": 0.3568005050505051, "width": 0.3998790849673203, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3706376262626263, "width": 0.0851437908496732, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "637"}, {"text": "We thought that GAT with a lesser number of touch gestures may require less effort of touch input by eliminating difcult gestures, i.e. , diagonal swipe gestures [11] and vertical swipe gestures.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.11353282828282829, "width": 0.39793464052287597, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.12736994949494948, "width": 0.39717483660130715, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.1409179292929293, "width": 0.39717810457516345, "height": 0.012868686868686863, "page": 4}, {"left": 0.5246633986928104, "top": 0.15504419191919191, "width": 0.05856372549019606, "height": 0.012579545454545454, "page": 4}], "section": "EXPERIMENT 1: CAN MORE EXPRESSIVE GAZE INPUT MAKE \"GAT\" BETTER?", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "638"}, {"text": "We designed three variations of GAT: GAT3, GAT6, and GAT9.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.27064015151515153, "width": 0.40079411764705897, "height": 0.012579545454545427, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "639"}, {"text": "We analyzed the text entry speed, corrected error rate (CER), and uncorrected error rate (UER) statistically using a two-way (2 techniques  3 blocks) repeated measure ANOVA (RMANOVA) on each.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.39248737373737375, "width": 0.39996405228758164, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.40632449494949496, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 4}, {"left": 0.08758169934640524, "top": 0.41988510101010096, "width": 0.4004101307189542, "height": 0.012856060606060649, "page": 4}, {"left": 0.08753267973856209, "top": 0.43399873737373734, "width": 0.12134803921568628, "height": 0.012579545454545482, "page": 4}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "640"}, {"text": "Because we varied the number of touch gestures, the keyboard layouts were correspondingly varied.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.40272095959595955, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.41655808080808077, "width": 0.25012581699346403, "height": 0.012579545454545482, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "641"}, {"text": "We recruited 18 participants (8 females, 10 males, mean age: 20.72, from 17 to 24 years) from our university.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.6326792929292929, "width": 0.400202614379085, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6465151515151515, "width": 0.31986437908496734, "height": 0.012579545454545538, "page": 5}], "section": "Design and Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "642"}, {"text": "We analyzed the results statistically for both NC and VGC, separately.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.5751502525252525, "width": 0.3999803921568629, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.5889873737373738, "width": 0.06900490196078435, "height": 0.012579545454545427, "page": 5}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "643"}, {"text": "We used the within-subject design, with the layout as a factor, i.e. , GAT3, GAT6, and GAT9.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.3274785353535353, "width": 0.3999673202614379, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.34102651515151516, "width": 0.2010767973856209, "height": 0.012868686868686863, "page": 5}], "section": "Design and Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "644"}, {"text": "We were also uncertain of how fast a user will get used to the layout.", "label": "Method", "bboxes": [{"left": 0.392359477124183, "top": 0.8367222222222221, "width": 0.09293300653594777, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3626160130718955, "height": 0.012579545454545427, "page": 5}], "section": "Apparatus and Task", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "645"}, {"text": "we conducted one additional block with VGC to simulate users who are familiar with each keyboard layout.", "label": "Method", "bboxes": [{"left": 0.5240784313725491, "top": 0.4096893939393939, "width": 0.39775490196078434, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.4235265151515151, "width": 0.32071078431372557, "height": 0.012579545454545482, "page": 5}], "section": "Apparatus and Task", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "646"}, {"text": "Owing to the differences in remote and wearable eye tracking environments, and the slippage and HWD wobbling problems of wearable eye trackers, we conducted Experiment 2 utilizing an eye-trackable HWD, FOVE VR 4 .", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.6991464646464647, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7129835858585859, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7268207070707071, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.738655303030303, "width": 0.23382679738562107, "height": 0.014580808080808083, "page": 6}], "section": "Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "647"}, {"text": "We analyzed the key input time to better understand the typing skills of users utilizing GAT, with only characters that were not erased and typed correctly (27,383 key entries).", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.3511691919191919, "width": 0.3979379084967321, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3650063131313131, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3788434343434343, "width": 0.3345767973856209, "height": 0.012579545454545482, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "648"}, {"text": "Thus, we argue that further reduction of T1 after long-term utilization and/or incorporation of an optimized key placement that requires less SWITCH , can make GAT better.", "label": "Method", "bboxes": [{"left": 0.8252630718954248, "top": 0.5346515151515152, "width": 0.09658006535947716, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5484015151515151, "width": 0.39745915032679746, "height": 0.012666666666666715, "page": 6}, {"left": 0.5246633986928104, "top": 0.5623257575757576, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5760757575757576, "width": 0.21686601307189535, "height": 0.012666666666666715, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "649"}, {"text": "To understand the observation on T1 , we calculated the average number of sub-keyboard switches per letter inputted ( SWITCH , Figure 11(e)).", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.6429166666666667, "width": 0.4003790849673202, "height": 0.012666666666666604, "page": 6}, {"left": 0.08811437908496732, "top": 0.6568409090909091, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 6}, {"left": 0.08758169934640524, "top": 0.670590909090909, "width": 0.17174836601307192, "height": 0.012666666666666715, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "650"}, {"text": "We decomposed the key input time into four parts considering eyeand touch input phase; Eye movement data was logged every 30 ms. T1 is the time from the moment the previous key was entered to the moment the eye cursor entered a target sub-keyboard containing a target key.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.40022727272727276, "width": 0.39794281045751634, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.414064393939394, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.42781439393939397, "width": 0.39717810457516345, "height": 0.01266666666666666, "page": 6}, {"left": 0.08811928104575163, "top": 0.4417386363636364, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.4555757575757575, "width": 0.24215032679738563, "height": 0.012579545454545482, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "651"}, {"text": "To validate that GAT has better usability than the monomodal text entry methods, we compared GAT to eye-only typing (adjustable dwell time [22]) and touch-only typing (SwipeZone [11]) utilizing eye-trackable HWD.", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.61689898989899, "width": 0.3976781045751634, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6307361111111112, "width": 0.3998725490196079, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6445732323232324, "width": 0.39716830065359465, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6584103535353536, "width": 0.22940032679738565, "height": 0.012579545454545427, "page": 6}], "section": "EXPERIMENT 2: \"GAT\" EVALUATION ON \"HWD\"", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "652"}, {"text": "We recruited 12 participants (3 females, 9 males, mean age: 22.33, from 18 to 28 years) from our university.", "label": "Method", "bboxes": [{"left": 0.8408480392156864, "top": 0.7385681818181818, "width": 0.08099509803921556, "height": 0.012579545454545427, "page": 7}, {"left": 0.5234428104575163, "top": 0.7524040404040404, "width": 0.3983970588235296, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7662411616161617, "width": 0.21573856209150322, "height": 0.012579545454545427, "page": 7}], "section": "Task and Participants", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "653"}, {"text": "We used plain QWERTY layout for the eye only typing as shown in gure 13 (d).", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.5712272727272727, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5850643939393939, "width": 0.14745588235294116, "height": 0.012579545454545538, "page": 7}], "section": "Eye-only typing: Adjustable dwell time selection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "654"}, {"text": "We used the within-subject design with the technique as a factor; touch-only (SwipeZone), eye-only (A-Dwell), and GAT.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.8782335858585859, "width": 0.4006339869281046, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.39323039215686273, "height": 0.012579545454545427, "page": 7}], "section": "Design and Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "655"}, {"text": "Thus, we used the multinger swipe gestures for the keys, which are same as GAT.", "label": "Method", "bboxes": [{"left": 0.7968692810457517, "top": 0.3600833333333333, "width": 0.12496405228758167, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246601307189542, "top": 0.37392045454545453, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.38775757575757575, "width": 0.034477124183006524, "height": 0.012579545454545482, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "656"}, {"text": "We constructed a VR environment that was similar to the smart glass-based environment.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.5704444444444444, "width": 0.3979379084967321, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.5842815656565656, "width": 0.16820098039215686, "height": 0.012579545454545427, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "657"}, {"text": "To help a user distinguish between the regions, we attached two strips of tape on the backward and forward regions of the touchpad to provide a tactile feedback, similar to the original Swipezone.", "label": "Method", "bboxes": [{"left": 0.8573627450980392, "top": 0.2695138888888889, "width": 0.06447385620915025, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.2833510101010101, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.2971881313131313, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.31102525252525254, "width": 0.39580228758169944, "height": 0.012579545454545482, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "658"}, {"text": "We increased the keyboard size to more than that of Experiment 1 considering the mean accuracy of FOVE.", "label": "Method", "bboxes": [{"left": 0.4010588235294118, "top": 0.7813737373737373, "width": 0.08423366013071892, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.39987091503267974, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.2191764705882353, "height": 0.012579545454545538, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "659"}, {"text": "We chose the Adjustable dwell time selection method [22] as the eye only typing baseline, i.e. , A-Dwell.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.4668219696969697, "width": 0.39793790849673216, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246617647058823, "top": 0.4803699494949495, "width": 0.31614542483660124, "height": 0.012868686868686863, "page": 7}], "section": "Eye-only typing: Adjustable dwell time selection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "660"}, {"text": "We chose the fastest letter-wise text entry method, i.e. , SwipeZone [11].", "label": "Method", "bboxes": [{"left": 0.646217320261438, "top": 0.10976010101010102, "width": 0.27617973856209144, "height": 0.01257954545454544, "page": 7}, {"left": 0.5246633986928104, "top": 0.1233080808080808, "width": 0.1941732026143791, "height": 0.012868686868686877, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "661"}, {"text": "Regardless, we chose GAT9 to compare the other variations because GAT6 cannot use a QWERTY-like layout, which may be more familiar to users, and GAT3 requires complicated touch gestures.", "label": "Method", "bboxes": [{"left": 0.3864003267973856, "top": 0.7184785353535353, "width": 0.09889705882352945, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.3998839869281046, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.759989898989899, "width": 0.3888022875816993, "height": 0.012579545454545427, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "662"}, {"text": "To conduct statistical tests, we simulated the missing data.", "label": "Method", "bboxes": [{"left": 0.18784477124183005, "top": 0.554239898989899, "width": 0.2974477124183007, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5680770202020202, "width": 0.08772058823529415, "height": 0.012579545454545427, "page": 8}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "663"}, {"text": "At the end of the last session, we surveyed a questionnaire with ve questions using a ve-point Likert scale; four of the questions were same to Experiment 1; and, the fth option was Eye fatigue .", "label": "Method", "bboxes": [{"left": 0.2785049019607843, "top": 0.2625959595959596, "width": 0.2067875816993464, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.2764330808080808, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.29027020202020204, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08689052287581699, "top": 0.30401893939393937, "width": 0.2804084967320261, "height": 0.012667929292929314, "page": 8}], "section": "Design and Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "664"}, {"text": "The analysis methods were nearly the same as those for Experiment 1; however, we conducted three post-hoc tests only in the last block, and we used a one-tailed assumption for the tests because we assumed that GAT outperforms the others.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.47750757575757574, "width": 0.4003774509803921, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.49134343434343436, "width": 0.39773692810457517, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.5051805555555555, "width": 0.397171568627451, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5190176767676767, "width": 0.38745098039215686, "height": 0.012579545454545427, "page": 8}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "665"}, {"text": "Finally, we gathered 12 participants  2 days  3 sessions (techniques)  3 blocks  4 tasks = 864 phrases.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.3805631313131313, "width": 0.39717156862745095, "height": 0.012856060606060649, "page": 8}, {"left": 0.08758660130718955, "top": 0.39439898989898986, "width": 0.3307173202614379, "height": 0.012857323232323303, "page": 8}], "section": "Design and Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "666"}, {"text": "At the start of each session, we calibrated the eye tracker with the default calibration method of FOVE.", "label": "Method", "bboxes": [{"left": 0.08753267973856209, "top": 0.14435227272727272, "width": 0.3977549019607843, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.15818939393939394, "width": 0.26888888888888896, "height": 0.012579545454545454, "page": 8}], "section": "Design and Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "667"}, {"text": "Therefore, we conducted post-hoc Wilcoxon signed-rank tests with Bonferroni correction.", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.49995454545454543, "width": 0.3976813725490196, "height": 0.012579545454545538, "page": 8}, {"left": 0.5240784313725491, "top": 0.5137916666666666, "width": 0.17857843137254903, "height": 0.012579545454545427, "page": 8}], "section": "Questionnaire", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "668"}, {"text": "In our study, we considered rst the input modalities for the conventional gesture input and nalized on a touchpad.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.5162007575757576, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5300378787878788, "width": 0.3582450980392158, "height": 0.012579545454545427, "page": 9}], "section": "Scalability of GAT", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "669"}, {"text": "We presented the concept of GAT and the rst adaptation of a complementary combination of the gaze and touch input modalities for wearable text entry.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.6706780303030303, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246617647058823, "top": 0.6845151515151515, "width": 0.39717483660130715, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.6983522727272727, "width": 0.21854248366013063, "height": 0.012579545454545427, "page": 9}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "670"}, {"text": "For the \"Prefer to use\" question, we asked the reason for the highest/lowest rating.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.0959229797979798, "width": 0.3971748366013072, "height": 0.01257954545454544, "page": 9}, {"left": 0.08811928104575163, "top": 0.10976010101010102, "width": 0.14101143790849674, "height": 0.01257954545454544, "page": 9}], "section": "Questionnaire", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "671"}, {"text": "Our future work will handle such environmental validity issues using an eyetracking AR glasses which can meet the requirements of GAT.", "label": "Method", "bboxes": [{"left": 0.8111960784313725, "top": 0.10912626262626263, "width": 0.11103267973856212, "height": 0.012579545454545468, "page": 9}, {"left": 0.5240751633986928, "top": 0.12296338383838384, "width": 0.40046078431372556, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.13680429292929294, "width": 0.40003267973856205, "height": 0.012579545454545454, "page": 9}], "section": "Validity of the Simulated Environment", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "672"}, {"text": "If we consider that the eye-tracking accuracy may affect the performance of GAT variations, the results of Exp 1 may differ in the wearable context, because a wearable eye tracker may be less accurate than a stationary one.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.3974591503267974, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.22814215686274514, "height": 0.012579545454545538, "page": 9}], "section": "Validity of the Simulated Environment", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "673"}, {"text": "We conducted the experiments in simulated environments.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.6417462121212121, "width": 0.4007941176470589, "height": 0.012579545454545427, "page": 9}], "section": "Validity of the Simulated Environment", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "674"}, {"text": "We demonstrated that GAT (11.04 wpm, after 40 min of usage) is faster than SwipeZone (8.53 wpm, after 50 min of usage) in Experiment 2.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.1805858585858586, "width": 0.39903921568627454, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246650326797386, "top": 0.19442297979797982, "width": 0.39717156862745084, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.208260101010101, "width": 0.09251307189542479, "height": 0.012579545454545454, "page": 9}], "section": "Performance Comparison with Literature", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "675"}, {"text": "This research was supported by Basic Science ResearchProgram through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF2015R1D1A1A01058992).", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.0959229797979798, "width": 0.4003774509803922, "height": 0.01257954545454544, "page": 10}, {"left": 0.08811274509803921, "top": 0.10976010101010102, "width": 0.3998790849673203, "height": 0.01257954545454544, "page": 10}, {"left": 0.08811274509803921, "top": 0.12359722222222222, "width": 0.3998790849673203, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.13743434343434344, "width": 0.17769934640522878, "height": 0.012579545454545454, "page": 10}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "676"}, {"text": "The others said that they had to observe the keyboard in the same manner for both techniques; therefore, they could not feel any difference in the eye fatigue.", "label": "Result", "bboxes": [{"left": 0.2833218954248366, "top": 0.8643964646464646, "width": 0.20197549019607847, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3985245098039216, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.39232352941176474, "height": 0.012579545454545427, "page": 4}], "section": "Eye fatigue", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "677"}, {"text": "As the results show, T2 and T3 were not dependent on the layout.", "label": "Result", "bboxes": [{"left": 0.39206699346405227, "top": 0.5247613636363636, "width": 0.09323366013071899, "height": 0.012579545454545538, "page": 6}, {"left": 0.08812745098039215, "top": 0.538510101010101, "width": 0.34539215686274516, "height": 0.012667929292929259, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "678"}, {"text": "Friedman tests did not show signicant effects on the other two questions.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.6257462121212122, "width": 0.39745588235294116, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6395833333333334, "width": 0.09668627450980405, "height": 0.012579545454545427, "page": 8}], "section": "Questionnaire", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "679"}, {"text": "Further, it may be accurate initially, but becomes inaccurate over time owing to the slippage of the HWD.", "label": "Conclusion", "bboxes": [{"left": 0.685890522875817, "top": 0.5276376262626262, "width": 0.23797222222222225, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.5414747474747474, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.555310606060606, "width": 0.06615032679738564, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "680"}, {"text": "In addition, the Midas touch [15] problem in smart glasses should be considered more seriously because a user always has to see the display while s/he sees the \"real world\" over the display.", "label": "Conclusion", "bboxes": [{"left": 0.9080049019607842, "top": 0.6244962121212121, "width": 0.013831699346405268, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.6383333333333333, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.6521704545454545, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.6660075757575757, "width": 0.39090196078431383, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "681"}, {"text": "is difcult and inefcient because of the narrow input space.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.2282550505050505, "width": 0.400013071895425, "height": 0.012579545454545482, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "682"}, {"text": "Additionally, the on-frame touchpad is perpendicular to the display of the glasses; therefore, the mapping between the touchpad and the display is often confusing.", "label": "Conclusion", "bboxes": [{"left": 0.5240816993464052, "top": 0.2420921717171717, "width": 0.3977549019607842, "height": 0.012579545454545454, "page": 0}, {"left": 0.5246633986928104, "top": 0.25592929292929295, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.2697651515151515, "width": 0.2871078431372549, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "683"}, {"text": "The rst approach utilized a two-step input to enter a character because of the small input space.", "label": "Conclusion", "bboxes": [{"left": 0.167109477124183, "top": 0.4973333333333333, "width": 0.3208823529411765, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.511169191919192, "width": 0.3103349673202615, "height": 0.012579545454545538, "page": 1}], "section": "On glasses frame input", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "684"}, {"text": "However, a hand-held device may constitute an additional input device rather than being a primary input device for the glasses users due to carriage load.", "label": "Conclusion", "bboxes": [{"left": 0.2872042483660131, "top": 0.7154570707070708, "width": 0.19808823529411768, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7292941919191919, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7431313131313131, "width": 0.3999542483660131, "height": 0.012579545454545538, "page": 1}], "section": "On glasses frame input", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "685"}, {"text": "[44], in-air typing performance may deteriorate, subject to the quality of the hand tracker.", "label": "Conclusion", "bboxes": [{"left": 0.6819395424836601, "top": 0.2870972222222222, "width": 0.2404787581699347, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.30093434343434344, "width": 0.34343300653594766, "height": 0.012579545454545427, "page": 1}], "section": "Mid-air input", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "686"}, {"text": "To enter text, voice input is a natural and fast technique [3]; however, its use is limited because", "label": "Conclusion", "bboxes": [{"left": 0.08761437908496732, "top": 0.33455681818181815, "width": 0.39903267973856205, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.34839393939393937, "width": 0.22026633986928107, "height": 0.012579545454545482, "page": 1}], "section": "Possible Text Entry Methods for Smart Glasses", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "687"}, {"text": "3) voice input can raise privacy issues, which may be dependent on the applications.", "label": "Conclusion", "bboxes": [{"left": 0.39194444444444443, "top": 0.3760681818181818, "width": 0.09334803921568635, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.389905303030303, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.4037411616161616, "width": 0.06724346405228758, "height": 0.012579545454545482, "page": 1}], "section": "Possible Text Entry Methods for Smart Glasses", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "688"}, {"text": "In the subsequent section, we review the literature concerning possible text entry methods for smart glasses that can be used in a general mobile environment.", "label": "Conclusion", "bboxes": [{"left": 0.16039869281045752, "top": 0.4037411616161616, "width": 0.3248921568627451, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4175782828282828, "width": 0.39717647058823524, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.431415404040404, "width": 0.29501960784313724, "height": 0.012579545454545482, "page": 1}], "section": "Possible Text Entry Methods for Smart Glasses", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "689"}, {"text": "The on-body touch input is a possible option for a larger and more comfortable touch input space than on-glasses input, without carrying a hand-held device.", "label": "Conclusion", "bboxes": [{"left": 0.36406372549019606, "top": 0.8505593434343435, "width": 0.12122875816993472, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.11709803921568628, "height": 0.012579545454545427, "page": 1}], "section": "On Body input", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "690"}, {"text": "It may be more physically accessible [24]", "label": "Conclusion", "bboxes": [{"left": 0.21024509803921568, "top": 0.892070707070707, "width": 0.27504738562091513, "height": 0.012579545454545427, "page": 1}], "section": "On Body input", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "691"}, {"text": "Finally, to demonstrate that the GAT method with larger keys for eye selection and simpler touchpad gestures would lead to improved performance and usability benets over eye-only and touch-only typing methods, we conducted Experiment 2.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.3971764705882353, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.24121085858585856, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.3977385620915033, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.268885101010101, "width": 0.3921372549019607, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "692"}, {"text": "The eye cursor can provide a sense of control for the eye input; however, it may distract a user or occlude a key.", "label": "Conclusion", "bboxes": [{"left": 0.23546568627450978, "top": 0.5522613636363637, "width": 0.24982679738562094, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5660984848484849, "width": 0.3974558823529412, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5799356060606061, "width": 0.09037581699346407, "height": 0.012579545454545427, "page": 2}], "section": "Sub-keyboard Selection", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "693"}, {"text": "In addition, looking at a key before entering the key is a relatively natural behavior; therefore, a user is not required to spent time on accommodating the gaze input.", "label": "Conclusion", "bboxes": [{"left": 0.6796470588235295, "top": 0.822885101010101, "width": 0.24218954248365998, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.3991993464052288, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.0710261437908497, "height": 0.012579545454545538, "page": 2}], "section": "Expected advantage of GAT", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "694"}, {"text": "Thus, to enter \"c\" in Figure 1(c), a user performed the lower-forward direction swipe gesture.", "label": "Conclusion", "bboxes": [{"left": 0.21416176470588236, "top": 0.7461527777777778, "width": 0.27141339869281056, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.759989898989899, "width": 0.3619395424836601, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "695"}, {"text": "Further, it is easy to confuse the forward and backward directions because the display and the touchpad are placed perpendicularly.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.5231919191919192, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5370290404040404, "width": 0.399874183006536, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.5508661616161616, "width": 0.08609313725490197, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "696"}, {"text": "An intentional eye movement of the user may occur before starting the touch input; therefore, we xed the eye cursor once a user started the touch input.", "label": "Conclusion", "bboxes": [{"left": 0.6186601307189542, "top": 0.46029671717171716, "width": 0.3037418300653595, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.4741338383838384, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.4879709595959596, "width": 0.29561437908496735, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "697"}, {"text": "Thus, when a nger touches the touchpad, the touch cursor is rst placed at the center of the selected sub-keyboard, regardless of the position of placement.", "label": "Conclusion", "bboxes": [{"left": 0.5241601307189543, "top": 0.6691098484848484, "width": 0.3979542483660129, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246584967320261, "top": 0.6829469696969697, "width": 0.3998790849673203, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6967840909090909, "width": 0.2389477124183006, "height": 0.012579545454545538, "page": 2}], "section": "Key Selection", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "698"}, {"text": "The target acquisition (approaching a target) may be above accuracy (AA) or below accuracy (BA), and the target action (selecting a target) may be a discrete action (DA) or a continuous manipulation (CM).", "label": "Conclusion", "bboxes": [{"left": 0.08761437908496732, "top": 0.205989898989899, "width": 0.397671568627451, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.21982702020202022, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 2}, {"left": 0.08758169934640524, "top": 0.23366414141414144, "width": 0.40041830065359474, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.2475012626262626, "width": 0.1598496732026144, "height": 0.012579545454545482, "page": 2}], "section": "Gaze-based Text entry methods", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "699"}, {"text": "Conversely, GAT belongs to the BA + DA category because it chose BA to overcome the inaccuracy of a mobile eye tracker, and DA to overcome the limited gesture space of the oblong touchpad.", "label": "Conclusion", "bboxes": [{"left": 0.31791503267973853, "top": 0.28901136363636365, "width": 0.1673790849673203, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.30284848484848487, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3166856060606061, "width": 0.3971683006535947, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3305227272727273, "width": 0.29614379084967324, "height": 0.012579545454545482, "page": 2}], "section": "Gaze-based Text entry methods", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "700"}, {"text": "Thus, we used the GP3 eye tracker 3 with a laptop as shown in gure", "label": "Conclusion", "bboxes": [{"left": 0.3350457516339869, "top": 0.7252297979797979, "width": 0.15024509803921565, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.7370656565656565, "width": 0.2918366013071895, "height": 0.014580808080808194, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "701"}, {"text": "To verify GAT, initially, we demonstrate the net effect of introducing gaze modality with a minimum expression of the eye input.", "label": "Conclusion", "bboxes": [{"left": 0.08761437908496732, "top": 0.2108838383838384, "width": 0.4003774509803922, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811274509803921, "top": 0.2247209595959596, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811274509803921, "top": 0.2385580808080808, "width": 0.06527124183006536, "height": 0.012579545454545454, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "702"}, {"text": "Thus, we choose SwipeBoard as a baseline technique in this study, to demonstrate the net effect of replacing an input step with gaze input.", "label": "Conclusion", "bboxes": [{"left": 0.37100653594771243, "top": 0.29390530303030304, "width": 0.11428594771241835, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.30774242424242426, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3215795454545455, "width": 0.4000261437908497, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "703"}, {"text": "The participants were instructed to enter the text as fast and accurately as possible.", "label": "Conclusion", "bboxes": [{"left": 0.660656862745098, "top": 0.44744065656565657, "width": 0.26145751633986913, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4612777777777778, "width": 0.2741601307189543, "height": 0.012579545454545482, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "704"}, {"text": "Thus, we decided to use 21 as the length of the square bound, tted in the  15 round.", "label": "Conclusion", "bboxes": [{"left": 0.32385130718954247, "top": 0.8434734848484848, "width": 0.16144281045751635, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.8570340909090909, "width": 0.4000245098039216, "height": 0.012856060606060593, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "705"}, {"text": "To demonstrate the net effect, we modied the SwipeBoard design to be similar to our GAT design; we called this MSwipeBoard.", "label": "Conclusion", "bboxes": [{"left": 0.08761437908496732, "top": 0.3429633838383839, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811274509803921, "top": 0.3568005050505051, "width": 0.3998790849673203, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3706376262626263, "width": 0.0851437908496732, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "706"}, {"text": "Five of twelve experienced more eye fatigue while using GAT because they had to pay more attention to the eye movement; however, another one said that M-SwipeBoard was more tiring owing to the long usage time.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.39768137254901964, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3985163398692811, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.19015359477124183, "height": 0.012579545454545538, "page": 4}], "section": "Eye fatigue", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "707"}, {"text": "The others said that they had to observe the keyboard in the same manner for both techniques; therefore, they could not feel any difference in the eye fatigue.", "label": "Conclusion", "bboxes": [{"left": 0.2833218954248366, "top": 0.8643964646464646, "width": 0.20197549019607847, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3985245098039216, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.39232352941176474, "height": 0.012579545454545427, "page": 4}], "section": "Eye fatigue", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "708"}, {"text": "We thought that GAT with a lesser number of touch gestures may require less effort of touch input by eliminating difcult gestures, i.e. , diagonal swipe gestures [11] and vertical swipe gestures.", "label": "Conclusion", "bboxes": [{"left": 0.5238986928104574, "top": 0.11353282828282829, "width": 0.39793464052287597, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.12736994949494948, "width": 0.39717483660130715, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.1409179292929293, "width": 0.39717810457516345, "height": 0.012868686868686863, "page": 4}, {"left": 0.5246633986928104, "top": 0.15504419191919191, "width": 0.05856372549019606, "height": 0.012579545454545454, "page": 4}], "section": "EXPERIMENT 1: CAN MORE EXPRESSIVE GAZE INPUT MAKE \"GAT\" BETTER?", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "709"}, {"text": "Thus, explicit eye movement to select a sub-keyboard will occur more frequently.", "label": "Conclusion", "bboxes": [{"left": 0.8867385620915033, "top": 0.16888131313131313, "width": 0.03713888888888894, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.18271843434343435, "width": 0.39745588235294116, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.19655555555555554, "width": 0.10960457516339883, "height": 0.012579545454545454, "page": 4}], "section": "EXPERIMENT 1: CAN MORE EXPRESSIVE GAZE INPUT MAKE \"GAT\" BETTER?", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "710"}, {"text": "GAT6 was designed to eliminate the four diagonal swipe gestures; thus, each sub-keyboard has ve keys for the ve touch gestures, and the keyboard has six sub-keyboards (Figure 6", "label": "Conclusion", "bboxes": [{"left": 0.687452614379085, "top": 0.2983143939393939, "width": 0.23438562091503257, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.3121515151515151, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.32598863636363634, "width": 0.3975784313725491, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.33982575757575756, "width": 0.14369117647058827, "height": 0.012579545454545482, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "711"}, {"text": "GAT9 was designed to eliminate both the diagonal and vertical swipe gestures; therefore, each sub-keyboard has three keys for the three touch gestures, and the keyboard has nine sub-keyboards as shown in Figure 6", "label": "Conclusion", "bboxes": [{"left": 0.713406862745098, "top": 0.33982575757575756, "width": 0.20842973856209157, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.3536628787878788, "width": 0.39717156862745107, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.3675, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.3813371212121212, "width": 0.36569117647058835, "height": 0.012579545454545482, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "712"}, {"text": "The UER violated normality assumption; therefore, we performed an aligned rank transform (ART) [42] on the UER before conducting RM-ANOVA.", "label": "Conclusion", "bboxes": [{"left": 0.2139591503267974, "top": 0.43399873737373734, "width": 0.2726813725490196, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.44783585858585856, "width": 0.39826960784313725, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4616729797979798, "width": 0.33811274509803924, "height": 0.012579545454545482, "page": 4}], "section": "Results", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "713"}, {"text": "Because we varied the number of touch gestures, the keyboard layouts were correspondingly varied.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.40272095959595955, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.41655808080808077, "width": 0.25012581699346403, "height": 0.012579545454545482, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "714"}, {"text": "Thus, we placed the keys in an alphabetical order to minimize the memorability effect of the key placement.", "label": "Conclusion", "bboxes": [{"left": 0.7843709150326797, "top": 0.41655808080808077, "width": 0.13746568627450972, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.430395202020202, "width": 0.39773856209150327, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4442323232323232, "width": 0.17660294117647068, "height": 0.012579545454545482, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "715"}, {"text": "In addition, considering the usage environment of smart glasses, it may require minimized visual representation of the keyboard.", "label": "Conclusion", "bboxes": [{"left": 0.7062598039215686, "top": 0.4442323232323232, "width": 0.21557679738562086, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4580694444444444, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.47190656565656564, "width": 0.2012205882352941, "height": 0.012579545454545482, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "716"}, {"text": "Thus, we removed the boundary of sub-keyboards, and displayed the visualization to notify the sub-keyboard activation as shown in Figure 6.", "label": "Conclusion", "bboxes": [{"left": 0.730985294117647, "top": 0.47190656565656564, "width": 0.1935588235294119, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.48574242424242425, "width": 0.39774673202614375, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.4995795454545454, "width": 0.32300326797385637, "height": 0.012579545454545538, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "717"}, {"text": "We controlled the interval of sessions to approximately 24 h as much as possible (from 15 to 41 h; mean = 23.58,  = 4.12).", "label": "Conclusion", "bboxes": [{"left": 0.28951143790849676, "top": 0.47968560606060606, "width": 0.19578104575163396, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4935227272727273, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.08689052287581699, "top": 0.5073598484848485, "width": 0.26542320261437913, "height": 0.012843434343434268, "page": 5}], "section": "Design and Procedure", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "718"}, {"text": "Because the keyboard layouts were not typical, a novice user may spend a lot of time to discover a key.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3974460784313726, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.29313071895424836, "height": 0.012579545454545538, "page": 5}], "section": "Apparatus and Task", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "719"}, {"text": "This study was conducted to demonstrate the effect of trade-off between the number of touch gestures and the number of subkeyboards, rather than the keyboard layout familiarity.", "label": "Conclusion", "bboxes": [{"left": 0.45579411764705885, "top": 0.8505593434343435, "width": 0.029493464052287532, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39987091503267974, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.3571111111111111, "height": 0.012579545454545427, "page": 5}], "section": "Apparatus and Task", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "720"}, {"text": "Thus,", "label": "Conclusion", "bboxes": [{"left": 0.45027124183006534, "top": 0.892070707070707, "width": 0.03706699346405229, "height": 0.012579545454545427, "page": 5}], "section": "Apparatus and Task", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "721"}, {"text": "Seven out of eighteen participants reported eye fatigue for GAT9, because they needed to move their eyes more to select or to nd a key, than the other layouts.", "label": "Conclusion", "bboxes": [{"left": 0.2690457516339869, "top": 0.2837954545454545, "width": 0.21624673202614386, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.29763257575757573, "width": 0.3971781045751634, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.31146969696969695, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 6}], "section": "Comments from the participants", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "722"}, {"text": "In fact, the virtual reality environment and the smart glass-based environment may differ from each other.", "label": "Conclusion", "bboxes": [{"left": 0.7625751633986928, "top": 0.7406565656565657, "width": 0.15983006535947708, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7544936868686868, "width": 0.39745915032679746, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.7683308080808081, "width": 0.10950490196078433, "height": 0.012579545454545427, "page": 6}], "section": "Implementation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "723"}, {"text": "Thus, we argue that further reduction of T1 after long-term utilization and/or incorporation of an optimized key placement that requires less SWITCH , can make GAT better.", "label": "Conclusion", "bboxes": [{"left": 0.8252630718954248, "top": 0.5346515151515152, "width": 0.09658006535947716, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5484015151515151, "width": 0.39745915032679746, "height": 0.012666666666666715, "page": 6}, {"left": 0.5246633986928104, "top": 0.5623257575757576, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5760757575757576, "width": 0.21686601307189535, "height": 0.012666666666666715, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "724"}, {"text": "Thus, we concluded that T1 increased with more sub-keyboards because with the larger number of sub-keyboards, a user might have been exploring other sub-keyboards to nd a target key.", "label": "Conclusion", "bboxes": [{"left": 0.4225702614379085, "top": 0.712189393939394, "width": 0.0627140522875817, "height": 0.012579545454545538, "page": 6}, {"left": 0.08812745098039215, "top": 0.7259393939393939, "width": 0.3998627450980392, "height": 0.012666666666666715, "page": 6}, {"left": 0.08811928104575163, "top": 0.7398636363636364, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.7537007575757576, "width": 0.40002614379084966, "height": 0.012579545454545427, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "725"}, {"text": "For example, for a word, \"the\", while using GAT3 (Figure 6(a)), \"t\" and \"h\" are on different sub-keyboards; therefore, one sub-keyboard switch is required; however, \"h\" and \"e\" are on the same sub-keyboard; hence, no sub-keyboard switch is required.", "label": "Conclusion", "bboxes": [{"left": 0.3221846405228758, "top": 0.7813737373737373, "width": 0.1651307189542484, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.39987091503267974, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3985294117647059, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.27482189542483665, "height": 0.012579545454545538, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "726"}, {"text": "Thus, the required number of sub-keyboard switches is one and the word length is three; therefore, the optimal SWITCH for \"the\" using GAT3 is 1/3 = 0.3.", "label": "Conclusion", "bboxes": [{"left": 0.3679820261437909, "top": 0.8367222222222221, "width": 0.11731535947712413, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8643093434343434, "width": 0.39717647058823524, "height": 0.012666666666666715, "page": 6}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.07745424836601307, "height": 0.012579545454545427, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "727"}, {"text": "Considering swipe gestures are slower than tap gesture [11], T4 may decreased as the number of swipe gestures decreased, e.g., GAT3 use eight out of nine touch gestures are swipe gestures, otherwise, GAT9 use two out of three touch gestures are swipe gestures (Figure 6).", "label": "Conclusion", "bboxes": [{"left": 0.2784624183006536, "top": 0.5662714646464647, "width": 0.2068382352941177, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5800214646464646, "width": 0.39745261437908497, "height": 0.012666666666666715, "page": 6}, {"left": 0.08811928104575163, "top": 0.5936565656565657, "width": 0.3971699346405229, "height": 0.012868686868686807, "page": 6}, {"left": 0.08811928104575163, "top": 0.6077828282828283, "width": 0.39716830065359476, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.6216199494949495, "width": 0.3698039215686275, "height": 0.012579545454545427, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "728"}, {"text": "Thus the dimension of the keyboard is 60 x 29.", "label": "Conclusion", "bboxes": [{"left": 0.7460604575163399, "top": 0.5989015151515151, "width": 0.17577287581699352, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6127386363636363, "width": 0.14127614379084963, "height": 0.012579545454545538, "page": 7}], "section": "Eye-only typing: Adjustable dwell time selection", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "729"}, {"text": "Thus, we used the multinger swipe gestures for the keys, which are same as GAT.", "label": "Conclusion", "bboxes": [{"left": 0.7968692810457517, "top": 0.3600833333333333, "width": 0.12496405228758167, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246601307189542, "top": 0.37392045454545453, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.38775757575757575, "width": 0.034477124183006524, "height": 0.012579545454545482, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "730"}, {"text": "In addition, the backward and forward directions of the touchpad matched the display in reverse from the original, to prevent confusion regarding the gestures for deletion and adding a space; thus, the matched direction same as GAT.", "label": "Conclusion", "bboxes": [{"left": 0.5631895424836602, "top": 0.38775757575757575, "width": 0.3586503267973856, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.40159469696969696, "width": 0.3991993464052287, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4154318181818182, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4292689393939394, "width": 0.3744934640522877, "height": 0.012579545454545482, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "731"}, {"text": "Thus, the dimensions of the GAT keyboard are 30  21; however, the required minimum FoV is 23  16 considering blank space as shown in Figure 13", "label": "Conclusion", "bboxes": [{"left": 0.29894607843137255, "top": 0.822885101010101, "width": 0.18634967320261436, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.8364457070707071, "width": 0.39718137254901964, "height": 0.012856060606060593, "page": 7}, {"left": 0.08811928104575163, "top": 0.8502828282828283, "width": 0.3998709150326797, "height": 0.012856060606060593, "page": 7}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.020954248366013076, "height": 0.012579545454545538, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "732"}, {"text": "Because it is commonly considered outperformed than xed dwell time technique.", "label": "Conclusion", "bboxes": [{"left": 0.8513676470588236, "top": 0.4806590909090909, "width": 0.07046895424836586, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.4944949494949495, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.5083320707070706, "width": 0.06870915032679736, "height": 0.012579545454545427, "page": 7}], "section": "Eye-only typing: Adjustable dwell time selection", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "733"}, {"text": "Regardless, we chose GAT9 to compare the other variations because GAT6 cannot use a QWERTY-like layout, which may be more familiar to users, and GAT3 requires complicated touch gestures.", "label": "Conclusion", "bboxes": [{"left": 0.3864003267973856, "top": 0.7184785353535353, "width": 0.09889705882352945, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.3998839869281046, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.759989898989899, "width": 0.3888022875816993, "height": 0.012579545454545427, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "734"}, {"text": "The analysis methods were nearly the same as those for Experiment 1; however, we conducted three post-hoc tests only in the last block, and we used a one-tailed assumption for the tests because we assumed that GAT outperforms the others.", "label": "Conclusion", "bboxes": [{"left": 0.08761437908496732, "top": 0.47750757575757574, "width": 0.4003774509803921, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.49134343434343436, "width": 0.39773692810457517, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.5051805555555555, "width": 0.397171568627451, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5190176767676767, "width": 0.38745098039215686, "height": 0.012579545454545427, "page": 8}], "section": "Results", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "735"}, {"text": "However, four participants could not complete all tasks for the eye-only technique owing to a time constraint.", "label": "Conclusion", "bboxes": [{"left": 0.42403267973856207, "top": 0.3946755050505051, "width": 0.06329575163398699, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.40851262626262624, "width": 0.3977385620915033, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.42234974747474746, "width": 0.24625816993464056, "height": 0.012579545454545482, "page": 8}], "section": "Design and Procedure", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "736"}, {"text": "Therefore, we conducted post-hoc Wilcoxon signed-rank tests with Bonferroni correction.", "label": "Conclusion", "bboxes": [{"left": 0.5241601307189543, "top": 0.49995454545454543, "width": 0.3976813725490196, "height": 0.012579545454545538, "page": 8}, {"left": 0.5240784313725491, "top": 0.5137916666666666, "width": 0.17857843137254903, "height": 0.012579545454545427, "page": 8}], "section": "Questionnaire", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "737"}, {"text": "They mentioned that eye-only and GAT techniques required a focus on eye movements; further, errors often occurred because of unintentional eye movements.", "label": "Conclusion", "bboxes": [{"left": 0.15919117647058822, "top": 0.3358800505050505, "width": 0.32610130718954256, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.3497171717171717, "width": 0.3998807189542484, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.3635542929292929, "width": 0.3266062091503268, "height": 0.012579545454545482, "page": 9}], "section": "Touch-only: SwipeZone", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "738"}, {"text": "This is because it was not only slow, but also difcult to perform the touch gestures.", "label": "Conclusion", "bboxes": [{"left": 0.08761437908496732, "top": 0.41890151515151514, "width": 0.40038562091503266, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.43273863636363635, "width": 0.1625392156862745, "height": 0.012579545454545482, "page": 9}], "section": "Touch-only: SwipeZone", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "739"}, {"text": "However, when compared to eye-only typing, the typing speed may be questionable as the previous studies were conducted in a desk-based environment.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.2434810606060606, "width": 0.39717320261437905, "height": 0.01257954545454551, "page": 9}, {"left": 0.5246633986928104, "top": 0.25731818181818183, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.27115530303030305, "width": 0.19403758169934648, "height": 0.012579545454545427, "page": 9}], "section": "Performance Comparison with Literature", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "740"}, {"text": "Seven out of twelve participants chose GAT as the best technique because it was the fastest.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.1743901515151515, "width": 0.39986928104575165, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.18822727272727272, "width": 0.20704411764705882, "height": 0.012579545454545454, "page": 9}], "section": "Questionnaire", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "741"}, {"text": "It was also easy to use because GAT does not use vertical swipe gestures and there was no requirement to distinguish a touched region on the touchpad for selecting a sub-keyboard.", "label": "Conclusion", "bboxes": [{"left": 0.27808823529411764, "top": 0.24357575757575758, "width": 0.2072042483660131, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.2574128787878788, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.2712487373737374, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.2850858585858586, "width": 0.18678431372549026, "height": 0.012579545454545427, "page": 9}], "section": "Questionnaire", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "742"}, {"text": "The main differences between VR and AR may be in the visual delity of the environment.", "label": "Conclusion", "bboxes": [{"left": 0.5826209150326798, "top": 0.09528914141414141, "width": 0.3392091503267972, "height": 0.012579545454545468, "page": 9}, {"left": 0.524656862745098, "top": 0.10912626262626263, "width": 0.2762941176470588, "height": 0.012579545454545468, "page": 9}], "section": "Validity of the Simulated Environment", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "743"}, {"text": "If we consider that the eye-tracking accuracy may affect the performance of GAT variations, the results of Exp 1 may differ in the wearable context, because a wearable eye tracker may be less accurate than a stationary one.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.3974591503267974, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.22814215686274514, "height": 0.012579545454545538, "page": 9}], "section": "Validity of the Simulated Environment", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "744"}, {"text": "Further, it may be accurate initially, but becomes inaccurate over time owing to the slippage of the HWD.", "label": "Future Work", "bboxes": [{"left": 0.685890522875817, "top": 0.5276376262626262, "width": 0.23797222222222225, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.5414747474747474, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.555310606060606, "width": 0.06615032679738564, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "745"}, {"text": "Further, it is easy to confuse the forward and backward directions because the display and the touchpad are placed perpendicularly.", "label": "Future Work", "bboxes": [{"left": 0.5246633986928104, "top": 0.5231919191919192, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5370290404040404, "width": 0.399874183006536, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.5508661616161616, "width": 0.08609313725490197, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "746"}, {"text": "Thus, explicit eye movement to select a sub-keyboard will occur more frequently.", "label": "Future Work", "bboxes": [{"left": 0.8867385620915033, "top": 0.16888131313131313, "width": 0.03713888888888894, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.18271843434343435, "width": 0.39745588235294116, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.19655555555555554, "width": 0.10960457516339883, "height": 0.012579545454545454, "page": 4}], "section": "EXPERIMENT 1: CAN MORE EXPRESSIVE GAZE INPUT MAKE \"GAT\" BETTER?", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "747"}, {"text": "The sub-keyboard size for all variations was 6.85  6.85; however, if the eye cursor out of boundary of the keyboard, a closest sub-keyboard will be activated.", "label": "Future Work", "bboxes": [{"left": 0.08761437908496732, "top": 0.22955681818181817, "width": 0.39903104575163395, "height": 0.012856060606060649, "page": 5}, {"left": 0.08812745098039215, "top": 0.24367045454545455, "width": 0.39919934640522886, "height": 0.012579545454545427, "page": 5}, {"left": 0.08812745098039215, "top": 0.25750757575757577, "width": 0.2793496732026144, "height": 0.012579545454545427, "page": 5}], "section": "GAT Variations: Different Number of Gestures", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "748"}, {"text": "We were also uncertain of how fast a user will get used to the layout.", "label": "Future Work", "bboxes": [{"left": 0.392359477124183, "top": 0.8367222222222221, "width": 0.09293300653594777, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3626160130718955, "height": 0.012579545454545427, "page": 5}], "section": "Apparatus and Task", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "749"}, {"text": "Thus, we argue that further reduction of T1 after long-term utilization and/or incorporation of an optimized key placement that requires less SWITCH , can make GAT better.", "label": "Future Work", "bboxes": [{"left": 0.8252630718954248, "top": 0.5346515151515152, "width": 0.09658006535947716, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5484015151515151, "width": 0.39745915032679746, "height": 0.012666666666666715, "page": 6}, {"left": 0.5246633986928104, "top": 0.5623257575757576, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5760757575757576, "width": 0.21686601307189535, "height": 0.012666666666666715, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "750"}, {"text": "They mentioned that eye-only and GAT techniques required a focus on eye movements; further, errors often occurred because of unintentional eye movements.", "label": "Future Work", "bboxes": [{"left": 0.15919117647058822, "top": 0.3358800505050505, "width": 0.32610130718954256, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.3497171717171717, "width": 0.3998807189542484, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.3635542929292929, "width": 0.3266062091503268, "height": 0.012579545454545482, "page": 9}], "section": "Touch-only: SwipeZone", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "751"}, {"text": "Further, we explored the variations in the number of touch gestures.", "label": "Future Work", "bboxes": [{"left": 0.672607843137255, "top": 0.7260265151515152, "width": 0.2492287581699345, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.7398636363636364, "width": 0.1879297385620916, "height": 0.012579545454545427, "page": 9}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "752"}, {"text": "Our future work will handle such environmental validity issues using an eyetracking AR glasses which can meet the requirements of GAT.", "label": "Future Work", "bboxes": [{"left": 0.8111960784313725, "top": 0.10912626262626263, "width": 0.11103267973856212, "height": 0.012579545454545468, "page": 9}, {"left": 0.5240751633986928, "top": 0.12296338383838384, "width": 0.40046078431372556, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.13680429292929294, "width": 0.40003267973856205, "height": 0.012579545454545454, "page": 9}], "section": "Validity of the Simulated Environment", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "753"}, {"text": "To achieve this goal, we focused on the gaze input modality.", "label": "Objective", "bboxes": [{"left": 0.7866617647058823, "top": 0.3606439393939394, "width": 0.1372009803921569, "height": 0.012579545454545482, "page": 1}, {"left": 0.5240816993464052, "top": 0.3744810606060606, "width": 0.27235947712418307, "height": 0.012579545454545482, "page": 1}], "section": "Gaze-based Interaction for Head-worn Display (HWD)", "prob": 0.718773603439331, "is_author_statement": true, "is_in_expected_section": false, "id": "754"}, {"text": "Our proposed approach is a two-step method using a combination of gaze and touch gesture for text entry.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.3722070707070707, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3860441919191919, "width": 0.3324003267973856, "height": 0.012579545454545427, "page": 2}], "section": "Gaze-based Text entry methods", "prob": 0.8111849427223206, "is_author_statement": true, "is_in_expected_section": true, "id": "755"}, {"text": "To distinguish the touch gestures, we used a vector from the placement to release positions.", "label": "Method", "bboxes": [{"left": 0.7686209150326797, "top": 0.6967840909090909, "width": 0.15321568627450988, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.7106212121212121, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7244583333333333, "width": 0.04752124183006545, "height": 0.012579545454545538, "page": 2}], "section": "Key Selection", "prob": 0.6966213583946228, "is_author_statement": true, "is_in_expected_section": true, "id": "756"}, {"text": "SwipeZone is a two-step key selection method, similar to SwipeBoard, but using different sub-keyboard selection gestures.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.1449810606060606, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 7}, {"left": 0.5246633986928104, "top": 0.15881818181818183, "width": 0.39988235294117647, "height": 0.012579545454545454, "page": 7}, {"left": 0.5246633986928104, "top": 0.17265530303030302, "width": 0.03642973856209153, "height": 0.012579545454545454, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.6749954223632812, "is_author_statement": false, "is_in_expected_section": true, "id": "757"}, {"text": "To validate that GAT has better usability than the monomodal text entry methods, we compared GAT to eye-only typing (adjustable dwell time [22]) and touch-only typing (SwipeZone [11]) utilizing eye-trackable HWD.", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.61689898989899, "width": 0.3976781045751634, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6307361111111112, "width": 0.3998725490196079, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6445732323232324, "width": 0.39716830065359465, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6584103535353536, "width": 0.22940032679738565, "height": 0.012579545454545427, "page": 6}], "section": "EXPERIMENT 2: \"GAT\" EVALUATION ON \"HWD\"", "prob": 0.6685924530029297, "is_author_statement": true, "is_in_expected_section": false, "id": "758"}, {"text": "Methods such as SwipeBoard (7.14 wpm) and SwipeZone (8.73 wpm) [11] use a two-step scheme to enter a character.", "label": "Method", "bboxes": [{"left": 0.18243790849673203, "top": 0.7474204545454546, "width": 0.3028611111111111, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.7612575757575758, "width": 0.39745588235294127, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.775094696969697, "width": 0.07666176470588236, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": 0.6498486995697021, "is_author_statement": false, "is_in_expected_section": true, "id": "759"}, {"text": "To overcome the problems of the touchpadand eye-gazebased methods, we propose the gaze-assisted typing (GAT).", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.6873914141414141, "width": 0.4003774509803921, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246584967320261, "top": 0.7012285353535355, "width": 0.40002941176470586, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": 0.6226657629013062, "is_author_statement": true, "is_in_expected_section": true, "id": "760"}, {"text": "The only difference between GAT and M-SwipeBoard is the sub-keyboard selection method.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.48888131313131317, "width": 0.39768137254901953, "height": 0.012579545454545371, "page": 3}, {"left": 0.08811928104575163, "top": 0.5027184343434343, "width": 0.2072892156862745, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.6178158521652222, "is_author_statement": false, "is_in_expected_section": false, "id": "761"}, {"text": "Finally, to demonstrate that the GAT method with larger keys for eye selection and simpler touchpad gestures would lead to improved performance and usability benets over eye-only and touch-only typing methods, we conducted Experiment 2.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.3971764705882353, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.24121085858585856, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.3977385620915033, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.268885101010101, "width": 0.3921372549019607, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5835129618644714, "is_author_statement": true, "is_in_expected_section": true, "id": "762"}, {"text": "At the start of each session, we calibrated the eye tracker with the default calibration method of FOVE.", "label": "Method", "bboxes": [{"left": 0.08753267973856209, "top": 0.14435227272727272, "width": 0.3977549019607843, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.15818939393939394, "width": 0.26888888888888896, "height": 0.012579545454545454, "page": 8}], "section": "Design and Procedure", "prob": 0.5781197547912598, "is_author_statement": true, "is_in_expected_section": true, "id": "763"}, {"text": "To avoid the issues, we simulated the environment instead of using a mobile eye tracker.", "label": "Method", "bboxes": [{"left": 0.14512091503267974, "top": 0.7113926767676768, "width": 0.340171568627451, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7252297979797979, "width": 0.2418741830065359, "height": 0.012579545454545538, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.5524033308029175, "is_author_statement": true, "is_in_expected_section": false, "id": "764"}, {"text": "To reduce the typing error by confusing the swipe direction, we provided a visual feedback (touch cursor in Figure 2(b)) for the touch position to permit adjustments in the swipe direction before the swipe gesture is nalized.", "label": "Method", "bboxes": [{"left": 0.6214330065359477, "top": 0.5508661616161616, "width": 0.3004035947712418, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5647032828282829, "width": 0.3974460784313726, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.5785404040404041, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5923775252525253, "width": 0.4000130718954249, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": 0.5408702492713928, "is_author_statement": true, "is_in_expected_section": true, "id": "765"}, {"text": "In each session, three blocks were performed using one of the three techniques.", "label": "Method", "bboxes": [{"left": 0.3842205882352941, "top": 0.09529419191919192, "width": 0.10310294117647067, "height": 0.012579545454545468, "page": 8}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.40002777777777776, "height": 0.012579545454545454, "page": 8}], "section": "Design and Procedure", "prob": 0.537702202796936, "is_author_statement": false, "is_in_expected_section": true, "id": "766"}, {"text": "Another approach utilized complex gestures to enter a character.", "label": "Method", "bboxes": [{"left": 0.4069656862745098, "top": 0.5526805555555556, "width": 0.08102777777777781, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5665176767676768, "width": 0.34099836601307193, "height": 0.012579545454545427, "page": 1}], "section": "On glasses frame input", "prob": 0.5373203158378601, "is_author_statement": false, "is_in_expected_section": true, "id": "767"}, {"text": "Then, we used a center position of the ray-casted positions of both eyes as the gaze position on the plane.", "label": "Method", "bboxes": [{"left": 0.2617205882352941, "top": 0.6396300505050505, "width": 0.22357516339869277, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6534671717171717, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6673030303030303, "width": 0.06329248366013072, "height": 0.012579545454545427, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.5367642045021057, "is_author_statement": true, "is_in_expected_section": true, "id": "768"}, {"text": "To reduce the jitter, we also used an average lter.", "label": "Method", "bboxes": [{"left": 0.15645751633986926, "top": 0.6673030303030303, "width": 0.32523366013071897, "height": 0.012579545454545427, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.535217821598053, "is_author_statement": true, "is_in_expected_section": true, "id": "769"}, {"text": "At the start of the session, we instructed a user on the process of entering text using a given technique and we calibrated the eye tracker with a ve-point calibration.", "label": "Method", "bboxes": [{"left": 0.9052434640522876, "top": 0.5590593434343434, "width": 0.01658823529411757, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5728964646464646, "width": 0.39717647058823546, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.5867335858585858, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.600570707070707, "width": 0.23665359477124182, "height": 0.012579545454545538, "page": 3}], "section": "Design and Procedure", "prob": 0.5346853137016296, "is_author_statement": true, "is_in_expected_section": true, "id": "770"}, {"text": "In this 2 x 2 taxonomy, the existing Gaze + Gesture text entry methods belong either to the AA + DA category (the click-alternative methods) or the BA + CM category (MAGIC-based method).", "label": "Method", "bboxes": [{"left": 0.25203594771241833, "top": 0.2475012626262626, "width": 0.23325653594771245, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.2613383838383838, "width": 0.39947058823529413, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.27517424242424243, "width": 0.3971797385620915, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.28901136363636365, "width": 0.2246797385620915, "height": 0.012579545454545482, "page": 2}], "section": "Gaze-based Text entry methods", "prob": 0.5316548347473145, "is_author_statement": false, "is_in_expected_section": true, "id": "771"}, {"text": "we conducted one additional block with VGC to simulate users who are familiar with each keyboard layout.", "label": "Method", "bboxes": [{"left": 0.5240784313725491, "top": 0.4096893939393939, "width": 0.39775490196078434, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.4235265151515151, "width": 0.32071078431372557, "height": 0.012579545454545482, "page": 5}], "section": "Apparatus and Task", "prob": 0.43854111433029175, "is_author_statement": true, "is_in_expected_section": true, "id": "772"}, {"text": "To select a key, a user can perform one of the nine touch gestures, similar to SwipeBoard [11], to enter a key as shown in Figure 1(c), i.e. , a tap gesture to select a center-positioned key on the sub-keyboard, and one of the eight directional swipe gestures to select a key corresponding to the swipe direction from the center-positioned key as shown in Figure 1(c).", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.6354570707070707, "width": 0.4003790849673202, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.6492941919191919, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6628421717171717, "width": 0.39774509803921565, "height": 0.012868686868686918, "page": 2}, {"left": 0.08811928104575163, "top": 0.6769671717171717, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6908042929292929, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.3753725490196078, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": 0.434969425201416, "is_author_statement": false, "is_in_expected_section": true, "id": "773"}, {"text": "The dwell-click methods were extensively used to avoid this problem.", "label": "Method", "bboxes": [{"left": 0.7262794117647059, "top": 0.5725618686868686, "width": 0.19556372549019607, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.5863977272727273, "width": 0.2594248366013072, "height": 0.012579545454545427, "page": 1}], "section": "Gaze-based Text entry methods", "prob": 0.4269034266471863, "is_author_statement": false, "is_in_expected_section": true, "id": "774"}, {"text": "We also used a Google Glass Enterprise Edition to use the built-in touchpad on the glasses.", "label": "Method", "bboxes": [{"left": 0.28903104575163396, "top": 0.7667411616161617, "width": 0.19626143790849676, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7805782828282828, "width": 0.39938235294117647, "height": 0.012579545454545538, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.398292601108551, "is_author_statement": true, "is_in_expected_section": false, "id": "775"}, {"text": "Thus, we choose SwipeBoard as a baseline technique in this study, to demonstrate the net effect of replacing an input step with gaze input.", "label": "Method", "bboxes": [{"left": 0.37100653594771243, "top": 0.29390530303030304, "width": 0.11428594771241835, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.30774242424242426, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3215795454545455, "width": 0.4000261437908497, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.37412410974502563, "is_author_statement": true, "is_in_expected_section": false, "id": "776"}, {"text": "Initially, a user can select one of the sections with the gaze input.", "label": "Method", "bboxes": [{"left": 0.7057990196078432, "top": 0.8194722222222223, "width": 0.21603758169934628, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8333093434343434, "width": 0.20287745098039212, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": 0.3729075491428375, "is_author_statement": false, "is_in_expected_section": true, "id": "777"}, {"text": "In the GAT session, if a user identied a mismatch in the position of the eye cursor, we recalibrated the eye tracker and discarded the task in progress.", "label": "Method", "bboxes": [{"left": 0.7663627450980393, "top": 0.600570707070707, "width": 0.15547058823529414, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.6144078282828283, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6282449494949495, "width": 0.40002450980392157, "height": 0.012579545454545538, "page": 3}], "section": "Design and Procedure", "prob": 0.37063100934028625, "is_author_statement": true, "is_in_expected_section": true, "id": "778"}, {"text": "The original SwipeBoard [11], consists of nine sub-keyboards; moreover, the key selection utilized three easyto-use touch gestures to reduce the input error at the key selection.", "label": "Method", "bboxes": [{"left": 0.17829575163398695, "top": 0.3706376262626263, "width": 0.30700163398692804, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3844747474747475, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.39831186868686874, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.4121489898989899, "width": 0.028812091503267967, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.35044097900390625, "is_author_statement": false, "is_in_expected_section": false, "id": "779"}, {"text": "In each session, by using one of the three layouts, a participant completed three blocks in NC; then, completed a block in VGC.", "label": "Method", "bboxes": [{"left": 0.1966781045751634, "top": 0.35515277777777776, "width": 0.28861437908496734, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.368989898989899, "width": 0.39919934640522875, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.3828270202020202, "width": 0.18540359477124185, "height": 0.012579545454545427, "page": 5}], "section": "Design and Procedure", "prob": 0.3471115827560425, "is_author_statement": false, "is_in_expected_section": true, "id": "780"}, {"text": "The literature presents several text entry methods using the on-frame touchpad.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.7059103535353536, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811274509803921, "top": 0.7197474747474747, "width": 0.13079084967320262, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": 0.3408927917480469, "is_author_statement": false, "is_in_expected_section": true, "id": "781"}, {"text": "We also provided audio feedback for key entered, it is same to the other techniques.", "label": "Method", "bboxes": [{"left": 0.6754624183006536, "top": 0.6819242424242423, "width": 0.24665686274509813, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.6957613636363637, "width": 0.30070098039215687, "height": 0.012579545454545427, "page": 7}], "section": "Eye-only typing: Adjustable dwell time selection", "prob": 0.32175788283348083, "is_author_statement": true, "is_in_expected_section": true, "id": "782"}, {"text": "GAT is two-step input approach for entering a character.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.7917979797979798, "width": 0.36678594771241846, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": 0.293315589427948, "is_author_statement": false, "is_in_expected_section": true, "id": "783"}, {"text": "Assuming the stationary setup might have been favorable to GAT9 than the others, the main results of Exp 1 would have remained essentially the same if a wearable setup had been used.", "label": "Result", "bboxes": [{"left": 0.417937908496732, "top": 0.8153383838383839, "width": 0.06734640522875823, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.8291742424242424, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.8430113636363636, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.8568484848484849, "width": 0.3571388888888889, "height": 0.012579545454545427, "page": 9}], "section": "Validity of the Simulated Environment", "prob": 0.8861883878707886, "is_author_statement": false, "is_in_expected_section": false, "id": "784"}, {"text": "For NC, two-way (the technique and the block) RM-ANOVA analysis on text entry speed and on CER showed signicant effects of the block (Speed: F ( 2 , 34 ) = 156.903, p < 0.01, CER: F ( 2 , 34 ) = 20.775, p < 0.01); however, the effects of layout and interaction were not signicant.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.6709911616161616, "width": 0.39775653594771243, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.6848282828282829, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.698388888888889, "width": 0.3994395424836602, "height": 0.014803030303030207, "page": 5}, {"left": 0.5246633986928104, "top": 0.71222601010101, "width": 0.39716993464052297, "height": 0.014803030303030318, "page": 5}, {"left": 0.5246633986928104, "top": 0.7263396464646464, "width": 0.20920424836601315, "height": 0.012579545454545538, "page": 5}], "section": "Text entry speed and corrected error rate", "prob": 0.8847801685333252, "is_author_statement": false, "is_in_expected_section": false, "id": "785"}, {"text": "RM-ANOVA demonstrated signicant effects on the technique (F ( 1 , 11 ) = 42.763, p < 0.01) and the block (F ( 2 , 22 ) = 53.962, p < 0.01); however, the interaction was not signicant.", "label": "Result", "bboxes": [{"left": 0.1808545751633987, "top": 0.5544027777777778, "width": 0.3044379084967321, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.5679633838383839, "width": 0.39758006535947715, "height": 0.014803030303030318, "page": 4}, {"left": 0.08757843137254902, "top": 0.5818005050505051, "width": 0.39771241830065357, "height": 0.014803030303030318, "page": 4}, {"left": 0.08811928104575163, "top": 0.5959141414141415, "width": 0.0715441176470588, "height": 0.012579545454545538, "page": 4}], "section": "Text entry speed", "prob": 0.8516501784324646, "is_author_statement": false, "is_in_expected_section": false, "id": "786"}, {"text": "As the results show, T2 and T3 were not dependent on the layout.", "label": "Result", "bboxes": [{"left": 0.39206699346405227, "top": 0.5247613636363636, "width": 0.09323366013071899, "height": 0.012579545454545538, "page": 6}, {"left": 0.08812745098039215, "top": 0.538510101010101, "width": 0.34539215686274516, "height": 0.012667929292929259, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": 0.8323380351066589, "is_author_statement": false, "is_in_expected_section": false, "id": "787"}, {"text": "When compared to eye-only, GAT required less eye movements and demonstrated better accuracy.", "label": "Result", "bboxes": [{"left": 0.3002124183006536, "top": 0.18822727272727272, "width": 0.18711437908496736, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.20206439393939393, "width": 0.39745588235294127, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.21590151515151515, "width": 0.061289215686274504, "height": 0.012579545454545454, "page": 9}], "section": "Questionnaire", "prob": 0.8187397718429565, "is_author_statement": false, "is_in_expected_section": false, "id": "788"}, {"text": "The RM-ANOVA analysis on CER showed signicant effects of block (F ( 5 , 55 ) = 3.260, p < 0.05); however, the effects of technique and interaction were not signicant.", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.8291742424242424, "width": 0.39767320261437905, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8427348484848485, "width": 0.3971650326797386, "height": 0.014803030303030318, "page": 8}, {"left": 0.08811928104575163, "top": 0.8568484848484849, "width": 0.3011078431372549, "height": 0.012579545454545427, "page": 8}], "section": "Error rate", "prob": 0.815356433391571, "is_author_statement": false, "is_in_expected_section": false, "id": "789"}, {"text": "Thus, we argue that further reduction of T1 after long-term utilization and/or incorporation of an optimized key placement that requires less SWITCH , can make GAT better.", "label": "Result", "bboxes": [{"left": 0.8252630718954248, "top": 0.5346515151515152, "width": 0.09658006535947716, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5484015151515151, "width": 0.39745915032679746, "height": 0.012666666666666715, "page": 6}, {"left": 0.5246633986928104, "top": 0.5623257575757576, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5760757575757576, "width": 0.21686601307189535, "height": 0.012666666666666715, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": 0.8111094236373901, "is_author_statement": true, "is_in_expected_section": false, "id": "790"}, {"text": "We analyzed the results statistically for both NC and VGC, separately.", "label": "Result", "bboxes": [{"left": 0.5238986928104574, "top": 0.5751502525252525, "width": 0.3999803921568629, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.5889873737373738, "width": 0.06900490196078435, "height": 0.012579545454545427, "page": 5}], "section": "Results", "prob": 0.8097468614578247, "is_author_statement": true, "is_in_expected_section": true, "id": "791"}, {"text": "The RM-ANOVA analysis on text entry speed showed significant effects of the technique (F ( 2 , 22 ) = 11.020, p < 0.01) and block (F ( 2 . 696 , 29 . 654 ) = 48.822, p < 0.01).", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.6464229797979798, "width": 0.40037581699346403, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6599823232323233, "width": 0.3971764705882353, "height": 0.014804292929292862, "page": 8}, {"left": 0.08811928104575163, "top": 0.6738194444444444, "width": 0.2563562091503268, "height": 0.014803030303030318, "page": 8}], "section": "Text entry speed", "prob": 0.8005924820899963, "is_author_statement": false, "is_in_expected_section": false, "id": "792"}, {"text": "Different wearable eye trackers have different problems; consequently, for studies conducted with a particular tracker, the results would be dependent on the specic problems of that tracker.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.6698825757575758, "width": 0.39987091503267974, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6837184343434344, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6975555555555556, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7113926767676768, "width": 0.04918137254901962, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.7922788262367249, "is_author_statement": false, "is_in_expected_section": true, "id": "793"}, {"text": "For the UER, RM-ANOVA analysis with ART showed a signicant effect of technique (F ( 1 , 11 ) = 5.309, p < 0.05).", "label": "Result", "bboxes": [{"left": 0.3043398692810457, "top": 0.7163181818181817, "width": 0.18153431372549023, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7301553030303031, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.08758823529411765, "top": 0.7437159090909091, "width": 0.17457843137254903, "height": 0.014803030303030207, "page": 4}], "section": "Text entry speed", "prob": 0.7809209823608398, "is_author_statement": false, "is_in_expected_section": false, "id": "794"}, {"text": "According to our micro analysis results, the gesture completion time ( T4 ) increased with the number of touch gestures.", "label": "Result", "bboxes": [{"left": 0.7175065359477124, "top": 0.7398636363636364, "width": 0.20433006535947718, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.7536136363636363, "width": 0.39716666666666667, "height": 0.012666666666666715, "page": 9}, {"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.1646372549019608, "height": 0.012579545454545427, "page": 9}], "section": "CONCLUSION", "prob": 0.7644515633583069, "is_author_statement": true, "is_in_expected_section": true, "id": "795"}, {"text": "Friedman tests did not show signicant effects on the other two questions.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.6257462121212122, "width": 0.39745588235294116, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6395833333333334, "width": 0.09668627450980405, "height": 0.012579545454545427, "page": 8}], "section": "Questionnaire", "prob": 0.7399365901947021, "is_author_statement": false, "is_in_expected_section": false, "id": "796"}, {"text": "GAT9 also got a signicantly higher rating than GAT3 for the Easy to use option (Z = -2.967, p < 0.01); however, for the option of Eye feels natural , GAT9 got a lower rating than GAT3 (Z = -2.423, p < 0.05).", "label": "Result", "bboxes": [{"left": 0.16131045751633985, "top": 0.13680429292929294, "width": 0.3239820261437909, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.150364898989899, "width": 0.3998709150326797, "height": 0.012856060606060593, "page": 6}, {"left": 0.08811928104575163, "top": 0.16439141414141414, "width": 0.3974575163398693, "height": 0.01266666666666666, "page": 6}, {"left": 0.08811928104575163, "top": 0.17803914141414143, "width": 0.2637156862745098, "height": 0.012856060606060593, "page": 6}], "section": "Questionnaire", "prob": 0.7227213382720947, "is_author_statement": false, "is_in_expected_section": false, "id": "797"}, {"text": "Tasks for the NC and VGC were the same as in the preliminary study.", "label": "Result", "bboxes": [{"left": 0.44805392156862744, "top": 0.7718345959595959, "width": 0.037241830065359416, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7856717171717171, "width": 0.40002777777777776, "height": 0.012579545454545538, "page": 5}], "section": "Apparatus and Task", "prob": 0.7200624346733093, "is_author_statement": false, "is_in_expected_section": false, "id": "798"}, {"text": "We demonstrated that GAT (11.04 wpm, after 40 min of usage) is faster than SwipeZone (8.53 wpm, after 50 min of usage) in Experiment 2.", "label": "Result", "bboxes": [{"left": 0.5238986928104574, "top": 0.1805858585858586, "width": 0.39903921568627454, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246650326797386, "top": 0.19442297979797982, "width": 0.39717156862745084, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.208260101010101, "width": 0.09251307189542479, "height": 0.012579545454545454, "page": 9}], "section": "Performance Comparison with Literature", "prob": 0.7168091535568237, "is_author_statement": true, "is_in_expected_section": false, "id": "799"}, {"text": "(a) shows the presented layout for GAT.", "label": "Result", "bboxes": [{"left": 0.1329313725490196, "top": 0.7813737373737373, "width": 0.26496895424836603, "height": 0.012579545454545538, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.6998785138130188, "is_author_statement": false, "is_in_expected_section": false, "id": "800"}, {"text": "Figure 7 shows a visual guidance.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.47258459595959595, "width": 0.2243725490196079, "height": 0.012579545454545482, "page": 5}], "section": "Apparatus and Task", "prob": 0.6955816745758057, "is_author_statement": false, "is_in_expected_section": false, "id": "801"}, {"text": "The post-hoc tests showed that GAT is signicantly faster than both touch-only (t 11 = 6.919, p < 0.01) and eye-only (t 11 = 3.882, p < 0.01) techniques.", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.7093181818181817, "width": 0.39795424836601306, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.7228787878787879, "width": 0.3971748366013072, "height": 0.014049242424242436, "page": 8}, {"left": 0.08811928104575163, "top": 0.7367159090909091, "width": 0.1858382352941177, "height": 0.012856060606060482, "page": 8}], "section": "Text entry speed", "prob": 0.6946559548377991, "is_author_statement": false, "is_in_expected_section": false, "id": "802"}, {"text": "We conducted the experiments in simulated environments.", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.6417462121212121, "width": 0.4007941176470589, "height": 0.012579545454545427, "page": 9}], "section": "Validity of the Simulated Environment", "prob": 0.6912708878517151, "is_author_statement": true, "is_in_expected_section": false, "id": "803"}, {"text": "Three of the ve participants chose touch-only as the best technique.", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.32204292929292927, "width": 0.39767156862745096, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.3358800505050505, "width": 0.06601470588235293, "height": 0.012579545454545482, "page": 9}], "section": "Touch-only: SwipeZone", "prob": 0.6897674202919006, "is_author_statement": false, "is_in_expected_section": false, "id": "804"}, {"text": "We were also uncertain of how fast a user will get used to the layout.", "label": "Result", "bboxes": [{"left": 0.392359477124183, "top": 0.8367222222222221, "width": 0.09293300653594777, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3626160130718955, "height": 0.012579545454545427, "page": 5}], "section": "Apparatus and Task", "prob": 0.6870003342628479, "is_author_statement": true, "is_in_expected_section": false, "id": "805"}, {"text": "The other two participants chose eye-only as the best.", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.48353156565656563, "width": 0.35825326797385615, "height": 0.012579545454545482, "page": 9}], "section": "Touch-only: SwipeZone", "prob": 0.6831738352775574, "is_author_statement": false, "is_in_expected_section": false, "id": "806"}, {"text": "The sessions were split across three days.", "label": "Result", "bboxes": [{"left": 0.4000637254901961, "top": 0.46584848484848485, "width": 0.08522875816993464, "height": 0.012579545454545482, "page": 5}, {"left": 0.0875375816993464, "top": 0.47968560606060606, "width": 0.1924264705882353, "height": 0.012579545454545482, "page": 5}], "section": "Design and Procedure", "prob": 0.682392418384552, "is_author_statement": false, "is_in_expected_section": false, "id": "807"}, {"text": "At the end of the last session, we surveyed a questionnaire with ve questions using a ve-point Likert scale; four of the questions were same to Experiment 1; and, the fth option was Eye fatigue .", "label": "Result", "bboxes": [{"left": 0.2785049019607843, "top": 0.2625959595959596, "width": 0.2067875816993464, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.2764330808080808, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.29027020202020204, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08689052287581699, "top": 0.30401893939393937, "width": 0.2804084967320261, "height": 0.012667929292929314, "page": 8}], "section": "Design and Procedure", "prob": 0.6724767684936523, "is_author_statement": true, "is_in_expected_section": false, "id": "808"}, {"text": "The RM-ANOVA with ART analysis on UER showed that both the effects and the interaction were not signicant.", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.8782335858585859, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.36173039215686276, "height": 0.012579545454545427, "page": 8}], "section": "Error rate", "prob": 0.6651859879493713, "is_author_statement": false, "is_in_expected_section": false, "id": "809"}, {"text": "For VGC, the layout effect also was not signicant.", "label": "Result", "bboxes": [{"left": 0.8992892156862745, "top": 0.7945063131313131, "width": 0.02283006535947707, "height": 0.012579545454545427, "page": 5}, {"left": 0.5240784313725491, "top": 0.8083434343434343, "width": 0.30799346405228756, "height": 0.012579545454545538, "page": 5}], "section": "Uncorected Error rate", "prob": 0.6535938382148743, "is_author_statement": false, "is_in_expected_section": false, "id": "810"}, {"text": "Moreover, in our pilot study, we also observed that users were inclined to not blink until a", "label": "Result", "bboxes": [{"left": 0.30829084967320264, "top": 0.8782335858585859, "width": 0.17904248366013065, "height": 0.012579545454545427, "page": 2}, {"left": 0.08753267973856209, "top": 0.892070707070707, "width": 0.3977549019607843, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": 0.6509550213813782, "is_author_statement": true, "is_in_expected_section": false, "id": "811"}, {"text": "Therefore, we conducted post-hoc Wilcoxon signed-rank tests with Bonferroni correction.", "label": "Result", "bboxes": [{"left": 0.5241601307189543, "top": 0.49995454545454543, "width": 0.3976813725490196, "height": 0.012579545454545538, "page": 8}, {"left": 0.5240784313725491, "top": 0.5137916666666666, "width": 0.17857843137254903, "height": 0.012579545454545427, "page": 8}], "section": "Questionnaire", "prob": 0.6491422653198242, "is_author_statement": true, "is_in_expected_section": false, "id": "812"}, {"text": "The accuracy of the tracker is less than 1 in specication.", "label": "Result", "bboxes": [{"left": 0.8372843137254902, "top": 0.8312260101010102, "width": 0.085125816993464, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.8450631313131313, "width": 0.28228921568627463, "height": 0.012579545454545538, "page": 6}], "section": "Implementation", "prob": 0.645764172077179, "is_author_statement": false, "is_in_expected_section": false, "id": "813"}, {"text": "As shown in Figure 11, T1 and SWITCH are highly correlated.", "label": "Result", "bboxes": [{"left": 0.3712990196078432, "top": 0.6983522727272727, "width": 0.11669444444444438, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811437908496732, "top": 0.7121022727272728, "width": 0.3247924836601307, "height": 0.012666666666666715, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": 0.6455420851707458, "is_author_statement": false, "is_in_expected_section": false, "id": "814"}, {"text": "Finally, we gathered 12 participants  2 techniques  3 blocks  5 tasks = 360 phrases.", "label": "Result", "bboxes": [{"left": 0.5246699346405229, "top": 0.7525012626262626, "width": 0.39717483660130715, "height": 0.012856060606060704, "page": 3}, {"left": 0.5246633986928104, "top": 0.7663383838383839, "width": 0.16365032679738567, "height": 0.012856060606060482, "page": 3}], "section": "Design and Procedure", "prob": 0.6432615518569946, "is_author_statement": true, "is_in_expected_section": false, "id": "815"}, {"text": "GAT got signicantly better ratings than touch-only for Easy to learn (Z= -2.565, p < 0.05), and better ratings than eyeonly for Prefer to use (Z= -2.280, p< 0.05) and Eye fatigue (Z= -2.980, p < 0.01).", "label": "Result", "bboxes": [{"left": 0.5246650326797386, "top": 0.5350883838383839, "width": 0.39773856209150316, "height": 0.012666666666666604, "page": 8}, {"left": 0.5246650326797386, "top": 0.5487373737373737, "width": 0.399875816993464, "height": 0.012856060606060593, "page": 8}, {"left": 0.5246617647058823, "top": 0.5627626262626263, "width": 0.39717810457516356, "height": 0.012667929292929259, "page": 8}, {"left": 0.5241339869281045, "top": 0.5764103535353535, "width": 0.14168790849673207, "height": 0.012856060606060593, "page": 8}], "section": "Questionnaire", "prob": 0.6373711824417114, "is_author_statement": false, "is_in_expected_section": false, "id": "816"}, {"text": "Friedman tests showed signicant effects of the layouts on all of the four questions: Easy to learn , Easy to use , Prefer to use , and Eye feels natural : p < 0.05, 0.01, 0.01, and 0.01; and,  ( 22 ) = 8.561, 15.524, 9.660, and 13.661, respectively.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.8488358585858586, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8625858585858586, "width": 0.3975343137254903, "height": 0.012666666666666715, "page": 5}, {"left": 0.5246699346405229, "top": 0.8762335858585858, "width": 0.39851470588235294, "height": 0.012856060606060704, "page": 5}, {"left": 0.5246633986928104, "top": 0.8883472222222223, "width": 0.38145098039215686, "height": 0.018300505050505067, "page": 5}], "section": "Questionnaire", "prob": 0.6355046629905701, "is_author_statement": false, "is_in_expected_section": false, "id": "817"}, {"text": "Consequently, Experiment 2 was conducted, which demonstrated that GAT causes less eye fatigue and is sufciently more robust than the eye-only text entry when considering the inaccuracy of eye tracker; consequently, it can be used with a wearable eye tracker.", "label": "Result", "bboxes": [{"left": 0.7532892156862745, "top": 0.41707196969696975, "width": 0.16854738562091498, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.43090909090909085, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.44474621212121207, "width": 0.39773856209150327, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.4585833333333333, "width": 0.39851797385620913, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.4724204545454545, "width": 0.3678676470588237, "height": 0.012579545454545482, "page": 9}], "section": "Performance Comparison with Literature", "prob": 0.6294530630111694, "is_author_statement": false, "is_in_expected_section": false, "id": "818"}, {"text": "Fifteen of the eighteen participants faced difculty in using the diagonal gestures and six out of these fteen also faced difculty in using the vertical swipe for GAT3.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.2146111111111111, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.22844823232323233, "width": 0.3998774509803921, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.24228409090909092, "width": 0.2753137254901961, "height": 0.012579545454545427, "page": 6}], "section": "Comments from the participants", "prob": 0.5885974168777466, "is_author_statement": false, "is_in_expected_section": false, "id": "819"}, {"text": "In Experiment 2, GAT outperformed an eye-typing method, A-Dwell (8.81 wpm, after 50 min of usage), when using the mobile eye tracker after 40 min of use.", "label": "Result", "bboxes": [{"left": 0.7240800653594771, "top": 0.27115530303030305, "width": 0.20046405228758168, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.2849924242424242, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.29882954545454543, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.31266666666666665, "width": 0.043627450980392224, "height": 0.012579545454545482, "page": 9}], "section": "Performance Comparison with Literature", "prob": 0.5876569747924805, "is_author_statement": false, "is_in_expected_section": false, "id": "820"}, {"text": "GAT3 has three sub-keyboards with nine keys for each, similar to the preliminary study.", "label": "Result", "bboxes": [{"left": 0.5246650326797386, "top": 0.28447727272727275, "width": 0.39745915032679735, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.2983143939393939, "width": 0.15770751633986924, "height": 0.012579545454545482, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "prob": 0.5846211314201355, "is_author_statement": false, "is_in_expected_section": false, "id": "821"}, {"text": "As shown in Figure 13(c), SwipeZone is nearly the same as the original.", "label": "Result", "bboxes": [{"left": 0.5240784313725491, "top": 0.3324090909090909, "width": 0.39775490196078434, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246601307189542, "top": 0.3462462121212121, "width": 0.10204901960784318, "height": 0.012579545454545482, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.5796695351600647, "is_author_statement": false, "is_in_expected_section": false, "id": "822"}, {"text": "The task was same as that of the previous studies.", "label": "Result", "bboxes": [{"left": 0.5241601307189543, "top": 0.7385681818181818, "width": 0.31185457516339865, "height": 0.012579545454545427, "page": 7}], "section": "Task and Participants", "prob": 0.5787248015403748, "is_author_statement": false, "is_in_expected_section": false, "id": "823"}, {"text": "The UER violated normality assumption; therefore, we performed an aligned rank transform (ART) [42] on the UER before conducting RM-ANOVA.", "label": "Result", "bboxes": [{"left": 0.2139591503267974, "top": 0.43399873737373734, "width": 0.2726813725490196, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.44783585858585856, "width": 0.39826960784313725, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4616729797979798, "width": 0.33811274509803924, "height": 0.012579545454545482, "page": 4}], "section": "Results", "prob": 0.5764380693435669, "is_author_statement": true, "is_in_expected_section": true, "id": "824"}, {"text": "The analysis methods were nearly the same as those for Experiment 1; however, we conducted three post-hoc tests only in the last block, and we used a one-tailed assumption for the tests because we assumed that GAT outperforms the others.", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.47750757575757574, "width": 0.4003774509803921, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.49134343434343436, "width": 0.39773692810457517, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.5051805555555555, "width": 0.397171568627451, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5190176767676767, "width": 0.38745098039215686, "height": 0.012579545454545427, "page": 8}], "section": "Results", "prob": 0.5756829977035522, "is_author_statement": true, "is_in_expected_section": true, "id": "825"}, {"text": "In the eye-only technique, four participants did not complete all the blocks.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.5404027777777778, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.554239898989899, "width": 0.09339869281045753, "height": 0.012579545454545427, "page": 8}], "section": "Results", "prob": 0.5746533870697021, "is_author_statement": false, "is_in_expected_section": true, "id": "826"}, {"text": "A target phrase was randomly presented at each trial.", "label": "Result", "bboxes": [{"left": 0.8678349673202614, "top": 0.3920921717171717, "width": 0.053996732026143635, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246584967320261, "top": 0.4059292929292929, "width": 0.29809640522875824, "height": 0.012579545454545482, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.5715838074684143, "is_author_statement": false, "is_in_expected_section": true, "id": "827"}, {"text": "Five participants were wearing glasses.", "label": "Result", "bboxes": [{"left": 0.3441209150326797, "top": 0.6880265151515151, "width": 0.141171568627451, "height": 0.012579545454545427, "page": 5}, {"left": 0.08753267973856209, "top": 0.7018636363636364, "width": 0.10669117647058825, "height": 0.012579545454545538, "page": 5}], "section": "Design and Procedure", "prob": 0.5554764270782471, "is_author_statement": false, "is_in_expected_section": false, "id": "828"}, {"text": "We rst conducted a preliminary study to validate the effect of introducing eye input modality to a two-step touch-only text entry with a minimal load on eye input.", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.11667803030303031, "width": 0.39793790849673205, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.1305151515151515, "width": 0.39717647058823524, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.14435227272727272, "width": 0.24770588235294122, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "prob": 0.5327445268630981, "is_author_statement": true, "is_in_expected_section": true, "id": "829"}, {"text": "After a users eye dwelling on a key over 30% of the dwell time, boundary of the key change its color to blue.", "label": "Result", "bboxes": [{"left": 0.8870849673202613, "top": 0.6265757575757576, "width": 0.03503431372549026, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6402878787878787, "width": 0.3992156862745099, "height": 0.012704545454545468, "page": 7}, {"left": 0.5246633986928104, "top": 0.6542500000000001, "width": 0.29504248366013075, "height": 0.012579545454545427, "page": 7}], "section": "Eye-only typing: Adjustable dwell time selection", "prob": 0.5307307243347168, "is_author_statement": false, "is_in_expected_section": false, "id": "830"}, {"text": "Because we varied the number of touch gestures, the keyboard layouts were correspondingly varied.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.40272095959595955, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.41655808080808077, "width": 0.25012581699346403, "height": 0.012579545454545482, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "prob": 0.528250515460968, "is_author_statement": true, "is_in_expected_section": false, "id": "831"}, {"text": "We constructed a VR environment that was similar to the smart glass-based environment.", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.5704444444444444, "width": 0.3979379084967321, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.5842815656565656, "width": 0.16820098039215686, "height": 0.012579545454545427, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.5256376266479492, "is_author_statement": true, "is_in_expected_section": false, "id": "832"}, {"text": "In Experiment 1, GAT3, GAT6, and GAT9 were equally effective; however, GAT6 was the most preferred.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.2932320261437909, "height": 0.012579545454545538, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.5255432724952698, "is_author_statement": false, "is_in_expected_section": false, "id": "833"}, {"text": "designed a unistroke gesture technique [46] and showed that 1D Handwriting (4.7 wpm) outperformed the selection-based text entry technique, i.e., 1Line-Keyboard (4.2 wpm).", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.580354797979798, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 1}, {"left": 0.08689869281045752, "top": 0.5941919191919193, "width": 0.398390522875817, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.607739898989899, "width": 0.34787745098039213, "height": 0.012868686868686918, "page": 1}], "section": "On glasses frame input", "prob": 0.5104252099990845, "is_author_statement": false, "is_in_expected_section": false, "id": "834"}, {"text": "When compared to the touch-only text entry, GAT can achieve better input speed than SwipeBoard [11] owing to the higher eye movement speed.", "label": "Result", "bboxes": [{"left": 0.5238986928104574, "top": 0.7952108585858586, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39746078431372556, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.1454869281045752, "height": 0.012579545454545427, "page": 2}], "section": "Expected advantage of GAT", "prob": 0.5078282952308655, "is_author_statement": false, "is_in_expected_section": false, "id": "835"}, {"text": "We follow dwell time adjust fomula used in the original [22], Dwell = 300  EXP ( X / 12 )  150, where as X, from 1 to 20, can be adjust in a digit (gure 13 (e)).", "label": "Result", "bboxes": [{"left": 0.6009771241830065, "top": 0.5083320707070706, "width": 0.32085947712418295, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5218800505050505, "width": 0.39921568627450976, "height": 0.012868686868686918, "page": 7}, {"left": 0.5246633986928104, "top": 0.5360063131313131, "width": 0.33710947712418304, "height": 0.012579545454545427, "page": 7}], "section": "Eye-only typing: Adjustable dwell time selection", "prob": 0.5004227161407471, "is_author_statement": true, "is_in_expected_section": false, "id": "836"}, {"text": "To verify GAT, initially, we demonstrate the net effect of introducing gaze modality with a minimum expression of the eye input.", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.2108838383838384, "width": 0.4003774509803922, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811274509803921, "top": 0.2247209595959596, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811274509803921, "top": 0.2385580808080808, "width": 0.06527124183006536, "height": 0.012579545454545454, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.46878886222839355, "is_author_statement": true, "is_in_expected_section": true, "id": "837"}, {"text": "To help a user distinguish between the regions, we attached two strips of tape on the backward and forward regions of the touchpad to provide a tactile feedback, similar to the original Swipezone.", "label": "Result", "bboxes": [{"left": 0.8573627450980392, "top": 0.2695138888888889, "width": 0.06447385620915025, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.2833510101010101, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.2971881313131313, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.31102525252525254, "width": 0.39580228758169944, "height": 0.012579545454545482, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.4298577904701233, "is_author_statement": true, "is_in_expected_section": false, "id": "838"}, {"text": "We also provided an audio feedback from the glasses for the touch input.", "label": "Result", "bboxes": [{"left": 0.7872614379084967, "top": 0.633888888888889, "width": 0.1345784313725491, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6477260101010102, "width": 0.33771895424836607, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "prob": 0.4095339775085449, "is_author_statement": true, "is_in_expected_section": false, "id": "839"}, {"text": "There were few options for the touch-only typing baseline for smart glasses.", "label": "Result", "bboxes": [{"left": 0.5241601307189543, "top": 0.0959229797979798, "width": 0.3976715686274508, "height": 0.01257954545454544, "page": 7}, {"left": 0.5246584967320261, "top": 0.10976010101010102, "width": 0.11596405228758166, "height": 0.01257954545454544, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "prob": 0.4056355059146881, "is_author_statement": false, "is_in_expected_section": false, "id": "840"}, {"text": "At the start of VGC, we explained the reason to the participants.", "label": "Result", "bboxes": [{"left": 0.8504150326797385, "top": 0.4235265151515151, "width": 0.07142647058823537, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.43736363636363634, "width": 0.35091339869281046, "height": 0.012579545454545482, "page": 5}], "section": "Apparatus and Task", "prob": 0.39064013957977295, "is_author_statement": true, "is_in_expected_section": false, "id": "841"}, {"text": "Owing to the differences in remote and wearable eye tracking environments, and the slippage and HWD wobbling problems of wearable eye trackers, we conducted Experiment 2 utilizing an eye-trackable HWD, FOVE VR 4 .", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.6991464646464647, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7129835858585859, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7268207070707071, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.738655303030303, "width": 0.23382679738562107, "height": 0.014580808080808083, "page": 6}], "section": "Implementation", "prob": 0.3510410785675049, "is_author_statement": true, "is_in_expected_section": false, "id": "842"}, {"text": "1) environmental noise can hamper recognition accuracy,", "label": "Result", "bboxes": [{"left": 0.3124542483660131, "top": 0.34839393939393937, "width": 0.17283823529411763, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.3622310606060606, "width": 0.19685947712418306, "height": 0.012579545454545482, "page": 1}], "section": "Possible Text Entry Methods for Smart Glasses", "prob": 0.3482941687107086, "is_author_statement": false, "is_in_expected_section": false, "id": "843"}, {"text": "We analyzed the key input time to better understand the typing skills of users utilizing GAT, with only characters that were not erased and typed correctly (27,383 key entries).", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.3511691919191919, "width": 0.3979379084967321, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3650063131313131, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3788434343434343, "width": 0.3345767973856209, "height": 0.012579545454545482, "page": 6}], "section": "Micro Analysis on Key Input Time", "prob": 0.3470366299152374, "is_author_statement": true, "is_in_expected_section": false, "id": "844"}, {"text": "To demonstrate the net effect, we modied the SwipeBoard design to be similar to our GAT design; we called this MSwipeBoard.", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.3429633838383839, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811274509803921, "top": 0.3568005050505051, "width": 0.3998790849673203, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3706376262626263, "width": 0.0851437908496732, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "prob": 0.33350419998168945, "is_author_statement": true, "is_in_expected_section": true, "id": "845"}, {"text": "For the \"Prefer to use\" question, we asked the reason for the highest/lowest rating.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.0959229797979798, "width": 0.3971748366013072, "height": 0.01257954545454544, "page": 9}, {"left": 0.08811928104575163, "top": 0.10976010101010102, "width": 0.14101143790849674, "height": 0.01257954545454544, "page": 9}], "section": "Questionnaire", "prob": 0.3315267860889435, "is_author_statement": true, "is_in_expected_section": false, "id": "846"}, {"text": "For dwell-free eye typing, gesture-based eye typing approaches were thoroughly explored: Examples include a keystroke gesture approach (EyeWrite [43]: 4.9 wpm), continuous gesture approach (Dasher: 17.26 wpm [40], and wordstroke gesture approach (EyeSwipe [16]: 11.7 wpm).", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.7046414141414141, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.759989898989899, "width": 0.34530555555555575, "height": 0.012579545454545427, "page": 1}], "section": "Gaze-based Text entry methods", "prob": 0.27164754271507263, "is_author_statement": false, "is_in_expected_section": false, "id": "847"}], "uist-4": [{"text": "In this paper, we introduce Lip-Interact, which enables users to access functionality on the smartphone by issuing silent speech commands (Figure 1).", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.3971944444444445, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.2023153594771242, "height": 0.012579545454545538, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "848"}, {"text": "Copyrights for components of this work owned by others than ACM mustbehonored.", "label": "Author", "bboxes": [{"left": 0.170890522875817, "top": 0.8376679292929293, "width": 0.3111748366013072, "height": 0.008805555555555622, "page": 0}, {"left": 0.0913611111111111, "top": 0.8477310606060606, "width": 0.07771732026143792, "height": 0.008805555555555511, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "849"}, {"text": "Although voice interaction is accurate and efcient, in our pilot survey of 15 users, only one user occasionally uses voice assistant to set calendar and another two users use speech for text input.", "label": "Author", "bboxes": [{"left": 0.8214493464052287, "top": 0.4002828282828283, "width": 0.10040849673202623, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.4141186868686869, "width": 0.3971895424836601, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.4279558080808081, "width": 0.3971911764705882, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.44179292929292924, "width": 0.37116993464052295, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "850"}, {"text": "To our knowledge, none has studied to use the on-device front camera to combine silent speech with the smartphone as a command input approach.", "label": "Author", "bboxes": [{"left": 0.1965326797385621, "top": 0.4839873737373737, "width": 0.2887777777777778, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.49782449494949493, "width": 0.3971846405228758, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.5116616161616161, "width": 0.28441013071895427, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "851"}, {"text": "We deem this to be a promising setup for practical use.", "label": "Author", "bboxes": [{"left": 0.3780261437908497, "top": 0.5116616161616161, "width": 0.10728431372549013, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5254987373737373, "width": 0.2569297385620915, "height": 0.012579545454545538, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "852"}, {"text": "We also for the first time study the feasibility of Lip-Interact from three aspects:", "label": "Author", "bboxes": [{"left": 0.3522222222222222, "top": 0.5254987373737373, "width": 0.13308823529411762, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5393358585858585, "width": 0.3888333333333333, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "853"}, {"text": "We discuss the design principles and present a set of interface mechanisms that support novice learning, visual feedback and online model personalization.", "label": "Author", "bboxes": [{"left": 0.30546405228758167, "top": 0.5607196969696969, "width": 0.18255555555555558, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5745568181818181, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5883939393939394, "width": 0.39989869281045753, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.6022310606060606, "width": 0.029900326797385607, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "854"}, {"text": "We describe algorithms that automatically segment the mouth image sequences and recognize the commands with an end-to-end deep learning model.", "label": "Method", "bboxes": [{"left": 0.12306862745098039, "top": 0.6022310606060606, "width": 0.36223366013071895, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6160681818181818, "width": 0.39719607843137256, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.629905303030303, "width": 0.20599509803921573, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "855"}, {"text": "We describe algorithms that automatically segment the mouth image sequences and recognize the commands with an end-to-end deep learning model.", "label": "Author", "bboxes": [{"left": 0.12306862745098039, "top": 0.6022310606060606, "width": 0.36223366013071895, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6160681818181818, "width": 0.39719607843137256, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.629905303030303, "width": 0.20599509803921573, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "856"}, {"text": "We nally implement a working Lip-Interact system that allows users to operate the smartphone in real time.", "label": "Author", "bboxes": [{"left": 0.29898039215686273, "top": 0.629905303030303, "width": 0.18903758169934637, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6437424242424242, "width": 0.3998856209150327, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.6575795454545454, "width": 0.12736111111111115, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "857"}, {"text": "We collect silent speech data of 44 commands from 22 participants.", "label": "Author", "bboxes": [{"left": 0.3228186274509804, "top": 0.6575795454545454, "width": 0.16248039215686272, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6714166666666666, "width": 0.28817973856209145, "height": 0.012579545454545538, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "858"}, {"text": "With two controlled experiments, we show that Lip-Interact provides a more efcient and stable way of accessing functionality on a smartphone than touch, especially for single-handed input.", "label": "Author", "bboxes": [{"left": 0.453687908496732, "top": 0.7544381313131313, "width": 0.03160457516339876, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7682752525252526, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 1}, {"left": 0.0877124183006536, "top": 0.7821123737373737, "width": 0.3981601307189542, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.795949494949495, "width": 0.40004248366013073, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "859"}, {"text": "Before implementing the Lip-Interact system, we rst describe several key design principles of the technique.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.3692020202020202, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3830391414141414, "width": 0.30050163398692814, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "860"}, {"text": "In order to make silent speech truly usable on a smartphone, we add the following two design constraints:", "label": "Author", "bboxes": [{"left": 0.43440032679738566, "top": 0.7233017676767677, "width": 0.05119281045751628, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7371388888888889, "width": 0.39719117647058827, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.7509760101010101, "width": 0.24090196078431375, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "861"}, {"text": "We see this as a great opportunity to apply silent speech on mobile devices to expand the interaction channel and improve user experience.", "label": "Author", "bboxes": [{"left": 0.39432679738562093, "top": 0.0814570707070707, "width": 0.0909836601307189, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3971862745098039, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.40003431372549025, "height": 0.012579545454545454, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "862"}, {"text": "Different from previous lip-reading researches that use recorded video or news as the dataset, we validate the feasibility of Lip-Interact by collecting data and evaluating the system when users are actually using the smartphone.", "label": "Author", "bboxes": [{"left": 0.4248692810457516, "top": 0.13680429292929294, "width": 0.06044117647058822, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.3974738562091504, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.19215277777777778, "width": 0.20112418300653595, "height": 0.012579545454545454, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "863"}, {"text": "We also improve the recognition model with Spatial Transformer Networks [24] to solve the variability of mouth orientation in the mobile context.", "label": "Author", "bboxes": [{"left": 0.2942679738562092, "top": 0.19215277777777778, "width": 0.1937434640522876, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.205989898989899, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.21982702020202022, "width": 0.3742140522875817, "height": 0.012579545454545454, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "864"}, {"text": "Please note that in this work, we cannot cover all the functionalities on smartphones in Lip-Interact because of their immense quantity, especially the built-in features of various apps.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.6014924242424242, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6153295454545454, "width": 0.3974738562091502, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6291666666666667, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6430037878787879, "width": 0.0345882352941177, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "865"}, {"text": "To evaluate the feasibility of Lip-Interact, we selected 44 popular commands (including those for two representative apps) to collect data, implement system and conduct user studies.", "label": "Author", "bboxes": [{"left": 0.5644656862745098, "top": 0.6430037878787879, "width": 0.35738888888888887, "height": 0.012579545454545427, "page": 2}, {"left": 0.52409477124183, "top": 0.6568409090909091, "width": 0.40046241830065354, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.6706767676767678, "width": 0.39746078431372556, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6845138888888889, "width": 0.048357843137255, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "866"}, {"text": "We will describe the details of the commands and their elicitation process in the next section.", "label": "Author", "bboxes": [{"left": 0.7241781045751634, "top": 0.6983510101010102, "width": 0.19766993464052296, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7121881313131313, "width": 0.40003921568627454, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "867"}, {"text": "To implement the Lip-Interact prototype, we equipped a smartphone (Huawei P9 Plus with a 5.5 inch screen running Android 7.0) with a 720p/30-fps camera at the top (see Figure 5.a).", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.45580934343434343, "width": 0.4003937908496732, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.46964646464646465, "width": 0.3971862745098039, "height": 0.012579545454545482, "page": 3}, {"left": 0.0877124183006536, "top": 0.48348358585858586, "width": 0.3678627450980393, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "868"}, {"text": "is a common mobile activity, and we hoped to learn how LipInteract can assist touch and improve the experience on some tasks where touch could be limited (e.g. undo/redo, adjust cursor position).", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.37509595959595965, "width": 0.39989705882352955, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.38893308080808087, "width": 0.39718464052287594, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.40277020202020203, "width": 0.3971911764705881, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4166073232323232, "width": 0.10639215686274517, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "869"}, {"text": "We developed the Notepad app (3.b) by ourselves which supported basic text editing functionalities.", "label": "Author", "bboxes": [{"left": 0.6361029411764706, "top": 0.4163181818181818, "width": 0.2884395424836602, "height": 0.012868686868686863, "page": 3}, {"left": 0.5246633986928104, "top": 0.4304444444444444, "width": 0.36992647058823536, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "870"}, {"text": "We used Standard Chinese as the experiment language to facilitate data collection and user evaluation locally.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.6529608585858586, "width": 0.4006650326797386, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6667967171717171, "width": 0.3008267973856209, "height": 0.012579545454545538, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "871"}, {"text": "For each category, we calculated the average ranking of every item across participants and selected the top results into the nal command set.", "label": "Author", "bboxes": [{"left": 0.34018300653594774, "top": 0.7221452020202019, "width": 0.14512745098039215, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.7359823232323233, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7498194444444444, "width": 0.3947434640522876, "height": 0.012579545454545538, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "872"}, {"text": "Our recognition method built on the latest research [4, 12] on lip reading in computer vision and was improved to adapt to", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3971944444444445, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "873"}, {"text": "We also chose two representative apps for evaluation: WeChat 1 and Notepad .", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.7692020202020201, "width": 0.39802777777777776, "height": 0.014580808080808194, "page": 3}, {"left": 0.08811928104575163, "top": 0.7847512626262626, "width": 0.08617320261437907, "height": 0.012868686868686918, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "874"}, {"text": "We selected Notepad as the second app because text editing", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.8677739898989899, "width": 0.3979477124183007, "height": 0.012868686868686918, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "875"}, {"text": "Our spatial transformer module uses bilinear sampling and a 16-point thin plate spline transformation (TPS) with a regular grid of control points as the transformation function to solve relative mouth positioning towards the camera.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.4702891414141414, "width": 0.39719281045751637, "height": 0.012579545454545482, "page": 4}, {"left": 0.08689869281045752, "top": 0.4841262626262626, "width": 0.3986813725490196, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.49796338383838384, "width": 0.397186274509804, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.511800505050505, "width": 0.3084983660130719, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "876"}, {"text": "We use the ReLU activation function and batch-normalize [23] the outputs of each convolution layer.", "label": "Author", "bboxes": [{"left": 0.27533660130718957, "top": 0.8090479797979797, "width": 0.21268137254901953, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.03644771241830065, "height": 0.012579545454545538, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "877"}, {"text": "Figure 6 illustrates our deep-learning architecture, combining the steps of spatial transformation, representation learning and dynamic modelling into a single end-to-end model.", "label": "Author", "bboxes": [{"left": 0.33179738562091504, "top": 0.2593989898989899, "width": 0.15379575163398695, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.27323484848484847, "width": 0.39989869281045753, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.2870719696969697, "width": 0.3971862745098039, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.3009090909090909, "width": 0.17361928104575164, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "878"}, {"text": "We rst apply a band-pass lter on the audio signal to remove part of the background noise, and then calculate the peaks (syllables) of the signal.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.7437967171717171, "width": 0.39794934640522894, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7576338383838385, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7714709595959596, "width": 0.1291454248366013, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "879"}, {"text": "Our implementation on the smartphone uses Android Accessibility Service 2 .", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.8425037878787879, "width": 0.39989215686274504, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8543383838383838, "width": 0.11296895424836595, "height": 0.014582070707070738, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "880"}, {"text": "Participants were rst given a short introduction to our task.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.40003104575163395, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "881"}, {"text": "We use this service to monitor the state of the device, set the suitable recognition model and automatically perform input actions (e.g. tapping on a view).", "label": "Author", "bboxes": [{"left": 0.4372565359477124, "top": 0.2788964646464646, "width": 0.04805392156862748, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.29273358585858583, "width": 0.3971960784313725, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.30657070707070705, "width": 0.39719117647058827, "height": 0.012579545454545427, "page": 5}, {"left": 0.08758169934640524, "top": 0.32040782828282827, "width": 0.16814215686274508, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "882"}, {"text": "As described previously, we also create system-level oating windows to guide novice to use Lip-Interact and provide real-time visual feedbacks.", "label": "Author", "bboxes": [{"left": 0.2637761437908497, "top": 0.32040782828282827, "width": 0.22152777777777777, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.3342449494949495, "width": 0.39719117647058827, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.3480820707070707, "width": 0.33791176470588236, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "883"}, {"text": "We conducted the experiment in a laboratory space with participants sitting or standing comfortably (Figure 5.a) near a window.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.6559722222222222, "width": 0.40064869281045756, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6698093434343435, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 5}, {"left": 0.08753267973856209, "top": 0.6836464646464646, "width": 0.05397875816993465, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "884"}, {"text": "We recorded participants mouth image sequences of issuing silent speech commands using the system described above.", "label": "Author", "bboxes": [{"left": 0.14655882352941177, "top": 0.6836464646464646, "width": 0.3387565359477124, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6974835858585859, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.711320707070707, "width": 0.043321895424836596, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "885"}, {"text": "We were trying to make the data diverse and the result representative.", "label": "Author", "bboxes": [{"left": 0.42984967320261436, "top": 0.7666679292929294, "width": 0.05545588235294119, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7805050505050505, "width": 0.3909656862745098, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "886"}, {"text": "We recruited 22 participants (12M/10F, P1-P22), aged between 20 and 28 years ( M = 24.5).", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.5582436868686869, "width": 0.39795261437908497, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5717916666666666, "width": 0.1799673202614379, "height": 0.012868686868686918, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "887"}, {"text": "464%, indicating that our system could accurately identify even a new users commands.", "label": "Author", "bboxes": [{"left": 0.5727336601307189, "top": 0.755469696969697, "width": 0.35182189542483666, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7693068181818182, "width": 0.22081209150326808, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "888"}, {"text": "54% misclassied samples, we found that these errors were not evenly distributed in different classes.", "label": "Author", "bboxes": [{"left": 0.8926830065359477, "top": 0.7693068181818182, "width": 0.02983986928104565, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7831439393939394, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7969810606060606, "width": 0.2518153594771242, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "889"}, {"text": "The goal of our rst study was to verify the feasibility of using Lip-Interact on a mobile device.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.4881906565656566, "width": 0.3976895424836601, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.5020265151515152, "width": 0.24889215686274513, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "890"}, {"text": "We collected data and examined the recognition performance.", "label": "Author", "bboxes": [{"left": 0.34205555555555556, "top": 0.5020265151515152, "width": 0.14324836601307184, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5158636363636364, "width": 0.25563562091503267, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "891"}, {"text": "Before feeding the data to our model, we augmented the dataset by applying a horizontally mirrored transformation on the mouth crop images.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.397030303030303, "width": 0.3971911764705882, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.41086742424242423, "width": 0.3971960784313725, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.42470454545454545, "width": 0.1525424836601308, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "892"}, {"text": "For training and evaluation, we divided the 44 commands into four groups based on the principle of [ system functionality + current context ].", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.47561742424242426, "width": 0.3971977124183006, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.48916540404040404, "width": 0.39947058823529424, "height": 0.012868686868686863, "page": 5}, {"left": 0.5246633986928104, "top": 0.5030025252525252, "width": 0.10742973856209148, "height": 0.012868686868686918, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "893"}, {"text": "We performed model training and evaluation on each group.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.5863131313131313, "width": 0.3916666666666667, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "894"}, {"text": "We quantied the exaggeration of lip movements by comparing the Maximum Mouth Opening Degree ( MMOD ) of the \"exaggerated\" silent speech vs. normal audible speech when the participants were issuing the same commands.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.263094696969697, "width": 0.40065359477124196, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.2766426767676768, "width": 0.3971911764705882, "height": 0.012868686868686863, "page": 5}, {"left": 0.5246633986928104, "top": 0.2907689393939394, "width": 0.3971895424836601, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3046060606060606, "width": 0.3180228758169935, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "895"}, {"text": "All participants understood the \"exaggerating lip movements\" requirement after we explained the reason and felt comfortable throughout the experiment.", "label": "Author", "bboxes": [{"left": 0.6920653594771241, "top": 0.33228030303030304, "width": 0.23248529411764718, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.34611742424242425, "width": 0.39718627450980404, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.35995454545454547, "width": 0.3591895424836602, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "896"}, {"text": "For each participant, we denoted the pre-trained leave-her/him-out model as M 0 and all of her/his samples as S 0 .", "label": "Author", "bboxes": [{"left": 0.35179738562091506, "top": 0.4849924242424242, "width": 0.13555555555555548, "height": 0.012579545454545482, "page": 6}, {"left": 0.08753267973856209, "top": 0.498540404040404, "width": 0.3969477124183006, "height": 0.02789898989898998, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "897"}, {"text": "The task simulated daily smartphone usage, including 22 of the 44 functionalities supported by our Lip-Interact system.", "label": "Author", "bboxes": [{"left": 0.679016339869281, "top": 0.6992487373737373, "width": 0.24283986928104595, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.7130858585858586, "width": 0.39746568627450984, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7269217171717172, "width": 0.1341013071895425, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "898"}, {"text": "Our deep learning model has sufcient generalization ability for new users with a recognition accuracy of over 95%.", "label": "Author", "bboxes": [{"left": 0.370390522875817, "top": 0.8367222222222221, "width": 0.11491503267973857, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.2230604575163399, "height": 0.012579545454545538, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "899"}, {"text": "We recruited 10 right-handed participants (P1-P10, 5M/5F), aged between 22 and 30.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.56177398989899, "width": 0.3999983660130719, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5756111111111112, "width": 0.15590032679738564, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "900"}, {"text": "An analysis using Friedman test showed that for our tasks participants reported signicantly stronger agreement for Lip-Interact than the two touch conditions on input easiness ( p = . 00015,  2 r = 17 . 64).", "label": "Author", "bboxes": [{"left": 0.664124183006536, "top": 0.5093952020202019, "width": 0.2577254901960784, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5232310606060606, "width": 0.3974738562091503, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5370681818181818, "width": 0.3971911764705882, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.5506161616161617, "width": 0.26988725490196086, "height": 0.013835858585858563, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "901"}, {"text": "We calculated the whole input time from the beginning of the word selection to the end of marking it.", "label": "Author", "bboxes": [{"left": 0.694640522875817, "top": 0.8505593434343435, "width": 0.2272189542483659, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39718627450980404, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.012859477124183005, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "902"}, {"text": "We measured input time in seconds to quantify the efciency.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.31793560606060606, "width": 0.40079901960784314, "height": 0.012868686868686863, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "903"}, {"text": "For analysis, we divided the tasks into six types based on their touch interaction properties.", "label": "Author", "bboxes": [{"left": 0.16533333333333333, "top": 0.3735732323232323, "width": 0.3199771241830065, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.3874103535353535, "width": 0.27736111111111117, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "904"}, {"text": "We collected a total of 90 blocks of task (10 participants  3 input conditions  3 replications).", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.704364898989899, "width": 0.39793954248366015, "height": 0.012856060606060593, "page": 7}, {"left": 0.08811928104575163, "top": 0.7182020202020203, "width": 0.22248202614379087, "height": 0.012856060606060593, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "905"}, {"text": "For each task type, we ran a one-way repeated-measures ANOVA for input condition .", "label": "Author", "bboxes": [{"left": 0.3156781045751634, "top": 0.7184785353535353, "width": 0.16962418300653598, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7320265151515152, "width": 0.38244117647058823, "height": 0.012868686868686807, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "906"}, {"text": "With each technique, P a was free to use the device for three minutes, accessing any of the functionalities that our system supported.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.6786704545454545, "width": 0.39794444444444443, "height": 0.013834595959596019, "page": 8}, {"left": 0.08811928104575163, "top": 0.6927967171717172, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7066338383838384, "width": 0.0696421568627451, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "907"}, {"text": "For the implementation of input control, we used the XunFei SDK 3 for command recognition.", "label": "Author", "bboxes": [{"left": 0.27230228758169933, "top": 0.8034924242424242, "width": 0.2130081699346405, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8153282828282828, "width": 0.3999705882352942, "height": 0.014580808080808083, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "908"}, {"text": "We argue that this simple difference signicantly enhances the user experience.", "label": "Author", "bboxes": [{"left": 0.36656535947712415, "top": 0.4641313131313131, "width": 0.1187369281045752, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.47796717171717173, "width": 0.4000392156862745, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "909"}, {"text": "In this study, we preliminarily investigate the user preference between Lip-Interact and voice input in public environment.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.49180429292929295, "width": 0.3971846405228758, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5056414141414142, "width": 0.400031045751634, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "910"}, {"text": "Finally, we encouraged participants to suggest more applications for Lip-Interact.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.5893914141414142, "width": 0.39989052287581706, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.6032272727272727, "width": 0.13901470588235298, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "911"}, {"text": "We list several representative examples.", "label": "Author", "bboxes": [{"left": 0.6687287581699346, "top": 0.6032272727272727, "width": 0.2559607843137255, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "912"}, {"text": "This example application comes from a real-life experience of one of our participants.", "label": "Author", "bboxes": [{"left": 0.5241601307189543, "top": 0.8782335858585859, "width": 0.39768954248366006, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.1733562091503268, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "913"}, {"text": "We chose the subway as our experimental environment.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.5607159090909091, "width": 0.354828431372549, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "914"}, {"text": "Although in our study, Lip-Interact outperformed touch-based input on most of the investigated tasks with a shorter completion time and higher input easiness, our purpose is not to substitute touch with Lip-Interact but to provide an alternative input approach.", "label": "Author", "bboxes": [{"left": 0.08753267973856209, "top": 0.5989027777777778, "width": 0.39776960784313725, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.612739898989899, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6265770202020202, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6404141414141414, "width": 0.3998986928104575, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.6542512626262627, "width": 0.12908006535947714, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "915"}, {"text": "Although the size of the command set is still limited in our work, Lip-Interact can be easy to be generalized to support other applications.", "label": "Author", "bboxes": [{"left": 0.6197107843137255, "top": 0.15064141414141416, "width": 0.30214379084967313, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.16447853535353535, "width": 0.39989869281045765, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.17831565656565657, "width": 0.22439705882352945, "height": 0.012579545454545454, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "916"}, {"text": "Our empirical experience indicates that as long as the mouth image sequence of the commands can be distinguished by human eyes, the model can accurately recognize them.", "label": "Author", "bboxes": [{"left": 0.7541209150326798, "top": 0.17831565656565657, "width": 0.1677320261437908, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.19215277777777778, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.205989898989899, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.21982702020202022, "width": 0.19733333333333336, "height": 0.012579545454545454, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "917"}, {"text": "For the \"exaggeration\" requirement, our participants understood the necessity, and did not report any discomfort during the studies.", "label": "Author", "bboxes": [{"left": 0.7743251633986928, "top": 0.23366414141414144, "width": 0.14752287581699342, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.2475012626262626, "width": 0.3971911764705881, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.2613383838383838, "width": 0.3144035947712419, "height": 0.012579545454545482, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "918"}, {"text": "At the same time, we admit that this is an interaction limitation on users.", "label": "Author", "bboxes": [{"left": 0.8441143790849672, "top": 0.2613383838383838, "width": 0.07773366013071914, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.27517424242424243, "width": 0.39070588235294124, "height": 0.012579545454545482, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "919"}, {"text": "For the intention recognition, at present we solve it by detecting the changes of the mouth opening degree and the audio signal.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.40182449494949496, "width": 0.39990032679738563, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.4156616161616162, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.4294987373737374, "width": 0.04231699346405238, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "920"}, {"text": "It worked well under our experimental conditions.", "label": "Author", "bboxes": [{"left": 0.5720147058823529, "top": 0.4294987373737374, "width": 0.3216356209150327, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "921"}, {"text": "But we do not rule out that the performance will be affected by more noise in real-life use cases.", "label": "Author", "bboxes": [{"left": 0.8986846405228758, "top": 0.4294987373737374, "width": 0.023171568627451067, "height": 0.012579545454545427, "page": 9}, {"left": 0.5240784313725491, "top": 0.4433358585858586, "width": 0.3983366013071895, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.45717297979797983, "width": 0.2174591503267974, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "922"}, {"text": "To address this, we can also consider some explicit mode-switch methods, such as pressing a button or squeezing the phones edge.", "label": "Author", "bboxes": [{"left": 0.853998366013072, "top": 0.4710088383838384, "width": 0.06785130718954246, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.4848459595959596, "width": 0.39922875816993475, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.4986830808080808, "width": 0.35940849673202624, "height": 0.012579545454545482, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "923"}, {"text": "Our research shows high accuracy for recognizing about 20 commands.", "label": "Author", "bboxes": [{"left": 0.6774019607843137, "top": 0.5540315656565656, "width": 0.24472222222222229, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5678686868686869, "width": 0.21250653594771252, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "924"}, {"text": "This work focuses on the implementation and evaluation of Lip-Interact for smartphone, but we also foresee the potential of Lip-Interact on other platforms.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.19111742424242426, "width": 0.3976895424836601, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.20495454545454547, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.21879166666666666, "width": 0.22865032679738562, "height": 0.012579545454545482, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "925"}, {"text": "We envision that a tiny camera is integrated into the microphone, capturing users mouth movement and enabling Lip-Interact.", "label": "Author", "bboxes": [{"left": 0.4033986928104575, "top": 0.32948737373737375, "width": 0.0819117647058824, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.34332449494949496, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.3571616161616162, "width": 0.3304722222222223, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "926"}, {"text": "Our implementation uses an additional mounted camera and host server for computing.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.6316224747474748, "width": 0.3971830065359476, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.645459595959596, "width": 0.17984803921568637, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "927"}, {"text": "We are trying to integrate LipInteract into a self-contained smartphone, which will reduce the response time and improve the user experience further.", "label": "Author", "bboxes": [{"left": 0.7146584967320261, "top": 0.645459595959596, "width": 0.20990359477124187, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.6592967171717172, "width": 0.3971895424836601, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.6731338383838383, "width": 0.3678480392156862, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "928"}, {"text": "Our experiment results reect the performance of Lip-Interact in a controlled setting.", "label": "Author", "bboxes": [{"left": 0.8973284313725489, "top": 0.6731338383838383, "width": 0.024808823529411717, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.6869709595959597, "width": 0.3971960784313725, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.7008080808080808, "width": 0.1152826797385621, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "929"}, {"text": "We compared Lip-Interact with touch input on only one device.", "label": "Author", "bboxes": [{"left": 0.6450032679738562, "top": 0.7008080808080808, "width": 0.27684640522875825, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.7146452020202021, "width": 0.1256650326797385, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "930"}, {"text": "This is also why we classied each input into different types and analyzed respectively.", "label": "Author", "bboxes": [{"left": 0.7258692810457517, "top": 0.7284810606060605, "width": 0.19598366013071888, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.7423181818181819, "width": 0.3712271241830065, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "931"}, {"text": "Besides, we have only tested the Chinese language.", "label": "Author", "bboxes": [{"left": 0.9009297385620916, "top": 0.7423181818181819, "width": 0.023624183006535904, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.756155303030303, "width": 0.3101258169934642, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "932"}, {"text": "But we think Lip-Interact can be easily applied to other languages as long as the mouth movements of commands are differentiable.", "label": "Author", "bboxes": [{"left": 0.8398153594771242, "top": 0.756155303030303, "width": 0.08244934640522883, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.7699924242424242, "width": 0.3971944444444445, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.7838295454545454, "width": 0.3734656862745098, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "933"}, {"text": "We have described Lip-Interact, a novel interaction technique that repurposes the front camera of smartphones to detect lip movement and recognize it into commands.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.3976376262626262, "width": 0.39795261437908497, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.41147474747474744, "width": 0.39719444444444446, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.42531186868686865, "width": 0.28545751633986927, "height": 0.012579545454545482, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "934"}, {"text": "We verify the feasibility of LipInteract by implementing a real-working prototype and conducting three experiments.", "label": "Author", "bboxes": [{"left": 0.2758888888888889, "top": 0.4529861111111111, "width": 0.2121290849673202, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4668232323232323, "width": 0.3998986928104575, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4806590909090909, "width": 0.1685686274509804, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "935"}, {"text": "We demonstrate that the model of a [ spatial transformer network + CNN + RNN ] architecture can accurately recognize around 20 silent speech commands with training data collected from dozens of users.", "label": "Author", "bboxes": [{"left": 0.26155392156862745, "top": 0.4806590909090909, "width": 0.2237516339869281, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.4942070707070707, "width": 0.39717647058823524, "height": 0.012868686868686807, "page": 9}, {"left": 0.08811928104575163, "top": 0.5083333333333334, "width": 0.39719117647058827, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5221704545454546, "width": 0.29161928104575163, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "936"}, {"text": "We incorporate voice check and design visual feedback to offer an accurate, robust and friendly user experience.", "label": "Author", "bboxes": [{"left": 0.38483496732026146, "top": 0.5221704545454546, "width": 0.10047875816993468, "height": 0.012579545454545427, "page": 9}, {"left": 0.0877124183006536, "top": 0.5360075757575757, "width": 0.3996323529411765, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.549844696969697, "width": 0.23250326797385623, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "937"}, {"text": "We also research on-line personalization, which conrms the potential and benet of adapting the machine learning model to end users.", "label": "Author", "bboxes": [{"left": 0.3256699346405229, "top": 0.549844696969697, "width": 0.15964379084967317, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.5636818181818182, "width": 0.39719117647058827, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5775189393939394, "width": 0.3273464052287582, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "938"}, {"text": "This work is supported by the National Key Research and Development Plan under Grant No. 2016YFB1001200, the Natural Science Foundation of China under Grant No. 61672314 and No. 61572276, Tsinghua University Research Funding No. 20151080408, and also by Beijing Key Lab of Networked Multimedia.", "label": "Author", "bboxes": [{"left": 0.5241601307189543, "top": 0.822885101010101, "width": 0.4003986928104576, "height": 0.012579545454545427, "page": 9}, {"left": 0.5242565359477125, "top": 0.8367222222222221, "width": 0.400299019607843, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.3971911764705882, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.08004575163398697, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "939"}, {"text": "In order to raise the recognition accuracy of silent speech to a practically usable level in a mobile input system, we add two", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.3971960784313725, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "940"}, {"text": "Lastly, we discuss the main limitations of our work, which also points to the directions of future work.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.8278421717171717, "width": 0.39719117647058827, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.841679292929293, "width": 0.2810800653594771, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "941"}, {"text": "Finally, in contrast to general audible voice input, the main difference is that Lip-Interact commands are silent and recognized by the camera.", "label": "Novelty", "bboxes": [{"left": 0.4192238562091503, "top": 0.3380707070707071, "width": 0.06608006535947708, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.3519078282828283, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.36574368686868686, "width": 0.3971895424836602, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.3795808080808081, "width": 0.05004738562091503, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "942"}, {"text": "Touch on GUIs is straightforward but may require a lot of search and multiple operations to reach the desired item [13].", "label": "Novelty", "bboxes": [{"left": 0.5241601307189543, "top": 0.12359722222222222, "width": 0.3976895424836602, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.13743434343434344, "width": 0.40003267973856205, "height": 0.012579545454545454, "page": 1}], "section": "Accessing Functionality with Gestures", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "943"}, {"text": "However, with larger sets, the design and the scalability of the gesture set, the effort required to learn and memorize it limit its widespread use.", "label": "Novelty", "bboxes": [{"left": 0.560828431372549, "top": 0.19278156565656565, "width": 0.36158986928104575, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.20661868686868687, "width": 0.3971895424836601, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.22045580808080809, "width": 0.15826797385620928, "height": 0.012579545454545454, "page": 1}], "section": "Accessing Functionality with Gestures", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "944"}, {"text": "[22] adopted a customized helmet where an ultrasound transducer was locked at a xed position beneath the chin while an optical camera was xed in front of the mouth.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39989869281045753, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.3124558823529413, "height": 0.012579545454545538, "page": 1}], "section": "Silent Speech Interface and Lip Reading", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "945"}, {"text": "The key difference between AlterEgo and previous SSIs is that their system works even when the user does not open the mouth.", "label": "Novelty", "bboxes": [{"left": 0.8699330065359477, "top": 0.8015012626262626, "width": 0.05248529411764702, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.8153383838383839, "width": 0.3974738562091502, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.8291742424242424, "width": 0.40003104575163406, "height": 0.012579545454545427, "page": 1}], "section": "Silent Speech Interface and Lip Reading", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "946"}, {"text": "However, these prior works all use an invasive setup, impeding the scalability of these solutions in real-world scenarios.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.8430113636363636, "width": 0.39718627450980404, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.8568484848484849, "width": 0.3660751633986927, "height": 0.012579545454545427, "page": 1}], "section": "Silent Speech Interface and Lip Reading", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "947"}, {"text": "Lip-Interact moves one step further by keeping the external form of voice interaction but not vocalizing any sound.", "label": "Novelty", "bboxes": [{"left": 0.5621078431372549, "top": 0.5109785353535353, "width": 0.36245424836601303, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.5248156565656565, "width": 0.40003267973856216, "height": 0.012579545454545427, "page": 1}], "section": "Multimodal Interfaces and Voice Interaction", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "948"}, {"text": "The user silently issues commands by mouthing the verbal commands but not vocalizing the sound.", "label": "Novelty", "bboxes": [{"left": 0.4299150326797386, "top": 0.313854797979798, "width": 0.055683006535947654, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.32769191919191915, "width": 0.39719117647058827, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.34152777777777776, "width": 0.18572712418300652, "height": 0.012579545454545427, "page": 2}], "section": "LIP-INTERACT DESIGNS", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "949"}, {"text": "However, silent speech recognition has intrinsic limitations.", "label": "Novelty", "bboxes": [{"left": 0.1463970588235294, "top": 0.5987689393939394, "width": 0.34162254901960787, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6126060606060606, "width": 0.053052287581699345, "height": 0.012579545454545427, "page": 2}], "section": "High Level of Recognition Accuracy", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "950"}, {"text": "Different from previous lip-reading researches that use recorded video or news as the dataset, we validate the feasibility of Lip-Interact by collecting data and evaluating the system when users are actually using the smartphone.", "label": "Novelty", "bboxes": [{"left": 0.4248692810457516, "top": 0.13680429292929294, "width": 0.06044117647058822, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.3974738562091504, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.19215277777777778, "width": 0.20112418300653595, "height": 0.012579545454545454, "page": 2}], "section": "Silent Speech Interface and Lip Reading", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "951"}, {"text": "For example, a user may prefer to type with ngers while using Lip-Interact for fast text editing (e.g. bold, highlight, undo, etc).", "label": "Novelty", "bboxes": [{"left": 0.27233823529411766, "top": 0.4909040404040404, "width": 0.2129673202614379, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.5047411616161616, "width": 0.39719117647058827, "height": 0.012579545454545538, "page": 2}, {"left": 0.08758169934640524, "top": 0.5185782828282828, "width": 0.20717156862745095, "height": 0.012579545454545427, "page": 2}], "section": "Compatible with Existing Touch Interaction", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "952"}, {"text": "Lip-Interact is context aware: covering different aspects of functionality when using different applications.", "label": "Novelty", "bboxes": [{"left": 0.6135147058823529, "top": 0.38639015151515155, "width": 0.30834150326797394, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.40022727272727276, "width": 0.3674803921568627, "height": 0.012579545454545427, "page": 2}], "section": "Context-Aware Functionality Support", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "953"}, {"text": "However, with full access to a cutting-edge devices camera and neural processing unit, it seems likely that Lip-Interact could be realized on a real product with no additional computing resources.", "label": "Novelty", "bboxes": [{"left": 0.31505228758169934, "top": 0.5665050505050505, "width": 0.17025, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5803421717171717, "width": 0.39922385620915035, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.594179292929293, "width": 0.3971911764705882, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.6080164141414142, "width": 0.315609477124183, "height": 0.012579545454545427, "page": 3}], "section": "SYSTEM IMPLEMENTATION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "954"}, {"text": "Unlike conventional lip-reading tasks [47, 4, 12] where the mouth position is relatively xed, for Lip-Interact in smartphone use, the orientation between the user and the camera may vary each time a command is issued, and may even change while the user issues a single command.", "label": "Novelty", "bboxes": [{"left": 0.28326633986928107, "top": 0.4011035353535353, "width": 0.20203921568627448, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.41494065656565654, "width": 0.3992320261437909, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.42877777777777776, "width": 0.397186274509804, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.442614898989899, "width": 0.39922875816993464, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4564520202020202, "width": 0.40003921568627454, "height": 0.012579545454545482, "page": 4}], "section": "Spatial Transformer", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "955"}, {"text": "For example, talking to other people while using the device will also result in mouth movement but should not be detected as issuing a Lip-Interact command.", "label": "Novelty", "bboxes": [{"left": 0.8994460784313726, "top": 0.6532272727272728, "width": 0.022681372549019585, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.667064393939394, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6809015151515152, "width": 0.3971928104575164, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6947386363636363, "width": 0.20527777777777778, "height": 0.012579545454545538, "page": 4}], "section": "Post- Vocal Check", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "956"}, {"text": "screen showed the progress: red indicated that the participant was issuing a lip command with signicant mouth movements, while green indicated the other states.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.0814570707070707, "width": 0.3971944444444444, "height": 0.012579545454545468, "page": 5}, {"left": 0.5240784313725491, "top": 0.09529419191919192, "width": 0.3998022875816992, "height": 0.012579545454545468, "page": 5}, {"left": 0.5240784313725491, "top": 0.10913131313131313, "width": 0.24445424836601304, "height": 0.012579545454545454, "page": 5}], "section": "Experimental Setup and Design", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "957"}, {"text": "54% misclassied samples, we found that these errors were not evenly distributed in different classes.", "label": "Novelty", "bboxes": [{"left": 0.8926830065359477, "top": 0.7693068181818182, "width": 0.02983986928104565, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7831439393939394, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7969810606060606, "width": 0.2518153594771242, "height": 0.012579545454545538, "page": 5}], "section": "Leave-One-Subject-Out Cross-Validation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "958"}, {"text": "But for the command \" Like \", 25 of the 47 samples were misclassied into \" Bluetooth \".", "label": "Novelty", "bboxes": [{"left": 0.21169444444444444, "top": 0.3895858585858586, "width": 0.2744346405228758, "height": 0.012868686868686918, "page": 6}, {"left": 0.08811928104575163, "top": 0.4034229797979798, "width": 0.29047385620915034, "height": 0.012868686868686863, "page": 6}], "section": "Leave-One-Subject-Out Cross-Validation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "959"}, {"text": "P4 commented: \" For the task of selecting a word and then highlight/bold it, I initially performed the two steps in sequence in the first block. But since using Lip-Interact always took some time, in the next blocks, I tried to start issuing the command before I nished the selection. It is faster .\"", "label": "Novelty", "bboxes": [{"left": 0.7563856209150327, "top": 0.781084595959596, "width": 0.16546895424836594, "height": 0.012868686868686807, "page": 7}, {"left": 0.5246633986928104, "top": 0.7949217171717171, "width": 0.39718300653594774, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.8087588383838384, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8225959595959595, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8364330808080809, "width": 0.39718300653594774, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.850270202020202, "width": 0.16590686274509814, "height": 0.012868686868686918, "page": 7}], "section": "User Behavior and Feedback with Lip-Interact", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "960"}, {"text": "Type II : Accessing between pages across different levels of the hierarchy.", "label": "Novelty", "bboxes": [{"left": 0.08768954248366012, "top": 0.45030555555555557, "width": 0.39761274509803923, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.4641426767676768, "width": 0.06570588235294118, "height": 0.012579545454545482, "page": 7}], "section": "Measures", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "961"}, {"text": "First, P a simulated the person using the phone, while P b simulated a stranger nearby.", "label": "Novelty", "bboxes": [{"left": 0.18786764705882353, "top": 0.637159090909091, "width": 0.29947222222222225, "height": 0.013835858585858563, "page": 8}, {"left": 0.08753267973856209, "top": 0.6509962121212122, "width": 0.23995915032679743, "height": 0.014030303030302949, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "962"}, {"text": "The main difference is that LipInteract does not require the vocalization.", "label": "Novelty", "bboxes": [{"left": 0.2836519607843137, "top": 0.4502941919191919, "width": 0.20435294117647063, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.4641313131313131, "width": 0.27341013071895426, "height": 0.012579545454545482, "page": 8}], "section": "STUDY 3: EVALUATE LIP-INTERACT VS. VOICE INPUT", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "963"}, {"text": "We argue that this simple difference signicantly enhances the user experience.", "label": "Novelty", "bboxes": [{"left": 0.36656535947712415, "top": 0.4641313131313131, "width": 0.1187369281045752, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.47796717171717173, "width": 0.4000392156862745, "height": 0.012579545454545427, "page": 8}], "section": "STUDY 3: EVALUATE LIP-INTERACT VS. VOICE INPUT", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "964"}, {"text": "For example, holding the device with one hand while having meals, or waching the phone that is xed on a stand while lying down.", "label": "Novelty", "bboxes": [{"left": 0.6612761437908498, "top": 0.7819520202020203, "width": 0.2605735294117646, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.7957891414141414, "width": 0.39718627450980404, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.8096262626262627, "width": 0.168156862745098, "height": 0.012579545454545427, "page": 8}], "section": "MORE EXAMPLE APPLICATIONS", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "965"}, {"text": "With Lip-Interact, however, \" it was much less noticeable \".", "label": "Novelty", "bboxes": [{"left": 0.7476192810457516, "top": 0.4440517676767677, "width": 0.1762565359477125, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.45759974747474746, "width": 0.1952712418300655, "height": 0.012868686868686863, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "966"}, {"text": "However, participants also thought that there was room for improvement for Lip-Interact.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.2983611111111111, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.3121982323232323, "width": 0.1755702614379085, "height": 0.012579545454545427, "page": 8}], "section": "User Behavior and Feedback with Lip-Interact", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "967"}, {"text": "P5 said \" There was a time when I wanted to say back to home instead of the required home. Although I ended up successfully issuing the correct command, a more exible mapping would further improve the experience of this technology. \"", "label": "Novelty", "bboxes": [{"left": 0.4682352941176471, "top": 0.3260340909090909, "width": 0.01705392156862745, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.33958207070707075, "width": 0.3998986928104575, "height": 0.012868686868686863, "page": 8}, {"left": 0.08811928104575163, "top": 0.35341919191919197, "width": 0.3998888888888889, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.36725631313131313, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.38109343434343435, "width": 0.3730751633986928, "height": 0.012868686868686863, "page": 8}], "section": "User Behavior and Feedback with Lip-Interact", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "968"}, {"text": "P5 commented \" The feeling of being a passerby was related to the current environment. It did not matter in most cases, but sometimes I could not help but wonder what he or she was doing \".", "label": "Novelty", "bboxes": [{"left": 0.8225735294117648, "top": 0.4931098484848485, "width": 0.09928104575163388, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.5066578282828283, "width": 0.3971911764705881, "height": 0.012868686868686807, "page": 8}, {"left": 0.5246633986928104, "top": 0.5204949494949496, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 8}, {"left": 0.5243872549019608, "top": 0.5343320707070707, "width": 0.3686045751633986, "height": 0.012868686868686918, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "969"}, {"text": "But for Lip-Interact, \" I rarely detected a noticeable difference. \".", "label": "Novelty", "bboxes": [{"left": 0.898032679738562, "top": 0.5346212121212122, "width": 0.023805555555555524, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.5481691919191919, "width": 0.3892401960784314, "height": 0.012868686868686918, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "970"}, {"text": "Although in our study, Lip-Interact outperformed touch-based input on most of the investigated tasks with a shorter completion time and higher input easiness, our purpose is not to substitute touch with Lip-Interact but to provide an alternative input approach.", "label": "Novelty", "bboxes": [{"left": 0.08753267973856209, "top": 0.5989027777777778, "width": 0.39776960784313725, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.612739898989899, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6265770202020202, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6404141414141414, "width": 0.3998986928104575, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.6542512626262627, "width": 0.12908006535947714, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "971"}, {"text": "Although the size of the command set is still limited in our work, Lip-Interact can be easy to be generalized to support other applications.", "label": "Novelty", "bboxes": [{"left": 0.6197107843137255, "top": 0.15064141414141416, "width": 0.30214379084967313, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.16447853535353535, "width": 0.39989869281045765, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.17831565656565657, "width": 0.22439705882352945, "height": 0.012579545454545454, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "972"}, {"text": "But we do not rule out that the performance will be affected by more noise in real-life use cases.", "label": "Novelty", "bboxes": [{"left": 0.8986846405228758, "top": 0.4294987373737374, "width": 0.023171568627451067, "height": 0.012579545454545427, "page": 9}, {"left": 0.5240784313725491, "top": 0.4433358585858586, "width": 0.3983366013071895, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.45717297979797983, "width": 0.2174591503267974, "height": 0.012579545454545427, "page": 9}], "section": "System Robustness", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "973"}, {"text": "This work focuses on the implementation and evaluation of Lip-Interact for smartphone, but we also foresee the potential of Lip-Interact on other platforms.", "label": "Novelty", "bboxes": [{"left": 0.08761437908496732, "top": 0.19111742424242426, "width": 0.3976895424836601, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.20495454545454547, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.21879166666666666, "width": 0.22865032679738562, "height": 0.012579545454545482, "page": 9}], "section": "Smart Watch and Head-worn Device", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "974"}, {"text": "For current mainstream solutions, mid-air gestures suffer from limited size of the command set and arm fatigue, while voice input has the inherent issue about privacy and social norms.", "label": "Novelty", "bboxes": [{"left": 0.41206372549019604, "top": 0.28797727272727275, "width": 0.07325000000000004, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.3018131313131313, "width": 0.397186274509804, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.31565025252525253, "width": 0.397186274509804, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.32948737373737375, "width": 0.3088529411764706, "height": 0.012579545454545427, "page": 9}], "section": "Smart Watch and Head-worn Device", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "975"}, {"text": "Different device sizes and UI designs can lead to different performance.", "label": "Novelty", "bboxes": [{"left": 0.6553627450980392, "top": 0.7146452020202021, "width": 0.2664967320261439, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.7284810606060605, "width": 0.19614869281045766, "height": 0.012579545454545427, "page": 9}], "section": "Other Limitations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "976"}, {"text": "This is also why we classied each input into different types and analyzed respectively.", "label": "Novelty", "bboxes": [{"left": 0.7258692810457517, "top": 0.7284810606060605, "width": 0.19598366013071888, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.7423181818181819, "width": 0.3712271241830065, "height": 0.012579545454545427, "page": 9}], "section": "Other Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "977"}, {"text": "But we think Lip-Interact can be easily applied to other languages as long as the mouth movements of commands are differentiable.", "label": "Novelty", "bboxes": [{"left": 0.8398153594771242, "top": 0.756155303030303, "width": 0.08244934640522883, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.7699924242424242, "width": 0.3971944444444445, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.7838295454545454, "width": 0.3734656862745098, "height": 0.012579545454545427, "page": 9}], "section": "Other Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "978"}, {"text": "The goal of Lip-Interact is to provide a simple, yet powerful approach to issuing commands on the smartphone.", "label": "Objective", "bboxes": [{"left": 0.08761437908496732, "top": 0.21353661616161618, "width": 0.39768954248366006, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.32528431372549016, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "979"}, {"text": "Since touch is straightforward and performs well in many contexts on mobile devices, the goal of Lip-Interact is to offer users an alternative based on their current cognitive and motor conditions when touch is inferior.", "label": "Objective", "bboxes": [{"left": 0.08811928104575163, "top": 0.4217184343434343, "width": 0.39775653594771243, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.43555555555555553, "width": 0.3974673202614379, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.44939267676767675, "width": 0.3974738562091504, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.46322979797979796, "width": 0.21304738562091502, "height": 0.012579545454545482, "page": 2}], "section": "Compatible with Existing Touch Interaction", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "980"}, {"text": "WeChat is a multi-purpose social media mobile app and the largest app by active users in China.", "label": "Objective", "bboxes": [{"left": 0.179390522875817, "top": 0.7847512626262626, "width": 0.3059199346405228, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.7988775252525253, "width": 0.31482843137254907, "height": 0.012579545454545427, "page": 3}], "section": "Command Set", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "981"}, {"text": "The goal of our first study was to verify the feasibility of using Lip-Interact on a mobile device.", "label": "Objective", "bboxes": [{"left": 0.08761437908496732, "top": 0.4881906565656566, "width": 0.3976895424836601, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.5020265151515152, "width": 0.24889215686274513, "height": 0.012579545454545427, "page": 5}], "section": "Real-time considerations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "982"}, {"text": "Research questions include:", "label": "Objective", "bboxes": [{"left": 0.7069444444444444, "top": 0.45826388888888886, "width": 0.18175980392156865, "height": 0.012579545454545482, "page": 6}], "section": "Task and Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "983"}, {"text": "Although in our study, Lip-Interact outperformed touch-based input on most of the investigated tasks with a shorter completion time and higher input easiness, our purpose is not to substitute touch with Lip-Interact but to provide an alternative input approach.", "label": "Objective", "bboxes": [{"left": 0.08753267973856209, "top": 0.5989027777777778, "width": 0.39776960784313725, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.612739898989899, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6265770202020202, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6404141414141414, "width": 0.3998986928104575, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.6542512626262627, "width": 0.12908006535947714, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "984"}, {"text": "In this paper, we introduce Lip-Interact, which enables users to access functionality on the smartphone by issuing silent speech commands (Figure 1).", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.3971944444444445, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.2023153594771242, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "985"}, {"text": "To our knowledge, none has studied to use the on-device front camera to combine silent speech with the smartphone as a command input approach.", "label": "Method", "bboxes": [{"left": 0.1965326797385621, "top": 0.4839873737373737, "width": 0.2887777777777778, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.49782449494949493, "width": 0.3971846405228758, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.5116616161616161, "width": 0.28441013071895427, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "986"}, {"text": "We discuss the design principles and present a set of interface mechanisms that support novice learning, visual feedback and online model personalization.", "label": "Method", "bboxes": [{"left": 0.30546405228758167, "top": 0.5607196969696969, "width": 0.18255555555555558, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5745568181818181, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5883939393939394, "width": 0.39989869281045753, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.6022310606060606, "width": 0.029900326797385607, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "987"}, {"text": "In order to make silent speech truly usable on a smartphone, we add the following two design constraints:", "label": "Method", "bboxes": [{"left": 0.43440032679738566, "top": 0.7233017676767677, "width": 0.05119281045751628, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7371388888888889, "width": 0.39719117647058827, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.7509760101010101, "width": 0.24090196078431375, "height": 0.012579545454545427, "page": 2}], "section": "High Level of Recognition Accuracy", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "988"}, {"text": "We see this as a great opportunity to apply silent speech on mobile devices to expand the interaction channel and improve user experience.", "label": "Method", "bboxes": [{"left": 0.39432679738562093, "top": 0.0814570707070707, "width": 0.0909836601307189, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3971862745098039, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.40003431372549025, "height": 0.012579545454545454, "page": 2}], "section": "Silent Speech Interface and Lip Reading", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "989"}, {"text": "Please note that in this work, we cannot cover all the functionalities on smartphones in Lip-Interact because of their immense quantity, especially the built-in features of various apps.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.6014924242424242, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6153295454545454, "width": 0.3974738562091502, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6291666666666667, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6430037878787879, "width": 0.0345882352941177, "height": 0.012579545454545427, "page": 2}], "section": "Context-Aware Functionality Support", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "990"}, {"text": "To implement the Lip-Interact prototype, we equipped a smartphone (Huawei P9 Plus with a 5.5 inch screen running Android 7.0) with a 720p/30-fps camera at the top (see Figure 5.a).", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.45580934343434343, "width": 0.4003937908496732, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.46964646464646465, "width": 0.3971862745098039, "height": 0.012579545454545482, "page": 3}, {"left": 0.0877124183006536, "top": 0.48348358585858586, "width": 0.3678627450980393, "height": 0.012579545454545482, "page": 3}], "section": "SYSTEM IMPLEMENTATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "991"}, {"text": "is a common mobile activity, and we hoped to learn how LipInteract can assist touch and improve the experience on some tasks where touch could be limited (e.g. undo/redo, adjust cursor position).", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.37509595959595965, "width": 0.39989705882352955, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.38893308080808087, "width": 0.39718464052287594, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.40277020202020203, "width": 0.3971911764705881, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4166073232323232, "width": 0.10639215686274517, "height": 0.012579545454545482, "page": 3}], "section": "Command Set", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "992"}, {"text": "We used Standard Chinese as the experiment language to facilitate data collection and user evaluation locally.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.6529608585858586, "width": 0.4006650326797386, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6667967171717171, "width": 0.3008267973856209, "height": 0.012579545454545538, "page": 3}], "section": "Command Set", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "993"}, {"text": "Our recognition method built on the latest research [4, 12] on lip reading in computer vision and was improved to adapt to", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3971944444444445, "height": 0.012579545454545427, "page": 3}], "section": "End-to-End Deep-Learning Command Recognition", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "994"}, {"text": "We also chose two representative apps for evaluation: WeChat 1 and Notepad .", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.7692020202020201, "width": 0.39802777777777776, "height": 0.014580808080808194, "page": 3}, {"left": 0.08811928104575163, "top": 0.7847512626262626, "width": 0.08617320261437907, "height": 0.012868686868686918, "page": 3}], "section": "Command Set", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "995"}, {"text": "Our spatial transformer module uses bilinear sampling and a 16-point thin plate spline transformation (TPS) with a regular grid of control points as the transformation function to solve relative mouth positioning towards the camera.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.4702891414141414, "width": 0.39719281045751637, "height": 0.012579545454545482, "page": 4}, {"left": 0.08689869281045752, "top": 0.4841262626262626, "width": 0.3986813725490196, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.49796338383838384, "width": 0.397186274509804, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.511800505050505, "width": 0.3084983660130719, "height": 0.012579545454545427, "page": 4}], "section": "Spatial Transformer", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "996"}, {"text": "We use the ReLU activation function and batch-normalize [23] the outputs of each convolution layer.", "label": "Method", "bboxes": [{"left": 0.27533660130718957, "top": 0.8090479797979797, "width": 0.21268137254901953, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.03644771241830065, "height": 0.012579545454545538, "page": 4}], "section": "Implementation and Training Details", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "997"}, {"text": "Figure 6 illustrates our deep-learning architecture, combining the steps of spatial transformation, representation learning and dynamic modelling into a single end-to-end model.", "label": "Method", "bboxes": [{"left": 0.33179738562091504, "top": 0.2593989898989899, "width": 0.15379575163398695, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.27323484848484847, "width": 0.39989869281045753, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.2870719696969697, "width": 0.3971862745098039, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.3009090909090909, "width": 0.17361928104575164, "height": 0.012579545454545427, "page": 4}], "section": "End-to-End Deep-Learning Command Recognition", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "998"}, {"text": "We first apply a band-pass filter on the audio signal to remove part of the background noise, and then calculate the peaks (syllables) of the signal.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.7437967171717171, "width": 0.39794934640522894, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7576338383838385, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7714709595959596, "width": 0.1291454248366013, "height": 0.012579545454545427, "page": 4}], "section": "Post- Vocal Check", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "999"}, {"text": "Our implementation on the smartphone uses Android Accessibility Service 2 .", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.8425037878787879, "width": 0.39989215686274504, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8543383838383838, "width": 0.11296895424836595, "height": 0.014582070707070738, "page": 4}], "section": "Device Control and Feedback Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1000"}, {"text": "Participants were first given a short introduction to our task.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.40003104575163395, "height": 0.012579545454545427, "page": 5}], "section": "Experimental Setup and Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1001"}, {"text": "We use this service to monitor the state of the device, set the suitable recognition model and automatically perform input actions (e.g. tapping on a view).", "label": "Method", "bboxes": [{"left": 0.4372565359477124, "top": 0.2788964646464646, "width": 0.04805392156862748, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.29273358585858583, "width": 0.3971960784313725, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.30657070707070705, "width": 0.39719117647058827, "height": 0.012579545454545427, "page": 5}, {"left": 0.08758169934640524, "top": 0.32040782828282827, "width": 0.16814215686274508, "height": 0.012579545454545427, "page": 5}], "section": "Device Control and Feedback Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1002"}, {"text": "We conducted the experiment in a laboratory space with participants sitting or standing comfortably (Figure 5.a) near a window.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.6559722222222222, "width": 0.40064869281045756, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6698093434343435, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 5}, {"left": 0.08753267973856209, "top": 0.6836464646464646, "width": 0.05397875816993465, "height": 0.012579545454545427, "page": 5}], "section": "Experimental Setup and Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1003"}, {"text": "464%, indicating that our system could accurately identify even a new users commands.", "label": "Method", "bboxes": [{"left": 0.5727336601307189, "top": 0.755469696969697, "width": 0.35182189542483666, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7693068181818182, "width": 0.22081209150326808, "height": 0.012579545454545427, "page": 5}], "section": "Leave-One-Subject-Out Cross-Validation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1004"}, {"text": "Before feeding the data to our model, we augmented the dataset by applying a horizontally mirrored transformation on the mouth crop images.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.397030303030303, "width": 0.3971911764705882, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.41086742424242423, "width": 0.3971960784313725, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.42470454545454545, "width": 0.1525424836601308, "height": 0.012579545454545482, "page": 5}], "section": "Data Augmentation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1005"}, {"text": "For training and evaluation, we divided the 44 commands into four groups based on the principle of [ system functionality + current context ].", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.47561742424242426, "width": 0.3971977124183006, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.48916540404040404, "width": 0.39947058823529424, "height": 0.012868686868686863, "page": 5}, {"left": 0.5246633986928104, "top": 0.5030025252525252, "width": 0.10742973856209148, "height": 0.012868686868686918, "page": 5}], "section": "Recognition Group", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1006"}, {"text": "We quantied the exaggeration of lip movements by comparing the Maximum Mouth Opening Degree ( MMOD ) of the \"exaggerated\" silent speech vs. normal audible speech when the participants were issuing the same commands.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.263094696969697, "width": 0.40065359477124196, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.2766426767676768, "width": 0.3971911764705882, "height": 0.012868686868686863, "page": 5}, {"left": 0.5246633986928104, "top": 0.2907689393939394, "width": 0.3971895424836601, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3046060606060606, "width": 0.3180228758169935, "height": 0.012579545454545482, "page": 5}], "section": "Quantifying the Exaggeration of Lip Movements", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1007"}, {"text": "The task simulated daily smartphone usage, including 22 of the 44 functionalities supported by our Lip-Interact system.", "label": "Method", "bboxes": [{"left": 0.679016339869281, "top": 0.6992487373737373, "width": 0.24283986928104595, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.7130858585858586, "width": 0.39746568627450984, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7269217171717172, "width": 0.1341013071895425, "height": 0.012579545454545427, "page": 6}], "section": "Experimental Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1008"}, {"text": "(1) On a limited set of mobile interactions, the commands issued by users through silent speech can be distinguished accurately by vision-only methods.", "label": "Result", "bboxes": [{"left": 0.17547875816993463, "top": 0.8090479797979797, "width": 0.3098316993464052, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.27729411764705886, "height": 0.012579545454545538, "page": 6}], "section": "Summary", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1009"}, {"text": "Our deep learning model has sufcient generalization ability for new users with a recognition accuracy of over 95%.", "label": "Result", "bboxes": [{"left": 0.370390522875817, "top": 0.8367222222222221, "width": 0.11491503267973857, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.2230604575163399, "height": 0.012579545454545538, "page": 6}], "section": "Summary", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1010"}, {"text": "We recruited 10 right-handed participants (P1-P10, 5M/5F), aged between 22 and 30.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.56177398989899, "width": 0.3999983660130719, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5756111111111112, "width": 0.15590032679738564, "height": 0.012579545454545427, "page": 6}], "section": "Participants", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1011"}, {"text": "An analysis using Friedman test showed that for our tasks participants reported signicantly stronger agreement for Lip-Interact than the two touch conditions on input easiness ( p = . 00015,  2 r = 17 . 64).", "label": "Method", "bboxes": [{"left": 0.664124183006536, "top": 0.5093952020202019, "width": 0.2577254901960784, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5232310606060606, "width": 0.3974738562091503, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5370681818181818, "width": 0.3971911764705882, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.5506161616161617, "width": 0.26988725490196086, "height": 0.013835858585858563, "page": 7}], "section": "Input Easiness", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1012"}, {"text": "We calculated the whole input time from the beginning of the word selection to the end of marking it.", "label": "Method", "bboxes": [{"left": 0.694640522875817, "top": 0.8505593434343435, "width": 0.2272189542483659, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39718627450980404, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.012859477124183005, "height": 0.012579545454545427, "page": 7}], "section": "User Behavior and Feedback with Lip-Interact", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1013"}, {"text": "We measured input time in seconds to quantify the efciency.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.31793560606060606, "width": 0.40079901960784314, "height": 0.012868686868686863, "page": 7}], "section": "Measures", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1014"}, {"text": "With each technique, P a was free to use the device for three minutes, accessing any of the functionalities that our system supported.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.6786704545454545, "width": 0.39794444444444443, "height": 0.013834595959596019, "page": 8}, {"left": 0.08811928104575163, "top": 0.6927967171717172, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7066338383838384, "width": 0.0696421568627451, "height": 0.012579545454545427, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1015"}, {"text": "We argue that this simple difference signicantly enhances the user experience.", "label": "Method", "bboxes": [{"left": 0.36656535947712415, "top": 0.4641313131313131, "width": 0.1187369281045752, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.47796717171717173, "width": 0.4000392156862745, "height": 0.012579545454545427, "page": 8}], "section": "STUDY 3: EVALUATE LIP-INTERACT VS. VOICE INPUT", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1016"}, {"text": "Finally, we encouraged participants to suggest more applications for Lip-Interact.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.5893914141414142, "width": 0.39989052287581706, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.6032272727272727, "width": 0.13901470588235298, "height": 0.012579545454545427, "page": 8}], "section": "MORE EXAMPLE APPLICATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1017"}, {"text": "This example application comes from a real-life experience of one of our participants.", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.8782335858585859, "width": 0.39768954248366006, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.1733562091503268, "height": 0.012579545454545427, "page": 8}], "section": "Shared Bike", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1018"}, {"text": "We chose the subway as our experimental environment.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.5607159090909091, "width": 0.354828431372549, "height": 0.012579545454545427, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1019"}, {"text": "Although in our study, Lip-Interact outperformed touch-based input on most of the investigated tasks with a shorter completion time and higher input easiness, our purpose is not to substitute touch with Lip-Interact but to provide an alternative input approach.", "label": "Method", "bboxes": [{"left": 0.08753267973856209, "top": 0.5989027777777778, "width": 0.39776960784313725, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.612739898989899, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6265770202020202, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6404141414141414, "width": 0.3998986928104575, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.6542512626262627, "width": 0.12908006535947714, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1020"}, {"text": "Although the size of the command set is still limited in our work, Lip-Interact can be easy to be generalized to support other applications.", "label": "Method", "bboxes": [{"left": 0.6197107843137255, "top": 0.15064141414141416, "width": 0.30214379084967313, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.16447853535353535, "width": 0.39989869281045765, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.17831565656565657, "width": 0.22439705882352945, "height": 0.012579545454545454, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1021"}, {"text": "For the intention recognition, at present we solve it by detecting the changes of the mouth opening degree and the audio signal.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.40182449494949496, "width": 0.39990032679738563, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.4156616161616162, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.4294987373737374, "width": 0.04231699346405238, "height": 0.012579545454545427, "page": 9}], "section": "System Robustness", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1022"}, {"text": "This work focuses on the implementation and evaluation of Lip-Interact for smartphone, but we also foresee the potential of Lip-Interact on other platforms.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.19111742424242426, "width": 0.3976895424836601, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.20495454545454547, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.21879166666666666, "width": 0.22865032679738562, "height": 0.012579545454545482, "page": 9}], "section": "Smart Watch and Head-worn Device", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1023"}, {"text": "Our implementation uses an additional mounted camera and host server for computing.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.6316224747474748, "width": 0.3971830065359476, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.645459595959596, "width": 0.17984803921568637, "height": 0.012579545454545538, "page": 9}], "section": "Other Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1024"}, {"text": "We have described Lip-Interact, a novel interaction technique that repurposes the front camera of smartphones to detect lip movement and recognize it into commands.", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.3976376262626262, "width": 0.39795261437908497, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.41147474747474744, "width": 0.39719444444444446, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.42531186868686865, "width": 0.28545751633986927, "height": 0.012579545454545482, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1025"}, {"text": "This work is supported by the National Key Research and Development Plan under Grant No. 2016YFB1001200, the Natural Science Foundation of China under Grant No. 61672314 and No. 61572276, Tsinghua University Research Funding No. 20151080408, and also by Beijing Key Lab of Networked Multimedia.", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.822885101010101, "width": 0.4003986928104576, "height": 0.012579545454545427, "page": 9}, {"left": 0.5242565359477125, "top": 0.8367222222222221, "width": 0.400299019607843, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.3971911764705882, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.08004575163398697, "height": 0.012579545454545427, "page": 9}], "section": "ACKNOWLEDGEMENT", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1026"}, {"text": "In order to raise the recognition accuracy of silent speech to a practically usable level in a mobile input system, we add two", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.3971960784313725, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1027"}, {"text": "Lastly, we discuss the main limitations of our work, which also points to the directions of future work.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.8278421717171717, "width": 0.39719117647058827, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.841679292929293, "width": 0.2810800653594771, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1028"}, {"text": "Results show that for a context-based set of around 20 commands, the mean recognition accuracy is 95 .", "label": "Result", "bboxes": [{"left": 0.3286911764705882, "top": 0.7129267676767678, "width": 0.1566029411764706, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7267638888888889, "width": 0.3998872549019608, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7403244949494949, "width": 0.12933823529411764, "height": 0.012856060606060704, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1029"}, {"text": "With two controlled experiments, we show that Lip-Interact provides a more efcient and stable way of accessing functionality on a smartphone than touch, especially for single-handed input.", "label": "Result", "bboxes": [{"left": 0.453687908496732, "top": 0.7544381313131313, "width": 0.03160457516339876, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7682752525252526, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 1}, {"left": 0.0877124183006536, "top": 0.7821123737373737, "width": 0.3981601307189542, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.795949494949495, "width": 0.40004248366013073, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1030"}, {"text": "We see this as a great opportunity to apply silent speech on mobile devices to expand the interaction channel and improve user experience.", "label": "Result", "bboxes": [{"left": 0.39432679738562093, "top": 0.0814570707070707, "width": 0.0909836601307189, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3971862745098039, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.40003431372549025, "height": 0.012579545454545454, "page": 2}], "section": "Silent Speech Interface and Lip Reading", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1031"}, {"text": "Compared with search-and-click-based touch interaction, Lip-Interact is essentially a kind of gesture (where the user performs one of a set of dened actions to express the input intention), thus enabling efcient one-step functionality access.", "label": "Conclusion", "bboxes": [{"left": 0.4184428104575163, "top": 0.2273737373737374, "width": 0.06687091503267972, "height": 0.012579545454545454, "page": 1}, {"left": 0.08753267973856209, "top": 0.24121085858585856, "width": 0.3977712418300653, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.3971960784313725, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.268885101010101, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.2827222222222222, "width": 0.31524673202614384, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1032"}, {"text": "Meanwhile, because the \"gestures\" in Lip-Interact are based on the natural language, it also avoids many limitations of conventional gesture input, such as the goodness of gesture-command mapping, the scalability of gesture set and the learning cost.", "label": "Novelty", "bboxes": [{"left": 0.4084787581699347, "top": 0.2827222222222222, "width": 0.07885784313725491, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.2965593434343434, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.31039646464646464, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.32423358585858586, "width": 0.39922875816993464, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.3380707070707071, "width": 0.3260261437908497, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1033"}, {"text": "Therefore, the interaction is immune to ambient noise and largely alleviates the issues surrounding personal privacy and social norms in public environment.", "label": "Conclusion", "bboxes": [{"left": 0.1431437908496732, "top": 0.3795808080808081, "width": 0.3421617647058824, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.3934179292929293, "width": 0.3977549019607844, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4072550505050505, "width": 0.26160947712418303, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1034"}, {"text": "Touch on GUIs is straightforward but may require a lot of search and multiple operations to reach the desired item [13].", "label": "Conclusion", "bboxes": [{"left": 0.5241601307189543, "top": 0.12359722222222222, "width": 0.3976895424836602, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.13743434343434344, "width": 0.40003267973856205, "height": 0.012579545454545454, "page": 1}], "section": "Accessing Functionality with Gestures", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1035"}, {"text": "Multimodal interfaces can support more exible, efcient and expressive means of humancomputer interaction [14], especially in mobile interaction when mobility may cause temporary disability and the user is unable to use a particular input [37].", "label": "Conclusion", "bboxes": [{"left": 0.715421568627451, "top": 0.2895858585858586, "width": 0.2091241830065359, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.30342297979797983, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.31726010101010105, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 1}, {"left": 0.5240784313725491, "top": 0.3310972222222222, "width": 0.3977826797385621, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.3449343434343434, "width": 0.23587908496732024, "height": 0.012579545454545482, "page": 1}], "section": "Multimodal Interfaces and Voice Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1036"}, {"text": "All of them concerned about privacy issues (i.e. \"I dont want my interactions to be observed by others in my immediate surroundings\") and social norms (i.e. \"my voice may disturb the surrounding environment\"), especially in public environments [40].", "label": "Conclusion", "bboxes": [{"left": 0.9008872549019608, "top": 0.44179292929292924, "width": 0.02097222222222228, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.45563005050505045, "width": 0.39775816993464064, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.46946717171717167, "width": 0.39989869281045753, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.4833042929292929, "width": 0.3971862745098039, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.4971414141414141, "width": 0.3971895424836601, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.5109785353535353, "width": 0.031815359477124217, "height": 0.012579545454545538, "page": 1}], "section": "Multimodal Interfaces and Voice Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1037"}, {"text": "Therefore, Lip-Interact is expected to be less noticeable and be able to largely increase users privacy.", "label": "Novelty", "bboxes": [{"left": 0.5241601307189543, "top": 0.5386527777777778, "width": 0.39768954248366006, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.5524886363636364, "width": 0.2674705882352941, "height": 0.012579545454545427, "page": 1}], "section": "Multimodal Interfaces and Voice Interaction", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "1038"}, {"text": "We discuss the design principles and present a set of interface mechanisms that support novice learning, visual feedback and online model personalization.", "label": "Conclusion", "bboxes": [{"left": 0.30546405228758167, "top": 0.5607196969696969, "width": 0.18255555555555558, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5745568181818181, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5883939393939394, "width": 0.39989869281045753, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.6022310606060606, "width": 0.029900326797385607, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1039"}, {"text": "For example, a user may prefer to type with ngers while using Lip-Interact for fast text editing (e.g. bold, highlight, undo, etc).", "label": "Conclusion", "bboxes": [{"left": 0.27233823529411766, "top": 0.4909040404040404, "width": 0.2129673202614379, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.5047411616161616, "width": 0.39719117647058827, "height": 0.012579545454545538, "page": 2}, {"left": 0.08758169934640524, "top": 0.5185782828282828, "width": 0.20717156862745095, "height": 0.012579545454545427, "page": 2}], "section": "Compatible with Existing Touch Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1040"}, {"text": "Please note that in this work, we cannot cover all the functionalities on smartphones in Lip-Interact because of their immense quantity, especially the built-in features of various apps.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.6014924242424242, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6153295454545454, "width": 0.3974738562091502, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6291666666666667, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6430037878787879, "width": 0.0345882352941177, "height": 0.012579545454545427, "page": 2}], "section": "Context-Aware Functionality Support", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1041"}, {"text": "However, with full access to a cutting-edge devices camera and neural processing unit, it seems likely that Lip-Interact could be realized on a real product with no additional computing resources.", "label": "Conclusion", "bboxes": [{"left": 0.31505228758169934, "top": 0.5665050505050505, "width": 0.17025, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5803421717171717, "width": 0.39922385620915035, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.594179292929293, "width": 0.3971911764705882, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.6080164141414142, "width": 0.315609477124183, "height": 0.012579545454545427, "page": 3}], "section": "SYSTEM IMPLEMENTATION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1042"}, {"text": "is a common mobile activity, and we hoped to learn how LipInteract can assist touch and improve the experience on some tasks where touch could be limited (e.g. undo/redo, adjust cursor position).", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.37509595959595965, "width": 0.39989705882352955, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.38893308080808087, "width": 0.39718464052287594, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.40277020202020203, "width": 0.3971911764705881, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4166073232323232, "width": 0.10639215686274517, "height": 0.012579545454545482, "page": 3}], "section": "Command Set", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1043"}, {"text": "We selected Notepad as the second app because text editing", "label": "Conclusion", "bboxes": [{"left": 0.08735457516339869, "top": 0.8677739898989899, "width": 0.3979477124183007, "height": 0.012868686868686918, "page": 3}], "section": "Command Set", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1044"}, {"text": "Unlike conventional lip-reading tasks [47, 4, 12] where the mouth position is relatively xed, for Lip-Interact in smartphone use, the orientation between the user and the camera may vary each time a command is issued, and may even change while the user issues a single command.", "label": "Conclusion", "bboxes": [{"left": 0.28326633986928107, "top": 0.4011035353535353, "width": 0.20203921568627448, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.41494065656565654, "width": 0.3992320261437909, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.42877777777777776, "width": 0.397186274509804, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.442614898989899, "width": 0.39922875816993464, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4564520202020202, "width": 0.40003921568627454, "height": 0.012579545454545482, "page": 4}], "section": "Spatial Transformer", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1045"}, {"text": "tree in which each node gives information (e.g. content, size, position, possible actions) about a window element.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.26506060606060605, "width": 0.39922712418300654, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.2788964646464646, "width": 0.34378921568627446, "height": 0.012579545454545427, "page": 5}], "section": "Device Control and Feedback Implementation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1046"}, {"text": "464%, indicating that our system could accurately identify even a new users commands.", "label": "Conclusion", "bboxes": [{"left": 0.5727336601307189, "top": 0.755469696969697, "width": 0.35182189542483666, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7693068181818182, "width": 0.22081209150326808, "height": 0.012579545454545427, "page": 5}], "section": "Leave-One-Subject-Out Cross-Validation", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1047"}, {"text": "The numbers in parentheses indicate the number of commands in the group.", "label": "Conclusion", "bboxes": [{"left": 0.836156862745098, "top": 0.5586388888888889, "width": 0.08569117647058833, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.5724760101010101, "width": 0.40003594771241846, "height": 0.012579545454545427, "page": 5}], "section": "Recognition Group", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1048"}, {"text": "The results showed that Lip-Interact could quickly learn and personalized the model as users made more use of it.", "label": "Conclusion", "bboxes": [{"left": 0.4118169934640523, "top": 0.7139318181818182, "width": 0.07349346405228752, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7277689393939394, "width": 0.3971895424836601, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.7416060606060606, "width": 0.25726307189542486, "height": 0.012579545454545538, "page": 6}], "section": "Progress over Time", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1049"}, {"text": "Among the 19 commands, 18 could be accurately recognized.", "label": "Conclusion", "bboxes": [{"left": 0.1879591503267974, "top": 0.3760391414141414, "width": 0.3000522875816994, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.389875, "width": 0.11854248366013072, "height": 0.012579545454545482, "page": 6}], "section": "Leave-One-Subject-Out Cross-Validation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1050"}, {"text": "Current smartphones use buttons, icons and menus to trigger functionality because they are easy to perceive and use.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.4029166666666667, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.4167525252525252, "width": 0.40003104575163406, "height": 0.012579545454545482, "page": 6}], "section": "STUDY 2: EVALUATE LIP-INTERACT VS. TOUCH", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1051"}, {"text": "Upon hearing it, the participant was asked to complete the subtask as quickly as possible and then wait for the next sound.", "label": "Conclusion", "bboxes": [{"left": 0.35767483660130717, "top": 0.15064141414141416, "width": 0.12763071895424843, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.39719117647058827, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.26746568627450984, "height": 0.012579545454545454, "page": 7}], "section": "Task and Procedure", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1052"}, {"text": "P2 said \" It felt like reading incantations. I did not have to select the command from the menu every time so that I could concentrate on the text itself \".", "label": "Conclusion", "bboxes": [{"left": 0.871985294117647, "top": 0.712189393939394, "width": 0.049864379084967325, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246650326797386, "top": 0.7257373737373737, "width": 0.3971911764705882, "height": 0.012868686868686918, "page": 7}, {"left": 0.5246633986928104, "top": 0.739574494949495, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7534116161616161, "width": 0.11913398692810462, "height": 0.012868686868686918, "page": 7}], "section": "User Behavior and Feedback with Lip-Interact", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1053"}, {"text": "P3 commented that \" I could remember all the commands right after the practice session. These commands are concise and intuitive. \" Six participants expressed surprise at the accuracy of the recognition.", "label": "Conclusion", "bboxes": [{"left": 0.630452614379085, "top": 0.6074936868686869, "width": 0.2913970588235295, "height": 0.012868686868686918, "page": 7}, {"left": 0.5246633986928104, "top": 0.6213308080808081, "width": 0.3971928104575164, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.6351679292929293, "width": 0.39718464052287583, "height": 0.012868686868686918, "page": 7}, {"left": 0.5246633986928104, "top": 0.6492941919191919, "width": 0.21352614379084967, "height": 0.012579545454545427, "page": 7}], "section": "User Behavior and Feedback with Lip-Interact", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1054"}, {"text": "P9 said \" I had never thought these things could be identied. When I first silently spoke the command and saw the correct response, It felt really good. \"", "label": "Conclusion", "bboxes": [{"left": 0.7429918300653595, "top": 0.6490050505050504, "width": 0.17885784313725495, "height": 0.012868686868686918, "page": 7}, {"left": 0.5246633986928104, "top": 0.6628421717171717, "width": 0.3971928104575164, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6766780303030303, "width": 0.38855718954248364, "height": 0.012868686868686807, "page": 7}], "section": "User Behavior and Feedback with Lip-Interact", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1055"}, {"text": "Type VII : Precise pointing which is difcult for touch because of the \" fat nger \"problem [42] (e.g. adjust a cursor in text).", "label": "Conclusion", "bboxes": [{"left": 0.08768954248366012, "top": 0.6025126262626262, "width": 0.3976143790849673, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6160606060606061, "width": 0.38723366013071897, "height": 0.012868686868686807, "page": 7}], "section": "Measures", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1056"}, {"text": "Finally, we encouraged participants to suggest more applications for Lip-Interact.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.5893914141414142, "width": 0.39989052287581706, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.6032272727272727, "width": 0.13901470588235298, "height": 0.012579545454545427, "page": 8}], "section": "MORE EXAMPLE APPLICATIONS", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1057"}, {"text": "The first concern was that \" making sound may disturb people around me \" (P6).", "label": "Conclusion", "bboxes": [{"left": 0.6568643790849673, "top": 0.38841414141414143, "width": 0.26499183006535965, "height": 0.012868686868686807, "page": 8}, {"left": 0.5246633986928104, "top": 0.40225126262626265, "width": 0.25249509803921577, "height": 0.012868686868686807, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1058"}, {"text": "Three expressed that the mapping from command words to a single function could be n to 1.", "label": "Conclusion", "bboxes": [{"left": 0.2688039215686275, "top": 0.3121982323232323, "width": 0.21650816993464056, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.32574494949494953, "width": 0.37507679738562094, "height": 0.012868686868686863, "page": 8}], "section": "User Behavior and Feedback with Lip-Interact", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1059"}, {"text": "P5 commented \" The feeling of being a passerby was related to the current environment. It did not matter in most cases, but sometimes I could not help but wonder what he or she was doing \".", "label": "Conclusion", "bboxes": [{"left": 0.8225735294117648, "top": 0.4931098484848485, "width": 0.09928104575163388, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.5066578282828283, "width": 0.3971911764705881, "height": 0.012868686868686807, "page": 8}, {"left": 0.5246633986928104, "top": 0.5204949494949496, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 8}, {"left": 0.5243872549019608, "top": 0.5343320707070707, "width": 0.3686045751633986, "height": 0.012868686868686918, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1060"}, {"text": "This is because touch-based input is still faster and more direct for many tasks (e.g. scrolling a list) and enables more complex interactions (e.g. typing).", "label": "Conclusion", "bboxes": [{"left": 0.22224509803921572, "top": 0.6542512626262627, "width": 0.2630588235294117, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6680883838383838, "width": 0.39828758169934636, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.6819255050505051, "width": 0.3413088235294118, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1061"}, {"text": "Although the size of the command set is still limited in our work, Lip-Interact can be easy to be generalized to support other applications.", "label": "Conclusion", "bboxes": [{"left": 0.6197107843137255, "top": 0.15064141414141416, "width": 0.30214379084967313, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.16447853535353535, "width": 0.39989869281045765, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.17831565656565657, "width": 0.22439705882352945, "height": 0.012579545454545454, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "1062"}, {"text": "Therefore, the robustness of the Lip-Interact system mainly relies on two factors: the robustness of recognizing the users intention to use silent speech and the robustness of recognizing the issued commands.", "label": "Conclusion", "bboxes": [{"left": 0.6271993464052288, "top": 0.33892929292929297, "width": 0.29465522875816985, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.3527664141414142, "width": 0.39989215686274504, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.3666035353535354, "width": 0.39989379084967314, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.3804393939393939, "width": 0.27399836601307204, "height": 0.012579545454545482, "page": 9}], "section": "System Robustness", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1063"}, {"text": "We demonstrate that the model of a [ spatial transformer network + CNN + RNN ] architecture can accurately recognize around 20 silent speech commands with training data collected from dozens of users.", "label": "Conclusion", "bboxes": [{"left": 0.26155392156862745, "top": 0.4806590909090909, "width": 0.2237516339869281, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.4942070707070707, "width": 0.39717647058823524, "height": 0.012868686868686807, "page": 9}, {"left": 0.08811928104575163, "top": 0.5083333333333334, "width": 0.39719117647058827, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5221704545454546, "width": 0.29161928104575163, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "1064"}, {"text": "Lip-Interact moves one step further by keeping the external form of voice interaction but not vocalizing any sound.", "label": "Novelty", "bboxes": [{"left": 0.5621078431372549, "top": 0.5109785353535353, "width": 0.36245424836601303, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.5248156565656565, "width": 0.40003267973856216, "height": 0.012579545454545427, "page": 1}], "section": "Multimodal Interfaces and Voice Interaction", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1065"}, {"text": "Recognition results will be displayed at the indicators location for two seconds after the user nishes issuing a command.", "label": "Future Work", "bboxes": [{"left": 0.7390996732026144, "top": 0.8367222222222221, "width": 0.18275490196078414, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.3971960784313726, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.2133676470588236, "height": 0.012579545454545538, "page": 2}], "section": "User Learning and System Feedback", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1066"}, {"text": "We will describe the details of the commands and their elicitation process in the next section.", "label": "Future Work", "bboxes": [{"left": 0.7241781045751634, "top": 0.6983510101010102, "width": 0.19766993464052296, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7121881313131313, "width": 0.40003921568627454, "height": 0.012579545454545427, "page": 2}], "section": "Context-Aware Functionality Support", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1067"}, {"text": "Users only need to mouth the command by thinking that their lip movement can present each syllable clearly.", "label": "Future Work", "bboxes": [{"left": 0.7952924836601307, "top": 0.30336742424242424, "width": 0.12656209150326792, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.31720454545454546, "width": 0.3971928104575164, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.3310404040404041, "width": 0.18700163398692815, "height": 0.012579545454545427, "page": 2}], "section": "High Level of Recognition Accuracy", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1068"}, {"text": "Lip-Interact only considers short and visually distinguishable commands rather than long sentences such as those in a conversational system.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.7869305555555556, "width": 0.3971846405228758, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8007676767676767, "width": 0.39989052287581695, "height": 0.012579545454545538, "page": 2}, {"left": 0.0877124183006536, "top": 0.814604797979798, "width": 0.12161601307189546, "height": 0.012579545454545427, "page": 2}], "section": "High Level of Recognition Accuracy", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1069"}, {"text": "Users are instructed to issue commands in a more \"standard\" way than when they are casually talking by exaggerating their", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 2}, {"left": 0.08753267973856209, "top": 0.892070707070707, "width": 0.39806045751633984, "height": 0.012579545454545427, "page": 2}], "section": "High Level of Recognition Accuracy", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1070"}, {"text": "To access frequently-used features, users often need to tap and swipe multiple times or choose from a hierarchical menu (3.a).", "label": "Future Work", "bboxes": [{"left": 0.4682303921568628, "top": 0.8265517676767677, "width": 0.01708660130718953, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.8403888888888889, "width": 0.39719117647058827, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.8542260101010102, "width": 0.40004248366013073, "height": 0.012579545454545427, "page": 3}], "section": "Command Set", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1071"}, {"text": "If and only if all of these features satisfy certain thresholds (learned from training data with a C4.5 decision tree [41]), the algorithm decides whether or not to remain in the current state or move to the next state.", "label": "Future Work", "bboxes": [{"left": 0.8830800653594773, "top": 0.6996237373737374, "width": 0.03877614379084959, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.7134608585858586, "width": 0.3971944444444445, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.7272979797979797, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.741135101010101, "width": 0.39746895424836604, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.7549722222222222, "width": 0.14506699346405227, "height": 0.012579545454545427, "page": 3}], "section": "Segmenting Mouth Sequence of Silent Speaking", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1072"}, {"text": "For example, talking to other people while using the device will also result in mouth movement but should not be detected as issuing a Lip-Interact command.", "label": "Future Work", "bboxes": [{"left": 0.8994460784313726, "top": 0.6532272727272728, "width": 0.022681372549019585, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.667064393939394, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6809015151515152, "width": 0.3971928104575164, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6947386363636363, "width": 0.20527777777777778, "height": 0.012579545454545538, "page": 4}], "section": "Post- Vocal Check", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1073"}, {"text": "P5 said \" There was a time when I wanted to say back to home instead of the required home. Although I ended up successfully issuing the correct command, a more exible mapping would further improve the experience of this technology. \"", "label": "Future Work", "bboxes": [{"left": 0.4682352941176471, "top": 0.3260340909090909, "width": 0.01705392156862745, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.33958207070707075, "width": 0.3998986928104575, "height": 0.012868686868686863, "page": 8}, {"left": 0.08811928104575163, "top": 0.35341919191919197, "width": 0.3998888888888889, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.36725631313131313, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.38109343434343435, "width": 0.3730751633986928, "height": 0.012868686868686863, "page": 8}], "section": "User Behavior and Feedback with Lip-Interact", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1074"}, {"text": "For Privacy , P4 commented that \" If there are other people around me, there is no privacy when using the voice interaction. People will know what I am doing on my device. Using the silent speech can largely protect my interaction privacy, unless others are looking at me very carefully. \"", "label": "Future Work", "bboxes": [{"left": 0.5246633986928104, "top": 0.29784469696969695, "width": 0.39717973856209154, "height": 0.012868686868686863, "page": 8}, {"left": 0.5246633986928104, "top": 0.31168181818181817, "width": 0.39989705882352955, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.3255189393939394, "width": 0.39718300653594774, "height": 0.012579545454545482, "page": 8}, {"left": 0.5246633986928104, "top": 0.3393560606060606, "width": 0.40003104575163406, "height": 0.012579545454545482, "page": 8}, {"left": 0.5246633986928104, "top": 0.3531931818181818, "width": 0.3064117647058824, "height": 0.012868686868686863, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1075"}, {"text": "In addition, Lip-Interact also has potential to be applied to other interaction platforms (e.g. accessibility, smartwatch, head-worn devices), which deserves to be further studied.", "label": "Future Work", "bboxes": [{"left": 0.3284035947712418, "top": 0.7649469696969696, "width": 0.15690032679738558, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.778784090909091, "width": 0.397187908496732, "height": 0.012579545454545427, "page": 9}, {"left": 0.08758169934640524, "top": 0.7926212121212121, "width": 0.39772222222222214, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.8064583333333334, "width": 0.19551633986928108, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1076"}, {"text": "Future work should be done to try Lip-Interact on more applications.", "label": "Future Work", "bboxes": [{"left": 0.7269901960784314, "top": 0.21982702020202022, "width": 0.19485947712418306, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.23366414141414144, "width": 0.24461437908496741, "height": 0.012579545454545454, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1077"}, {"text": "But we do not rule out that the performance will be affected by more noise in real-life use cases.", "label": "Future Work", "bboxes": [{"left": 0.8986846405228758, "top": 0.4294987373737374, "width": 0.023171568627451067, "height": 0.012579545454545427, "page": 9}, {"left": 0.5240784313725491, "top": 0.4433358585858586, "width": 0.3983366013071895, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.45717297979797983, "width": 0.2174591503267974, "height": 0.012579545454545427, "page": 9}], "section": "System Robustness", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1078"}, {"text": "The algorithms need to be improved and tested in more various environments.", "label": "Future Work", "bboxes": [{"left": 0.7478513071895425, "top": 0.45717297979797983, "width": 0.174003267973856, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.4710088383838384, "width": 0.32443627450980395, "height": 0.012579545454545482, "page": 9}], "section": "System Robustness", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1079"}, {"text": "Future work should be done to improve the algorithm and explore the theoretical upper bound.", "label": "Future Work", "bboxes": [{"left": 0.7422696078431372, "top": 0.5678686868686869, "width": 0.1795915032679739, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5817058080808081, "width": 0.3974738562091502, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5955429292929293, "width": 0.04476797385620923, "height": 0.012579545454545427, "page": 9}], "section": "System Robustness", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1080"}, {"text": "We are trying to integrate LipInteract into a self-contained smartphone, which will reduce the response time and improve the user experience further.", "label": "Future Work", "bboxes": [{"left": 0.7146584967320261, "top": 0.645459595959596, "width": 0.20990359477124187, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.6592967171717172, "width": 0.3971895424836601, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.6731338383838383, "width": 0.3678480392156862, "height": 0.012579545454545538, "page": 9}], "section": "Other Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1081"}, {"text": "Lastly, we discuss the main limitations of our work, which also points to the directions of future work.", "label": "Future Work", "bboxes": [{"left": 0.08811928104575163, "top": 0.8278421717171717, "width": 0.39719117647058827, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.841679292929293, "width": 0.2810800653594771, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1082"}, {"text": "The goal of our first study was to verify the feasibility of using Lip-Interact on a mobile device.", "label": "Objective", "bboxes": [{"left": 0.08761437908496732, "top": 0.4881906565656566, "width": 0.3976895424836601, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.5020265151515152, "width": 0.24889215686274513, "height": 0.012579545454545427, "page": 5}], "section": "Real-time considerations", "prob": 0.824285626411438, "is_author_statement": true, "is_in_expected_section": false, "id": "1083"}, {"text": "The goal of Lip-Interact is to provide a simple, yet powerful approach to issuing commands on the smartphone.", "label": "Objective", "bboxes": [{"left": 0.08761437908496732, "top": 0.21353661616161618, "width": 0.39768954248366006, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.32528431372549016, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "prob": 0.8155174255371094, "is_author_statement": false, "is_in_expected_section": true, "id": "1084"}, {"text": "In each frame, the method of Kazemi and Sullivan [27] is used to identify the facial landmarks, 20 of which describe the feature points of the mouth (Figure 5.b).", "label": "Method", "bboxes": [{"left": 0.6494019607843138, "top": 0.5260315656565657, "width": 0.27245751633986925, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5398686868686869, "width": 0.3971911764705881, "height": 0.012579545454545427, "page": 3}, {"left": 0.5240784313725491, "top": 0.5537058080808082, "width": 0.4006290849673203, "height": 0.012579545454545427, "page": 3}], "section": "Segmenting Mouth Sequence of Silent Speaking", "prob": 0.7330785393714905, "is_author_statement": false, "is_in_expected_section": true, "id": "1085"}, {"text": "Before feeding the data to our model, we augmented the dataset by applying a horizontally mirrored transformation on the mouth crop images.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.397030303030303, "width": 0.3971911764705882, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.41086742424242423, "width": 0.3971960784313725, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.42470454545454545, "width": 0.1525424836601308, "height": 0.012579545454545482, "page": 5}], "section": "Data Augmentation", "prob": 0.5919922590255737, "is_author_statement": true, "is_in_expected_section": true, "id": "1086"}, {"text": "Our spatial transformer module uses bilinear sampling and a 16-point thin plate spline transformation (TPS) with a regular grid of control points as the transformation function to solve relative mouth positioning towards the camera.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.4702891414141414, "width": 0.39719281045751637, "height": 0.012579545454545482, "page": 4}, {"left": 0.08689869281045752, "top": 0.4841262626262626, "width": 0.3986813725490196, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.49796338383838384, "width": 0.397186274509804, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.511800505050505, "width": 0.3084983660130719, "height": 0.012579545454545427, "page": 4}], "section": "Spatial Transformer", "prob": 0.5813753008842468, "is_author_statement": true, "is_in_expected_section": true, "id": "1087"}, {"text": "We first apply a band-pass filter on the audio signal to remove part of the background noise, and then calculate the peaks (syllables) of the signal.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.7437967171717171, "width": 0.39794934640522894, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7576338383838385, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7714709595959596, "width": 0.1291454248366013, "height": 0.012579545454545427, "page": 4}], "section": "Post- Vocal Check", "prob": 0.5721179842948914, "is_author_statement": true, "is_in_expected_section": true, "id": "1088"}, {"text": "The features extracted by the convolutional nets are then attened and passed into two bidirectional Gated Recurrent Units (GRU) [10] for the modelling of dynamics.", "label": "Method", "bboxes": [{"left": 0.3371437908496732, "top": 0.6535050505050505, "width": 0.1481666666666666, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.6673421717171717, "width": 0.3971911764705882, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.6811792929292929, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.6950164141414141, "width": 0.15015686274509804, "height": 0.012579545454545538, "page": 4}], "section": "Representation Learning and Dynamic Modelling", "prob": 0.571015477180481, "is_author_statement": false, "is_in_expected_section": true, "id": "1089"}, {"text": "Dropout [43] of 0.5 is applied after each max-pooling layer.", "label": "Method", "bboxes": [{"left": 0.1296143790849673, "top": 0.8367222222222221, "width": 0.3556960784313725, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.03644771241830065, "height": 0.012579545454545427, "page": 4}], "section": "Implementation and Training Details", "prob": 0.5210630297660828, "is_author_statement": false, "is_in_expected_section": true, "id": "1090"}, {"text": "Finally, the output at the last time-step of the second GRU is followed by a fully connected layer with a softmax function to predict the lip-command class.", "label": "Method", "bboxes": [{"left": 0.2431421568627451, "top": 0.6950164141414141, "width": 0.2421633986928105, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7088535353535353, "width": 0.397186274509804, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7226906565656566, "width": 0.3427303921568628, "height": 0.012579545454545427, "page": 4}], "section": "Representation Learning and Dynamic Modelling", "prob": 0.47315123677253723, "is_author_statement": false, "is_in_expected_section": true, "id": "1091"}, {"text": "We use the ReLU activation function and batch-normalize [23] the outputs of each convolution layer.", "label": "Method", "bboxes": [{"left": 0.27533660130718957, "top": 0.8090479797979797, "width": 0.21268137254901953, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.03644771241830065, "height": 0.012579545454545538, "page": 4}], "section": "Implementation and Training Details", "prob": 0.4704661965370178, "is_author_statement": true, "is_in_expected_section": true, "id": "1092"}, {"text": "Our implementation uses an additional mounted camera and host server for computing.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.6316224747474748, "width": 0.3971830065359476, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.645459595959596, "width": 0.17984803921568637, "height": 0.012579545454545538, "page": 9}], "section": "Other Limitations", "prob": 0.46119004487991333, "is_author_statement": true, "is_in_expected_section": true, "id": "1093"}, {"text": "Before implementing the Lip-Interact system, we first describe several key design principles of the technique.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.3692020202020202, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3830391414141414, "width": 0.30050163398692814, "height": 0.012579545454545427, "page": 2}], "section": "LIP-INTERACT DESIGNS", "prob": 0.425923615694046, "is_author_statement": true, "is_in_expected_section": true, "id": "1094"}, {"text": "We use this service to monitor the state of the device, set the suitable recognition model and automatically perform input actions (e.g. tapping on a view).", "label": "Method", "bboxes": [{"left": 0.4372565359477124, "top": 0.2788964646464646, "width": 0.04805392156862748, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.29273358585858583, "width": 0.3971960784313725, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.30657070707070705, "width": 0.39719117647058827, "height": 0.012579545454545427, "page": 5}, {"left": 0.08758169934640524, "top": 0.32040782828282827, "width": 0.16814215686274508, "height": 0.012579545454545427, "page": 5}], "section": "Device Control and Feedback Implementation", "prob": 0.42325374484062195, "is_author_statement": true, "is_in_expected_section": true, "id": "1095"}, {"text": "For training and evaluation, we divided the 44 commands into four groups based on the principle of [ system functionality + current context ].", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.47561742424242426, "width": 0.3971977124183006, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.48916540404040404, "width": 0.39947058823529424, "height": 0.012868686868686863, "page": 5}, {"left": 0.5246633986928104, "top": 0.5030025252525252, "width": 0.10742973856209148, "height": 0.012868686868686918, "page": 5}], "section": "Recognition Group", "prob": 0.42257755994796753, "is_author_statement": true, "is_in_expected_section": true, "id": "1096"}, {"text": "For analysis, we divided the tasks into six types based on their touch interaction properties.", "label": "Method", "bboxes": [{"left": 0.16533333333333333, "top": 0.3735732323232323, "width": 0.3199771241830065, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.3874103535353535, "width": 0.27736111111111117, "height": 0.012579545454545427, "page": 7}], "section": "Measures", "prob": 0.4132451117038727, "is_author_statement": true, "is_in_expected_section": true, "id": "1097"}, {"text": "To evaluate the feasibility of Lip-Interact, we selected 44 popular commands (including those for two representative apps) to collect data, implement system and conduct user studies.", "label": "Method", "bboxes": [{"left": 0.5644656862745098, "top": 0.6430037878787879, "width": 0.35738888888888887, "height": 0.012579545454545427, "page": 2}, {"left": 0.52409477124183, "top": 0.6568409090909091, "width": 0.40046241830065354, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.6706767676767678, "width": 0.39746078431372556, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6845138888888889, "width": 0.048357843137255, "height": 0.012579545454545427, "page": 2}], "section": "Context-Aware Functionality Support", "prob": 0.3801322877407074, "is_author_statement": true, "is_in_expected_section": true, "id": "1098"}, {"text": "Within each sliding window, seven statistical features of OD are calculated: current value, sum, mean, max, standard deviation, number of frames between max and min, and absolute energy .", "label": "Method", "bboxes": [{"left": 0.8420179738562092, "top": 0.6581123737373737, "width": 0.07983006535947712, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6716603535353536, "width": 0.399437908496732, "height": 0.012868686868686918, "page": 3}, {"left": 0.5246633986928104, "top": 0.6854974747474747, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.699334595959596, "width": 0.3533807189542485, "height": 0.012868686868686918, "page": 3}], "section": "Segmenting Mouth Sequence of Silent Speaking", "prob": 0.3664095401763916, "is_author_statement": false, "is_in_expected_section": true, "id": "1099"}, {"text": "Each participant was asked to complete a series of interaction tasks on the smartphone under three input conditions : Lip-Interact, touch operation with two hands, and touch operation with one hand.", "label": "Method", "bboxes": [{"left": 0.7327810457516339, "top": 0.6439002525252525, "width": 0.18906862745098052, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6577373737373737, "width": 0.3974738562091502, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6712853535353536, "width": 0.39718137254901964, "height": 0.012868686868686807, "page": 6}, {"left": 0.5246633986928104, "top": 0.6854116161616162, "width": 0.27112418300653596, "height": 0.012579545454545427, "page": 6}], "section": "Experimental Design", "prob": 0.3567027151584625, "is_author_statement": false, "is_in_expected_section": false, "id": "1100"}, {"text": "We also improve the recognition model with Spatial Transformer Networks [24] to solve the variability of mouth orientation in the mobile context.", "label": "Method", "bboxes": [{"left": 0.2942679738562092, "top": 0.19215277777777778, "width": 0.1937434640522876, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.205989898989899, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.21982702020202022, "width": 0.3742140522875817, "height": 0.012579545454545454, "page": 2}], "section": "Silent Speech Interface and Lip Reading", "prob": 0.34067603945732117, "is_author_statement": true, "is_in_expected_section": true, "id": "1101"}, {"text": "The camera recognizes the commands, and the system triggers corresponding functionalities.", "label": "Method", "bboxes": [{"left": 0.27893954248366015, "top": 0.34152777777777776, "width": 0.2090751633986928, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.355364898989899, "width": 0.40004248366013073, "height": 0.012579545454545427, "page": 2}], "section": "LIP-INTERACT DESIGNS", "prob": 0.32692277431488037, "is_author_statement": false, "is_in_expected_section": true, "id": "1102"}, {"text": "The above results verify the feasibility of Lip-Interact from two aspects.", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.7952108585858586, "width": 0.39768954248366006, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.08126470588235295, "height": 0.012579545454545538, "page": 6}], "section": "Summary", "prob": 0.8688287734985352, "is_author_statement": false, "is_in_expected_section": false, "id": "1103"}, {"text": "Figure 8 shows the results.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.6309090909090909, "width": 0.17119444444444443, "height": 0.012579545454545427, "page": 6}], "section": "Progress over Time", "prob": 0.8511965274810791, "is_author_statement": false, "is_in_expected_section": false, "id": "1104"}, {"text": "Our experiment results reect the performance of Lip-Interact in a controlled setting.", "label": "Result", "bboxes": [{"left": 0.8973284313725489, "top": 0.6731338383838383, "width": 0.024808823529411717, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.6869709595959597, "width": 0.3971960784313725, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.7008080808080808, "width": 0.1152826797385621, "height": 0.012579545454545538, "page": 9}], "section": "Other Limitations", "prob": 0.8342774510383606, "is_author_statement": true, "is_in_expected_section": false, "id": "1105"}, {"text": "P3 commented that \" I could remember all the commands right after the practice session. These commands are concise and intuitive. \" Six participants expressed surprise at the accuracy of the recognition.", "label": "Result", "bboxes": [{"left": 0.630452614379085, "top": 0.6074936868686869, "width": 0.2913970588235295, "height": 0.012868686868686918, "page": 7}, {"left": 0.5246633986928104, "top": 0.6213308080808081, "width": 0.3971928104575164, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.6351679292929293, "width": 0.39718464052287583, "height": 0.012868686868686918, "page": 7}, {"left": 0.5246633986928104, "top": 0.6492941919191919, "width": 0.21352614379084967, "height": 0.012579545454545427, "page": 7}], "section": "User Behavior and Feedback with Lip-Interact", "prob": 0.8045495748519897, "is_author_statement": false, "is_in_expected_section": false, "id": "1106"}, {"text": "For command recognition, the mean accuracy was 97.5%.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.21385479797979798, "width": 0.37730718954248366, "height": 0.012579545454545482, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": 0.7952869534492493, "is_author_statement": false, "is_in_expected_section": true, "id": "1107"}, {"text": "For the segmentation of the silent speech sequence, the recall was 99.18%, and the precision was 99.59%.", "label": "Result", "bboxes": [{"left": 0.3836029411764706, "top": 0.2374810606060606, "width": 0.10441339869281047, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.25131818181818183, "width": 0.3992173202614379, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.2651540404040404, "width": 0.1980245098039216, "height": 0.012579545454545427, "page": 7}], "section": "Results and Analysis", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1108"}, {"text": "For the command recognition, the mean accuracy was 97.9%.", "label": "Result", "bboxes": [{"left": 0.29118790849673204, "top": 0.2651540404040404, "width": 0.19616013071895422, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.2789911616161616, "width": 0.20030228758169938, "height": 0.012579545454545427, "page": 7}], "section": "Results and Analysis", "prob": 0.7888373732566833, "is_author_statement": false, "is_in_expected_section": true, "id": "1109"}, {"text": "On average, MMOD increased by only 7% when participants were issuing silent speech commands.", "label": "Result", "bboxes": [{"left": 0.8476323529411764, "top": 0.3046060606060606, "width": 0.07625980392156873, "height": 0.012579545454545482, "page": 5}, {"left": 0.5239803921568628, "top": 0.31815404040404044, "width": 0.3978741830065359, "height": 0.012868686868686863, "page": 5}, {"left": 0.5246633986928104, "top": 0.33228030303030304, "width": 0.16233823529411773, "height": 0.012579545454545482, "page": 5}], "section": "Quantifying the Exaggeration of Lip Movements", "prob": 0.7484244108200073, "is_author_statement": false, "is_in_expected_section": false, "id": "1110"}, {"text": "Our research shows high accuracy for recognizing about 20 commands.", "label": "Result", "bboxes": [{"left": 0.6774019607843137, "top": 0.5540315656565656, "width": 0.24472222222222229, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5678686868686869, "width": 0.21250653594771252, "height": 0.012579545454545427, "page": 9}], "section": "System Robustness", "prob": 0.7426536083221436, "is_author_statement": true, "is_in_expected_section": false, "id": "1111"}, {"text": "Table 2 summarizes the overall classication accuracies of the four groups.", "label": "Result", "bboxes": [{"left": 0.5241601307189543, "top": 0.7277954545454546, "width": 0.39769771241830065, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7416325757575758, "width": 0.07713562091503268, "height": 0.012579545454545427, "page": 5}], "section": "Leave-One-Subject-Out Cross-Validation", "prob": 0.7375227212905884, "is_author_statement": false, "is_in_expected_section": false, "id": "1112"}, {"text": "Table 3 shows the mean input time of accessing the seven types of functionality under three input conditions .", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.7537007575757576, "width": 0.39768954248366006, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7672474747474747, "width": 0.3586176470588236, "height": 0.012868686868686918, "page": 7}], "section": "Measures", "prob": 0.7321366667747498, "is_author_statement": false, "is_in_expected_section": false, "id": "1113"}, {"text": "Lip-Interact also had the smallest standard deviation of input time on all interaction types, showing that Lip-Interact provided a more stable and dependable way to access functionality.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.3833320707070707, "width": 0.39989869281045753, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.3971691919191919, "width": 0.3971911764705882, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.41100631313131314, "width": 0.39989869281045753, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.42484343434343436, "width": 0.0313006535947713, "height": 0.012579545454545482, "page": 7}], "section": "Measures", "prob": 0.7078165411949158, "is_author_statement": false, "is_in_expected_section": false, "id": "1114"}, {"text": "Results show that for a context-based set of around 20 commands, the mean recognition accuracy is 95 .", "label": "Result", "bboxes": [{"left": 0.3286911764705882, "top": 0.7129267676767678, "width": 0.1566029411764706, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7267638888888889, "width": 0.3998872549019608, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7403244949494949, "width": 0.12933823529411764, "height": 0.012856060606060704, "page": 1}], "section": "INTRODUCTION", "prob": 0.7014800906181335, "is_author_statement": false, "is_in_expected_section": true, "id": "1115"}, {"text": "However, participants also thought that there was room for improvement for Lip-Interact.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.2983611111111111, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.3121982323232323, "width": 0.1755702614379085, "height": 0.012579545454545427, "page": 8}], "section": "User Behavior and Feedback with Lip-Interact", "prob": 0.6842480897903442, "is_author_statement": false, "is_in_expected_section": false, "id": "1116"}, {"text": "compared to touch with one hand.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.3481111111111111, "width": 0.22738888888888886, "height": 0.012579545454545427, "page": 7}], "section": "Measures", "prob": 0.639270007610321, "is_author_statement": false, "is_in_expected_section": false, "id": "1117"}, {"text": "P4 commented: \" For the task of selecting a word and then highlight/bold it, I initially performed the two steps in sequence in the first block. But since using Lip-Interact always took some time, in the next blocks, I tried to start issuing the command before I nished the selection. It is faster .\"", "label": "Result", "bboxes": [{"left": 0.7563856209150327, "top": 0.781084595959596, "width": 0.16546895424836594, "height": 0.012868686868686807, "page": 7}, {"left": 0.5246633986928104, "top": 0.7949217171717171, "width": 0.39718300653594774, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.8087588383838384, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8225959595959595, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8364330808080809, "width": 0.39718300653594774, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.850270202020202, "width": 0.16590686274509814, "height": 0.012868686868686918, "page": 7}], "section": "User Behavior and Feedback with Lip-Interact", "prob": 0.6250287890434265, "is_author_statement": false, "is_in_expected_section": false, "id": "1118"}, {"text": "We verify the feasibility of LipInteract by implementing a real-working prototype and conducting three experiments.", "label": "Method", "bboxes": [{"left": 0.2758888888888889, "top": 0.4529861111111111, "width": 0.2121290849673202, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4668232323232323, "width": 0.3998986928104575, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4806590909090909, "width": 0.1685686274509804, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": 0.6233242750167847, "is_author_statement": true, "is_in_expected_section": true, "id": "1119"}, {"text": "We chose the subway as our experimental environment.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.5607159090909091, "width": 0.354828431372549, "height": 0.012579545454545427, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": 0.618323564529419, "is_author_statement": true, "is_in_expected_section": true, "id": "1120"}, {"text": "For both apps, we elicited the most frequently-used functionalities as above.", "label": "Result", "bboxes": [{"left": 0.8996339869281046, "top": 0.4304444444444444, "width": 0.02250163398692806, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4442815656565656, "width": 0.39718627450980404, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.45811868686868684, "width": 0.061671568627451046, "height": 0.012579545454545482, "page": 3}], "section": "Command Set", "prob": 0.5903097987174988, "is_author_statement": true, "is_in_expected_section": false, "id": "1121"}, {"text": "We also chose two representative apps for evaluation: WeChat 1 and Notepad .", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.7692020202020201, "width": 0.39802777777777776, "height": 0.014580808080808194, "page": 3}, {"left": 0.08811928104575163, "top": 0.7847512626262626, "width": 0.08617320261437907, "height": 0.012868686868686918, "page": 3}], "section": "Command Set", "prob": 0.5783315896987915, "is_author_statement": true, "is_in_expected_section": false, "id": "1122"}, {"text": "For the \"exaggeration\" requirement, our participants understood the necessity, and did not report any discomfort during the studies.", "label": "Result", "bboxes": [{"left": 0.7743251633986928, "top": 0.23366414141414144, "width": 0.14752287581699342, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.2475012626262626, "width": 0.3971911764705881, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.2613383838383838, "width": 0.3144035947712419, "height": 0.012579545454545482, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1123"}, {"text": "At the same time, we admit that this is an interaction limitation on users.", "label": "Result", "bboxes": [{"left": 0.8441143790849672, "top": 0.2613383838383838, "width": 0.07773366013071914, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.27517424242424243, "width": 0.39070588235294124, "height": 0.012579545454545482, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": 0.5779037475585938, "is_author_statement": true, "is_in_expected_section": true, "id": "1124"}, {"text": "The whole experiment took around 90 minutes.", "label": "Result", "bboxes": [{"left": 0.7746944444444445, "top": 0.19215277777777778, "width": 0.14716339869281048, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.205989898989899, "width": 0.15826797385620917, "height": 0.012579545454545454, "page": 5}], "section": "Experimental Setup and Design", "prob": 0.5738669037818909, "is_author_statement": false, "is_in_expected_section": true, "id": "1125"}, {"text": "We collected a total of 90 blocks of task (10 participants  3 input conditions  3 replications).", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.704364898989899, "width": 0.39793954248366015, "height": 0.012856060606060593, "page": 7}, {"left": 0.08811928104575163, "top": 0.7182020202020203, "width": 0.22248202614379087, "height": 0.012856060606060593, "page": 7}], "section": "Measures", "prob": 0.5650738477706909, "is_author_statement": true, "is_in_expected_section": false, "id": "1126"}, {"text": "Different from previous lip-reading researches that use recorded video or news as the dataset, we validate the feasibility of Lip-Interact by collecting data and evaluating the system when users are actually using the smartphone.", "label": "Result", "bboxes": [{"left": 0.4248692810457516, "top": 0.13680429292929294, "width": 0.06044117647058822, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.3974738562091504, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.19215277777777778, "width": 0.20112418300653595, "height": 0.012579545454545454, "page": 2}], "section": "Silent Speech Interface and Lip Reading", "prob": 0.5650471448898315, "is_author_statement": true, "is_in_expected_section": false, "id": "1127"}, {"text": "As for Comfort as people in the surrounding environment , most participants rated 3 (neutral) on voice input.", "label": "Result", "bboxes": [{"left": 0.5240784313725491, "top": 0.47898358585858586, "width": 0.397767973856209, "height": 0.012868686868686918, "page": 8}, {"left": 0.5246633986928104, "top": 0.4931098484848485, "width": 0.29228104575163405, "height": 0.012579545454545427, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": 0.5429524779319763, "is_author_statement": false, "is_in_expected_section": true, "id": "1128"}, {"text": "During this study, participants issued a total of 367 Lip-Interact commands.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.8585669191919192, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8724040404040404, "width": 0.07610130718954249, "height": 0.012579545454545538, "page": 8}], "section": "Experimental Design and Task Procedure", "prob": 0.46082279086112976, "is_author_statement": false, "is_in_expected_section": true, "id": "1129"}, {"text": "We measured input time in seconds to quantify the efciency.", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.31793560606060606, "width": 0.40079901960784314, "height": 0.012868686868686863, "page": 7}], "section": "Measures", "prob": 0.4438991844654083, "is_author_statement": true, "is_in_expected_section": false, "id": "1130"}, {"text": "As a counter-example, Figure 7 shows the analog audio signal from the devices microphone when a user speaks \" open WeChat \" in a dining room.", "label": "Result", "bboxes": [{"left": 0.7347271241830066, "top": 0.6947386363636363, "width": 0.1871225490196079, "height": 0.012579545454545538, "page": 4}, {"left": 0.5242565359477125, "top": 0.7085757575757576, "width": 0.39760130718954245, "height": 0.012579545454545427, "page": 4}, {"left": 0.5240784313725491, "top": 0.7221237373737374, "width": 0.3457483660130718, "height": 0.012868686868686918, "page": 4}], "section": "Post- Vocal Check", "prob": 0.4280930757522583, "is_author_statement": false, "is_in_expected_section": false, "id": "1131"}, {"text": "Our recognition method built on the latest research [4, 12] on lip reading in computer vision and was improved to adapt to", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3971944444444445, "height": 0.012579545454545427, "page": 3}], "section": "End-to-End Deep-Learning Command Recognition", "prob": 0.41245803236961365, "is_author_statement": true, "is_in_expected_section": false, "id": "1132"}, {"text": "None had participated in the previous data-collection experiment.", "label": "Result", "bboxes": [{"left": 0.6853986928104575, "top": 0.5756111111111112, "width": 0.23645098039215695, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5894482323232323, "width": 0.17788562091503268, "height": 0.012579545454545427, "page": 6}], "section": "Participants", "prob": 0.38753581047058105, "is_author_statement": false, "is_in_expected_section": false, "id": "1133"}, {"text": "Participants were first given a short introduction to our task.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.40003104575163395, "height": 0.012579545454545427, "page": 5}], "section": "Experimental Setup and Design", "prob": 0.37913891673088074, "is_author_statement": true, "is_in_expected_section": true, "id": "1134"}, {"text": "This experiment examines the interaction performance of LipInteract on a smartphone, with a focus on efciency in comparison with general touch.", "label": "Result", "bboxes": [{"left": 0.5241601307189543, "top": 0.4305896464646464, "width": 0.4003970588235294, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.44442676767676764, "width": 0.39989869281045753, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.45826388888888886, "width": 0.17723366013071895, "height": 0.012579545454545482, "page": 6}], "section": "STUDY 2: EVALUATE LIP-INTERACT VS. TOUCH", "prob": 0.37635594606399536, "is_author_statement": false, "is_in_expected_section": true, "id": "1135"}, {"text": "The computing is conducted on a host with CPU of Intel Core i7 4.20 GHz  8 and GPU of GeForce GTX 1080 Ti.", "label": "Result", "bboxes": [{"left": 0.08761437908496732, "top": 0.3904621212121212, "width": 0.3976977124183007, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.40402272727272726, "width": 0.32462254901960785, "height": 0.012856060606060649, "page": 5}], "section": "Real-time considerations", "prob": 0.3614528179168701, "is_author_statement": false, "is_in_expected_section": false, "id": "1136"}, {"text": "For per frame between begin speaking and stop speaking , an afne transformation is applied based on the lip landmarks to extract a mouth crop image of size H  W = 100  80 pixels (Figure 5.b).", "label": "Result", "bboxes": [{"left": 0.674795751633987, "top": 0.7546830808080808, "width": 0.2470424836601307, "height": 0.012868686868686807, "page": 3}, {"left": 0.5246633986928104, "top": 0.7685189393939394, "width": 0.397187908496732, "height": 0.012868686868686807, "page": 3}, {"left": 0.5246633986928104, "top": 0.782645202020202, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.7961931818181819, "width": 0.2508970588235293, "height": 0.012868686868686807, "page": 3}], "section": "Segmenting Mouth Sequence of Silent Speaking", "prob": 0.34306755661964417, "is_author_statement": false, "is_in_expected_section": false, "id": "1137"}, {"text": "Figure 2 illustrates how Lip-Interact is integrated into smartphone usage.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.37255303030303033, "width": 0.39988725490196086, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.38639015151515155, "width": 0.08378431372549022, "height": 0.012579545454545427, "page": 2}], "section": "Context-Aware Functionality Support", "prob": 0.34274935722351074, "is_author_statement": false, "is_in_expected_section": false, "id": "1138"}, {"text": "Improve Recognition Accuracy of Silent Speech Commands", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.8647209595959596, "width": 0.38764705882352934, "height": 0.011320707070707092, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": 0.3409053087234497, "is_author_statement": false, "is_in_expected_section": true, "id": "1139"}, {"text": "Lastly, we discuss the main limitations of our work, which also points to the directions of future work.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.8278421717171717, "width": 0.39719117647058827, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.841679292929293, "width": 0.2810800653594771, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "prob": 0.3285699784755707, "is_author_statement": true, "is_in_expected_section": true, "id": "1140"}], "uist-5": [{"text": "Our research explores AR visualization designs to facilitate stair navigation by leveraging PLVs residual vision.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.7945984848484848, "width": 0.3941503267973855, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8091464646464647, "width": 0.33934313725490184, "height": 0.012727272727272587, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1141"}, {"text": "We sought to design effective visualizations for PLV, which balance visibility and distraction, while providing alternative choices to support a wide range of visual abilities.", "label": "Author", "bboxes": [{"left": 0.3071535947712418, "top": 0.11370454545454546, "width": 0.1753480392156863, "height": 0.01272727272727274, "page": 1}, {"left": 0.08829248366013072, "top": 0.12825126262626263, "width": 0.39415686274509804, "height": 0.012727272727272726, "page": 1}, {"left": 0.08827450980392157, "top": 0.14249368686868688, "width": 0.39421241830065357, "height": 0.012727272727272726, "page": 1}, {"left": 0.08827450980392157, "top": 0.15704040404040404, "width": 0.192937908496732, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1142"}, {"text": "We designed visualizations on two AR platforms that can generate immersive virtual content in the physical environment: projection-based AR and smartglasses.", "label": "Author", "bboxes": [{"left": 0.08827450980392157, "top": 0.17916035353535356, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 1}, {"left": 0.08827450980392157, "top": 0.19370833333333334, "width": 0.39412581699346405, "height": 0.012727272727272754, "page": 1}, {"left": 0.08827450980392157, "top": 0.2082550505050505, "width": 0.3062107843137255, "height": 0.012727272727272754, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1143"}, {"text": "Our designs considered the different characteristics of the two platforms: (1) For projection, which can augment a large physical space, we designed visual highlights with different patterns that are directly projected onto the stairs to enhance their visibility (Figure 1a).", "label": "Author", "bboxes": [{"left": 0.40116666666666667, "top": 0.2082550505050505, "width": 0.08134150326797385, "height": 0.012727272727272754, "page": 1}, {"left": 0.08824183006535948, "top": 0.2228030303030303, "width": 0.3941421568627451, "height": 0.012727272727272698, "page": 1}, {"left": 0.08822549019607843, "top": 0.23734974747474746, "width": 0.39424999999999993, "height": 0.012727272727272754, "page": 1}, {"left": 0.08824183006535948, "top": 0.25189646464646465, "width": 0.3941781045751634, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.26644444444444443, "width": 0.39420751633986933, "height": 0.012727272727272754, "page": 1}, {"left": 0.08822549019607843, "top": 0.2809911616161616, "width": 0.07741176470588236, "height": 0.012727272727272754, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1144"}, {"text": "(2) For smartglasses that have a limited vertical field of view (FOV), we designed visualizations in the users central FOV to indicate the users exact position on the stairs (Figure 1b).", "label": "Author", "bboxes": [{"left": 0.16991993464052288, "top": 0.2809911616161616, "width": 0.3124640522875817, "height": 0.012727272727272754, "page": 1}, {"left": 0.08824183006535948, "top": 0.29553914141414145, "width": 0.39414542483660137, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.3100858585858586, "width": 0.3941421568627451, "height": 0.012727272727272754, "page": 1}, {"left": 0.08824183006535948, "top": 0.3246338383838384, "width": 0.07808823529411764, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1145"}, {"text": "In summary, we contribute the first exploration of AR visualizations to facilitate stair navigation for PLV.", "label": "Author", "bboxes": [{"left": 0.08825816993464053, "top": 0.48494570707070705, "width": 0.39414215686274506, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.49918813131313133, "width": 0.3113937908496732, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1146"}, {"text": "Our evaluations demonstrated the effectiveness of our visualizations and provide insights for the design of AR visualizations for PLV that support other tasks as well.", "label": "Author", "bboxes": [{"left": 0.40423039215686274, "top": 0.49918813131313133, "width": 0.07813725490196083, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.5137348484848485, "width": 0.39409477124183007, "height": 0.01272727272727281, "page": 1}, {"left": 0.08824183006535948, "top": 0.5282828282828282, "width": 0.3941290849673203, "height": 0.01272727272727281, "page": 1}, {"left": 0.08825816993464053, "top": 0.5428295454545454, "width": 0.2410147058823529, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1147"}, {"text": "We evaluated our visualizations on each platform with 12 PLV.", "label": "Author", "bboxes": [{"left": 0.08824183006535948, "top": 0.3464482323232323, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 1}, {"left": 0.08822549019607843, "top": 0.36099494949494954, "width": 0.03498366013071895, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1148"}, {"text": "We found that the visualizations on both platforms increased participants self-reported psychological security.", "label": "Author", "bboxes": [{"left": 0.1273921568627451, "top": 0.36099494949494954, "width": 0.3549918300653595, "height": 0.012727272727272698, "page": 1}, {"left": 0.08822549019607843, "top": 0.3755429292929293, "width": 0.39417156862745095, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1149"}, {"text": "Our visualizations also changed participants behaviors.", "label": "Author", "bboxes": [{"left": 0.08824183006535948, "top": 0.3900896464646465, "width": 0.39415849673202613, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1150"}, {"text": "Many participants didnt stare down at the stairs when walking with our visualizations; some stopped holding the railing.", "label": "Author", "bboxes": [{"left": 0.08824183006535948, "top": 0.4046363636363636, "width": 0.39417483660130714, "height": 0.012727272727272754, "page": 1}, {"left": 0.08824183006535948, "top": 0.41918434343434346, "width": 0.39415849673202613, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1151"}, {"text": "We sought to facilitate stair navigation by augmenting the stairs with AR visualizations.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.6130517676767676, "width": 0.39428594771241826, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.6275997474747476, "width": 0.1897565359477124, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1152"}, {"text": "Considering the different visual abilities of PLV and our new use case for AR, we did not know a-priori what AR platform would be most appropriate for the stair navigation task.", "label": "Author", "bboxes": [{"left": 0.290921568627451, "top": 0.7148825757575757, "width": 0.19143790849673203, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.7294305555555555, "width": 0.39415032679738565, "height": 0.01272727272727281, "page": 2}, {"left": 0.08818627450980392, "top": 0.7436717171717172, "width": 0.3941895424836601, "height": 0.012727272727272587, "page": 2}, {"left": 0.08820261437908496, "top": 0.7582196969696969, "width": 0.1830947712418301, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1153"}, {"text": "Thus, we designed visualizations for such a projection-based AR smartphone to augment the stairs for PLV.", "label": "Author", "bboxes": [{"left": 0.5663513071895425, "top": 0.5848926767676768, "width": 0.3454526143790849, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5994393939393939, "width": 0.3772859477124182, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1154"}, {"text": "To our knowledge, our research is the first attempt to facilitate stair navigation for PLV.", "label": "Author", "bboxes": [{"left": 0.15576470588235294, "top": 0.5633876262626263, "width": 0.3265915032679738, "height": 0.01272727272727281, "page": 2}, {"left": 0.08821895424836601, "top": 0.5779356060606061, "width": 0.2414264705882353, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1155"}, {"text": "From an interaction perspective, we aimed to simulate use of a flashlight, which is commonly used by PLV in dark places [79]: when a user points the projection-based AR phone at the stairs, it recognizes several stairs in front of her and projects visualizations on those stairs in real time (Figure 1a).", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.6345669191919192, "width": 0.39411274509803906, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.649114898989899, "width": 0.394142156862745, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.6636616161616161, "width": 0.3941454248366014, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176307189542484, "top": 0.6782095959595961, "width": 0.3941078431372549, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176307189542484, "top": 0.6927563131313131, "width": 0.39412581699346394, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1156"}, {"text": "Inspired by the contrast stripes that many PLV used to distinguish stair edges [79], we project highlights on the stair edges to increase their visibility.", "label": "Author", "bboxes": [{"left": 0.5176307189542484, "top": 0.7073030303030302, "width": 0.3943856209150326, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176307189542484, "top": 0.7218510101010102, "width": 0.39407516339869286, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176307189542484, "top": 0.7363977272727272, "width": 0.21194607843137248, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1157"}, {"text": "Based on the formative study, we narrowed down our target platforms to immersive AR platforms, specifically (1) handheld projection-based AR, and (2) optical see-through smartglasses.", "label": "Author", "bboxes": [{"left": 0.5175898692810458, "top": 0.16459343434343435, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.17914141414141413, "width": 0.39422222222222214, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.19368813131313134, "width": 0.39416503267973846, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.20823611111111112, "width": 0.0503986928104575, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1158"}, {"text": "We designed and evaluated visualizations for both platforms, given that each platform has its own strength: projectionbased AR can augment large physical surfaces but projects content publicly, which may be better suited to private places with few people ( e.g ., home, workspace); meanwhile, smartglasses present information only to the user, which may be better for crowded public places ( e.g ., subway stations).", "label": "Author", "bboxes": [{"left": 0.8890522875816994, "top": 0.22278282828282828, "width": 0.022735294117647076, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.2373308080808081, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.25187752525252527, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.26642424242424245, "width": 0.3940915032679738, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.28097222222222223, "width": 0.3941584967320261, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.29551893939393936, "width": 0.39412418300653596, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.3100669191919192, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.3246136363636364, "width": 0.3652679738562091, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1159"}, {"text": "We therefore designed our visualizations to help them perceive the stairs from a greater distance, so they can better plan and prepare their steps.", "label": "Author", "bboxes": [{"left": 0.5624640522875817, "top": 0.8164015151515152, "width": 0.34943954248366027, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176797385620915, "top": 0.8309494949494949, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176797385620915, "top": 0.8454962121212122, "width": 0.1838349673202614, "height": 0.012727272727272587, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1160"}, {"text": "We first explored the design space of hand-held projectionbased AR, which combines a camera that recognizes the environment and a projector that projects visual contents into that environment [61].", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.359415404040404, "width": 0.3942401960784314, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176470588235295, "top": 0.3739633838383838, "width": 0.39407516339869286, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176470588235295, "top": 0.38851010101010097, "width": 0.39420751633986917, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.4030580808080808, "width": 0.14836764705882355, "height": 0.012727272727272754, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1161"}, {"text": "To determine what platforms would be appropriate, we began by conducting a formative study with 11 PLV (7 female, 4 male; age: 2870, mean = 40) to evaluate prototype visualizations for a smartphone.", "label": "Author", "bboxes": [{"left": 0.08820261437908496, "top": 0.7803396464646465, "width": 0.39422385620915035, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.7948863636363636, "width": 0.3942107843137255, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.8094343434343434, "width": 0.3942336601307189, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.8239810606060606, "width": 0.18408823529411766, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1162"}, {"text": "We presented the real-time captured image of the stairs on the phone screen and enhanced the stair edges with yellow highlights.", "label": "Author", "bboxes": [{"left": 0.2719738562091503, "top": 0.8530757575757575, "width": 0.2104624183006536, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.8676224747474748, "width": 0.39413725490196083, "height": 0.012727272727272587, "page": 2}, {"left": 0.08820261437908496, "top": 0.8821704545454545, "width": 0.24663071895424843, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1163"}, {"text": "To alert users of the presence of stairs as they approach, we first generate auditory feedback to provide an overview of", "label": "Author", "bboxes": [{"left": 0.5176797385620915, "top": 0.8676161616161616, "width": 0.39416503267973846, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176633986928104, "top": 0.8821641414141413, "width": 0.39417483660130725, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1164"}, {"text": "In contrast, our work addresses this gap by designing AR visualizations to assist PLV in navigating stairs.", "label": "Author", "bboxes": [{"left": 0.2446111111111111, "top": 0.1357979797979798, "width": 0.23786437908496727, "height": 0.012727272727272726, "page": 2}, {"left": 0.0883218954248366, "top": 0.15004040404040403, "width": 0.39417320261437905, "height": 0.012727272727272754, "page": 2}, {"left": 0.08833823529411765, "top": 0.1645871212121212, "width": 0.08359477124183005, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1165"}, {"text": "To minimize the confounding effect of computer vision accuracy, we prototyped our design with a Wizard of Oz protocol [65].", "label": "Author", "bboxes": [{"left": 0.6927124183006536, "top": 0.7154823232323233, "width": 0.21910947712418305, "height": 0.012727272727272587, "page": 3}, {"left": 0.5176633986928104, "top": 0.730030303030303, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176470588235295, "top": 0.7442714646464647, "width": 0.2311323529411764, "height": 0.01272727272727281, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1166"}, {"text": "We created all visualizations with PowerPoint.", "label": "Author", "bboxes": [{"left": 0.6195343137254902, "top": 0.7879128787878789, "width": 0.2922107843137256, "height": 0.012727272727272587, "page": 3}, {"left": 0.5176143790849673, "top": 0.8024608585858586, "width": 0.03861437908496734, "height": 0.01272727272727281, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1167"}, {"text": "To simulate the limited projection area of a handheld projector, we projected visualizations only on the three stairs in front of the participant (Figure 1a).", "label": "Author", "bboxes": [{"left": 0.8331748366013072, "top": 0.8315555555555555, "width": 0.07861274509803928, "height": 0.01272727272727281, "page": 3}, {"left": 0.5175980392156863, "top": 0.8461022727272728, "width": 0.3941895424836601, "height": 0.012727272727272587, "page": 3}, {"left": 0.5175980392156863, "top": 0.8606502525252525, "width": 0.39417973856209154, "height": 0.01272727272727281, "page": 3}, {"left": 0.5175980392156863, "top": 0.8751969696969696, "width": 0.15113398692810454, "height": 0.01272727272727281, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1168"}, {"text": "We provide three different auditory feedback choices: (1) Sonification that indicates stair direction: one ding sound for going up and two ding sounds for going down, adapted from the sonic alerts for some elevators; (2) a human voice that verbally reported stair direction and number of stairs: Approaching upstairs, 14 stairs going up; and (3) a combined sonification and human voice: ding, approaching upstairs, 14 stairs going up.", "label": "Author", "bboxes": [{"left": 0.40587091503267975, "top": 0.21638888888888888, "width": 0.0765049019607843, "height": 0.012727272727272754, "page": 3}, {"left": 0.08820261437908496, "top": 0.23093686868686866, "width": 0.39420261437908505, "height": 0.012727272727272754, "page": 3}, {"left": 0.08821895424836601, "top": 0.24548358585858587, "width": 0.3941993464052288, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.260030303030303, "width": 0.3941764705882353, "height": 0.012727272727272754, "page": 3}, {"left": 0.08821895424836601, "top": 0.27457828282828284, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.289125, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.0882516339869281, "top": 0.3036729797979798, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.318219696969697, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.33276767676767677, "width": 0.12806862745098035, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1169"}, {"text": "We recruited 12 PLV (6 female, 6 male; mean age=53.9) with different low-vision conditions, as shown in Table 1 (P1  P12).", "label": "Author", "bboxes": [{"left": 0.6077075163398692, "top": 0.5906275252525253, "width": 0.30414869281045764, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.605175505050505, "width": 0.39418627450980404, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.6197222222222222, "width": 0.12820751633986938, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1170"}, {"text": "We conducted a phone screen to ensure participants were eligible.", "label": "Author", "bboxes": [{"left": 0.8034117647058824, "top": 0.6636691919191919, "width": 0.1083758169934641, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176470588235295, "top": 0.6788156565656566, "width": 0.32262908496732023, "height": 0.01272727272727281, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1171"}, {"text": "We evaluated the visualizations for projection-based AR, aiming to answer three questions: (1) How do PLV perceive the different visualization designs?", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.4975972222222222, "width": 0.39420751633986917, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.5121452020202021, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.5266919191919192, "width": 0.2338006535947713, "height": 0.01272727272727281, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1172"}, {"text": "(3) How secure do people feel when using our visualizations?", "label": "Author", "bboxes": [{"left": 0.7451813725490196, "top": 0.5409343434343434, "width": 0.166640522875817, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.5554810606060606, "width": 0.23057189542483658, "height": 0.01272727272727281, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1173"}, {"text": "Since locating the first and last stairs was most important but challenging for PLV [86], we distinguish the first and last stairs from the rest by projecting thick highlights on them (Figure 2a), while projecting thin highlights on the middle stairs (Figure 3a).", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.3545820707070707, "width": 0.3941388888888889, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3691287878787879, "width": 0.39412745098039215, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.38367676767676767, "width": 0.39422058823529404, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3982234848484848, "width": 0.3942075163398692, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.4127714646464647, "width": 0.11595261437908495, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1174"}, {"text": "We call the highlights on the first and last stairs End Highlights , and we call those on the middle stairs Middle Highlights .", "label": "Author", "bboxes": [{"left": 0.20833823529411766, "top": 0.4127714646464647, "width": 0.27400490196078436, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.42731818181818176, "width": 0.3941633986928105, "height": 0.012727272727272754, "page": 3}, {"left": 0.08821895424836601, "top": 0.441864898989899, "width": 0.12319934640522878, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1175"}, {"text": "We needed a visible color for these highlights that would not be confused with natural light, so we used yellow.", "label": "Author", "bboxes": [{"left": 0.21519117647058825, "top": 0.441864898989899, "width": 0.2672009803921569, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.4564128787878788, "width": 0.39403921568627454, "height": 0.012727272727272754, "page": 3}, {"left": 0.08820261437908496, "top": 0.47095959595959597, "width": 0.08263235294117648, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1176"}, {"text": "Beyond these highlights, we sought ways to further emphasize the first and last stairs so that a user will notice them and perceive their exact location from a distance.", "label": "Author", "bboxes": [{"left": 0.08820261437908496, "top": 0.49307954545454546, "width": 0.394124183006536, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5076275252525253, "width": 0.3941993464052288, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5218686868686868, "width": 0.3023006535947713, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1177"}, {"text": "We designed five animations to achieve this:", "label": "Author", "bboxes": [{"left": 0.3960212418300654, "top": 0.5218686868686868, "width": 0.0863888888888889, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5364166666666667, "width": 0.20429084967320266, "height": 0.01272727272727281, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1178"}, {"text": "We designed two middle highlights to support the user in a minimally obtrusive way.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.2945669191919192, "width": 0.39420751633986917, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.30911489898989897, "width": 0.16747385620915034, "height": 0.012727272727272754, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1179"}, {"text": "(1) Flash: Since a flash can attract peoples attention [83, 84], we added this feature to the end highlights.", "label": "Author", "bboxes": [{"left": 0.08820261437908496, "top": 0.5585366161616161, "width": 0.3941568627450981, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5730833333333334, "width": 0.2921225490196079, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1180"}, {"text": "So in this design, we kept a stable line at the stair edge while flashing the rest of the highlighted strip (Figure 2b).", "label": "Author", "bboxes": [{"left": 0.14738071895424837, "top": 0.6388459595959596, "width": 0.33493790849673194, "height": 0.01272727272727281, "page": 3}, {"left": 0.08816993464052288, "top": 0.6533926767676768, "width": 0.39416666666666667, "height": 0.01272727272727281, "page": 3}, {"left": 0.08816993464052288, "top": 0.6679393939393939, "width": 0.025890522875817, "height": 0.012727272727272587, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1181"}, {"text": "With a stable line at the stair edge, we added another line moving towards the edge to generate movement (Figure 2c).", "label": "Author", "bboxes": [{"left": 0.08815359477124184, "top": 0.7046073232323232, "width": 0.3941781045751634, "height": 0.01272727272727281, "page": 3}, {"left": 0.08815359477124184, "top": 0.7191540404040405, "width": 0.394156862745098, "height": 0.012727272727272587, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1182"}, {"text": "(4) Moving Horizontal Zebra: Since movement can be distracting [84], we design a more subtle movement effect with a yellow and black zebra pattern moving back and forth at a frequency of 1Hz (Figure 2d).", "label": "Author", "bboxes": [{"left": 0.08815359477124184, "top": 0.740969696969697, "width": 0.39414052287581697, "height": 0.01272727272727281, "page": 3}, {"left": 0.08813725490196078, "top": 0.7555164141414141, "width": 0.3941764705882353, "height": 0.012727272727272587, "page": 3}, {"left": 0.08813725490196078, "top": 0.7700631313131313, "width": 0.39417156862745095, "height": 0.01272727272727281, "page": 3}, {"left": 0.08813725490196078, "top": 0.7846111111111111, "width": 0.19653921568627453, "height": 0.012727272727272587, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1183"}, {"text": "(5) Moving Vertical Zebra: Moving the highlight over the edge of the stair may distort the perceived location of the edge, so we also designed a zebra pattern that is perpendicular to the edge (Figure 2e).", "label": "Author", "bboxes": [{"left": 0.08813725490196078, "top": 0.8067310606060607, "width": 0.3941176470588235, "height": 0.012727272727272587, "page": 3}, {"left": 0.08815359477124184, "top": 0.8212777777777778, "width": 0.3941470588235294, "height": 0.01272727272727281, "page": 3}, {"left": 0.08815359477124184, "top": 0.8358257575757576, "width": 0.39420588235294113, "height": 0.012727272727272587, "page": 3}, {"left": 0.08815359477124184, "top": 0.8503724747474748, "width": 0.17424183006535948, "height": 0.01272727272727281, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1184"}, {"text": "In each phase, we presented all design options to the participant and asked about their experiences, including whether or not they liked the design, whether the design distracted them from seeing the environment, and how they wanted to improve it.", "label": "Author", "bboxes": [{"left": 0.0882843137254902, "top": 0.7594419191919192, "width": 0.39417156862745095, "height": 0.01272727272727281, "page": 4}, {"left": 0.08826797385620916, "top": 0.7739886363636364, "width": 0.39415849673202613, "height": 0.012727272727272587, "page": 4}, {"left": 0.08826797385620916, "top": 0.7885366161616161, "width": 0.3942761437908497, "height": 0.01272727272727281, "page": 4}, {"left": 0.08826797385620916, "top": 0.8030833333333334, "width": 0.39415849673202613, "height": 0.012727272727272587, "page": 4}, {"left": 0.08826797385620916, "top": 0.8176313131313131, "width": 0.055999999999999994, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1185"}, {"text": "To avoid order effects, we randomized the order of the design alternatives.", "label": "Author", "bboxes": [{"left": 0.3328888888888889, "top": 0.8321780303030304, "width": 0.1494869281045752, "height": 0.012727272727272587, "page": 4}, {"left": 0.08826797385620916, "top": 0.8467260101010101, "width": 0.3348643790849673, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1186"}, {"text": "During the visualization experience, we gave the participant our prototype smartphone and explained how to use it.", "label": "Author", "bboxes": [{"left": 0.0882516339869281, "top": 0.6357967171717172, "width": 0.39419444444444446, "height": 0.012727272727272698, "page": 4}, {"left": 0.0882516339869281, "top": 0.6503434343434343, "width": 0.36376307189542484, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1187"}, {"text": "The participant experienced our design in three phases: (1) Auditory feedback when approaching the stairs, with three alternatives: sound, human voice, and the combination of them; (2) End highlights on the first and last stairs with six design alternatives (Figure 2); and (3) Middle highlights on the middle stairs with three design alternatives (Figure 3).", "label": "Author", "bboxes": [{"left": 0.4569624183006536, "top": 0.6503434343434343, "width": 0.025464052287581695, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882516339869281, "top": 0.6648914141414142, "width": 0.39412418300653596, "height": 0.012727272727272587, "page": 4}, {"left": 0.0882516339869281, "top": 0.6794381313131314, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 4}, {"left": 0.08826797385620916, "top": 0.6939848484848484, "width": 0.39414869281045745, "height": 0.01272727272727281, "page": 4}, {"left": 0.08826797385620916, "top": 0.7085328282828283, "width": 0.3942173202614379, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882843137254902, "top": 0.7230795454545454, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882843137254902, "top": 0.7376275252525253, "width": 0.32853921568627453, "height": 0.012727272727272587, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1188"}, {"text": "We ended the study with an exit interview, asking about the participants general experience with the prototype.", "label": "Author", "bboxes": [{"left": 0.5177058823529412, "top": 0.7382638888888889, "width": 0.3941830065359476, "height": 0.012727272727272587, "page": 4}, {"left": 0.5176895424836602, "top": 0.752810606060606, "width": 0.3531781045751634, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1189"}, {"text": "We analyzed the effect of our visualizations on participants walking time when navigating stairs.", "label": "Author", "bboxes": [{"left": 0.5856307189542483, "top": 0.832814393939394, "width": 0.32633823529411765, "height": 0.012727272727272587, "page": 4}, {"left": 0.517673202614379, "top": 0.8473623737373738, "width": 0.33596732026143794, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1190"}, {"text": "Our experiment had one within-subject factor, Condition (Visualizations, No Visualizations), and one measure, Time .", "label": "Author", "bboxes": [{"left": 0.8595359477124183, "top": 0.8473623737373738, "width": 0.05236601307189537, "height": 0.01272727272727281, "page": 4}, {"left": 0.517673202614379, "top": 0.8619090909090908, "width": 0.3942042483660131, "height": 0.01272727272727281, "page": 4}, {"left": 0.517673202614379, "top": 0.8764558080808081, "width": 0.3419803921568627, "height": 0.012727272727272587, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1191"}, {"text": "We de-", "label": "Author", "bboxes": [{"left": 0.8639689542483661, "top": 0.8764558080808081, "width": 0.047844771241830064, "height": 0.012727272727272587, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1192"}, {"text": "We indicated the start points with yellow stickers on the landings, three feet away from the top and bottom stairs.", "label": "Author", "bboxes": [{"left": 0.5176895424836602, "top": 0.5634040404040405, "width": 0.39417320261437894, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.577645202020202, "width": 0.3675245098039215, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1193"}, {"text": "We recorded the time for each task.", "label": "Author", "bboxes": [{"left": 0.5177222222222222, "top": 0.6503813131313131, "width": 0.23283823529411773, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1194"}, {"text": "After the participant experienced all design alternatives in all three phases, we asked them to select one alternative from", "label": "Author", "bboxes": [{"left": 0.0882516339869281, "top": 0.8688459595959596, "width": 0.394140522875817, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882516339869281, "top": 0.8833926767676769, "width": 0.39422712418300654, "height": 0.012727272727272587, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1195"}, {"text": "We started the session with an interview, asking each participant about their demographics, visual condition, and technology use when navigating stairs.", "label": "Author", "bboxes": [{"left": 0.15950326797385622, "top": 0.5121515151515151, "width": 0.3229313725490196, "height": 0.012727272727272698, "page": 4}, {"left": 0.0882516339869281, "top": 0.5266982323232323, "width": 0.39412418300653596, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882516339869281, "top": 0.5412449494949495, "width": 0.2897549019607843, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1196"}, {"text": "After the interview, we walked the participant to the staircase and continued the study with a visualization experience session and a stair navigation session.", "label": "Author", "bboxes": [{"left": 0.4225882352941177, "top": 0.5700340909090909, "width": 0.05982189542483657, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882516339869281, "top": 0.5845820707070707, "width": 0.39421732026143796, "height": 0.01272727272727281, "page": 4}, {"left": 0.08823529411764706, "top": 0.5991287878787879, "width": 0.3941911764705882, "height": 0.012727272727272698, "page": 4}, {"left": 0.0882516339869281, "top": 0.6136767676767677, "width": 0.1679640522875817, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1197"}, {"text": "To reduce order effects, we used a simultaneous within-subjects design, switching the task condition after each walking up and down task.", "label": "Author", "bboxes": [{"left": 0.5177222222222222, "top": 0.6725012626262626, "width": 0.3941405228758169, "height": 0.01272727272727281, "page": 4}, {"left": 0.5177058823529412, "top": 0.6870492424242425, "width": 0.3941813725490195, "height": 0.012727272727272587, "page": 4}, {"left": 0.5177058823529412, "top": 0.7015959595959596, "width": 0.1301013071895425, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1198"}, {"text": "We counterbalanced the starting task (up/down) and condition (with/without the prototype).", "label": "Author", "bboxes": [{"left": 0.6555816993464052, "top": 0.7015959595959596, "width": 0.2562810457516339, "height": 0.01272727272727281, "page": 4}, {"left": 0.5177058823529412, "top": 0.7161439393939394, "width": 0.35490849673202607, "height": 0.012727272727272587, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1199"}, {"text": "They conducted each task in two conditions: (1) walking in their original way (participants could use a cane if desired, but nobody chose to use it); (2) walking using our prototype with their preferred combinations.", "label": "Author", "bboxes": [{"left": 0.6529133986928105, "top": 0.483094696969697, "width": 0.25903267973856203, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.49764141414141416, "width": 0.3941977124183007, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.5121893939393939, "width": 0.3941944444444444, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.5267361111111111, "width": 0.3563349673202614, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1200"}, {"text": "We asked the participant to hold a regular phone with the back camera facing the stairs, assuming the projected visualizations were from the smartphone.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.4172941919191919, "width": 0.39416666666666667, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4318421717171717, "width": 0.3940915032679739, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4463888888888889, "width": 0.22724183006535947, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1201"}, {"text": "We also implemented the auditory feedback on the smartphone.", "label": "Author", "bboxes": [{"left": 0.31862581699346404, "top": 0.4463888888888889, "width": 0.16376307189542483, "height": 0.012727272727272698, "page": 4}, {"left": 0.0882516339869281, "top": 0.46093686868686873, "width": 0.2534068627450981, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1202"}, {"text": "Although we provided different visualizations (flash or movement) to further enhance the end highlights, most participants (seven out of 12) liked the original design.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.8230833333333333, "width": 0.3942663398692811, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8376313131313131, "width": 0.3941421568627451, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8521780303030303, "width": 0.331998366013072, "height": 0.01272727272727281, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1203"}, {"text": "To validate counterbalancing, we added another between-subject factor, Order (two levels: WithWithout, WithoutWith), into our model.", "label": "Author", "bboxes": [{"left": 0.34990359477124183, "top": 0.24183964646464648, "width": 0.1324722222222222, "height": 0.012727272727272726, "page": 5}, {"left": 0.08821895424836601, "top": 0.25638762626262623, "width": 0.3941666666666667, "height": 0.012727272727272754, "page": 5}, {"left": 0.08820261437908496, "top": 0.27093434343434347, "width": 0.3941307189542484, "height": 0.012727272727272698, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1204"}, {"text": "participants felt our design was helpful and [would make] life easier (P4), especially in relatively dark environments, such as subway stations.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.40093308080808077, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 5}, {"left": 0.08823529411764706, "top": 0.4151742424242424, "width": 0.39414052287581697, "height": 0.012727272727272754, "page": 5}, {"left": 0.08821895424836601, "top": 0.42972222222222217, "width": 0.16533660130718952, "height": 0.012727272727272754, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1205"}, {"text": "We analyzed the participants qualitative feedback by coding the interview transcripts based on grounded theory [66].", "label": "Author", "bboxes": [{"left": 0.08823366013071895, "top": 0.3366868686868687, "width": 0.3941911764705883, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823366013071895, "top": 0.3512348484848485, "width": 0.3666683006535948, "height": 0.012727272727272698, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1206"}, {"text": "Next, we report participants responses on all design alternatives in the three design phases.", "label": "Author", "bboxes": [{"left": 0.08818627450980392, "top": 0.5536729797979798, "width": 0.39420261437908494, "height": 0.012727272727272698, "page": 5}, {"left": 0.08816993464052288, "top": 0.568219696969697, "width": 0.20695261437908502, "height": 0.01272727272727281, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1207"}, {"text": "As such, our visualizations aim to facilitate a comfortable head pose by indicating the users exact location on the stairs without augmenting the stairs directly.", "label": "Author", "bboxes": [{"left": 0.893653594771242, "top": 0.6721717171717172, "width": 0.018207516339869168, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176960784313726, "top": 0.6867196969696969, "width": 0.39421078431372547, "height": 0.01272727272727281, "page": 6}, {"left": 0.5177124183006535, "top": 0.7009608585858585, "width": 0.39422385620915046, "height": 0.01272727272727281, "page": 6}, {"left": 0.5177124183006535, "top": 0.7155088383838384, "width": 0.2228480392156864, "height": 0.01272727272727281, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1208"}, {"text": "Our visualizations improved participants psychological security when walking on stairs.", "label": "Author", "bboxes": [{"left": 0.6788235294117647, "top": 0.07005050505050504, "width": 0.2329934640522877, "height": 0.01272727272727274, "page": 6}, {"left": 0.517625816993464, "top": 0.08459722222222223, "width": 0.3616274509803922, "height": 0.012727272727272726, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1209"}, {"text": "Participants all gave high scores to their psychological security when using our prototype ( mean =6.6, SD =0.67), as shown in Figure 4.", "label": "Author", "bboxes": [{"left": 0.8845245098039215, "top": 0.08459722222222223, "width": 0.027274509803921543, "height": 0.012727272727272726, "page": 6}, {"left": 0.517625816993464, "top": 0.09914520202020201, "width": 0.3941013071895425, "height": 0.012727272727272726, "page": 6}, {"left": 0.517625816993464, "top": 0.11369191919191919, "width": 0.3942483660130719, "height": 0.01272727272727274, "page": 6}, {"left": 0.517625816993464, "top": 0.128239898989899, "width": 0.059802287581699365, "height": 0.012727272727272726, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1210"}, {"text": "Based on our observations of the walking tasks, some participants ( e.g ., P9, P4) looked down less when using our design since they could use their lower peripheral vision to notice the highlights.", "label": "Author", "bboxes": [{"left": 0.21390849673202617, "top": 0.5066843434343434, "width": 0.26850326797385615, "height": 0.01272727272727281, "page": 6}, {"left": 0.08823692810457516, "top": 0.5212323232323232, "width": 0.3941781045751634, "height": 0.01272727272727281, "page": 6}, {"left": 0.08822058823529412, "top": 0.5357790404040403, "width": 0.39414542483660137, "height": 0.012727272727272698, "page": 6}, {"left": 0.08822058823529412, "top": 0.5503270202020202, "width": 0.20637745098039217, "height": 0.01272727272727281, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1211"}, {"text": "The second platform we explored was optical see-through smartglasses.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.3885063131313131, "width": 0.3941503267973855, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.4030542929292929, "width": 0.08672058823529405, "height": 0.012727272727272698, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1212"}, {"text": "Some participants ( e.g ., P6, P3, P11) hesitated at the first and last stairs and felt the stairs with their feet when walking without our visualizations (especially in the first two trials of the walking tasks).", "label": "Author", "bboxes": [{"left": 0.08822058823529412, "top": 0.6597297979797979, "width": 0.3941748366013072, "height": 0.01272727272727281, "page": 6}, {"left": 0.08822058823529412, "top": 0.6742777777777778, "width": 0.3941437908496732, "height": 0.012727272727272587, "page": 6}, {"left": 0.08822058823529412, "top": 0.6885189393939394, "width": 0.3941584967320262, "height": 0.012727272727272587, "page": 6}, {"left": 0.08822058823529412, "top": 0.7030669191919191, "width": 0.13227614379084968, "height": 0.01272727272727281, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1213"}, {"text": "When using our visualizations, they stopped feeling the stairs with their feet.", "label": "Author", "bboxes": [{"left": 0.2291764705882353, "top": 0.7030669191919191, "width": 0.2531993464052288, "height": 0.01272727272727281, "page": 6}, {"left": 0.08818790849673203, "top": 0.7176136363636364, "width": 0.27010130718954245, "height": 0.012727272727272587, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1214"}, {"text": "Some participants ( e.g ., P7, P11) walked without holding the railing when using our visualizations.", "label": "Author", "bboxes": [{"left": 0.36356045751633986, "top": 0.7176136363636364, "width": 0.11880228758169936, "height": 0.012727272727272587, "page": 6}, {"left": 0.08818790849673203, "top": 0.7321616161616161, "width": 0.3941176470588235, "height": 0.01272727272727281, "page": 6}, {"left": 0.08818790849673203, "top": 0.7467083333333334, "width": 0.12219607843137253, "height": 0.012727272727272587, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1215"}, {"text": "P10 also changed how he balanced his body when using our prototype: without our design, he walked down leaning his left shoulder forward instead of facing forward.", "label": "Author", "bboxes": [{"left": 0.21679084967320258, "top": 0.7467083333333334, "width": 0.2655375816993464, "height": 0.012727272727272587, "page": 6}, {"left": 0.08818790849673203, "top": 0.761256313131313, "width": 0.3942075163398693, "height": 0.01272727272727281, "page": 6}, {"left": 0.08818790849673203, "top": 0.7758030303030303, "width": 0.3941748366013072, "height": 0.012727272727272587, "page": 6}, {"left": 0.08820424836601308, "top": 0.7903497474747475, "width": 0.08078104575163399, "height": 0.01272727272727281, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1216"}, {"text": "Our visualizations reduced the time participants spent during stair navigation.", "label": "Author", "bboxes": [{"left": 0.19405882352941176, "top": 0.23033459595959596, "width": 0.2883513071895425, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.24488131313131312, "width": 0.23941666666666667, "height": 0.012727272727272754, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1217"}, {"text": "With a paired t -test, we found a considerable trend towards significance when evaluating the effect of Condition on the time walking downstairs ( t 11 =-2.131, p =0.0565) with an effect size of 0.615 (Cohens d ).", "label": "Author", "bboxes": [{"left": 0.16894934640522877, "top": 0.3030707070707071, "width": 0.31342320261437906, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.3176186868686869, "width": 0.39419281045751625, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.3318396464646465, "width": 0.39415032679738565, "height": 0.01274747474747473, "page": 6}, {"left": 0.08824509803921568, "top": 0.34638762626262626, "width": 0.34104575163398687, "height": 0.012727272727272698, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1218"}, {"text": "With a paired t -test, we also found a trend towards significant effect of Condition on the time walking upstairs ( t 11 =1.9894, p =0.0721).", "label": "Author", "bboxes": [{"left": 0.2585996732026144, "top": 0.45579040404040405, "width": 0.22384477124183, "height": 0.012727272727272698, "page": 6}, {"left": 0.08824509803921568, "top": 0.47033838383838383, "width": 0.39413071895424834, "height": 0.012727272727272754, "page": 6}, {"left": 0.08824509803921568, "top": 0.4848699494949495, "width": 0.266436274509804, "height": 0.012742424242424222, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1219"}, {"text": "We conducted a user study to evaluate the visualizations we designed for commercial smartglasses.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.7388093434343433, "width": 0.3941911764705881, "height": 0.01272727272727281, "page": 7}, {"left": 0.5176633986928104, "top": 0.7533573232323232, "width": 0.25916503267973856, "height": 0.01272727272727281, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1220"}, {"text": "We aim to answer:", "label": "Author", "bboxes": [{"left": 0.7828725490196078, "top": 0.7533573232323232, "width": 0.12894934640522882, "height": 0.01272727272727281, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1221"}, {"text": "Thus, to better inform the user of their position on the stairs, we distinguish a users position on a set of stairs based on how close she is to a change in her step pattern.", "label": "Author", "bboxes": [{"left": 0.08821895424836601, "top": 0.31457575757575756, "width": 0.39408823529411774, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.32912247474747475, "width": 0.3942924836601307, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.3436691919191919, "width": 0.31083986928104573, "height": 0.012727272727272754, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1222"}, {"text": "We provide feedback to indicate that a change is approaching, and then that the change is about to occur.", "label": "Author", "bboxes": [{"left": 0.16447712418300653, "top": 0.3873118686868687, "width": 0.31792320261437906, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.40185858585858586, "width": 0.364468954248366, "height": 0.012727272727272698, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1223"}, {"text": "Similar to projection-based AR, when the user stands on the landing, our system verbally notifies the user of the existence of the stairs with stair direction and the number of stairs.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.21971843434343433, "width": 0.3941911764705882, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.23426641414141414, "width": 0.3941601307189542, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.24881313131313132, "width": 0.3698970588235294, "height": 0.012727272727272698, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1224"}, {"text": "Specifically, the following are the seven stages we used in our design, described for descending stairs as an example (Figure 6): (1) Upper landing: the flat surface that is more than 3' away from the edge of the top stair; (2) Upper preparation area: 1.5'3' away from the top stair edge where the person should prepare to step down; (3) Upper alert area: within 1.5' from the top stair edge where the persons next step would be stepping down; (4) Middle stairs: between the edge of the top stair and the edge of the second-to-last stair, where the person is stepping down repeatedly; (5) Lower preparation area: the last stair, where the person is one step away from the flat surface and should prepare for the imminent flat surface; (6) Lower alert area: within 1.5' from the last stair edge on the landing where the persons next step is on the flat surface (not stepping down); (7) Lower landing: 1.5' away from the last stair edge where the person is walking on flat surface again.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.42367297979797974, "width": 0.39420098039215684, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.43822095959595964, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.45276767676767676, "width": 0.39417320261437905, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.46731565656565655, "width": 0.3942124183006535, "height": 0.012727272727272754, "page": 7}, {"left": 0.08821895424836601, "top": 0.48186237373737373, "width": 0.3942091503267974, "height": 0.012727272727272698, "page": 7}, {"left": 0.08821895424836601, "top": 0.4964090909090909, "width": 0.39410784313725494, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5109570707070707, "width": 0.3942794117647059, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5255037878787879, "width": 0.39420424836601314, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5400517676767677, "width": 0.394140522875817, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5545984848484848, "width": 0.3941568627450981, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5688409090909091, "width": 0.3941568627450981, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5833876262626263, "width": 0.39419281045751636, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.597935606060606, "width": 0.3942042483660131, "height": 0.012727272727272698, "page": 7}, {"left": 0.08821895424836601, "top": 0.6124823232323232, "width": 0.3941666666666667, "height": 0.012727272727272698, "page": 7}, {"left": 0.08821895424836601, "top": 0.627030303030303, "width": 0.39409803921568626, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.6415770202020202, "width": 0.39420424836601314, "height": 0.012727272727272587, "page": 7}, {"left": 0.08820261437908496, "top": 0.6561237373737373, "width": 0.1398186274509804, "height": 0.01272727272727281, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1225"}, {"text": "Our visualizations inform PLV of the different stair stages via different design.", "label": "Author", "bboxes": [{"left": 0.23304411764705882, "top": 0.6561237373737373, "width": 0.2493480392156863, "height": 0.01272727272727281, "page": 7}, {"left": 0.08821895424836601, "top": 0.6706717171717173, "width": 0.26529084967320266, "height": 0.012727272727272587, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1226"}, {"text": "We design two visualizations and one sonification.", "label": "Author", "bboxes": [{"left": 0.3571503267973856, "top": 0.6706717171717173, "width": 0.12527614379084967, "height": 0.012727272727272587, "page": 7}, {"left": 0.08821895424836601, "top": 0.6852184343434343, "width": 0.21148202614379086, "height": 0.01272727272727281, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1227"}, {"text": "(1) Glow visualization (Figure 7ad): We generate a glow effect at the bottom of the display to simulate the experience of seeing the edge highlights on the stairs with peripheral vision.", "label": "Author", "bboxes": [{"left": 0.08821895424836601, "top": 0.7073383838383839, "width": 0.39422385620915035, "height": 0.01272727272727281, "page": 7}, {"left": 0.08823529411764706, "top": 0.7218863636363637, "width": 0.3941633986928104, "height": 0.012727272727272587, "page": 7}, {"left": 0.08823529411764706, "top": 0.7364330808080808, "width": 0.39417483660130714, "height": 0.01272727272727281, "page": 7}, {"left": 0.08823529411764706, "top": 0.7506755050505051, "width": 0.03136601307189542, "height": 0.01272727272727281, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1228"}, {"text": "We then gave the HoloLens to the participant and explained how to use it.", "label": "Author", "bboxes": [{"left": 0.33354901960784317, "top": 0.5779368686868687, "width": 0.14881045751633987, "height": 0.012727272727272698, "page": 8}, {"left": 0.08818627450980392, "top": 0.5924848484848485, "width": 0.3545588235294117, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1229"}, {"text": "We continued the study with a design exploration session and a stair navigation session.", "label": "Author", "bboxes": [{"left": 0.42598039215686273, "top": 0.6361262626262626, "width": 0.0563790849673203, "height": 0.01272727272727281, "page": 8}, {"left": 0.08818627450980392, "top": 0.6506729797979798, "width": 0.394140522875817, "height": 0.01272727272727281, "page": 8}, {"left": 0.08818627450980392, "top": 0.6652209595959596, "width": 0.12438562091503268, "height": 0.012727272727272587, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1230"}, {"text": "We conducted the design exploration session at an emergency staircase with 12 stairs (different stairs than those in the projection study).", "label": "Author", "bboxes": [{"left": 0.08818627450980392, "top": 0.6873409090909092, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 8}, {"left": 0.08818627450980392, "top": 0.7018876262626262, "width": 0.3941764705882353, "height": 0.01272727272727281, "page": 8}, {"left": 0.08820261437908496, "top": 0.7161300505050504, "width": 0.14408496732026144, "height": 0.01272727272727281, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1231"}, {"text": "We counterbalanced by randomizing the presentation order of the four designs.", "label": "Author", "bboxes": [{"left": 0.23737581699346405, "top": 0.8034128787878788, "width": 0.24497058823529416, "height": 0.01272727272727281, "page": 8}, {"left": 0.08816993464052288, "top": 0.8179608585858587, "width": 0.27782516339869284, "height": 0.012727272727272587, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1232"}, {"text": "After the participant experienced all the design alternatives, we asked for their preferred combination.", "label": "Author", "bboxes": [{"left": 0.3708218954248366, "top": 0.8179608585858587, "width": 0.1115212418300654, "height": 0.012727272727272587, "page": 8}, {"left": 0.08816993464052288, "top": 0.8325075757575757, "width": 0.3941078431372549, "height": 0.01272727272727281, "page": 8}, {"left": 0.08816993464052288, "top": 0.8470555555555557, "width": 0.1837663398692811, "height": 0.012727272727272587, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1233"}, {"text": "To minimize the confounding effect of general computer vision accuracy, we marked the position of the stairs with two Vuforia image targets [37] (on the side walls at the top and bottom landing of the stairs) that can be recognized by HoloLens.", "label": "Author", "bboxes": [{"left": 0.08821895424836601, "top": 0.3815555555555556, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 8}, {"left": 0.08821895424836601, "top": 0.3961022727272728, "width": 0.39419444444444446, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.41065025252525256, "width": 0.39425326797385624, "height": 0.012727272727272698, "page": 8}, {"left": 0.08821895424836601, "top": 0.4251969696969697, "width": 0.39415686274509804, "height": 0.012727272727272754, "page": 8}, {"left": 0.08821895424836601, "top": 0.43974368686868687, "width": 0.048620915032679735, "height": 0.012727272727272754, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1234"}, {"text": "This provided an anchor in the environment, which enabled our application to determine the position of the user on the stairs by tracking the motion of the HoloLens, improving the accuracy of our visualizations and sonification.", "label": "Author", "bboxes": [{"left": 0.1416830065359477, "top": 0.43974368686868687, "width": 0.34072058823529405, "height": 0.012727272727272754, "page": 8}, {"left": 0.08821895424836601, "top": 0.45429166666666665, "width": 0.39407189542483656, "height": 0.012727272727272754, "page": 8}, {"left": 0.08820261437908496, "top": 0.46883838383838383, "width": 0.3941732026143791, "height": 0.012727272727272698, "page": 8}, {"left": 0.08821895424836601, "top": 0.4833863636363636, "width": 0.35759313725490194, "height": 0.012727272727272754, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1235"}, {"text": "We first report the effect of the HoloLens on participants visual abilities.", "label": "Author", "bboxes": [{"left": 0.7512826797385621, "top": 0.7748699494949495, "width": 0.16049673202614378, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176307189542484, "top": 0.7894179292929293, "width": 0.3207320261437907, "height": 0.012727272727272587, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1236"}, {"text": "We analyzed the effect of our visualizations on participants walking time when navigating stairs.", "label": "Author", "bboxes": [{"left": 0.5854820261437909, "top": 0.4634419191919192, "width": 0.32633823529411765, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175245098039216, "top": 0.47798989898989896, "width": 0.33596732026143783, "height": 0.012727272727272754, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1237"}, {"text": "Our experiment had one within-subject factor, Condition (No HoloLens; HoloLens w/o visualizations; Visualizations), and one measure, Time .", "label": "Author", "bboxes": [{"left": 0.859388888888889, "top": 0.47798989898989896, "width": 0.052294117647058824, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175245098039216, "top": 0.49253661616161615, "width": 0.3941584967320262, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175081699346404, "top": 0.5067790404040404, "width": 0.39423366013071903, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175245098039216, "top": 0.5213257575757576, "width": 0.1266601307189541, "height": 0.01272727272727281, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1238"}, {"text": "We defined a Trial (15) as one walking task.", "label": "Author", "bboxes": [{"left": 0.6479901960784313, "top": 0.5213257575757576, "width": 0.26369281045751636, "height": 0.01272727272727281, "page": 8}, {"left": 0.5175245098039216, "top": 0.5358737373737373, "width": 0.030428104575163317, "height": 0.01272727272727281, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1239"}, {"text": "We determined Time from the video we recorded during the study.", "label": "Author", "bboxes": [{"left": 0.5525343137254902, "top": 0.5358737373737373, "width": 0.3592238562091503, "height": 0.01272727272727281, "page": 8}, {"left": 0.5175245098039216, "top": 0.5504204545454545, "width": 0.08653267973856205, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1240"}, {"text": "When analyzing data, we removed the first trial, treating it as a practice trial for participants to get used to the HoloLens.", "label": "Author", "bboxes": [{"left": 0.6072075163398692, "top": 0.5504204545454545, "width": 0.30865522875817, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175245098039216, "top": 0.5649684343434344, "width": 0.39403594771241823, "height": 0.01272727272727281, "page": 8}, {"left": 0.5175245098039216, "top": 0.5795151515151515, "width": 0.06856699346405226, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1241"}, {"text": "We asked them to score the usefulness and comfort level of the prototype on a Likert scale, as well as their psychological security when using the prototype, ranging from 1 (strongly negative) to 7 (strongly positive).", "label": "Author", "bboxes": [{"left": 0.8462124183006535, "top": 0.3831338383838384, "width": 0.0655065359477125, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175408496732027, "top": 0.3976805555555556, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175408496732027, "top": 0.4122272727272727, "width": 0.3941078431372548, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175245098039216, "top": 0.4267752525252525, "width": 0.39422385620915035, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175245098039216, "top": 0.4413219696969697, "width": 0.15342973856209152, "height": 0.012727272727272754, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1242"}, {"text": "We built our prototype on Microsoft HoloLens v1.", "label": "Author", "bboxes": [{"left": 0.16765686274509803, "top": 0.24336237373737374, "width": 0.3147483660130719, "height": 0.012727272727272754, "page": 8}, {"left": 0.08823529411764706, "top": 0.2579090909090909, "width": 0.02052287581699347, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1243"}, {"text": "We chose HoloLens because of its FOV (~34 diagonal), binocular displays, and ability to be worn with eyeglasses.", "label": "Author", "bboxes": [{"left": 0.11233169934640523, "top": 0.2579090909090909, "width": 0.370140522875817, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.2724570707070707, "width": 0.3941078431372549, "height": 0.012727272727272754, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1244"}, {"text": "We recruited 12 PLV (5 female, 7 male; mean age=51.6) with different low vision conditions (Table 1, P6 P17).", "label": "Author", "bboxes": [{"left": 0.17829575163398695, "top": 0.1342638888888889, "width": 0.30414869281045753, "height": 0.012727272727272726, "page": 8}, {"left": 0.0882516339869281, "top": 0.1488118686868687, "width": 0.3942565359477125, "height": 0.012727272727272726, "page": 8}, {"left": 0.0882516339869281, "top": 0.16335858585858584, "width": 0.03499019607843139, "height": 0.012727272727272754, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1245"}, {"text": "Seven participants had taken part in the evaluation of our projection-based AR visualizations, but they did not see the stairs used in this study.", "label": "Author", "bboxes": [{"left": 0.36164705882352943, "top": 0.16335858585858584, "width": 0.12077941176470586, "height": 0.012727272727272754, "page": 8}, {"left": 0.0882516339869281, "top": 0.17790656565656565, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 8}, {"left": 0.08823529411764706, "top": 0.19214772727272728, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.20669570707070709, "width": 0.039537581699346416, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1246"}, {"text": "We followed the same recruitment procedures as in the previous study.", "label": "Author", "bboxes": [{"left": 0.1336111111111111, "top": 0.20669570707070709, "width": 0.34882516339869274, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.22124242424242424, "width": 0.12385620915032682, "height": 0.012727272727272754, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1247"}, {"text": "We indicated the start and end points on the stairs with stickers that were three feet away from the top and bottom steps on the landings.", "label": "Author", "bboxes": [{"left": 0.5175898692810458, "top": 0.19372601010101012, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175898692810458, "top": 0.20827272727272728, "width": 0.3941813725490195, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175898692810458, "top": 0.22282070707070706, "width": 0.10745424836601292, "height": 0.012727272727272754, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1248"}, {"text": "We recorded the time for each task.", "label": "Author", "bboxes": [{"left": 0.657843137254902, "top": 0.281010101010101, "width": 0.2327516339869281, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1249"}, {"text": "To validate counterbalancing, we added another betweensubject factor, Order (six levels based on the three conditions), into our model.", "label": "Author", "bboxes": [{"left": 0.5175245098039216, "top": 0.601635101010101, "width": 0.3941584967320262, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175245098039216, "top": 0.6161818181818182, "width": 0.3941078431372549, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175081699346404, "top": 0.630729797979798, "width": 0.1468709150326798, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1250"}, {"text": "To reduce the effect of order on the results, we used a simultaneous within-subjects design by switching the task condition after each round of walking up and down.", "label": "Author", "bboxes": [{"left": 0.5175735294117647, "top": 0.30313005050505054, "width": 0.39399346405228763, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175735294117647, "top": 0.31767676767676767, "width": 0.3942075163398693, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175571895424836, "top": 0.3319191919191919, "width": 0.2976928104575164, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1251"}, {"text": "We also counterbalanced the starting task (up/down) and the conditions.", "label": "Author", "bboxes": [{"left": 0.8186732026143791, "top": 0.3319191919191919, "width": 0.09317320261437889, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175571895424836, "top": 0.34646590909090913, "width": 0.38311764705882345, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1252"}, {"text": "We analyzed participants qualitative responses with the same method we used in the previous study.", "label": "Author", "bboxes": [{"left": 0.517656862745098, "top": 0.7251717171717171, "width": 0.39416993464052286, "height": 0.01272727272727281, "page": 8}, {"left": 0.517640522875817, "top": 0.7397196969696969, "width": 0.28866339869281055, "height": 0.01272727272727281, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1253"}, {"text": "Interestingly, we found that participants had different preferences for Paths position in their visual field.", "label": "Author", "bboxes": [{"left": 0.517640522875817, "top": 0.3176818181818182, "width": 0.3941078431372549, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.33192424242424245, "width": 0.28760947712418305, "height": 0.012727272727272698, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1254"}, {"text": "As he said, [Path] would be my favorite if we were able to get it to [get close] to the stairs instead hanging up in the middle of everything.", "label": "Author", "bboxes": [{"left": 0.691140522875817, "top": 0.4337550505050505, "width": 0.22072058823529406, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176895424836602, "top": 0.44830176767676766, "width": 0.39416993464052275, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176895424836602, "top": 0.4628484848484849, "width": 0.3166339869281045, "height": 0.012727272727272698, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1255"}, {"text": "We", "label": "Author", "bboxes": [{"left": 0.45969607843137256, "top": 0.1357840909090909, "width": 0.022730392156862733, "height": 0.012727272727272726, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1256"}, {"text": "Our design changed peoples behaviors when walking on stairs.", "label": "Author", "bboxes": [{"left": 0.6475539215686275, "top": 0.7606654040404041, "width": 0.2642369281045751, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176503267973857, "top": 0.7752121212121211, "width": 0.1580653594771242, "height": 0.01272727272727281, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1257"}, {"text": "Moreover, we tracked participants head orientation with HoloLens during the walking tasks, and found that some participants ( e.g ., P6, P9) head orientation changed when using our visualizations.", "label": "Author", "bboxes": [{"left": 0.589093137254902, "top": 0.8043068181818182, "width": 0.32282026143790854, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176503267973857, "top": 0.8188535353535354, "width": 0.3940392156862744, "height": 0.012727272727272587, "page": 10}, {"left": 0.5176503267973857, "top": 0.8334015151515152, "width": 0.39415686274509787, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176503267973857, "top": 0.8476426767676767, "width": 0.19614869281045755, "height": 0.01272727272727281, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1258"}, {"text": "We found", "label": "Author", "bboxes": [{"left": 0.8476584967320262, "top": 0.8767373737373737, "width": 0.06399183006535936, "height": 0.01272727272727281, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1259"}, {"text": "We found that theres no significant effect of Condition (HoloLens with visualizations vs. HoloLens without visualizations) on participants walking time for both ascending ( F (1,10) =0.466, p =0.511) and descending stairs ( F (1,10) =0.114, p =0.742).", "label": "Author", "bboxes": [{"left": 0.7176797385620916, "top": 0.2733623737373737, "width": 0.1942173202614379, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176437908496732, "top": 0.2879090909090909, "width": 0.3982598039215687, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176437908496732, "top": 0.3024558080808081, "width": 0.39418137254901964, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176437908496732, "top": 0.31668813131313134, "width": 0.39422875816993475, "height": 0.012737373737373658, "page": 10}, {"left": 0.5176486928104574, "top": 0.33123358585858587, "width": 0.2712467320261438, "height": 0.012729797979797952, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1260"}, {"text": "Except for P17, who slowed down a lot when walking downstairs with our visualizations, all other participants times increased by less than 1 second.", "label": "Author", "bboxes": [{"left": 0.657718954248366, "top": 0.3894229797979798, "width": 0.2541323529411764, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176503267973857, "top": 0.403969696969697, "width": 0.39426960784313725, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176503267973857, "top": 0.41851767676767676, "width": 0.34263071895424824, "height": 0.012727272727272698, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1261"}, {"text": "We investigated and found that P17 had a hard time seeing the blue glow on middle stairs in the bright environment.", "label": "Author", "bboxes": [{"left": 0.8655718954248366, "top": 0.41851767676767676, "width": 0.046202614379084905, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176503267973857, "top": 0.43306439393939394, "width": 0.3941911764705882, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176503267973857, "top": 0.4476123737373738, "width": 0.31533006535947705, "height": 0.012727272727272698, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1262"}, {"text": "Paired Wilcoxon Signed-Rank tests showed that, while wearing HoloLens significantly reduced participants psychological security ( V =8, p =0.031), our visualizations significantly increased participant psychological security compared with not wearing HoloLens ( V =21, p =0.050).", "label": "Author", "bboxes": [{"left": 0.5896470588235294, "top": 0.6803560606060606, "width": 0.3221176470588235, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.6949027777777779, "width": 0.3942859477124183, "height": 0.012727272727272587, "page": 10}, {"left": 0.5176339869281046, "top": 0.7094507575757576, "width": 0.39417320261437894, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.7239974747474747, "width": 0.39426797385620904, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.7385454545454545, "width": 0.3647075163398693, "height": 0.01272727272727281, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1263"}, {"text": "We found that most participants (10 out of 12) combined a visualization with a sonification (Beep).", "label": "Author", "bboxes": [{"left": 0.0882516339869281, "top": 0.3242638888888889, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882516339869281, "top": 0.3388118686868687, "width": 0.2609101307189543, "height": 0.012727272727272698, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1264"}, {"text": "While there is no significant improvement in walking speed when using the visualizations, participants reported feeling safer and more confident when using our design.", "label": "Author", "bboxes": [{"left": 0.6847941176470588, "top": 0.48427904040404035, "width": 0.2270800653594771, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176666666666667, "top": 0.49852146464646463, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176666666666667, "top": 0.5130681818181818, "width": 0.39417156862745095, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176830065359477, "top": 0.5276161616161616, "width": 0.1139101307189544, "height": 0.012727272727272698, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1265"}, {"text": "P11 described her experience when using our prototype, I love the fact that the [visualizations] are there. Once you understand what they mean, you can actually move more confidently I would be very safe instead of falling down and kicking things.", "label": "Author", "bboxes": [{"left": 0.6365179738562091, "top": 0.5276161616161616, "width": 0.27536601307189545, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176830065359477, "top": 0.5421628787878787, "width": 0.39418464052287583, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176830065359477, "top": 0.5567108585858586, "width": 0.3941584967320262, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176830065359477, "top": 0.5712575757575757, "width": 0.3941405228758169, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176830065359477, "top": 0.5858042929292929, "width": 0.21925653594771244, "height": 0.012727272727272698, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1266"}, {"text": "With ANOVA, we found that participants walking time significantly increased when they walked downstairs wearing the HoloLens whether using our visualizations or not ( F (2,12) =8.783, p =0.0045).", "label": "Author", "bboxes": [{"left": 0.20690522875816994, "top": 0.7391477272727273, "width": 0.2754869281045751, "height": 0.01272727272727281, "page": 10}, {"left": 0.0882843137254902, "top": 0.7536957070707071, "width": 0.3942369281045752, "height": 0.012727272727272587, "page": 10}, {"left": 0.0882843137254902, "top": 0.7682424242424242, "width": 0.39422385620915035, "height": 0.01272727272727281, "page": 10}, {"left": 0.0882843137254902, "top": 0.7827487373737374, "width": 0.27779738562091505, "height": 0.01276767676767665, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1267"}, {"text": "With the condition of wearing HoloLens without visualizations as the baseline, we analyzed the effect of our visualiza-", "label": "Author", "bboxes": [{"left": 0.08822222222222222, "top": 0.8627487373737374, "width": 0.39427450980392165, "height": 0.012727272727272587, "page": 10}, {"left": 0.08822222222222222, "top": 0.8772967171717171, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1268"}, {"text": "In general, participants felt that our prototype was helpful, especially in unfamiliar places.", "label": "Author", "bboxes": [{"left": 0.08831699346405228, "top": 0.6155025252525252, "width": 0.3941584967320261, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.6300492424242424, "width": 0.22053104575163393, "height": 0.012727272727272698, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1269"}, {"text": "Our evaluation was conducted indoors, with no other people around.", "label": "Author", "bboxes": [{"left": 0.7362549019607844, "top": 0.5922184343434344, "width": 0.17555882352941177, "height": 0.01272727272727281, "page": 11}, {"left": 0.517640522875817, "top": 0.6067664141414142, "width": 0.2998464052287583, "height": 0.012727272727272698, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1270"}, {"text": "In future work, we will consider these real-world challenges when developing AR stair navigation systems.", "label": "Author", "bboxes": [{"left": 0.5176078431372549, "top": 0.6937436868686869, "width": 0.39417320261437905, "height": 0.012727272727272587, "page": 11}, {"left": 0.5175898692810458, "top": 0.7082916666666667, "width": 0.30403431372549017, "height": 0.01272727272727281, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1271"}, {"text": "For example, besides recognizing stairs with computer vision, we will consider instrumenting the environment ( e.g ., using RFID) to foster accurate and fast stair recognition in a complex environment.", "label": "Author", "bboxes": [{"left": 0.8256568627450981, "top": 0.7082916666666667, "width": 0.08623039215686268, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175898692810458, "top": 0.722838383838384, "width": 0.39420751633986917, "height": 0.012727272727272587, "page": 11}, {"left": 0.5175898692810458, "top": 0.7373863636363637, "width": 0.3941519607843137, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175571895424836, "top": 0.7519330808080807, "width": 0.39417483660130725, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175571895424836, "top": 0.7664810606060606, "width": 0.05858333333333332, "height": 0.01272727272727281, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1272"}, {"text": "We will also add face detection to avoid projecting in bystanders faces.", "label": "Author", "bboxes": [{"left": 0.5800147058823529, "top": 0.7664810606060606, "width": 0.3317500000000001, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175571895424836, "top": 0.7810277777777778, "width": 0.13383986928104574, "height": 0.01272727272727281, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1273"}, {"text": "Unlike prior research, which showed that PLV had very different preferences for visual augmentations [84, 85], our study revealed that some common preferences among PLV cross different visual abilities for stair navigation.", "label": "Author", "bboxes": [{"left": 0.45677124183006534, "top": 0.5615593434343434, "width": 0.025457516339869368, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.5761060606060606, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.5906527777777778, "width": 0.39419117647058827, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.6052007575757576, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.6197474747474747, "width": 0.28821895424836597, "height": 0.01272727272727281, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1274"}, {"text": "First, the HoloLenss weight strongly diminished PLVs experiences, which may have influenced our results.", "label": "Author", "bboxes": [{"left": 0.8234003267973856, "top": 0.8031477272727272, "width": 0.08831535947712421, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175408496732027, "top": 0.8173901515151516, "width": 0.3941584967320261, "height": 0.012727272727272587, "page": 11}, {"left": 0.5175408496732027, "top": 0.8319368686868687, "width": 0.2527581699346404, "height": 0.01272727272727281, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1275"}, {"text": "Our research is the first to explore AR visualizations for people with low vision in the context of stair navigation.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.33608207070707075, "width": 0.3942238562091503, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.35063005050505053, "width": 0.3627483660130719, "height": 0.012727272727272698, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1276"}, {"text": "Our studies demonstrate the effectiveness of our designs with both projection-based AR and smartglasses.", "label": "Author", "bboxes": [{"left": 0.4569624183006536, "top": 0.35063005050505053, "width": 0.025460784313725504, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.3651767676767677, "width": 0.39417810457516345, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.3797247474747475, "width": 0.29258333333333336, "height": 0.012727272727272698, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1277"}, {"text": "We found that our visualizations on both platforms largely increased peoples psychological security, making them feel confident and safe when walking on stairs.", "label": "Author", "bboxes": [{"left": 0.3862205882352941, "top": 0.3797247474747475, "width": 0.09615522875816995, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.3942714646464646, "width": 0.3942075163398693, "height": 0.012727272727272754, "page": 11}, {"left": 0.0882516339869281, "top": 0.4088181818181818, "width": 0.3941584967320262, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.4233661616161616, "width": 0.1843839869281046, "height": 0.012727272727272754, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1278"}, {"text": "We compared users experiences with the visualizations on both platforms given that seven participated in both studies.", "label": "Author", "bboxes": [{"left": 0.08821895424836601, "top": 0.6709621212121213, "width": 0.39417320261437905, "height": 0.012727272727272587, "page": 11}, {"left": 0.08821895424836601, "top": 0.6852045454545455, "width": 0.39422385620915035, "height": 0.012727272727272587, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1279"}, {"text": "While our study focused on the design and evaluation of the AR visualizations, we discuss the technical feasibility and challenges for our AR stair navigation systems.", "label": "Author", "bboxes": [{"left": 0.517673202614379, "top": 0.3521957070707071, "width": 0.394140522875817, "height": 0.012727272727272698, "page": 11}, {"left": 0.517673202614379, "top": 0.3667424242424242, "width": 0.3941862745098039, "height": 0.012727272727272754, "page": 11}, {"left": 0.5176895424836602, "top": 0.3812891414141414, "width": 0.315937908496732, "height": 0.012727272727272698, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1280"}, {"text": "While many stair detection methods have been presented in prior research [20, 58], algorithms that locate the exact position of each stair with high speed and accuracy should be investigated and tested to support the stair visualization systems we designed for PLV.", "label": "Author", "bboxes": [{"left": 0.6355196078431372, "top": 0.49736237373737374, "width": 0.2763807189542484, "height": 0.012727272727272754, "page": 11}, {"left": 0.517656862745098, "top": 0.511909090909091, "width": 0.39418954248366, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5264570707070707, "width": 0.3942516339869281, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5410037878787879, "width": 0.39413071895424834, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5555517676767677, "width": 0.28683496732026137, "height": 0.012727272727272698, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1281"}, {"text": "that, he looked much further down to the stairs when not using our visualizations, especially at the beginning and the end of the stairs ( e.g ., preparation area, alert area).", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.2721426767676768, "width": 0.39410784313725494, "height": 0.012727272727272698, "page": 11}, {"left": 0.08823529411764706, "top": 0.286385101010101, "width": 0.3941846405228758, "height": 0.012727272727272698, "page": 11}, {"left": 0.08823529411764706, "top": 0.3009318181818182, "width": 0.30114869281045753, "height": 0.012727272727272754, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1282"}, {"text": "In this paper, we designed AR visualizations to facilitate stair navigation for people with low vision.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.2357790404040404, "width": 0.39419444444444446, "height": 0.012727272727272754, "page": 12}, {"left": 0.08823529411764706, "top": 0.2503270202020202, "width": 0.2511928104575163, "height": 0.012727272727272754, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1283"}, {"text": "We designed visualizations (and sonification) for both projection-based AR and smartglasses based on the different characteristics of these platforms.", "label": "Author", "bboxes": [{"left": 0.3437924836601307, "top": 0.2503270202020202, "width": 0.13859967320261435, "height": 0.012727272727272754, "page": 12}, {"left": 0.08823529411764706, "top": 0.2648737373737374, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 12}, {"left": 0.08821895424836601, "top": 0.27942171717171715, "width": 0.39412418300653596, "height": 0.012727272727272754, "page": 12}, {"left": 0.08821895424836601, "top": 0.29396843434343434, "width": 0.06673856209150325, "height": 0.012727272727272698, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1284"}, {"text": "We evaluated the design on each platform with 12 participants, finding that both visualizations increased participants psychological security, making them feel safer and more confident when walking on stairs.", "label": "Author", "bboxes": [{"left": 0.15843300653594772, "top": 0.29396843434343434, "width": 0.3239901960784313, "height": 0.012727272727272698, "page": 12}, {"left": 0.08820261437908496, "top": 0.3085151515151515, "width": 0.3942107843137255, "height": 0.012727272727272754, "page": 12}, {"left": 0.08820261437908496, "top": 0.32306313131313136, "width": 0.3940751633986928, "height": 0.012727272727272754, "page": 12}, {"left": 0.08820261437908496, "top": 0.3376098484848485, "width": 0.2662287581699347, "height": 0.012727272727272698, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1285"}, {"text": "Moreover, our design for projection-based AR showed a trend towards significantly reducing participants walking time on stairs.", "label": "Author", "bboxes": [{"left": 0.359983660130719, "top": 0.3376098484848485, "width": 0.12244281045751632, "height": 0.012727272727272698, "page": 12}, {"left": 0.08820261437908496, "top": 0.3518522727272727, "width": 0.3942320261437908, "height": 0.012727272727272754, "page": 12}, {"left": 0.08821895424836601, "top": 0.3663989898989899, "width": 0.34444934640522873, "height": 0.012727272727272698, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1286"}, {"text": "the HoloLens, we designed visualizations in the users central vision instead of adding highlights to the stairs in our smartglasses prototype.", "label": "Author", "bboxes": [{"left": 0.08823202614379085, "top": 0.07018813131313131, "width": 0.39417320261437916, "height": 0.012727272727272726, "page": 12}, {"left": 0.0882483660130719, "top": 0.08473611111111112, "width": 0.39414052287581697, "height": 0.012727272727272726, "page": 12}, {"left": 0.0882483660130719, "top": 0.09928282828282829, "width": 0.15778758169934642, "height": 0.012727272727272726, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1287"}, {"text": "Third, we asked participants to score their feeling of psychological security, but these results could be influenced by a novelty effect.", "label": "Author", "bboxes": [{"left": 0.24340032679738563, "top": 0.1426199494949495, "width": 0.23893954248366014, "height": 0.012727272727272726, "page": 12}, {"left": 0.08823202614379085, "top": 0.15716666666666668, "width": 0.394202614379085, "height": 0.012727272727272754, "page": 12}, {"left": 0.08823202614379085, "top": 0.17171338383838386, "width": 0.23277450980392161, "height": 0.012727272727272698, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1288"}, {"text": "This work was supported in part by the National Science Foundation under grant no.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.4015366161616162, "width": 0.3941748366013072, "height": 0.012727272727272698, "page": 12}, {"left": 0.08823529411764706, "top": 0.41608459595959596, "width": 0.17816993464052283, "height": 0.012727272727272754, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1289"}, {"text": "We adjust the glow color and size to inform the user of their current stage on the stairs:", "label": "Author", "bboxes": [{"left": 0.7824869281045752, "top": 0.08461868686868687, "width": 0.1293790849673203, "height": 0.012727272727272726, "page": 7}, {"left": 0.517640522875817, "top": 0.09916666666666668, "width": 0.39417647058823535, "height": 0.012727272727272726, "page": 7}, {"left": 0.517640522875817, "top": 0.11371338383838385, "width": 0.03906699346405229, "height": 0.012727272727272712, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1290"}, {"text": "(2) Path visualization (Figure 7eg): Inspired by the railings, which PLV used as a visual cue to see where the stairs start and end [86], we designed this visualization to show the trend of the stairs.", "label": "Author", "bboxes": [{"left": 0.517640522875817, "top": 0.2631060606060606, "width": 0.39416666666666667, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176241830065359, "top": 0.2776527777777778, "width": 0.3943480392156862, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176241830065359, "top": 0.2922007575757576, "width": 0.394235294117647, "height": 0.012727272727272698, "page": 7}, {"left": 0.517640522875817, "top": 0.3067474747474747, "width": 0.08284640522875819, "height": 0.012727272727272754, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1291"}, {"text": "The Path is generated at the users eye level with a fixed distance from one side of the head (we adjusted its specific position based on the users visual field and preference), making sure that they can see it without looking too far down.", "label": "Author", "bboxes": [{"left": 0.8863169934640523, "top": 0.35008459595959596, "width": 0.02546405228758164, "height": 0.012727272727272698, "page": 7}, {"left": 0.517656862745098, "top": 0.36463131313131314, "width": 0.3941470588235294, "height": 0.012727272727272698, "page": 7}, {"left": 0.517656862745098, "top": 0.3791792929292929, "width": 0.394140522875817, "height": 0.012727272727272698, "page": 7}, {"left": 0.517656862745098, "top": 0.3937260101010101, "width": 0.39417320261437905, "height": 0.012727272727272698, "page": 7}, {"left": 0.517656862745098, "top": 0.40827272727272723, "width": 0.33115686274509815, "height": 0.012727272727272754, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1292"}, {"text": "To better distinguish the landing and the stairs, we colored the straight part of the visualization (over the landing) yellow and the slope blue.", "label": "Author", "bboxes": [{"left": 0.7153300653594771, "top": 0.4373674242424242, "width": 0.19646732026143798, "height": 0.012727272727272754, "page": 7}, {"left": 0.517640522875817, "top": 0.45191540404040403, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 7}, {"left": 0.517640522875817, "top": 0.4664621212121212, "width": 0.3639166666666668, "height": 0.012727272727272698, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1293"}, {"text": "We added virtual pillars to connect the Path to each stair to help users associate the visualization with the physical stairs.", "label": "Author", "bboxes": [{"left": 0.8890849673202614, "top": 0.4664621212121212, "width": 0.022735294117647076, "height": 0.012727272727272698, "page": 7}, {"left": 0.517640522875817, "top": 0.481010101010101, "width": 0.39421568627451, "height": 0.012727272727272754, "page": 7}, {"left": 0.517640522875817, "top": 0.4955568181818182, "width": 0.3666683006535948, "height": 0.012727272727272698, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1294"}, {"text": "Similar to glow, we adjusted the sound based on the different stages of the stairs:", "label": "Author", "bboxes": [{"left": 0.7188055555555556, "top": 0.5319191919191919, "width": 0.19301307189542483, "height": 0.01272727272727281, "page": 7}, {"left": 0.517640522875817, "top": 0.5464659090909091, "width": 0.3379395424836602, "height": 0.012727272727272698, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1295"}, {"text": "(3) How secure do PLV feel when using our visualizations?", "label": "Author", "bboxes": [{"left": 0.08822712418300653, "top": 0.09912878787878789, "width": 0.3908562091503268, "height": 0.012727272727272726, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1296"}, {"text": "In summary, we contribute the first exploration of AR visualizations to facilitate stair navigation for PLV.", "label": "Contribution", "bboxes": [{"left": 0.08825816993464053, "top": 0.48494570707070705, "width": 0.39414215686274506, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.49918813131313133, "width": 0.3113937908496732, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1297"}, {"text": "Despite the difficulty they experience, PLV use their residual vision extensively when navigating stairs [73].", "label": "Novelty", "bboxes": [{"left": 0.5176797385620915, "top": 0.503361111111111, "width": 0.3941078431372549, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176633986928104, "top": 0.5179078282828282, "width": 0.31419281045751646, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1298"}, {"text": "[86] found that they looked at contrast stripes ( i.e ., contrasting marking stripes on stair treads) to perceive the exact location of stair edges; some also observed the trend of the railing to understand the overall structure of a staircase.", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.5324545454545455, "width": 0.3942499999999999, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176633986928104, "top": 0.5470025252525252, "width": 0.39417156862745095, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176797385620915, "top": 0.5615492424242424, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176797385620915, "top": 0.5760972222222223, "width": 0.3941584967320262, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1299"}, {"text": "However, sometimes stairs do not have contrast stripes, and even when they do, their stripes are often not accessibly designed; for example, stripes may have low contrast with the stairs or be too thin to detect [86].", "label": "Novelty", "bboxes": [{"left": 0.5176797385620915, "top": 0.5906439393939394, "width": 0.3941911764705883, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176797385620915, "top": 0.6051919191919192, "width": 0.39425, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176960784313726, "top": 0.6197386363636364, "width": 0.394142156862745, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176960784313726, "top": 0.6339810606060606, "width": 0.23230228758169935, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1300"}, {"text": "Unlike people who are blind, people with low vision (PLV) have functional vision that they use extensively in daily activities [73, 74].", "label": "Novelty", "bboxes": [{"left": 0.26012254901960785, "top": 0.6824494949494949, "width": 0.22235294117647053, "height": 0.01272727272727281, "page": 0}, {"left": 0.0882516339869281, "top": 0.6966919191919192, "width": 0.3941748366013072, "height": 0.01272727272727281, "page": 0}, {"left": 0.08823529411764706, "top": 0.7112386363636364, "width": 0.28517156862745097, "height": 0.012727272727272587, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1301"}, {"text": "Low vision can be attributed to a variety of diseases ( e.g ., glaucoma, diabetic retinopathy) and affects many visual functions including visual acuity, contrast sensitivity, and peripheral vision [21].", "label": "Novelty", "bboxes": [{"left": 0.378843137254902, "top": 0.7112386363636364, "width": 0.10356699346405229, "height": 0.012727272727272587, "page": 0}, {"left": 0.0882516339869281, "top": 0.7257866161616161, "width": 0.3941846405228758, "height": 0.01272727272727281, "page": 0}, {"left": 0.0882516339869281, "top": 0.7403333333333334, "width": 0.394140522875817, "height": 0.012727272727272587, "page": 0}, {"left": 0.0882516339869281, "top": 0.7548800505050505, "width": 0.37666666666666665, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1302"}, {"text": "They found that PLV struggled but used their vision extensively, and lighting conditions affected their ability to notice obstacles and uneven pavement on the ground.", "label": "Novelty", "bboxes": [{"left": 0.5176078431372549, "top": 0.12823358585858588, "width": 0.39422549019607844, "height": 0.012727272727272726, "page": 1}, {"left": 0.5176078431372549, "top": 0.1424760101010101, "width": 0.394140522875817, "height": 0.012727272727272754, "page": 1}, {"left": 0.5176078431372549, "top": 0.15702272727272726, "width": 0.31494934640522876, "height": 0.012727272727272754, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1303"}, {"text": "[86] conducted a more in-depth study observing 14 PLV walking on different sets of stairs indoors and outdoors.", "label": "Novelty", "bboxes": [{"left": 0.5176078431372549, "top": 0.17157070707070704, "width": 0.3941960784313725, "height": 0.012727272727272754, "page": 1}, {"left": 0.5176078431372549, "top": 0.18611742424242425, "width": 0.3573774509803921, "height": 0.012727272727272754, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1304"}, {"text": "They found that most participants relied on their vision ( e.g ., looking at contrast stripes) to navigate stairs.", "label": "Novelty", "bboxes": [{"left": 0.8781486928104576, "top": 0.18611742424242425, "width": 0.03363235294117639, "height": 0.012727272727272754, "page": 1}, {"left": 0.5176078431372549, "top": 0.20066540404040403, "width": 0.3941568627450981, "height": 0.012727272727272754, "page": 1}, {"left": 0.5175898692810458, "top": 0.21521212121212122, "width": 0.27108660130718953, "height": 0.012727272727272698, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1305"}, {"text": "They found that people with low visual acuity and low contrast sensitivity reported difficulty walking up and down stairs without help.", "label": "Novelty", "bboxes": [{"left": 0.12783333333333333, "top": 0.7948888888888889, "width": 0.3543856209150327, "height": 0.012727272727272587, "page": 1}, {"left": 0.08821895424836601, "top": 0.809435606060606, "width": 0.39421078431372547, "height": 0.01272727272727281, "page": 1}, {"left": 0.08821895424836601, "top": 0.8239835858585859, "width": 0.128140522875817, "height": 0.012727272727272587, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1306"}, {"text": "We sought to design effective visualizations for PLV, which balance visibility and distraction, while providing alternative choices to support a wide range of visual abilities.", "label": "Novelty", "bboxes": [{"left": 0.3071535947712418, "top": 0.11370454545454546, "width": 0.1753480392156863, "height": 0.01272727272727274, "page": 1}, {"left": 0.08829248366013072, "top": 0.12825126262626263, "width": 0.39415686274509804, "height": 0.012727272727272726, "page": 1}, {"left": 0.08827450980392157, "top": 0.14249368686868688, "width": 0.39421241830065357, "height": 0.012727272727272726, "page": 1}, {"left": 0.08827450980392157, "top": 0.15704040404040404, "width": 0.192937908496732, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1307"}, {"text": "Our designs considered the different characteristics of the two platforms: (1) For projection, which can augment a large physical space, we designed visual highlights with different patterns that are directly projected onto the stairs to enhance their visibility (Figure 1a).", "label": "Novelty", "bboxes": [{"left": 0.40116666666666667, "top": 0.2082550505050505, "width": 0.08134150326797385, "height": 0.012727272727272754, "page": 1}, {"left": 0.08824183006535948, "top": 0.2228030303030303, "width": 0.3941421568627451, "height": 0.012727272727272698, "page": 1}, {"left": 0.08822549019607843, "top": 0.23734974747474746, "width": 0.39424999999999993, "height": 0.012727272727272754, "page": 1}, {"left": 0.08824183006535948, "top": 0.25189646464646465, "width": 0.3941781045751634, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.26644444444444443, "width": 0.39420751633986933, "height": 0.012727272727272754, "page": 1}, {"left": 0.08822549019607843, "top": 0.2809911616161616, "width": 0.07741176470588236, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1308"}, {"text": "Mobility is critical but challenging for PLV.", "label": "Novelty", "bboxes": [{"left": 0.08823529411764706, "top": 0.5985063131313131, "width": 0.29912254901960783, "height": 0.012727272727272698, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1309"}, {"text": "For example, Leat and LovieKitchin [45] found that visual field loss reduced walking speed, while reduced visual acuity and contrast sensitivity impacted distance and depth perception.", "label": "Novelty", "bboxes": [{"left": 0.28362581699346406, "top": 0.6421489898989899, "width": 0.19873366013071897, "height": 0.01272727272727281, "page": 1}, {"left": 0.08820261437908496, "top": 0.656695707070707, "width": 0.39417320261437916, "height": 0.01272727272727281, "page": 1}, {"left": 0.08820261437908496, "top": 0.6712424242424243, "width": 0.3942369281045752, "height": 0.012727272727272587, "page": 1}, {"left": 0.08820261437908496, "top": 0.685790404040404, "width": 0.2627385620915033, "height": 0.01272727272727281, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1310"}, {"text": "For each display type, devices exist (either commercially or as research prototypes) with different form factors and device characteristics.", "label": "Novelty", "bboxes": [{"left": 0.19626633986928105, "top": 0.6566944444444444, "width": 0.2860866013071895, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.6712411616161617, "width": 0.3941895424836601, "height": 0.012727272727272587, "page": 2}, {"left": 0.08820261437908496, "top": 0.6857878787878788, "width": 0.21957679738562091, "height": 0.01272727272727281, "page": 2}], "section": "INITIAL EXPLORATION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1311"}, {"text": "Considering the different visual abilities of PLV and our new use case for AR, we did not know a-priori what AR platform would be most appropriate for the stair navigation task.", "label": "Novelty", "bboxes": [{"left": 0.290921568627451, "top": 0.7148825757575757, "width": 0.19143790849673203, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.7294305555555555, "width": 0.39415032679738565, "height": 0.01272727272727281, "page": 2}, {"left": 0.08818627450980392, "top": 0.7436717171717172, "width": 0.3941895424836601, "height": 0.012727272727272587, "page": 2}, {"left": 0.08820261437908496, "top": 0.7582196969696969, "width": 0.1830947712418301, "height": 0.01272727272727281, "page": 2}], "section": "INITIAL EXPLORATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1312"}, {"text": "Although there are no popular commercial devices in the market, researchers have prototyped different hand-held projection-based AR platforms [16, 18, 63, 82].", "label": "Novelty", "bboxes": [{"left": 0.5176797385620915, "top": 0.48336616161616164, "width": 0.39426307189542487, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.4979141414141414, "width": 0.39422385620915046, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176797385620915, "top": 0.5124608585858587, "width": 0.28818627450980394, "height": 0.012727272727272698, "page": 2}], "section": "VISUALIZATIONS FOR PROJECTION-BASED AR", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1313"}, {"text": "[41] developed a HoloLens application that recolored the scene with high contrast colors for PLV based on the spatial information from the HoloLens.", "label": "Novelty", "bboxes": [{"left": 0.22945424836601305, "top": 0.46186237373737377, "width": 0.25293790849673203, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.4764090909090909, "width": 0.3942140522875818, "height": 0.012727272727272754, "page": 2}, {"left": 0.08823529411764706, "top": 0.49095707070707073, "width": 0.3396274509803921, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for PLV", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1314"}, {"text": "However, these specialized tools often stigmatize users in social settings [69]; thus, people avoid using them or abandon them altogether [25].", "label": "Novelty", "bboxes": [{"left": 0.4183382352941177, "top": 0.2945757575757576, "width": 0.06401797385620911, "height": 0.012727272727272698, "page": 2}, {"left": 0.08818627450980392, "top": 0.30912247474747473, "width": 0.39419281045751636, "height": 0.012727272727272754, "page": 2}, {"left": 0.08818627450980392, "top": 0.3236691919191919, "width": 0.3942058823529412, "height": 0.012727272727272698, "page": 2}, {"left": 0.08820261437908496, "top": 0.3382171717171717, "width": 0.10093137254901961, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for PLV", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1315"}, {"text": "Some PLV also use a white cane, especially at night and in unfamiliar places, but many prefer not using it because it exposes their disability [86].", "label": "Novelty", "bboxes": [{"left": 0.19336764705882353, "top": 0.3382171717171717, "width": 0.2890032679738562, "height": 0.012727272727272698, "page": 2}, {"left": 0.08820261437908496, "top": 0.3527638888888889, "width": 0.39417647058823535, "height": 0.012727272727272698, "page": 2}, {"left": 0.08820261437908496, "top": 0.3673118686868687, "width": 0.2682075163398693, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for PLV", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1316"}, {"text": "Inspired by the contrast stripes that many PLV used to distinguish stair edges [79], we project highlights on the stair edges to increase their visibility.", "label": "Novelty", "bboxes": [{"left": 0.5176307189542484, "top": 0.7073030303030302, "width": 0.3943856209150326, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176307189542484, "top": 0.7218510101010102, "width": 0.39407516339869286, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176307189542484, "top": 0.7363977272727272, "width": 0.21194607843137248, "height": 0.01272727272727281, "page": 2}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1317"}, {"text": "We designed and evaluated visualizations for both platforms, given that each platform has its own strength: projectionbased AR can augment large physical surfaces but projects content publicly, which may be better suited to private places with few people ( e.g ., home, workspace); meanwhile, smartglasses present information only to the user, which may be better for crowded public places ( e.g ., subway stations).", "label": "Novelty", "bboxes": [{"left": 0.8890522875816994, "top": 0.22278282828282828, "width": 0.022735294117647076, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.2373308080808081, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.25187752525252527, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.26642424242424245, "width": 0.3940915032679738, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.28097222222222223, "width": 0.3941584967320261, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.29551893939393936, "width": 0.39412418300653596, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.3100669191919192, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.3246136363636364, "width": 0.3652679738562091, "height": 0.012727272727272698, "page": 2}], "section": "INITIAL EXPLORATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1318"}, {"text": "However, participants", "label": "Novelty", "bboxes": [{"left": 0.33845588235294116, "top": 0.8821704545454545, "width": 0.1438709150326798, "height": 0.01272727272727281, "page": 2}], "section": "INITIAL EXPLORATION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1319"}, {"text": "In contrast, our work addresses this gap by designing AR visualizations to assist PLV in navigating stairs.", "label": "Novelty", "bboxes": [{"left": 0.2446111111111111, "top": 0.1357979797979798, "width": 0.23786437908496727, "height": 0.012727272727272726, "page": 2}, {"left": 0.0883218954248366, "top": 0.15004040404040403, "width": 0.39417320261437905, "height": 0.012727272727272754, "page": 2}, {"left": 0.08833823529411765, "top": 0.1645871212121212, "width": 0.08359477124183005, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for Blind People", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1320"}, {"text": "We provide three different auditory feedback choices: (1) Sonification that indicates stair direction: one ding sound for going up and two ding sounds for going down, adapted from the sonic alerts for some elevators; (2) a human voice that verbally reported stair direction and number of stairs: Approaching upstairs, 14 stairs going up; and (3) a combined sonification and human voice: ding, approaching upstairs, 14 stairs going up.", "label": "Novelty", "bboxes": [{"left": 0.40587091503267975, "top": 0.21638888888888888, "width": 0.0765049019607843, "height": 0.012727272727272754, "page": 3}, {"left": 0.08820261437908496, "top": 0.23093686868686866, "width": 0.39420261437908505, "height": 0.012727272727272754, "page": 3}, {"left": 0.08821895424836601, "top": 0.24548358585858587, "width": 0.3941993464052288, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.260030303030303, "width": 0.3941764705882353, "height": 0.012727272727272754, "page": 3}, {"left": 0.08821895424836601, "top": 0.27457828282828284, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.289125, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.0882516339869281, "top": 0.3036729797979798, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.318219696969697, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.33276767676767677, "width": 0.12806862745098035, "height": 0.012727272727272698, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1321"}, {"text": "We recruited 12 PLV (6 female, 6 male; mean age=53.9) with different low-vision conditions, as shown in Table 1 (P1  P12).", "label": "Novelty", "bboxes": [{"left": 0.6077075163398692, "top": 0.5906275252525253, "width": 0.30414869281045764, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.605175505050505, "width": 0.39418627450980404, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.6197222222222222, "width": 0.12820751633986938, "height": 0.012727272727272698, "page": 3}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1322"}, {"text": "We evaluated the visualizations for projection-based AR, aiming to answer three questions: (1) How do PLV perceive the different visualization designs?", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.4975972222222222, "width": 0.39420751633986917, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.5121452020202021, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.5266919191919192, "width": 0.2338006535947713, "height": 0.01272727272727281, "page": 3}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1323"}, {"text": "Since locating the first and last stairs was most important but challenging for PLV [86], we distinguish the first and last stairs from the rest by projecting thick highlights on them (Figure 2a), while projecting thin highlights on the middle stairs (Figure 3a).", "label": "Novelty", "bboxes": [{"left": 0.08823529411764706, "top": 0.3545820707070707, "width": 0.3941388888888889, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3691287878787879, "width": 0.39412745098039215, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.38367676767676767, "width": 0.39422058823529404, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3982234848484848, "width": 0.3942075163398692, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.4127714646464647, "width": 0.11595261437908495, "height": 0.012727272727272698, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1324"}, {"text": "So in this design, we kept a stable line at the stair edge while flashing the rest of the highlighted strip (Figure 2b).", "label": "Novelty", "bboxes": [{"left": 0.14738071895424837, "top": 0.6388459595959596, "width": 0.33493790849673194, "height": 0.01272727272727281, "page": 3}, {"left": 0.08816993464052288, "top": 0.6533926767676768, "width": 0.39416666666666667, "height": 0.01272727272727281, "page": 3}, {"left": 0.08816993464052288, "top": 0.6679393939393939, "width": 0.025890522875817, "height": 0.012727272727272587, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1325"}, {"text": "They conducted each task in two conditions: (1) walking in their original way (participants could use a cane if desired, but nobody chose to use it); (2) walking using our prototype with their preferred combinations.", "label": "Novelty", "bboxes": [{"left": 0.6529133986928105, "top": 0.483094696969697, "width": 0.25903267973856203, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.49764141414141416, "width": 0.3941977124183007, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.5121893939393939, "width": 0.3941944444444444, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.5267361111111111, "width": 0.3563349673202614, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1326"}, {"text": "Although we provided different visualizations (flash or movement) to further enhance the end highlights, most participants (seven out of 12) liked the original design.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.8230833333333333, "width": 0.3942663398692811, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8376313131313131, "width": 0.3941421568627451, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8521780303030303, "width": 0.331998366013072, "height": 0.01272727272727281, "page": 5}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1327"}, {"text": "However, none of the participants liked the Moving Horizontal Zebra since the parallel movement to the stair edge distorted its appearance.", "label": "Novelty", "bboxes": [{"left": 0.5175081699346404, "top": 0.44583459595959596, "width": 0.39417483660130737, "height": 0.012727272727272698, "page": 5}, {"left": 0.5175081699346404, "top": 0.46038131313131314, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 5}, {"left": 0.5175245098039216, "top": 0.4749292929292929, "width": 0.13930882352941176, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1328"}, {"text": "The yellow gives me more alert and the blue gives me a little bit more of a relaxed mode. But when I go up and down the steps, I wanna be alert (P5).", "label": "Novelty", "bboxes": [{"left": 0.8788562091503268, "top": 0.8322222222222222, "width": 0.032727124183006495, "height": 0.01272727272727281, "page": 5}, {"left": 0.5174101307189543, "top": 0.8467689393939394, "width": 0.39417320261437905, "height": 0.01272727272727281, "page": 5}, {"left": 0.5174101307189543, "top": 0.8613169191919192, "width": 0.3941503267973856, "height": 0.01272727272727281, "page": 5}, {"left": 0.5174101307189543, "top": 0.8758636363636363, "width": 0.1892205882352942, "height": 0.01272727272727281, "page": 5}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1329"}, {"text": "However, P11 preferred the Flash since the thin stable highlight of the Flashing Edge gave him an illusion of another small step (P11).", "label": "Novelty", "bboxes": [{"left": 0.656124183006536, "top": 0.3512840909090909, "width": 0.2555245098039215, "height": 0.012727272727272754, "page": 5}, {"left": 0.5175081699346404, "top": 0.3658308080808081, "width": 0.39417483660130737, "height": 0.012727272727272698, "page": 5}, {"left": 0.5175081699346404, "top": 0.38037878787878787, "width": 0.24877777777777776, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1330"}, {"text": "Although no participants chose the Moving Edge in the study, P6 felt it could be helpful since it indicated direction.", "label": "Novelty", "bboxes": [{"left": 0.5175245098039216, "top": 0.4970492424242424, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 5}, {"left": 0.5175081699346404, "top": 0.5115959595959596, "width": 0.39425980392156856, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1331"}, {"text": "However, most participants found it overwhelming; it made them feel like the ground is going to move (P9).", "label": "Novelty", "bboxes": [{"left": 0.8743921568627452, "top": 0.5261439393939394, "width": 0.03725653594771228, "height": 0.012727272727272698, "page": 5}, {"left": 0.5175081699346404, "top": 0.5406906565656565, "width": 0.39423366013071903, "height": 0.01272727272727281, "page": 5}, {"left": 0.5175081699346404, "top": 0.5552386363636364, "width": 0.29179901960784316, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1332"}, {"text": "They liked the idea of projecting highlights on the stair edges to simulate the physical contrast stripes.", "label": "Novelty", "bboxes": [{"left": 0.2594313725490196, "top": 0.42972222222222217, "width": 0.22293137254901962, "height": 0.012727272727272754, "page": 5}, {"left": 0.08821895424836601, "top": 0.44426893939393935, "width": 0.39417320261437905, "height": 0.012727272727272698, "page": 5}, {"left": 0.08821895424836601, "top": 0.4588169191919192, "width": 0.04678758169934638, "height": 0.012727272727272754, "page": 5}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1333"}, {"text": "P9 said, Having [the highlights] this bright is really good. Because usually [the contrast stripes] are painted, and theyre about to fade out, and theyre not as vibrant and bright as this is. This is great here because you can see it.", "label": "Novelty", "bboxes": [{"left": 0.13912745098039217, "top": 0.4588169191919192, "width": 0.3432320261437909, "height": 0.012727272727272754, "page": 5}, {"left": 0.08820261437908496, "top": 0.4733636363636364, "width": 0.39420098039215684, "height": 0.012727272727272698, "page": 5}, {"left": 0.08821895424836601, "top": 0.48791035353535356, "width": 0.3942075163398693, "height": 0.012727272727272754, "page": 5}, {"left": 0.08820261437908496, "top": 0.5024583333333333, "width": 0.3941732026143791, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1334"}, {"text": "However, current optical see-through smartglasses have a very limited FOV [88] ( e.g ., ca. 30 wide  17 high for HoloLens v1), largely limiting the area for presenting AR visualizations.", "label": "Novelty", "bboxes": [{"left": 0.5176960784313726, "top": 0.5124570707070707, "width": 0.39417483660130725, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176960784313726, "top": 0.526699494949495, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176797385620915, "top": 0.541246212121212, "width": 0.3941584967320261, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176797385620915, "top": 0.555794191919192, "width": 0.06672222222222224, "height": 0.012727272727272698, "page": 6}], "section": "VISUALIZATIONS FOR SMARTGLASSES", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1335"}, {"text": "While the recently announced HoloLens v2 is estimated to have a 29 vertical FOV, it is still much smaller than that of a typically-sighted human (120 vertical FOV).", "label": "Novelty", "bboxes": [{"left": 0.5885522875816994, "top": 0.555794191919192, "width": 0.3233022875816992, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176797385620915, "top": 0.5703409090909091, "width": 0.39415522875817, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176960784313726, "top": 0.5848888888888889, "width": 0.3941486928104574, "height": 0.012727272727272698, "page": 6}], "section": "VISUALIZATIONS FOR SMARTGLASSES", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1336"}, {"text": "As P9 mentioned, I know mentally Im looking in the bottom field of vision, even though Im looking straight aheadThe [highlight] stands out very bright and my peripheral catches it, it catches blue, it catches the yellowWithout the system, I have to stare a lot more at the stairs and, I have to look a little bit extra to make sure that that is really the last step.", "label": "Novelty", "bboxes": [{"left": 0.3006421568627451, "top": 0.5503270202020202, "width": 0.1818022875816993, "height": 0.01272727272727281, "page": 6}, {"left": 0.08822058823529412, "top": 0.5648737373737374, "width": 0.39417156862745095, "height": 0.012727272727272698, "page": 6}, {"left": 0.08822058823529412, "top": 0.5794217171717172, "width": 0.3941748366013072, "height": 0.01272727272727281, "page": 6}, {"left": 0.08823692810457516, "top": 0.5939684343434344, "width": 0.3942598039215686, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823692810457516, "top": 0.6085151515151516, "width": 0.3940751633986928, "height": 0.012727272727272698, "page": 6}, {"left": 0.08822058823529412, "top": 0.6230631313131313, "width": 0.39423692810457517, "height": 0.012727272727272698, "page": 6}, {"left": 0.08822058823529412, "top": 0.6376098484848485, "width": 0.27275326797385624, "height": 0.012727272727272698, "page": 6}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1337"}, {"text": "Meanwhile, four participants felt that the middle highlights should be a different color from the end highlights.", "label": "Novelty", "bboxes": [{"left": 0.08823529411764706, "top": 0.07002146464646465, "width": 0.3941372549019607, "height": 0.01272727272727274, "page": 6}, {"left": 0.08823529411764706, "top": 0.08456944444444445, "width": 0.34987418300653594, "height": 0.012727272727272726, "page": 6}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1338"}, {"text": "Three participants liked the blue color since its not as attracting as yellow but still sticks out (P9).", "label": "Novelty", "bboxes": [{"left": 0.44424673202614384, "top": 0.08456944444444445, "width": 0.03814542483660127, "height": 0.012727272727272726, "page": 6}, {"left": 0.08821895424836601, "top": 0.09911616161616162, "width": 0.3941944444444445, "height": 0.012727272727272726, "page": 6}, {"left": 0.08821895424836601, "top": 0.11366414141414143, "width": 0.2241045751633987, "height": 0.012727272727272712, "page": 6}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1339"}, {"text": "s study, knowing when the stairs start and end can help PLV plan their steps, while the middle stairs are less important because most stairs are uniform [86].", "label": "Novelty", "bboxes": [{"left": 0.25108006535947713, "top": 0.2709330808080808, "width": 0.23133006535947714, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.2854810606060606, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 7}, {"left": 0.08821895424836601, "top": 0.3000277777777778, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1340"}, {"text": "Our visualizations inform PLV of the different stair stages via different design.", "label": "Novelty", "bboxes": [{"left": 0.23304411764705882, "top": 0.6561237373737373, "width": 0.2493480392156863, "height": 0.01272727272727281, "page": 7}, {"left": 0.08821895424836601, "top": 0.6706717171717173, "width": 0.26529084967320266, "height": 0.012727272727272587, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1341"}, {"text": "Unlike the highlights that are attached to the stair edges, the glow is always at the bottom of the vertical FOV, so that", "label": "Novelty", "bboxes": [{"left": 0.12275816993464052, "top": 0.7506755050505051, "width": 0.3595784313725491, "height": 0.01272727272727281, "page": 7}, {"left": 0.08821895424836601, "top": 0.7652222222222222, "width": 0.39415686274509804, "height": 0.012727272727272587, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1342"}, {"text": "They conducted each task in three conditions: (1) walking on the stairs as they typically would (they could use a cane if desired, but none chose to use it), (2) walking on the stairs with HoloLens and no visualizations, and (3) walking on the stairs with HoloLens and their chosen designs.", "label": "Novelty", "bboxes": [{"left": 0.5956274509803922, "top": 0.09917550505050504, "width": 0.31613398692810457, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175898692810458, "top": 0.11372222222222222, "width": 0.39418137254901964, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175735294117647, "top": 0.12827020202020203, "width": 0.39422712418300665, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175898692810458, "top": 0.14251136363636363, "width": 0.3941813725490195, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175898692810458, "top": 0.15705934343434344, "width": 0.3564738562091504, "height": 0.012727272727272726, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1343"}, {"text": "We conducted the design exploration session at an emergency staircase with 12 stairs (different stairs than those in the projection study).", "label": "Novelty", "bboxes": [{"left": 0.08818627450980392, "top": 0.6873409090909092, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 8}, {"left": 0.08818627450980392, "top": 0.7018876262626262, "width": 0.3941764705882353, "height": 0.01272727272727281, "page": 8}, {"left": 0.08820261437908496, "top": 0.7161300505050504, "width": 0.14408496732026144, "height": 0.01272727272727281, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1344"}, {"text": "Participants wore the HoloLens and experienced four different designs: Glow, Path, Beep, and Edge Highlights as a baseline.", "label": "Novelty", "bboxes": [{"left": 0.2387107843137255, "top": 0.7161300505050504, "width": 0.24379738562091502, "height": 0.01272727272727281, "page": 8}, {"left": 0.08821895424836601, "top": 0.7306767676767677, "width": 0.39422385620915035, "height": 0.012727272727272587, "page": 8}, {"left": 0.08821895424836601, "top": 0.7452247474747474, "width": 0.20093954248366014, "height": 0.01272727272727281, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1345"}, {"text": "However, P12 experienced a decrease in visual acuity (from 20/200 to 20/400).", "label": "Novelty", "bboxes": [{"left": 0.575968954248366, "top": 0.8621540404040404, "width": 0.33591013071895426, "height": 0.012727272727272587, "page": 8}, {"left": 0.5176143790849673, "top": 0.8767007575757576, "width": 0.18503431372549017, "height": 0.01272727272727281, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1346"}, {"text": "We recruited 12 PLV (5 female, 7 male; mean age=51.6) with different low vision conditions (Table 1, P6 P17).", "label": "Novelty", "bboxes": [{"left": 0.17829575163398695, "top": 0.1342638888888889, "width": 0.30414869281045753, "height": 0.012727272727272726, "page": 8}, {"left": 0.0882516339869281, "top": 0.1488118686868687, "width": 0.3942565359477125, "height": 0.012727272727272726, "page": 8}, {"left": 0.0882516339869281, "top": 0.16335858585858584, "width": 0.03499019607843139, "height": 0.012727272727272754, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1347"}, {"text": "Seven participants had taken part in the evaluation of our projection-based AR visualizations, but they did not see the stairs used in this study.", "label": "Novelty", "bboxes": [{"left": 0.36164705882352943, "top": 0.16335858585858584, "width": 0.12077941176470586, "height": 0.012727272727272754, "page": 8}, {"left": 0.0882516339869281, "top": 0.17790656565656565, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 8}, {"left": 0.08823529411764706, "top": 0.19214772727272728, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.20669570707070709, "width": 0.039537581699346416, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1348"}, {"text": "On the other hand, P17 felt that Beep may not be distinguishable from environmental sounds: The world around you is so full of noise. I mean, if I use this in the city you have cars honking and everything like that, Im not sure if I would react in time.", "label": "Novelty", "bboxes": [{"left": 0.5176895424836602, "top": 0.7322601010101011, "width": 0.39426470588235285, "height": 0.012727272727272587, "page": 9}, {"left": 0.5177058823529412, "top": 0.7468068181818182, "width": 0.3941405228758168, "height": 0.01272727272727281, "page": 9}, {"left": 0.5177058823529412, "top": 0.761354797979798, "width": 0.3942401960784313, "height": 0.012727272727272587, "page": 9}, {"left": 0.5177058823529412, "top": 0.7759015151515152, "width": 0.3942075163398693, "height": 0.01272727272727281, "page": 9}, {"left": 0.5177058823529412, "top": 0.7904494949494949, "width": 0.09646895424836599, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1349"}, {"text": "P8 and P14 voiced the same concern about environmental noise but explained that along with the visualizations the sound would be recognizable.", "label": "Novelty", "bboxes": [{"left": 0.6199052287581699, "top": 0.7904494949494949, "width": 0.29194117647058826, "height": 0.01272727272727281, "page": 9}, {"left": 0.5177058823529412, "top": 0.8049962121212122, "width": 0.39425816993464036, "height": 0.012727272727272587, "page": 9}, {"left": 0.5177058823529412, "top": 0.8195441919191918, "width": 0.28593954248366016, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1350"}, {"text": "As P10 mentioned, I want more information while Im going down the stairs, The yellow color was helpful to let me know", "label": "Novelty", "bboxes": [{"left": 0.46422549019607845, "top": 0.805530303030303, "width": 0.018207516339869334, "height": 0.012727272727272587, "page": 9}, {"left": 0.08826797385620916, "top": 0.8200782828282828, "width": 0.3941617647058824, "height": 0.01272727272727281, "page": 9}, {"left": 0.08826797385620916, "top": 0.8346250000000001, "width": 0.39415849673202613, "height": 0.012727272727272587, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1351"}, {"text": "This one is my kind of style. Its subtle, simple, and I can keep my head wherever I want at the same time. And [the color of the Glow] changes exactly when I need to step. It warns me when Im about to take my last step Its very discreet but not distracting. So Ill still be able to see people, and things around me without falling over steps. If my real glasses could do this, it would be good.", "label": "Novelty", "bboxes": [{"left": 0.0882843137254902, "top": 0.5724823232323233, "width": 0.3942009803921569, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882843137254902, "top": 0.5870290404040405, "width": 0.39417973856209143, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882843137254902, "top": 0.6015757575757575, "width": 0.39416013071895417, "height": 0.01272727272727281, "page": 9}, {"left": 0.0882843137254902, "top": 0.6161237373737374, "width": 0.3942091503267974, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.6306704545454546, "width": 0.39407516339869286, "height": 0.01272727272727281, "page": 9}, {"left": 0.08826797385620916, "top": 0.6452184343434343, "width": 0.3941748366013072, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.6597651515151515, "width": 0.26652777777777775, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1352"}, {"text": "terms of visual field, no participants experienced a change while wearing the HoloLens.", "label": "Novelty", "bboxes": [{"left": 0.08823529411764706, "top": 0.07002146464646465, "width": 0.39412418300653596, "height": 0.01272727272727274, "page": 9}, {"left": 0.08823529411764706, "top": 0.08456944444444445, "width": 0.19058169934640518, "height": 0.012727272727272726, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1353"}, {"text": "that Im at the last step...but I didnt really see that [blue glow in the middle], I need to be reassured that Im still going down the stairs.", "label": "Novelty", "bboxes": [{"left": 0.517673202614379, "top": 0.07008585858585858, "width": 0.3942401960784314, "height": 0.012727272727272712, "page": 9}, {"left": 0.517673202614379, "top": 0.08463257575757575, "width": 0.39417973856209154, "height": 0.012727272727272726, "page": 9}, {"left": 0.5176895424836602, "top": 0.09918055555555556, "width": 0.11300490196078428, "height": 0.012727272727272726, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1354"}, {"text": "Interestingly, we found that participants had different preferences for Paths position in their visual field.", "label": "Novelty", "bboxes": [{"left": 0.517640522875817, "top": 0.3176818181818182, "width": 0.3941078431372549, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.33192424242424245, "width": 0.28760947712418305, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1355"}, {"text": "However, two participants (P6, P14) had difficulty using Glow because of difficulty distinguishing colors.", "label": "Novelty", "bboxes": [{"left": 0.08826797385620916, "top": 0.681885101010101, "width": 0.3941911764705882, "height": 0.01272727272727281, "page": 9}, {"left": 0.08826797385620916, "top": 0.6961275252525252, "width": 0.31449019607843137, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1356"}, {"text": "P14 doesnt have color vision, while P6s visual condition included auras of various colors that interfere with the colors of Glow.", "label": "Novelty", "bboxes": [{"left": 0.40580392156862743, "top": 0.6961275252525252, "width": 0.07660620915032684, "height": 0.01272727272727281, "page": 9}, {"left": 0.08826797385620916, "top": 0.7106742424242425, "width": 0.3941781045751634, "height": 0.012727272727272587, "page": 9}, {"left": 0.0882843137254902, "top": 0.7252222222222222, "width": 0.3612271241830065, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1357"}, {"text": "They felt the different colors can effectively inform them of their stage on the stairs, and the thicker and brighter glow colors at the preparation and alert area successfully attracted their attention.", "label": "Novelty", "bboxes": [{"left": 0.16856209150326798, "top": 0.4488358585858586, "width": 0.3139297385620916, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.4633838383838384, "width": 0.39418464052287583, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.4779305555555556, "width": 0.3940588235294118, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.49247853535353536, "width": 0.18803758169934637, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1358"}, {"text": "Moreover, participants enjoyed the freedom to move their head in any direction while still being able to see Glow.", "label": "Novelty", "bboxes": [{"left": 0.27948856209150325, "top": 0.49247853535353536, "width": 0.20292483660130728, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.506719696969697, "width": 0.3941552287581699, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.5212676767676767, "width": 0.15559477124183008, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1359"}, {"text": "P8 also felt Path could guide him along the stairs: Its like a reinforced railing but its also like a guide [showing] where Im stepping. Its like a good reference. I kinda like to have the guide.", "label": "Novelty", "bboxes": [{"left": 0.517640522875817, "top": 0.23737247474747475, "width": 0.3941535947712419, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.25192045454545453, "width": 0.3942565359477125, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2664671717171717, "width": 0.394295751633987, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2810151515151515, "width": 0.07294771241830067, "height": 0.012727272727272754, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1360"}, {"text": "Four participants (P6, P8, P12, P17) slowed down a little on ascending stairs with the visualizations, while five participants (P6, P13, P12, P16, P17) slowed down on descending stairs with their preferred visualizations.", "label": "Novelty", "bboxes": [{"left": 0.7968022875816994, "top": 0.33123358585858587, "width": 0.11511274509803926, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176503267973857, "top": 0.34578156565656565, "width": 0.3942598039215687, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176503267973857, "top": 0.36032828282828283, "width": 0.3941405228758169, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176503267973857, "top": 0.3748762626262626, "width": 0.39407516339869275, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176503267973857, "top": 0.3894229797979798, "width": 0.13654738562091506, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1361"}, {"text": "Participants gave scores to their psychological security during stair navigation in three conditions (Figure 9): (1) walking as they typically would ( mean =4.8, SD =1.60); (2) with HoloLens but no visualizations ( mean =3.9, SD =1.44); (3) with preferred visualizations or sonification ( mean =6.1, SD =1.38).", "label": "Novelty", "bboxes": [{"left": 0.5176830065359477, "top": 0.6079242424242425, "width": 0.3941405228758169, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176666666666667, "top": 0.6224722222222222, "width": 0.39412418300653584, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176503267973857, "top": 0.6370189393939394, "width": 0.39421568627450976, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176503267973857, "top": 0.6515669191919192, "width": 0.3942075163398693, "height": 0.012727272727272587, "page": 10}, {"left": 0.5176503267973857, "top": 0.6658080808080808, "width": 0.39417483660130725, "height": 0.012727272727272587, "page": 10}, {"left": 0.5176503267973857, "top": 0.6803560606060606, "width": 0.0674035947712418, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1362"}, {"text": "Paired Wilcoxon Signed-Rank tests showed that, while wearing HoloLens significantly reduced participants psychological security ( V =8, p =0.031), our visualizations significantly increased participant psychological security compared with not wearing HoloLens ( V =21, p =0.050).", "label": "Novelty", "bboxes": [{"left": 0.5896470588235294, "top": 0.6803560606060606, "width": 0.3221176470588235, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.6949027777777779, "width": 0.3942859477124183, "height": 0.012727272727272587, "page": 10}, {"left": 0.5176339869281046, "top": 0.7094507575757576, "width": 0.39417320261437894, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.7239974747474747, "width": 0.39426797385620904, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.7385454545454545, "width": 0.3647075163398693, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1363"}, {"text": "While they all mentioned that visualizations were more effective than audio feedback and used the visualization as a primary guide, participants also appreciated the beep and used it as a secondary complement to the visualizations.", "label": "Novelty", "bboxes": [{"left": 0.35291830065359475, "top": 0.3388118686868687, "width": 0.12949183006535953, "height": 0.012727272727272698, "page": 10}, {"left": 0.08826797385620916, "top": 0.35335858585858587, "width": 0.3941846405228759, "height": 0.012727272727272698, "page": 10}, {"left": 0.08826797385620916, "top": 0.36790656565656565, "width": 0.39415849673202613, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882843137254902, "top": 0.38245328282828284, "width": 0.39422058823529416, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882843137254902, "top": 0.39699999999999996, "width": 0.22379901960784312, "height": 0.012727272727272754, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1364"}, {"text": "One participant (P14) chose Path, while two participants (P6 and P10) chose Edge Highlights.", "label": "Novelty", "bboxes": [{"left": 0.3358382352941176, "top": 0.5060984848484849, "width": 0.14668627450980398, "height": 0.01272727272727281, "page": 10}, {"left": 0.08830065359477124, "top": 0.5206464646464646, "width": 0.39419117647058827, "height": 0.01272727272727281, "page": 10}, {"left": 0.08831699346405228, "top": 0.5351931818181818, "width": 0.07311437908496735, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1365"}, {"text": "P13 combined all four designs because he used each design for different purposes: Path as a reminder to look for a railing, Edge Highlights to get an overview of the stairs, and Glow when walking on stairs and scanning the environment for people or obstacles.", "label": "Novelty", "bboxes": [{"left": 0.16658660130718952, "top": 0.5351931818181818, "width": 0.3158725490196078, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.5497411616161616, "width": 0.3942107843137256, "height": 0.01272727272727281, "page": 10}, {"left": 0.08831699346405228, "top": 0.5642878787878788, "width": 0.3941584967320261, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.578834595959596, "width": 0.3942369281045751, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.5933825757575758, "width": 0.1892238562091503, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1366"}, {"text": "While there is no significant improvement in walking speed when using the visualizations, participants reported feeling safer and more confident when using our design.", "label": "Novelty", "bboxes": [{"left": 0.6847941176470588, "top": 0.48427904040404035, "width": 0.2270800653594771, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176666666666667, "top": 0.49852146464646463, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176666666666667, "top": 0.5130681818181818, "width": 0.39417156862745095, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176830065359477, "top": 0.5276161616161616, "width": 0.1139101307189544, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1367"}, {"text": "However, when walking upstairs, there was no significant effect of Condition on participants walking time ( F (2,10) =2.924, p =0.092).", "label": "Novelty", "bboxes": [{"left": 0.37454738562091505, "top": 0.7827487373737374, "width": 0.10796405228758171, "height": 0.01272727272727281, "page": 10}, {"left": 0.08823856209150327, "top": 0.7972967171717171, "width": 0.3941748366013072, "height": 0.01272727272727281, "page": 10}, {"left": 0.08823856209150327, "top": 0.8118396464646465, "width": 0.3539950980392157, "height": 0.01273106060606044, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1368"}, {"text": "Participants combined different visualizations and sonification based on their preferences, as shown in Figure 8.", "label": "Novelty", "bboxes": [{"left": 0.4342663398692811, "top": 0.273354797979798, "width": 0.04816013071895425, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882516339869281, "top": 0.2879027777777778, "width": 0.39415849673202613, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882516339869281, "top": 0.3024494949494949, "width": 0.31948039215686275, "height": 0.012727272727272754, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1369"}, {"text": "The system implementation should also take into account different real-world situations.", "label": "Novelty", "bboxes": [{"left": 0.517656862745098, "top": 0.5776717171717172, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5922184343434344, "width": 0.2094624183006536, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1370"}, {"text": "However, the real world could be much more complicated, raising all kinds of challenges.", "label": "Novelty", "bboxes": [{"left": 0.8226421568627451, "top": 0.6067664141414142, "width": 0.08915522875816984, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.6213131313131313, "width": 0.39417320261437905, "height": 0.01272727272727281, "page": 11}, {"left": 0.517640522875817, "top": 0.6355555555555555, "width": 0.09124673202614386, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1371"}, {"text": "Unlike prior research, which showed that PLV had very different preferences for visual augmentations [84, 85], our study revealed that some common preferences among PLV cross different visual abilities for stair navigation.", "label": "Novelty", "bboxes": [{"left": 0.45677124183006534, "top": 0.5615593434343434, "width": 0.025457516339869368, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.5761060606060606, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.5906527777777778, "width": 0.39419117647058827, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.6052007575757576, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.6197474747474747, "width": 0.28821895424836597, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1372"}, {"text": "Meanwhile, the design on smartglasses, especially Glow and Beep, proposed a new way to perceive stairs: it divided the stairs into different stages, providing only immediate information about the current stair without a preview of whats to come.", "label": "Novelty", "bboxes": [{"left": 0.301578431372549, "top": 0.7579406565656566, "width": 0.18084803921568626, "height": 0.01272727272727281, "page": 11}, {"left": 0.0882516339869281, "top": 0.7724873737373736, "width": 0.3941748366013072, "height": 0.01272727272727281, "page": 11}, {"left": 0.0882516339869281, "top": 0.7870353535353535, "width": 0.3941486928104575, "height": 0.01272727272727281, "page": 11}, {"left": 0.0882516339869281, "top": 0.8015820707070707, "width": 0.39421732026143796, "height": 0.01272727272727281, "page": 11}, {"left": 0.08826797385620916, "top": 0.8161300505050505, "width": 0.2996503267973856, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1373"}, {"text": "While our study focused on the design and evaluation of the AR visualizations, we discuss the technical feasibility and challenges for our AR stair navigation systems.", "label": "Novelty", "bboxes": [{"left": 0.517673202614379, "top": 0.3521957070707071, "width": 0.394140522875817, "height": 0.012727272727272698, "page": 11}, {"left": 0.517673202614379, "top": 0.3667424242424242, "width": 0.3941862745098039, "height": 0.012727272727272754, "page": 11}, {"left": 0.5176895424836602, "top": 0.3812891414141414, "width": 0.315937908496732, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1374"}, {"text": "While many stair detection methods have been presented in prior research [20, 58], algorithms that locate the exact position of each stair with high speed and accuracy should be investigated and tested to support the stair visualization systems we designed for PLV.", "label": "Novelty", "bboxes": [{"left": 0.6355196078431372, "top": 0.49736237373737374, "width": 0.2763807189542484, "height": 0.012727272727272754, "page": 11}, {"left": 0.517656862745098, "top": 0.511909090909091, "width": 0.39418954248366, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5264570707070707, "width": 0.3942516339869281, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5410037878787879, "width": 0.39413071895424834, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5555517676767677, "width": 0.28683496732026137, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1375"}, {"text": "platforms, The first experience [projection-based AR] gave me a better sense of a direction as to where this was goingBut the [glow] was like floating over the steps, and they didnt stay fixed in place. That was one big difference. I like the light fixed on the step.", "label": "Novelty", "bboxes": [{"left": 0.5177058823529412, "top": 0.2721919191919192, "width": 0.39415686274509787, "height": 0.012727272727272754, "page": 11}, {"left": 0.5176895424836602, "top": 0.2864330808080808, "width": 0.39418137254901964, "height": 0.012727272727272754, "page": 11}, {"left": 0.5176895424836602, "top": 0.3009810606060606, "width": 0.39412745098039215, "height": 0.012727272727272698, "page": 11}, {"left": 0.5176895424836602, "top": 0.3155277777777778, "width": 0.39417320261437905, "height": 0.012727272727272754, "page": 11}, {"left": 0.517673202614379, "top": 0.3300757575757576, "width": 0.17794771241830087, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1376"}, {"text": "We designed visualizations (and sonification) for both projection-based AR and smartglasses based on the different characteristics of these platforms.", "label": "Novelty", "bboxes": [{"left": 0.3437924836601307, "top": 0.2503270202020202, "width": 0.13859967320261435, "height": 0.012727272727272754, "page": 12}, {"left": 0.08823529411764706, "top": 0.2648737373737374, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 12}, {"left": 0.08821895424836601, "top": 0.27942171717171715, "width": 0.39412418300653596, "height": 0.012727272727272754, "page": 12}, {"left": 0.08821895424836601, "top": 0.29396843434343434, "width": 0.06673856209150325, "height": 0.012727272727272698, "page": 12}], "section": "CONCLUSIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1377"}, {"text": "Third, we asked participants to score their feeling of psychological security, but these results could be influenced by a novelty effect.", "label": "Novelty", "bboxes": [{"left": 0.24340032679738563, "top": 0.1426199494949495, "width": 0.23893954248366014, "height": 0.012727272727272726, "page": 12}, {"left": 0.08823202614379085, "top": 0.15716666666666668, "width": 0.394202614379085, "height": 0.012727272727272754, "page": 12}, {"left": 0.08823202614379085, "top": 0.17171338383838386, "width": 0.23277450980392161, "height": 0.012727272727272698, "page": 12}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1378"}, {"text": "Similar to glow, we adjusted the sound based on the different stages of the stairs:", "label": "Novelty", "bboxes": [{"left": 0.7188055555555556, "top": 0.5319191919191919, "width": 0.19301307189542483, "height": 0.01272727272727281, "page": 7}, {"left": 0.517640522875817, "top": 0.5464659090909091, "width": 0.3379395424836602, "height": 0.012727272727272698, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1379"}, {"text": "However, half of the participants felt Path was distracting and hard to understand.", "label": "Novelty", "bboxes": [{"left": 0.5176895424836602, "top": 0.484969696969697, "width": 0.3941911764705882, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176895424836602, "top": 0.49921085858585856, "width": 0.160390522875817, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1380"}, {"text": "P6 even felt it was misleading to have a virtual railing (Path) in a different place than the real railing because it changed her perception of the width of the staircase: It suggests that there is a railing and then I feel I have a very narrow staircase (P6).", "label": "Novelty", "bboxes": [{"left": 0.6844705882352942, "top": 0.49921085858585856, "width": 0.2274199346405228, "height": 0.012727272727272698, "page": 9}, {"left": 0.517673202614379, "top": 0.5137575757575757, "width": 0.3942042483660131, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176895424836602, "top": 0.5283055555555556, "width": 0.3941405228758168, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176895424836602, "top": 0.5428522727272728, "width": 0.3941013071895424, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176895424836602, "top": 0.5574002525252525, "width": 0.22959313725490182, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1381"}, {"text": "We evaluated the visualizations for projection-based AR, aiming to answer three questions: (1) How do PLV perceive the different visualization designs?", "label": "Objective", "bboxes": [{"left": 0.5176470588235295, "top": 0.4975972222222222, "width": 0.39420751633986917, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.5121452020202021, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.5266919191919192, "width": 0.2338006535947713, "height": 0.01272727272727281, "page": 3}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1382"}, {"text": "As such, our visualizations aim to facilitate a comfortable head pose by indicating the users exact location on the stairs without augmenting the stairs directly.", "label": "Objective", "bboxes": [{"left": 0.893653594771242, "top": 0.6721717171717172, "width": 0.018207516339869168, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176960784313726, "top": 0.6867196969696969, "width": 0.39421078431372547, "height": 0.01272727272727281, "page": 6}, {"left": 0.5177124183006535, "top": 0.7009608585858585, "width": 0.39422385620915046, "height": 0.01272727272727281, "page": 6}, {"left": 0.5177124183006535, "top": 0.7155088383838384, "width": 0.2228480392156864, "height": 0.01272727272727281, "page": 6}], "section": "VISUALIZATIONS FOR SMARTGLASSES", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1383"}, {"text": "We aim to answer:", "label": "Objective", "bboxes": [{"left": 0.7828725490196078, "top": 0.7533573232323232, "width": 0.12894934640522882, "height": 0.01272727272727281, "page": 7}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1384"}, {"text": "P13 combined all four designs because he used each design for different purposes: Path as a reminder to look for a railing, Edge Highlights to get an overview of the stairs, and Glow when walking on stairs and scanning the environment for people or obstacles.", "label": "Objective", "bboxes": [{"left": 0.16658660130718952, "top": 0.5351931818181818, "width": 0.3158725490196078, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.5497411616161616, "width": 0.3942107843137256, "height": 0.01272727272727281, "page": 10}, {"left": 0.08831699346405228, "top": 0.5642878787878788, "width": 0.3941584967320261, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.578834595959596, "width": 0.3942369281045751, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.5933825757575758, "width": 0.1892238562091503, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1385"}, {"text": "Future research should consider more objective measurements ( e.g ., biometrics) to evaluate psychological security.", "label": "Objective", "bboxes": [{"left": 0.32788562091503265, "top": 0.17171338383838386, "width": 0.15454738562091513, "height": 0.012727272727272698, "page": 12}, {"left": 0.08823202614379085, "top": 0.18626136363636364, "width": 0.3941764705882353, "height": 0.012727272727272698, "page": 12}, {"left": 0.08823202614379085, "top": 0.2008080808080808, "width": 0.20921405228758172, "height": 0.012727272727272754, "page": 12}], "section": "DISCUSSION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1386"}, {"text": "Our research explores AR visualization designs to facilitate stair navigation by leveraging PLVs residual vision.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.7945984848484848, "width": 0.3941503267973855, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8091464646464647, "width": 0.33934313725490184, "height": 0.012727272727272587, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1387"}, {"text": "We sought to design effective visualizations for PLV, which balance visibility and distraction, while providing alternative choices to support a wide range of visual abilities.", "label": "Method", "bboxes": [{"left": 0.3071535947712418, "top": 0.11370454545454546, "width": 0.1753480392156863, "height": 0.01272727272727274, "page": 1}, {"left": 0.08829248366013072, "top": 0.12825126262626263, "width": 0.39415686274509804, "height": 0.012727272727272726, "page": 1}, {"left": 0.08827450980392157, "top": 0.14249368686868688, "width": 0.39421241830065357, "height": 0.012727272727272726, "page": 1}, {"left": 0.08827450980392157, "top": 0.15704040404040404, "width": 0.192937908496732, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1388"}, {"text": "We designed visualizations on two AR platforms that can generate immersive virtual content in the physical environment: projection-based AR and smartglasses.", "label": "Method", "bboxes": [{"left": 0.08827450980392157, "top": 0.17916035353535356, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 1}, {"left": 0.08827450980392157, "top": 0.19370833333333334, "width": 0.39412581699346405, "height": 0.012727272727272754, "page": 1}, {"left": 0.08827450980392157, "top": 0.2082550505050505, "width": 0.3062107843137255, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1389"}, {"text": "In summary, we contribute the first exploration of AR visualizations to facilitate stair navigation for PLV.", "label": "Method", "bboxes": [{"left": 0.08825816993464053, "top": 0.48494570707070705, "width": 0.39414215686274506, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.49918813131313133, "width": 0.3113937908496732, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1390"}, {"text": "We evaluated our visualizations on each platform with 12 PLV.", "label": "Method", "bboxes": [{"left": 0.08824183006535948, "top": 0.3464482323232323, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 1}, {"left": 0.08822549019607843, "top": 0.36099494949494954, "width": 0.03498366013071895, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1391"}, {"text": "We sought to facilitate stair navigation by augmenting the stairs with AR visualizations.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.6130517676767676, "width": 0.39428594771241826, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.6275997474747476, "width": 0.1897565359477124, "height": 0.012727272727272698, "page": 2}], "section": "INITIAL EXPLORATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1392"}, {"text": "Thus, we designed visualizations for such a projection-based AR smartphone to augment the stairs for PLV.", "label": "Method", "bboxes": [{"left": 0.5663513071895425, "top": 0.5848926767676768, "width": 0.3454526143790849, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5994393939393939, "width": 0.3772859477124182, "height": 0.01272727272727281, "page": 2}], "section": "VISUALIZATIONS FOR PROJECTION-BASED AR", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1393"}, {"text": "To our knowledge, our research is the first attempt to facilitate stair navigation for PLV.", "label": "Method", "bboxes": [{"left": 0.15576470588235294, "top": 0.5633876262626263, "width": 0.3265915032679738, "height": 0.01272727272727281, "page": 2}, {"left": 0.08821895424836601, "top": 0.5779356060606061, "width": 0.2414264705882353, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for PLV", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1394"}, {"text": "From an interaction perspective, we aimed to simulate use of a flashlight, which is commonly used by PLV in dark places [79]: when a user points the projection-based AR phone at the stairs, it recognizes several stairs in front of her and projects visualizations on those stairs in real time (Figure 1a).", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.6345669191919192, "width": 0.39411274509803906, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.649114898989899, "width": 0.394142156862745, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.6636616161616161, "width": 0.3941454248366014, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176307189542484, "top": 0.6782095959595961, "width": 0.3941078431372549, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176307189542484, "top": 0.6927563131313131, "width": 0.39412581699346394, "height": 0.01272727272727281, "page": 2}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1395"}, {"text": "Based on the formative study, we narrowed down our target platforms to immersive AR platforms, specifically (1) handheld projection-based AR, and (2) optical see-through smartglasses.", "label": "Method", "bboxes": [{"left": 0.5175898692810458, "top": 0.16459343434343435, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.17914141414141413, "width": 0.39422222222222214, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.19368813131313134, "width": 0.39416503267973846, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.20823611111111112, "width": 0.0503986928104575, "height": 0.012727272727272698, "page": 2}], "section": "INITIAL EXPLORATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1396"}, {"text": "We therefore designed our visualizations to help them perceive the stairs from a greater distance, so they can better plan and prepare their steps.", "label": "Method", "bboxes": [{"left": 0.5624640522875817, "top": 0.8164015151515152, "width": 0.34943954248366027, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176797385620915, "top": 0.8309494949494949, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176797385620915, "top": 0.8454962121212122, "width": 0.1838349673202614, "height": 0.012727272727272587, "page": 2}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1397"}, {"text": "We first explored the design space of hand-held projectionbased AR, which combines a camera that recognizes the environment and a projector that projects visual contents into that environment [61].", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.359415404040404, "width": 0.3942401960784314, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176470588235295, "top": 0.3739633838383838, "width": 0.39407516339869286, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176470588235295, "top": 0.38851010101010097, "width": 0.39420751633986917, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.4030580808080808, "width": 0.14836764705882355, "height": 0.012727272727272754, "page": 2}], "section": "VISUALIZATIONS FOR PROJECTION-BASED AR", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1398"}, {"text": "To determine what platforms would be appropriate, we began by conducting a formative study with 11 PLV (7 female, 4 male; age: 2870, mean = 40) to evaluate prototype visualizations for a smartphone.", "label": "Method", "bboxes": [{"left": 0.08820261437908496, "top": 0.7803396464646465, "width": 0.39422385620915035, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.7948863636363636, "width": 0.3942107843137255, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.8094343434343434, "width": 0.3942336601307189, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.8239810606060606, "width": 0.18408823529411766, "height": 0.01272727272727281, "page": 2}], "section": "INITIAL EXPLORATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1399"}, {"text": "To alert users of the presence of stairs as they approach, we first generate auditory feedback to provide an overview of", "label": "Method", "bboxes": [{"left": 0.5176797385620915, "top": 0.8676161616161616, "width": 0.39416503267973846, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176633986928104, "top": 0.8821641414141413, "width": 0.39417483660130725, "height": 0.01272727272727281, "page": 2}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1400"}, {"text": "In contrast, our work addresses this gap by designing AR visualizations to assist PLV in navigating stairs.", "label": "Method", "bboxes": [{"left": 0.2446111111111111, "top": 0.1357979797979798, "width": 0.23786437908496727, "height": 0.012727272727272726, "page": 2}, {"left": 0.0883218954248366, "top": 0.15004040404040403, "width": 0.39417320261437905, "height": 0.012727272727272754, "page": 2}, {"left": 0.08833823529411765, "top": 0.1645871212121212, "width": 0.08359477124183005, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for Blind People", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1401"}, {"text": "To minimize the confounding effect of computer vision accuracy, we prototyped our design with a Wizard of Oz protocol [65].", "label": "Method", "bboxes": [{"left": 0.6927124183006536, "top": 0.7154823232323233, "width": 0.21910947712418305, "height": 0.012727272727272587, "page": 3}, {"left": 0.5176633986928104, "top": 0.730030303030303, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176470588235295, "top": 0.7442714646464647, "width": 0.2311323529411764, "height": 0.01272727272727281, "page": 3}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1402"}, {"text": "We provide three different auditory feedback choices: (1) Sonification that indicates stair direction: one ding sound for going up and two ding sounds for going down, adapted from the sonic alerts for some elevators; (2) a human voice that verbally reported stair direction and number of stairs: Approaching upstairs, 14 stairs going up; and (3) a combined sonification and human voice: ding, approaching upstairs, 14 stairs going up.", "label": "Method", "bboxes": [{"left": 0.40587091503267975, "top": 0.21638888888888888, "width": 0.0765049019607843, "height": 0.012727272727272754, "page": 3}, {"left": 0.08820261437908496, "top": 0.23093686868686866, "width": 0.39420261437908505, "height": 0.012727272727272754, "page": 3}, {"left": 0.08821895424836601, "top": 0.24548358585858587, "width": 0.3941993464052288, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.260030303030303, "width": 0.3941764705882353, "height": 0.012727272727272754, "page": 3}, {"left": 0.08821895424836601, "top": 0.27457828282828284, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.289125, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.0882516339869281, "top": 0.3036729797979798, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.318219696969697, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.33276767676767677, "width": 0.12806862745098035, "height": 0.012727272727272698, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1403"}, {"text": "We recruited 12 PLV (6 female, 6 male; mean age=53.9) with different low-vision conditions, as shown in Table 1 (P1  P12).", "label": "Method", "bboxes": [{"left": 0.6077075163398692, "top": 0.5906275252525253, "width": 0.30414869281045764, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.605175505050505, "width": 0.39418627450980404, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.6197222222222222, "width": 0.12820751633986938, "height": 0.012727272727272698, "page": 3}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1404"}, {"text": "We evaluated the visualizations for projection-based AR, aiming to answer three questions: (1) How do PLV perceive the different visualization designs?", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.4975972222222222, "width": 0.39420751633986917, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.5121452020202021, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.5266919191919192, "width": 0.2338006535947713, "height": 0.01272727272727281, "page": 3}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1405"}, {"text": "Since locating the first and last stairs was most important but challenging for PLV [86], we distinguish the first and last stairs from the rest by projecting thick highlights on them (Figure 2a), while projecting thin highlights on the middle stairs (Figure 3a).", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.3545820707070707, "width": 0.3941388888888889, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3691287878787879, "width": 0.39412745098039215, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.38367676767676767, "width": 0.39422058823529404, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3982234848484848, "width": 0.3942075163398692, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.4127714646464647, "width": 0.11595261437908495, "height": 0.012727272727272698, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1406"}, {"text": "Beyond these highlights, we sought ways to further emphasize the first and last stairs so that a user will notice them and perceive their exact location from a distance.", "label": "Method", "bboxes": [{"left": 0.08820261437908496, "top": 0.49307954545454546, "width": 0.394124183006536, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5076275252525253, "width": 0.3941993464052288, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5218686868686868, "width": 0.3023006535947713, "height": 0.012727272727272698, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1407"}, {"text": "We designed two middle highlights to support the user in a minimally obtrusive way.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.2945669191919192, "width": 0.39420751633986917, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.30911489898989897, "width": 0.16747385620915034, "height": 0.012727272727272754, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1408"}, {"text": "(1) Flash: Since a flash can attract peoples attention [83, 84], we added this feature to the end highlights.", "label": "Method", "bboxes": [{"left": 0.08820261437908496, "top": 0.5585366161616161, "width": 0.3941568627450981, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5730833333333334, "width": 0.2921225490196079, "height": 0.012727272727272698, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1409"}, {"text": "In each phase, we presented all design options to the participant and asked about their experiences, including whether or not they liked the design, whether the design distracted them from seeing the environment, and how they wanted to improve it.", "label": "Method", "bboxes": [{"left": 0.0882843137254902, "top": 0.7594419191919192, "width": 0.39417156862745095, "height": 0.01272727272727281, "page": 4}, {"left": 0.08826797385620916, "top": 0.7739886363636364, "width": 0.39415849673202613, "height": 0.012727272727272587, "page": 4}, {"left": 0.08826797385620916, "top": 0.7885366161616161, "width": 0.3942761437908497, "height": 0.01272727272727281, "page": 4}, {"left": 0.08826797385620916, "top": 0.8030833333333334, "width": 0.39415849673202613, "height": 0.012727272727272587, "page": 4}, {"left": 0.08826797385620916, "top": 0.8176313131313131, "width": 0.055999999999999994, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1410"}, {"text": "During the visualization experience, we gave the participant our prototype smartphone and explained how to use it.", "label": "Method", "bboxes": [{"left": 0.0882516339869281, "top": 0.6357967171717172, "width": 0.39419444444444446, "height": 0.012727272727272698, "page": 4}, {"left": 0.0882516339869281, "top": 0.6503434343434343, "width": 0.36376307189542484, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1411"}, {"text": "We ended the study with an exit interview, asking about the participants general experience with the prototype.", "label": "Method", "bboxes": [{"left": 0.5177058823529412, "top": 0.7382638888888889, "width": 0.3941830065359476, "height": 0.012727272727272587, "page": 4}, {"left": 0.5176895424836602, "top": 0.752810606060606, "width": 0.3531781045751634, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1412"}, {"text": "We analyzed the effect of our visualizations on participants walking time when navigating stairs.", "label": "Method", "bboxes": [{"left": 0.5856307189542483, "top": 0.832814393939394, "width": 0.32633823529411765, "height": 0.012727272727272587, "page": 4}, {"left": 0.517673202614379, "top": 0.8473623737373738, "width": 0.33596732026143794, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1413"}, {"text": "We indicated the start points with yellow stickers on the landings, three feet away from the top and bottom stairs.", "label": "Method", "bboxes": [{"left": 0.5176895424836602, "top": 0.5634040404040405, "width": 0.39417320261437894, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.577645202020202, "width": 0.3675245098039215, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1414"}, {"text": "After the participant experienced all design alternatives in all three phases, we asked them to select one alternative from", "label": "Method", "bboxes": [{"left": 0.0882516339869281, "top": 0.8688459595959596, "width": 0.394140522875817, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882516339869281, "top": 0.8833926767676769, "width": 0.39422712418300654, "height": 0.012727272727272587, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1415"}, {"text": "We started the session with an interview, asking each participant about their demographics, visual condition, and technology use when navigating stairs.", "label": "Method", "bboxes": [{"left": 0.15950326797385622, "top": 0.5121515151515151, "width": 0.3229313725490196, "height": 0.012727272727272698, "page": 4}, {"left": 0.0882516339869281, "top": 0.5266982323232323, "width": 0.39412418300653596, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882516339869281, "top": 0.5412449494949495, "width": 0.2897549019607843, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1416"}, {"text": "To reduce order effects, we used a simultaneous within-subjects design, switching the task condition after each walking up and down task.", "label": "Method", "bboxes": [{"left": 0.5177222222222222, "top": 0.6725012626262626, "width": 0.3941405228758169, "height": 0.01272727272727281, "page": 4}, {"left": 0.5177058823529412, "top": 0.6870492424242425, "width": 0.3941813725490195, "height": 0.012727272727272587, "page": 4}, {"left": 0.5177058823529412, "top": 0.7015959595959596, "width": 0.1301013071895425, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1417"}, {"text": "They conducted each task in two conditions: (1) walking in their original way (participants could use a cane if desired, but nobody chose to use it); (2) walking using our prototype with their preferred combinations.", "label": "Method", "bboxes": [{"left": 0.6529133986928105, "top": 0.483094696969697, "width": 0.25903267973856203, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.49764141414141416, "width": 0.3941977124183007, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.5121893939393939, "width": 0.3941944444444444, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.5267361111111111, "width": 0.3563349673202614, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1418"}, {"text": "We asked the participant to hold a regular phone with the back camera facing the stairs, assuming the projected visualizations were from the smartphone.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.4172941919191919, "width": 0.39416666666666667, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4318421717171717, "width": 0.3940915032679739, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4463888888888889, "width": 0.22724183006535947, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1419"}, {"text": "Although we provided different visualizations (flash or movement) to further enhance the end highlights, most participants (seven out of 12) liked the original design.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.8230833333333333, "width": 0.3942663398692811, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8376313131313131, "width": 0.3941421568627451, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8521780303030303, "width": 0.331998366013072, "height": 0.01272727272727281, "page": 5}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1420"}, {"text": "To validate counterbalancing, we added another between-subject factor, Order (two levels: WithWithout, WithoutWith), into our model.", "label": "Method", "bboxes": [{"left": 0.34990359477124183, "top": 0.24183964646464648, "width": 0.1324722222222222, "height": 0.012727272727272726, "page": 5}, {"left": 0.08821895424836601, "top": 0.25638762626262623, "width": 0.3941666666666667, "height": 0.012727272727272754, "page": 5}, {"left": 0.08820261437908496, "top": 0.27093434343434347, "width": 0.3941307189542484, "height": 0.012727272727272698, "page": 5}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1421"}, {"text": "participants felt our design was helpful and [would make] life easier (P4), especially in relatively dark environments, such as subway stations.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.40093308080808077, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 5}, {"left": 0.08823529411764706, "top": 0.4151742424242424, "width": 0.39414052287581697, "height": 0.012727272727272754, "page": 5}, {"left": 0.08821895424836601, "top": 0.42972222222222217, "width": 0.16533660130718952, "height": 0.012727272727272754, "page": 5}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1422"}, {"text": "We analyzed the participants qualitative feedback by coding the interview transcripts based on grounded theory [66].", "label": "Method", "bboxes": [{"left": 0.08823366013071895, "top": 0.3366868686868687, "width": 0.3941911764705883, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823366013071895, "top": 0.3512348484848485, "width": 0.3666683006535948, "height": 0.012727272727272698, "page": 5}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1423"}, {"text": "Next, we report participants responses on all design alternatives in the three design phases.", "label": "Method", "bboxes": [{"left": 0.08818627450980392, "top": 0.5536729797979798, "width": 0.39420261437908494, "height": 0.012727272727272698, "page": 5}, {"left": 0.08816993464052288, "top": 0.568219696969697, "width": 0.20695261437908502, "height": 0.01272727272727281, "page": 5}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1424"}, {"text": "As such, our visualizations aim to facilitate a comfortable head pose by indicating the users exact location on the stairs without augmenting the stairs directly.", "label": "Method", "bboxes": [{"left": 0.893653594771242, "top": 0.6721717171717172, "width": 0.018207516339869168, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176960784313726, "top": 0.6867196969696969, "width": 0.39421078431372547, "height": 0.01272727272727281, "page": 6}, {"left": 0.5177124183006535, "top": 0.7009608585858585, "width": 0.39422385620915046, "height": 0.01272727272727281, "page": 6}, {"left": 0.5177124183006535, "top": 0.7155088383838384, "width": 0.2228480392156864, "height": 0.01272727272727281, "page": 6}], "section": "VISUALIZATIONS FOR SMARTGLASSES", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1425"}, {"text": "Our visualizations improved participants psychological security when walking on stairs.", "label": "Method", "bboxes": [{"left": 0.6788235294117647, "top": 0.07005050505050504, "width": 0.2329934640522877, "height": 0.01272727272727274, "page": 6}, {"left": 0.517625816993464, "top": 0.08459722222222223, "width": 0.3616274509803922, "height": 0.012727272727272726, "page": 6}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1426"}, {"text": "Based on our observations of the walking tasks, some participants ( e.g ., P9, P4) looked down less when using our design since they could use their lower peripheral vision to notice the highlights.", "label": "Method", "bboxes": [{"left": 0.21390849673202617, "top": 0.5066843434343434, "width": 0.26850326797385615, "height": 0.01272727272727281, "page": 6}, {"left": 0.08823692810457516, "top": 0.5212323232323232, "width": 0.3941781045751634, "height": 0.01272727272727281, "page": 6}, {"left": 0.08822058823529412, "top": 0.5357790404040403, "width": 0.39414542483660137, "height": 0.012727272727272698, "page": 6}, {"left": 0.08822058823529412, "top": 0.5503270202020202, "width": 0.20637745098039217, "height": 0.01272727272727281, "page": 6}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1427"}, {"text": "The second platform we explored was optical see-through smartglasses.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.3885063131313131, "width": 0.3941503267973855, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.4030542929292929, "width": 0.08672058823529405, "height": 0.012727272727272698, "page": 6}], "section": "VISUALIZATIONS FOR SMARTGLASSES", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1428"}, {"text": "Some participants ( e.g ., P6, P3, P11) hesitated at the first and last stairs and felt the stairs with their feet when walking without our visualizations (especially in the first two trials of the walking tasks).", "label": "Method", "bboxes": [{"left": 0.08822058823529412, "top": 0.6597297979797979, "width": 0.3941748366013072, "height": 0.01272727272727281, "page": 6}, {"left": 0.08822058823529412, "top": 0.6742777777777778, "width": 0.3941437908496732, "height": 0.012727272727272587, "page": 6}, {"left": 0.08822058823529412, "top": 0.6885189393939394, "width": 0.3941584967320262, "height": 0.012727272727272587, "page": 6}, {"left": 0.08822058823529412, "top": 0.7030669191919191, "width": 0.13227614379084968, "height": 0.01272727272727281, "page": 6}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1429"}, {"text": "Our visualizations reduced the time participants spent during stair navigation.", "label": "Method", "bboxes": [{"left": 0.19405882352941176, "top": 0.23033459595959596, "width": 0.2883513071895425, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.24488131313131312, "width": 0.23941666666666667, "height": 0.012727272727272754, "page": 6}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1430"}, {"text": "With a paired t -test, we also found a trend towards significant effect of Condition on the time walking upstairs ( t 11 =1.9894, p =0.0721).", "label": "Method", "bboxes": [{"left": 0.2585996732026144, "top": 0.45579040404040405, "width": 0.22384477124183, "height": 0.012727272727272698, "page": 6}, {"left": 0.08824509803921568, "top": 0.47033838383838383, "width": 0.39413071895424834, "height": 0.012727272727272754, "page": 6}, {"left": 0.08824509803921568, "top": 0.4848699494949495, "width": 0.266436274509804, "height": 0.012742424242424222, "page": 6}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1431"}, {"text": "We conducted a user study to evaluate the visualizations we designed for commercial smartglasses.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.7388093434343433, "width": 0.3941911764705881, "height": 0.01272727272727281, "page": 7}, {"left": 0.5176633986928104, "top": 0.7533573232323232, "width": 0.25916503267973856, "height": 0.01272727272727281, "page": 7}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1432"}, {"text": "Thus, to better inform the user of their position on the stairs, we distinguish a users position on a set of stairs based on how close she is to a change in her step pattern.", "label": "Method", "bboxes": [{"left": 0.08821895424836601, "top": 0.31457575757575756, "width": 0.39408823529411774, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.32912247474747475, "width": 0.3942924836601307, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.3436691919191919, "width": 0.31083986928104573, "height": 0.012727272727272754, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1433"}, {"text": "Similar to projection-based AR, when the user stands on the landing, our system verbally notifies the user of the existence of the stairs with stair direction and the number of stairs.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.21971843434343433, "width": 0.3941911764705882, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.23426641414141414, "width": 0.3941601307189542, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.24881313131313132, "width": 0.3698970588235294, "height": 0.012727272727272698, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1434"}, {"text": "Specifically, the following are the seven stages we used in our design, described for descending stairs as an example (Figure 6): (1) Upper landing: the flat surface that is more than 3' away from the edge of the top stair; (2) Upper preparation area: 1.5'3' away from the top stair edge where the person should prepare to step down; (3) Upper alert area: within 1.5' from the top stair edge where the persons next step would be stepping down; (4) Middle stairs: between the edge of the top stair and the edge of the second-to-last stair, where the person is stepping down repeatedly; (5) Lower preparation area: the last stair, where the person is one step away from the flat surface and should prepare for the imminent flat surface; (6) Lower alert area: within 1.5' from the last stair edge on the landing where the persons next step is on the flat surface (not stepping down); (7) Lower landing: 1.5' away from the last stair edge where the person is walking on flat surface again.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.42367297979797974, "width": 0.39420098039215684, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.43822095959595964, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.45276767676767676, "width": 0.39417320261437905, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.46731565656565655, "width": 0.3942124183006535, "height": 0.012727272727272754, "page": 7}, {"left": 0.08821895424836601, "top": 0.48186237373737373, "width": 0.3942091503267974, "height": 0.012727272727272698, "page": 7}, {"left": 0.08821895424836601, "top": 0.4964090909090909, "width": 0.39410784313725494, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5109570707070707, "width": 0.3942794117647059, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5255037878787879, "width": 0.39420424836601314, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5400517676767677, "width": 0.394140522875817, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5545984848484848, "width": 0.3941568627450981, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5688409090909091, "width": 0.3941568627450981, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5833876262626263, "width": 0.39419281045751636, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.597935606060606, "width": 0.3942042483660131, "height": 0.012727272727272698, "page": 7}, {"left": 0.08821895424836601, "top": 0.6124823232323232, "width": 0.3941666666666667, "height": 0.012727272727272698, "page": 7}, {"left": 0.08821895424836601, "top": 0.627030303030303, "width": 0.39409803921568626, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.6415770202020202, "width": 0.39420424836601314, "height": 0.012727272727272587, "page": 7}, {"left": 0.08820261437908496, "top": 0.6561237373737373, "width": 0.1398186274509804, "height": 0.01272727272727281, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1435"}, {"text": "(1) Glow visualization (Figure 7ad): We generate a glow effect at the bottom of the display to simulate the experience of seeing the edge highlights on the stairs with peripheral vision.", "label": "Method", "bboxes": [{"left": 0.08821895424836601, "top": 0.7073383838383839, "width": 0.39422385620915035, "height": 0.01272727272727281, "page": 7}, {"left": 0.08823529411764706, "top": 0.7218863636363637, "width": 0.3941633986928104, "height": 0.012727272727272587, "page": 7}, {"left": 0.08823529411764706, "top": 0.7364330808080808, "width": 0.39417483660130714, "height": 0.01272727272727281, "page": 7}, {"left": 0.08823529411764706, "top": 0.7506755050505051, "width": 0.03136601307189542, "height": 0.01272727272727281, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1436"}, {"text": "We then gave the HoloLens to the participant and explained how to use it.", "label": "Method", "bboxes": [{"left": 0.33354901960784317, "top": 0.5779368686868687, "width": 0.14881045751633987, "height": 0.012727272727272698, "page": 8}, {"left": 0.08818627450980392, "top": 0.5924848484848485, "width": 0.3545588235294117, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1437"}, {"text": "We conducted the design exploration session at an emergency staircase with 12 stairs (different stairs than those in the projection study).", "label": "Method", "bboxes": [{"left": 0.08818627450980392, "top": 0.6873409090909092, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 8}, {"left": 0.08818627450980392, "top": 0.7018876262626262, "width": 0.3941764705882353, "height": 0.01272727272727281, "page": 8}, {"left": 0.08820261437908496, "top": 0.7161300505050504, "width": 0.14408496732026144, "height": 0.01272727272727281, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1438"}, {"text": "To minimize the confounding effect of general computer vision accuracy, we marked the position of the stairs with two Vuforia image targets [37] (on the side walls at the top and bottom landing of the stairs) that can be recognized by HoloLens.", "label": "Method", "bboxes": [{"left": 0.08821895424836601, "top": 0.3815555555555556, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 8}, {"left": 0.08821895424836601, "top": 0.3961022727272728, "width": 0.39419444444444446, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.41065025252525256, "width": 0.39425326797385624, "height": 0.012727272727272698, "page": 8}, {"left": 0.08821895424836601, "top": 0.4251969696969697, "width": 0.39415686274509804, "height": 0.012727272727272754, "page": 8}, {"left": 0.08821895424836601, "top": 0.43974368686868687, "width": 0.048620915032679735, "height": 0.012727272727272754, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1439"}, {"text": "We first report the effect of the HoloLens on participants visual abilities.", "label": "Method", "bboxes": [{"left": 0.7512826797385621, "top": 0.7748699494949495, "width": 0.16049673202614378, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176307189542484, "top": 0.7894179292929293, "width": 0.3207320261437907, "height": 0.012727272727272587, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1440"}, {"text": "We analyzed the effect of our visualizations on participants walking time when navigating stairs.", "label": "Method", "bboxes": [{"left": 0.5854820261437909, "top": 0.4634419191919192, "width": 0.32633823529411765, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175245098039216, "top": 0.47798989898989896, "width": 0.33596732026143783, "height": 0.012727272727272754, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1441"}, {"text": "We asked them to score the usefulness and comfort level of the prototype on a Likert scale, as well as their psychological security when using the prototype, ranging from 1 (strongly negative) to 7 (strongly positive).", "label": "Method", "bboxes": [{"left": 0.8462124183006535, "top": 0.3831338383838384, "width": 0.0655065359477125, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175408496732027, "top": 0.3976805555555556, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175408496732027, "top": 0.4122272727272727, "width": 0.3941078431372548, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175245098039216, "top": 0.4267752525252525, "width": 0.39422385620915035, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175245098039216, "top": 0.4413219696969697, "width": 0.15342973856209152, "height": 0.012727272727272754, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1442"}, {"text": "We built our prototype on Microsoft HoloLens v1.", "label": "Method", "bboxes": [{"left": 0.16765686274509803, "top": 0.24336237373737374, "width": 0.3147483660130719, "height": 0.012727272727272754, "page": 8}, {"left": 0.08823529411764706, "top": 0.2579090909090909, "width": 0.02052287581699347, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1443"}, {"text": "We recruited 12 PLV (5 female, 7 male; mean age=51.6) with different low vision conditions (Table 1, P6 P17).", "label": "Method", "bboxes": [{"left": 0.17829575163398695, "top": 0.1342638888888889, "width": 0.30414869281045753, "height": 0.012727272727272726, "page": 8}, {"left": 0.0882516339869281, "top": 0.1488118686868687, "width": 0.3942565359477125, "height": 0.012727272727272726, "page": 8}, {"left": 0.0882516339869281, "top": 0.16335858585858584, "width": 0.03499019607843139, "height": 0.012727272727272754, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1444"}, {"text": "We indicated the start and end points on the stairs with stickers that were three feet away from the top and bottom steps on the landings.", "label": "Method", "bboxes": [{"left": 0.5175898692810458, "top": 0.19372601010101012, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175898692810458, "top": 0.20827272727272728, "width": 0.3941813725490195, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175898692810458, "top": 0.22282070707070706, "width": 0.10745424836601292, "height": 0.012727272727272754, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1445"}, {"text": "To validate counterbalancing, we added another betweensubject factor, Order (six levels based on the three conditions), into our model.", "label": "Method", "bboxes": [{"left": 0.5175245098039216, "top": 0.601635101010101, "width": 0.3941584967320262, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175245098039216, "top": 0.6161818181818182, "width": 0.3941078431372549, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175081699346404, "top": 0.630729797979798, "width": 0.1468709150326798, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1446"}, {"text": "To reduce the effect of order on the results, we used a simultaneous within-subjects design by switching the task condition after each round of walking up and down.", "label": "Method", "bboxes": [{"left": 0.5175735294117647, "top": 0.30313005050505054, "width": 0.39399346405228763, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175735294117647, "top": 0.31767676767676767, "width": 0.3942075163398693, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175571895424836, "top": 0.3319191919191919, "width": 0.2976928104575164, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1447"}, {"text": "We analyzed participants qualitative responses with the same method we used in the previous study.", "label": "Method", "bboxes": [{"left": 0.517656862745098, "top": 0.7251717171717171, "width": 0.39416993464052286, "height": 0.01272727272727281, "page": 8}, {"left": 0.517640522875817, "top": 0.7397196969696969, "width": 0.28866339869281055, "height": 0.01272727272727281, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1448"}, {"text": "Interestingly, we found that participants had different preferences for Paths position in their visual field.", "label": "Method", "bboxes": [{"left": 0.517640522875817, "top": 0.3176818181818182, "width": 0.3941078431372549, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.33192424242424245, "width": 0.28760947712418305, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1449"}, {"text": "We", "label": "Method", "bboxes": [{"left": 0.45969607843137256, "top": 0.1357840909090909, "width": 0.022730392156862733, "height": 0.012727272727272726, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1450"}, {"text": "Our design changed peoples behaviors when walking on stairs.", "label": "Method", "bboxes": [{"left": 0.6475539215686275, "top": 0.7606654040404041, "width": 0.2642369281045751, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176503267973857, "top": 0.7752121212121211, "width": 0.1580653594771242, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1451"}, {"text": "We found that theres no significant effect of Condition (HoloLens with visualizations vs. HoloLens without visualizations) on participants walking time for both ascending ( F (1,10) =0.466, p =0.511) and descending stairs ( F (1,10) =0.114, p =0.742).", "label": "Method", "bboxes": [{"left": 0.7176797385620916, "top": 0.2733623737373737, "width": 0.1942173202614379, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176437908496732, "top": 0.2879090909090909, "width": 0.3982598039215687, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176437908496732, "top": 0.3024558080808081, "width": 0.39418137254901964, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176437908496732, "top": 0.31668813131313134, "width": 0.39422875816993475, "height": 0.012737373737373658, "page": 10}, {"left": 0.5176486928104574, "top": 0.33123358585858587, "width": 0.2712467320261438, "height": 0.012729797979797952, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1452"}, {"text": "Paired Wilcoxon Signed-Rank tests showed that, while wearing HoloLens significantly reduced participants psychological security ( V =8, p =0.031), our visualizations significantly increased participant psychological security compared with not wearing HoloLens ( V =21, p =0.050).", "label": "Method", "bboxes": [{"left": 0.5896470588235294, "top": 0.6803560606060606, "width": 0.3221176470588235, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.6949027777777779, "width": 0.3942859477124183, "height": 0.012727272727272587, "page": 10}, {"left": 0.5176339869281046, "top": 0.7094507575757576, "width": 0.39417320261437894, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.7239974747474747, "width": 0.39426797385620904, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.7385454545454545, "width": 0.3647075163398693, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1453"}, {"text": "We found that most participants (10 out of 12) combined a visualization with a sonification (Beep).", "label": "Method", "bboxes": [{"left": 0.0882516339869281, "top": 0.3242638888888889, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882516339869281, "top": 0.3388118686868687, "width": 0.2609101307189543, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1454"}, {"text": "While there is no significant improvement in walking speed when using the visualizations, participants reported feeling safer and more confident when using our design.", "label": "Method", "bboxes": [{"left": 0.6847941176470588, "top": 0.48427904040404035, "width": 0.2270800653594771, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176666666666667, "top": 0.49852146464646463, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176666666666667, "top": 0.5130681818181818, "width": 0.39417156862745095, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176830065359477, "top": 0.5276161616161616, "width": 0.1139101307189544, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1455"}, {"text": "With ANOVA, we found that participants walking time significantly increased when they walked downstairs wearing the HoloLens whether using our visualizations or not ( F (2,12) =8.783, p =0.0045).", "label": "Method", "bboxes": [{"left": 0.20690522875816994, "top": 0.7391477272727273, "width": 0.2754869281045751, "height": 0.01272727272727281, "page": 10}, {"left": 0.0882843137254902, "top": 0.7536957070707071, "width": 0.3942369281045752, "height": 0.012727272727272587, "page": 10}, {"left": 0.0882843137254902, "top": 0.7682424242424242, "width": 0.39422385620915035, "height": 0.01272727272727281, "page": 10}, {"left": 0.0882843137254902, "top": 0.7827487373737374, "width": 0.27779738562091505, "height": 0.01276767676767665, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1456"}, {"text": "With the condition of wearing HoloLens without visualizations as the baseline, we analyzed the effect of our visualiza-", "label": "Method", "bboxes": [{"left": 0.08822222222222222, "top": 0.8627487373737374, "width": 0.39427450980392165, "height": 0.012727272727272587, "page": 10}, {"left": 0.08822222222222222, "top": 0.8772967171717171, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1457"}, {"text": "In general, participants felt that our prototype was helpful, especially in unfamiliar places.", "label": "Method", "bboxes": [{"left": 0.08831699346405228, "top": 0.6155025252525252, "width": 0.3941584967320261, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.6300492424242424, "width": 0.22053104575163393, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1458"}, {"text": "Our evaluation was conducted indoors, with no other people around.", "label": "Method", "bboxes": [{"left": 0.7362549019607844, "top": 0.5922184343434344, "width": 0.17555882352941177, "height": 0.01272727272727281, "page": 11}, {"left": 0.517640522875817, "top": 0.6067664141414142, "width": 0.2998464052287583, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1459"}, {"text": "Unlike prior research, which showed that PLV had very different preferences for visual augmentations [84, 85], our study revealed that some common preferences among PLV cross different visual abilities for stair navigation.", "label": "Method", "bboxes": [{"left": 0.45677124183006534, "top": 0.5615593434343434, "width": 0.025457516339869368, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.5761060606060606, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.5906527777777778, "width": 0.39419117647058827, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.6052007575757576, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.6197474747474747, "width": 0.28821895424836597, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1460"}, {"text": "First, the HoloLenss weight strongly diminished PLVs experiences, which may have influenced our results.", "label": "Method", "bboxes": [{"left": 0.8234003267973856, "top": 0.8031477272727272, "width": 0.08831535947712421, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175408496732027, "top": 0.8173901515151516, "width": 0.3941584967320261, "height": 0.012727272727272587, "page": 11}, {"left": 0.5175408496732027, "top": 0.8319368686868687, "width": 0.2527581699346404, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1461"}, {"text": "Our research is the first to explore AR visualizations for people with low vision in the context of stair navigation.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.33608207070707075, "width": 0.3942238562091503, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.35063005050505053, "width": 0.3627483660130719, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1462"}, {"text": "We compared users experiences with the visualizations on both platforms given that seven participated in both studies.", "label": "Method", "bboxes": [{"left": 0.08821895424836601, "top": 0.6709621212121213, "width": 0.39417320261437905, "height": 0.012727272727272587, "page": 11}, {"left": 0.08821895424836601, "top": 0.6852045454545455, "width": 0.39422385620915035, "height": 0.012727272727272587, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1463"}, {"text": "While our study focused on the design and evaluation of the AR visualizations, we discuss the technical feasibility and challenges for our AR stair navigation systems.", "label": "Method", "bboxes": [{"left": 0.517673202614379, "top": 0.3521957070707071, "width": 0.394140522875817, "height": 0.012727272727272698, "page": 11}, {"left": 0.517673202614379, "top": 0.3667424242424242, "width": 0.3941862745098039, "height": 0.012727272727272754, "page": 11}, {"left": 0.5176895424836602, "top": 0.3812891414141414, "width": 0.315937908496732, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1464"}, {"text": "that, he looked much further down to the stairs when not using our visualizations, especially at the beginning and the end of the stairs ( e.g ., preparation area, alert area).", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.2721426767676768, "width": 0.39410784313725494, "height": 0.012727272727272698, "page": 11}, {"left": 0.08823529411764706, "top": 0.286385101010101, "width": 0.3941846405228758, "height": 0.012727272727272698, "page": 11}, {"left": 0.08823529411764706, "top": 0.3009318181818182, "width": 0.30114869281045753, "height": 0.012727272727272754, "page": 11}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1465"}, {"text": "In this paper, we designed AR visualizations to facilitate stair navigation for people with low vision.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.2357790404040404, "width": 0.39419444444444446, "height": 0.012727272727272754, "page": 12}, {"left": 0.08823529411764706, "top": 0.2503270202020202, "width": 0.2511928104575163, "height": 0.012727272727272754, "page": 12}], "section": "CONCLUSIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1466"}, {"text": "the HoloLens, we designed visualizations in the users central vision instead of adding highlights to the stairs in our smartglasses prototype.", "label": "Method", "bboxes": [{"left": 0.08823202614379085, "top": 0.07018813131313131, "width": 0.39417320261437916, "height": 0.012727272727272726, "page": 12}, {"left": 0.0882483660130719, "top": 0.08473611111111112, "width": 0.39414052287581697, "height": 0.012727272727272726, "page": 12}, {"left": 0.0882483660130719, "top": 0.09928282828282829, "width": 0.15778758169934642, "height": 0.012727272727272726, "page": 12}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1467"}, {"text": "This work was supported in part by the National Science Foundation under grant no.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.4015366161616162, "width": 0.3941748366013072, "height": 0.012727272727272698, "page": 12}, {"left": 0.08823529411764706, "top": 0.41608459595959596, "width": 0.17816993464052283, "height": 0.012727272727272754, "page": 12}], "section": "CONCLUSIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1468"}, {"text": "We adjust the glow color and size to inform the user of their current stage on the stairs:", "label": "Method", "bboxes": [{"left": 0.7824869281045752, "top": 0.08461868686868687, "width": 0.1293790849673203, "height": 0.012727272727272726, "page": 7}, {"left": 0.517640522875817, "top": 0.09916666666666668, "width": 0.39417647058823535, "height": 0.012727272727272726, "page": 7}, {"left": 0.517640522875817, "top": 0.11371338383838385, "width": 0.03906699346405229, "height": 0.012727272727272712, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1469"}, {"text": "(2) Path visualization (Figure 7eg): Inspired by the railings, which PLV used as a visual cue to see where the stairs start and end [86], we designed this visualization to show the trend of the stairs.", "label": "Method", "bboxes": [{"left": 0.517640522875817, "top": 0.2631060606060606, "width": 0.39416666666666667, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176241830065359, "top": 0.2776527777777778, "width": 0.3943480392156862, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176241830065359, "top": 0.2922007575757576, "width": 0.394235294117647, "height": 0.012727272727272698, "page": 7}, {"left": 0.517640522875817, "top": 0.3067474747474747, "width": 0.08284640522875819, "height": 0.012727272727272754, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1470"}, {"text": "Similar to glow, we adjusted the sound based on the different stages of the stairs:", "label": "Method", "bboxes": [{"left": 0.7188055555555556, "top": 0.5319191919191919, "width": 0.19301307189542483, "height": 0.01272727272727281, "page": 7}, {"left": 0.517640522875817, "top": 0.5464659090909091, "width": 0.3379395424836602, "height": 0.012727272727272698, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1471"}, {"text": "(3) How secure do PLV feel when using our visualizations?", "label": "Method", "bboxes": [{"left": 0.08822712418300653, "top": 0.09912878787878789, "width": 0.3908562091503268, "height": 0.012727272727272726, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1472"}, {"text": "They preferred devices, such as smartglasses, that would show the visualizations only to them.", "label": "Result", "bboxes": [{"left": 0.7865081699346406, "top": 0.3243156565656566, "width": 0.1251928104575163, "height": 0.012727272727272698, "page": 6}, {"left": 0.5175767973856209, "top": 0.33886363636363637, "width": 0.3942859477124183, "height": 0.012727272727272698, "page": 6}, {"left": 0.5175767973856209, "top": 0.35341035353535355, "width": 0.1225735294117648, "height": 0.012727272727272698, "page": 6}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1473"}, {"text": "(2) Path visualization (Figure 7eg): Inspired by the railings, which PLV used as a visual cue to see where the stairs start and end [86], we designed this visualization to show the trend of the stairs.", "label": "Result", "bboxes": [{"left": 0.517640522875817, "top": 0.2631060606060606, "width": 0.39416666666666667, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176241830065359, "top": 0.2776527777777778, "width": 0.3943480392156862, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176241830065359, "top": 0.2922007575757576, "width": 0.394235294117647, "height": 0.012727272727272698, "page": 7}, {"left": 0.517640522875817, "top": 0.3067474747474747, "width": 0.08284640522875819, "height": 0.012727272727272754, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1474"}, {"text": "The user can thus observe the start and end of the stairs by looking at the turning points of the Path.", "label": "Result", "bboxes": [{"left": 0.853968954248366, "top": 0.40827272727272723, "width": 0.05784477124183007, "height": 0.012727272727272754, "page": 7}, {"left": 0.517656862745098, "top": 0.4228207070707071, "width": 0.39426143790849677, "height": 0.012727272727272698, "page": 7}, {"left": 0.517656862745098, "top": 0.4373674242424242, "width": 0.1935947712418301, "height": 0.012727272727272754, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1475"}, {"text": "However, sometimes stairs do not have contrast stripes, and even when they do, their stripes are often not accessibly designed; for example, stripes may have low contrast with the stairs or be too thin to detect [86].", "label": "Conclusion", "bboxes": [{"left": 0.5176797385620915, "top": 0.5906439393939394, "width": 0.3941911764705883, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176797385620915, "top": 0.6051919191919192, "width": 0.39425, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176960784313726, "top": 0.6197386363636364, "width": 0.394142156862745, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176960784313726, "top": 0.6339810606060606, "width": 0.23230228758169935, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1476"}, {"text": "Thus, there is a gap in tools that support PLV in the basic task of stair navigation.", "label": "Conclusion", "bboxes": [{"left": 0.7350915032679738, "top": 0.6630757575757575, "width": 0.17674673202614377, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176797385620915, "top": 0.6776224747474747, "width": 0.34855065359477133, "height": 0.012727272727272587, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1477"}, {"text": "A visualization that a sighted person can easily see ( e.g ., a small arrow) may not be noticeable by PLV: it may be too small for them to see or outside their visual field [87].", "label": "Conclusion", "bboxes": [{"left": 0.8998970588235293, "top": 0.8524823232323233, "width": 0.011890522875817044, "height": 0.012727272727272587, "page": 0}, {"left": 0.5175816993464052, "top": 0.867030303030303, "width": 0.39414052287581713, "height": 0.01272727272727281, "page": 0}, {"left": 0.5175816993464052, "top": 0.8815770202020201, "width": 0.3941241830065362, "height": 0.01272727272727281, "page": 0}, {"left": 0.5175816993464052, "top": 0.8961237373737374, "width": 0.30958006535947724, "height": 0.012727272727272587, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1478"}, {"text": "This could be dangerous in the context of stair navigation.", "label": "Conclusion", "bboxes": [{"left": 0.3223218954248366, "top": 0.09915656565656565, "width": 0.16018627450980394, "height": 0.012727272727272726, "page": 1}, {"left": 0.08827450980392157, "top": 0.11370454545454546, "width": 0.21494771241830066, "height": 0.01272727272727274, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1479"}, {"text": "We sought to design effective visualizations for PLV, which balance visibility and distraction, while providing alternative choices to support a wide range of visual abilities.", "label": "Conclusion", "bboxes": [{"left": 0.3071535947712418, "top": 0.11370454545454546, "width": 0.1753480392156863, "height": 0.01272727272727274, "page": 1}, {"left": 0.08829248366013072, "top": 0.12825126262626263, "width": 0.39415686274509804, "height": 0.012727272727272726, "page": 1}, {"left": 0.08827450980392157, "top": 0.14249368686868688, "width": 0.39421241830065357, "height": 0.012727272727272726, "page": 1}, {"left": 0.08827450980392157, "top": 0.15704040404040404, "width": 0.192937908496732, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1480"}, {"text": "(2) For smartglasses that have a limited vertical field of view (FOV), we designed visualizations in the users central FOV to indicate the users exact position on the stairs (Figure 1b).", "label": "Conclusion", "bboxes": [{"left": 0.16991993464052288, "top": 0.2809911616161616, "width": 0.3124640522875817, "height": 0.012727272727272754, "page": 1}, {"left": 0.08824183006535948, "top": 0.29553914141414145, "width": 0.39414542483660137, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.3100858585858586, "width": 0.3941421568627451, "height": 0.012727272727272754, "page": 1}, {"left": 0.08824183006535948, "top": 0.3246338383838384, "width": 0.07808823529411764, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1481"}, {"text": "In summary, we contribute the first exploration of AR visualizations to facilitate stair navigation for PLV.", "label": "Conclusion", "bboxes": [{"left": 0.08825816993464053, "top": 0.48494570707070705, "width": 0.39414215686274506, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.49918813131313133, "width": 0.3113937908496732, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": 1, "is_author_statement": true, "is_in_expected_section": false, "id": "1482"}, {"text": "Our evaluations demonstrated the effectiveness of our visualizations and provide insights for the design of AR visualizations for PLV that support other tasks as well.", "label": "Conclusion", "bboxes": [{"left": 0.40423039215686274, "top": 0.49918813131313133, "width": 0.07813725490196083, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.5137348484848485, "width": 0.39409477124183007, "height": 0.01272727272727281, "page": 1}, {"left": 0.08824183006535948, "top": 0.5282828282828282, "width": 0.3941290849673203, "height": 0.01272727272727281, "page": 1}, {"left": 0.08825816993464053, "top": 0.5428295454545454, "width": 0.2410147058823529, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1483"}, {"text": "With a growing number of smartphones that have embedded depth sensors (e.g., iPhone XR, Samsung Galaxy S10) and projectors (e.g., Samsung Galaxy Beam [67]), smartphones may support projection-based AR with depth-sensing capabilities in the near future.", "label": "Conclusion", "bboxes": [{"left": 0.8096862745098039, "top": 0.5124608585858587, "width": 0.10215196078431366, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.5267032828282828, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.54125, "width": 0.3941617647058824, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.5557979797979798, "width": 0.3941584967320261, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.570344696969697, "width": 0.3941584967320261, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.5848926767676768, "width": 0.043143790849673236, "height": 0.012727272727272698, "page": 2}], "section": "VISUALIZATIONS FOR PROJECTION-BASED AR", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1484"}, {"text": "Thus, we designed visualizations for such a projection-based AR smartphone to augment the stairs for PLV.", "label": "Conclusion", "bboxes": [{"left": 0.5663513071895425, "top": 0.5848926767676768, "width": 0.3454526143790849, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5994393939393939, "width": 0.3772859477124182, "height": 0.01272727272727281, "page": 2}], "section": "VISUALIZATIONS FOR PROJECTION-BASED AR", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1485"}, {"text": "[64] built a real-time head-worn LED display with a depth camera to aid navigation by detecting the distance to nearby objects and changing the brightness of the objects to indicate their distances.", "label": "Conclusion", "bboxes": [{"left": 0.45513398692810453, "top": 0.5055037878787878, "width": 0.02727941176470594, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.5200517676767676, "width": 0.394156862745098, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.5342929292929294, "width": 0.39414052287581697, "height": 0.012727272727272698, "page": 2}, {"left": 0.08823529411764706, "top": 0.5488409090909091, "width": 0.39414052287581697, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.5633876262626263, "width": 0.06400490196078432, "height": 0.01272727272727281, "page": 2}], "section": "Safe Navigation for PLV", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1486"}, {"text": "However, these specialized tools often stigmatize users in social settings [69]; thus, people avoid using them or abandon them altogether [25].", "label": "Conclusion", "bboxes": [{"left": 0.4183382352941177, "top": 0.2945757575757576, "width": 0.06401797385620911, "height": 0.012727272727272698, "page": 2}, {"left": 0.08818627450980392, "top": 0.30912247474747473, "width": 0.39419281045751636, "height": 0.012727272727272754, "page": 2}, {"left": 0.08818627450980392, "top": 0.3236691919191919, "width": 0.3942058823529412, "height": 0.012727272727272698, "page": 2}, {"left": 0.08820261437908496, "top": 0.3382171717171717, "width": 0.10093137254901961, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for PLV", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1487"}, {"text": "Some PLV also use a white cane, especially at night and in unfamiliar places, but many prefer not using it because it exposes their disability [86].", "label": "Conclusion", "bboxes": [{"left": 0.19336764705882353, "top": 0.3382171717171717, "width": 0.2890032679738562, "height": 0.012727272727272698, "page": 2}, {"left": 0.08820261437908496, "top": 0.3527638888888889, "width": 0.39417647058823535, "height": 0.012727272727272698, "page": 2}, {"left": 0.08820261437908496, "top": 0.3673118686868687, "width": 0.2682075163398693, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for PLV", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1488"}, {"text": "We designed and evaluated visualizations for both platforms, given that each platform has its own strength: projectionbased AR can augment large physical surfaces but projects content publicly, which may be better suited to private places with few people ( e.g ., home, workspace); meanwhile, smartglasses present information only to the user, which may be better for crowded public places ( e.g ., subway stations).", "label": "Conclusion", "bboxes": [{"left": 0.8890522875816994, "top": 0.22278282828282828, "width": 0.022735294117647076, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.2373308080808081, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.25187752525252527, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.26642424242424245, "width": 0.3940915032679738, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.28097222222222223, "width": 0.3941584967320261, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.29551893939393936, "width": 0.39412418300653596, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.3100669191919192, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.3246136363636364, "width": 0.3652679738562091, "height": 0.012727272727272698, "page": 2}], "section": "INITIAL EXPLORATION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1489"}, {"text": "We therefore designed our visualizations to help them perceive the stairs from a greater distance, so they can better plan and prepare their steps.", "label": "Conclusion", "bboxes": [{"left": 0.5624640522875817, "top": 0.8164015151515152, "width": 0.34943954248366027, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176797385620915, "top": 0.8309494949494949, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176797385620915, "top": 0.8454962121212122, "width": 0.1838349673202614, "height": 0.012727272727272587, "page": 2}], "section": "Visualization (and Sonification) Design", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1490"}, {"text": "This platform has potential to facilitate mobility because it can project over a relatively large area [88] and provide visual augmentations in peoples peripheral vision, which is shown to be important for stair navigation [56].", "label": "Conclusion", "bboxes": [{"left": 0.6709558823529411, "top": 0.4030580808080808, "width": 0.2408986928104575, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176633986928104, "top": 0.417604797979798, "width": 0.3941977124183006, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.4321515151515152, "width": 0.394295751633987, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.44669949494949496, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.46124621212121214, "width": 0.08080555555555557, "height": 0.012727272727272698, "page": 2}], "section": "VISUALIZATIONS FOR PROJECTION-BASED AR", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1491"}, {"text": "wearable RGB-D camera, and could even work when the stairs were partially occluded.", "label": "Conclusion", "bboxes": [{"left": 0.08830555555555555, "top": 0.07003661616161616, "width": 0.394107843137255, "height": 0.01272727272727274, "page": 2}, {"left": 0.08830555555555555, "top": 0.08458333333333333, "width": 0.19603267973856206, "height": 0.012727272727272726, "page": 2}], "section": "Safe Navigation for Blind People", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1492"}, {"text": "We designed two middle highlights to support the user in a minimally obtrusive way.", "label": "Conclusion", "bboxes": [{"left": 0.5176470588235295, "top": 0.2945669191919192, "width": 0.39420751633986917, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.30911489898989897, "width": 0.16747385620915034, "height": 0.012727272727272754, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1493"}, {"text": "(2) Flashing Edge: When the end highlight flashes, the user may lose track of the edge position when the highlight disappears.", "label": "Conclusion", "bboxes": [{"left": 0.08818627450980392, "top": 0.6097512626262627, "width": 0.3941241830065359, "height": 0.012727272727272698, "page": 3}, {"left": 0.08816993464052288, "top": 0.6242979797979799, "width": 0.39400816993464055, "height": 0.012727272727272698, "page": 3}, {"left": 0.08816993464052288, "top": 0.6388459595959596, "width": 0.05400653594771242, "height": 0.01272727272727281, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1494"}, {"text": "(5) Moving Vertical Zebra: Moving the highlight over the edge of the stair may distort the perceived location of the edge, so we also designed a zebra pattern that is perpendicular to the edge (Figure 2e).", "label": "Conclusion", "bboxes": [{"left": 0.08813725490196078, "top": 0.8067310606060607, "width": 0.3941176470588235, "height": 0.012727272727272587, "page": 3}, {"left": 0.08815359477124184, "top": 0.8212777777777778, "width": 0.3941470588235294, "height": 0.01272727272727281, "page": 3}, {"left": 0.08815359477124184, "top": 0.8358257575757576, "width": 0.39420588235294113, "height": 0.012727272727272587, "page": 3}, {"left": 0.08815359477124184, "top": 0.8503724747474748, "width": 0.17424183006535948, "height": 0.01272727272727281, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1495"}, {"text": "To support a range of visual abilities, the design alternatives can be selected and combined by a user to optimize her experience for a particular environment.", "label": "Conclusion", "bboxes": [{"left": 0.5176633986928104, "top": 0.4333585858585859, "width": 0.3941437908496731, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176633986928104, "top": 0.447905303030303, "width": 0.39422875816993463, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176633986928104, "top": 0.4624532828282828, "width": 0.24727287581699353, "height": 0.012727272727272754, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1496"}, {"text": "Participants were asked to walk as quickly and safely as possible.", "label": "Conclusion", "bboxes": [{"left": 0.8637369281045751, "top": 0.6212878787878788, "width": 0.04816013071895431, "height": 0.012727272727272698, "page": 4}, {"left": 0.5177222222222222, "top": 0.635834595959596, "width": 0.39403594771241823, "height": 0.012727272727272587, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1497"}, {"text": "They conducted each task in two conditions: (1) walking in their original way (participants could use a cane if desired, but nobody chose to use it); (2) walking using our prototype with their preferred combinations.", "label": "Conclusion", "bboxes": [{"left": 0.6529133986928105, "top": 0.483094696969697, "width": 0.25903267973856203, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.49764141414141416, "width": 0.3941977124183007, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.5121893939393939, "width": 0.3941944444444444, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.5267361111111111, "width": 0.3563349673202614, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1498"}, {"text": "As P7 explained: I guess because I dont see details,", "label": "Conclusion", "bboxes": [{"left": 0.12941176470588237, "top": 0.8958194444444445, "width": 0.35281699346405226, "height": 0.012727272727272587, "page": 5}], "section": "Results", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1499"}, {"text": "Although no participants chose the Moving Edge in the study, P6 felt it could be helpful since it indicated direction.", "label": "Conclusion", "bboxes": [{"left": 0.5175245098039216, "top": 0.4970492424242424, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 5}, {"left": 0.5175081699346404, "top": 0.5115959595959596, "width": 0.39425980392156856, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1500"}, {"text": "P9 said, Having [the highlights] this bright is really good. Because usually [the contrast stripes] are painted, and theyre about to fade out, and theyre not as vibrant and bright as this is. This is great here because you can see it.", "label": "Conclusion", "bboxes": [{"left": 0.13912745098039217, "top": 0.4588169191919192, "width": 0.3432320261437909, "height": 0.012727272727272754, "page": 5}, {"left": 0.08820261437908496, "top": 0.4733636363636364, "width": 0.39420098039215684, "height": 0.012727272727272698, "page": 5}, {"left": 0.08821895424836601, "top": 0.48791035353535356, "width": 0.3942075163398693, "height": 0.012727272727272754, "page": 5}, {"left": 0.08820261437908496, "top": 0.5024583333333333, "width": 0.3941732026143791, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1501"}, {"text": "Even on a typical set of stairs, participants wanted the middle highlights to confirm that they are still on the stairs, which made them feel safe.", "label": "Conclusion", "bboxes": [{"left": 0.5174428104575164, "top": 0.7519128787878788, "width": 0.39416339869281036, "height": 0.01272727272727281, "page": 5}, {"left": 0.5174428104575164, "top": 0.7664595959595959, "width": 0.394156862745098, "height": 0.01272727272727281, "page": 5}, {"left": 0.5174428104575164, "top": 0.7810075757575757, "width": 0.13691339869281038, "height": 0.01272727272727281, "page": 5}], "section": "Results", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1502"}, {"text": "Meanwhile, three participants (P2, P8, P7) chose the nonverbal sound because they had relatively good vision and felt the human voice was unnecessary.", "label": "Conclusion", "bboxes": [{"left": 0.4396356209150327, "top": 0.6191287878787879, "width": 0.04267483660130711, "height": 0.012727272727272698, "page": 5}, {"left": 0.08815359477124184, "top": 0.6336767676767677, "width": 0.3941405228758169, "height": 0.012727272727272698, "page": 5}, {"left": 0.08815359477124184, "top": 0.6482234848484849, "width": 0.3941699346405229, "height": 0.012727272727272587, "page": 5}, {"left": 0.08815359477124184, "top": 0.6627714646464646, "width": 0.2078415032679738, "height": 0.01272727272727281, "page": 5}], "section": "Results", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1503"}, {"text": "This is the part where I probably trip the most, on that last step. The light [end highlights] is really important because it defines the end of the step, so youre not gonna miss a step (P5).", "label": "Conclusion", "bboxes": [{"left": 0.4033333333333333, "top": 0.7576275252525253, "width": 0.07899346405228769, "height": 0.012727272727272587, "page": 5}, {"left": 0.08813725490196078, "top": 0.7718686868686869, "width": 0.39415686274509804, "height": 0.012727272727272587, "page": 5}, {"left": 0.08813725490196078, "top": 0.7864166666666667, "width": 0.39420915032679743, "height": 0.01272727272727281, "page": 5}, {"left": 0.08813725490196078, "top": 0.800963383838384, "width": 0.325843137254902, "height": 0.012727272727272587, "page": 5}], "section": "Results", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1504"}, {"text": "They felt this technology was cool and could even be beneficial for people who are sighted, for example, in dark environments.", "label": "Conclusion", "bboxes": [{"left": 0.7647124183006536, "top": 0.22279040404040404, "width": 0.14708660130718954, "height": 0.012727272727272754, "page": 6}, {"left": 0.517625816993464, "top": 0.23733712121212122, "width": 0.3942009803921569, "height": 0.012727272727272726, "page": 6}, {"left": 0.517609477124183, "top": 0.25188510101010103, "width": 0.30480392156862746, "height": 0.012727272727272754, "page": 6}], "section": "Results", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1505"}, {"text": "P11 regarded the prototype as an identity tool (similar with the identity cane), which could indicate her disability to others, so that other people wont bump into her on stairs.", "label": "Conclusion", "bboxes": [{"left": 0.8257581699346405, "top": 0.25188510101010103, "width": 0.0860375816993465, "height": 0.012727272727272754, "page": 6}, {"left": 0.517593137254902, "top": 0.2664318181818182, "width": 0.3941470588235294, "height": 0.012727272727272698, "page": 6}, {"left": 0.517593137254902, "top": 0.280979797979798, "width": 0.3941568627450981, "height": 0.012727272727272754, "page": 6}, {"left": 0.517593137254902, "top": 0.2955265151515151, "width": 0.28526960784313726, "height": 0.012727272727272754, "page": 6}], "section": "Results", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1506"}, {"text": "P6 and P8 also said that the visualizations reduced their visual effort, so that they could look at the surroundings ( e.g ., other people and obstacles on the stairs), which also helped them feel safe.", "label": "Conclusion", "bboxes": [{"left": 0.8171601307189542, "top": 0.1424810606060606, "width": 0.0946552287581699, "height": 0.012727272727272726, "page": 6}, {"left": 0.517625816993464, "top": 0.15702904040404042, "width": 0.3941437908496733, "height": 0.012727272727272726, "page": 6}, {"left": 0.517625816993464, "top": 0.17157575757575758, "width": 0.394140522875817, "height": 0.012727272727272698, "page": 6}, {"left": 0.517625816993464, "top": 0.18612247474747473, "width": 0.37436601307189543, "height": 0.012727272727272754, "page": 6}], "section": "Results", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1507"}, {"text": "Based on our observations of the walking tasks, some participants ( e.g ., P9, P4) looked down less when using our design since they could use their lower peripheral vision to notice the highlights.", "label": "Conclusion", "bboxes": [{"left": 0.21390849673202617, "top": 0.5066843434343434, "width": 0.26850326797385615, "height": 0.01272727272727281, "page": 6}, {"left": 0.08823692810457516, "top": 0.5212323232323232, "width": 0.3941781045751634, "height": 0.01272727272727281, "page": 6}, {"left": 0.08822058823529412, "top": 0.5357790404040403, "width": 0.39414542483660137, "height": 0.012727272727272698, "page": 6}, {"left": 0.08822058823529412, "top": 0.5503270202020202, "width": 0.20637745098039217, "height": 0.01272727272727281, "page": 6}], "section": "Results", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1508"}, {"text": "She felt it unnecessary since the she could walk on stairs knowing the position of the first stair and the number of stairs (she counted stairs).", "label": "Conclusion", "bboxes": [{"left": 0.08821895424836601, "top": 0.16457323232323234, "width": 0.3941764705882353, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.1791199494949495, "width": 0.39419117647058827, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.19366666666666665, "width": 0.13746405228758168, "height": 0.012727272727272754, "page": 6}], "section": "Results", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1509"}, {"text": "s study, knowing when the stairs start and end can help PLV plan their steps, while the middle stairs are less important because most stairs are uniform [86].", "label": "Conclusion", "bboxes": [{"left": 0.25108006535947713, "top": 0.2709330808080808, "width": 0.23133006535947714, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.2854810606060606, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 7}, {"left": 0.08821895424836601, "top": 0.3000277777777778, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1510"}, {"text": "Thus, to better inform the user of their position on the stairs, we distinguish a users position on a set of stairs based on how close she is to a change in her step pattern.", "label": "Conclusion", "bboxes": [{"left": 0.08821895424836601, "top": 0.31457575757575756, "width": 0.39408823529411774, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.32912247474747475, "width": 0.3942924836601307, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.3436691919191919, "width": 0.31083986928104573, "height": 0.012727272727272754, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1511"}, {"text": "We provide feedback to indicate that a change is approaching, and then that the change is about to occur.", "label": "Conclusion", "bboxes": [{"left": 0.16447712418300653, "top": 0.3873118686868687, "width": 0.31792320261437906, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.40185858585858586, "width": 0.364468954248366, "height": 0.012727272727272698, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1512"}, {"text": "They conducted each task in three conditions: (1) walking on the stairs as they typically would (they could use a cane if desired, but none chose to use it), (2) walking on the stairs with HoloLens and no visualizations, and (3) walking on the stairs with HoloLens and their chosen designs.", "label": "Conclusion", "bboxes": [{"left": 0.5956274509803922, "top": 0.09917550505050504, "width": 0.31613398692810457, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175898692810458, "top": 0.11372222222222222, "width": 0.39418137254901964, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175735294117647, "top": 0.12827020202020203, "width": 0.39422712418300665, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175898692810458, "top": 0.14251136363636363, "width": 0.3941813725490195, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175898692810458, "top": 0.15705934343434344, "width": 0.3564738562091504, "height": 0.012727272727272726, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1513"}, {"text": "Some participants appreciated the tinted optics because they blocked environmental glare.", "label": "Conclusion", "bboxes": [{"left": 0.8432549019607843, "top": 0.7894179292929293, "width": 0.06851633986928107, "height": 0.012727272727272587, "page": 8}, {"left": 0.5176307189542484, "top": 0.8039646464646465, "width": 0.39417320261437905, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176307189542484, "top": 0.8185126262626262, "width": 0.1381454248366012, "height": 0.01272727272727281, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1514"}, {"text": "It is possible that the tint of the HoloLens made the environment too dark for him to see.", "label": "Conclusion", "bboxes": [{"left": 0.7069934640522876, "top": 0.8767007575757576, "width": 0.20495261437908496, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176143790849673, "top": 0.8909431818181818, "width": 0.3759052287581699, "height": 0.01272727272727281, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1515"}, {"text": "We chose HoloLens because of its FOV (~34 diagonal), binocular displays, and ability to be worn with eyeglasses.", "label": "Conclusion", "bboxes": [{"left": 0.11233169934640523, "top": 0.2579090909090909, "width": 0.370140522875817, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.2724570707070707, "width": 0.3941078431372549, "height": 0.012727272727272754, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1516"}, {"text": "Participants were asked to walk as quickly and safely as possible during the task.", "label": "Conclusion", "bboxes": [{"left": 0.5175735294117647, "top": 0.2664621212121212, "width": 0.3941094771241831, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175735294117647, "top": 0.281010101010101, "width": 0.13621405228758177, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1517"}, {"text": "On the other hand, P17 felt that Beep may not be distinguishable from environmental sounds: The world around you is so full of noise. I mean, if I use this in the city you have cars honking and everything like that, Im not sure if I would react in time.", "label": "Conclusion", "bboxes": [{"left": 0.5176895424836602, "top": 0.7322601010101011, "width": 0.39426470588235285, "height": 0.012727272727272587, "page": 9}, {"left": 0.5177058823529412, "top": 0.7468068181818182, "width": 0.3941405228758168, "height": 0.01272727272727281, "page": 9}, {"left": 0.5177058823529412, "top": 0.761354797979798, "width": 0.3942401960784313, "height": 0.012727272727272587, "page": 9}, {"left": 0.5177058823529412, "top": 0.7759015151515152, "width": 0.3942075163398693, "height": 0.01272727272727281, "page": 9}, {"left": 0.5177058823529412, "top": 0.7904494949494949, "width": 0.09646895424836599, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1518"}, {"text": "Nevertheless, some participants ( e.g ., P6, P10, P13) felt this design was helpful because it provided a preview for future steps, especially when they looked downstairs from the top landing.", "label": "Conclusion", "bboxes": [{"left": 0.0882516339869281, "top": 0.29609595959595963, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.3106439393939394, "width": 0.39416993464052286, "height": 0.012727272727272754, "page": 9}, {"left": 0.0882516339869281, "top": 0.32519065656565654, "width": 0.3941650326797385, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882516339869281, "top": 0.3394330808080808, "width": 0.05317647058823531, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1519"}, {"text": "Interestingly, P10 mentioned that he could combine his own vision (that is not covered by the HoloLens) with the Edge Highlights.", "label": "Conclusion", "bboxes": [{"left": 0.1453153594771242, "top": 0.3394330808080808, "width": 0.33706045751633984, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882516339869281, "top": 0.35397979797979795, "width": 0.39419117647058827, "height": 0.012727272727272754, "page": 9}, {"left": 0.0882516339869281, "top": 0.3685277777777778, "width": 0.11030392156862744, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1520"}, {"text": "He didnt feel the need to look down all the time because he has good peripheral vision to see the stairs, and he could use the Edge Highlights on the HoloLens to prepare for future steps and verify the last step.", "label": "Conclusion", "bboxes": [{"left": 0.20212908496732027, "top": 0.3685277777777778, "width": 0.28036601307189535, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882516339869281, "top": 0.3830744949494949, "width": 0.394140522875817, "height": 0.012727272727272754, "page": 9}, {"left": 0.0882516339869281, "top": 0.3976212121212121, "width": 0.3941846405228758, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882516339869281, "top": 0.4121691919191919, "width": 0.30803267973856213, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1521"}, {"text": "This one is my kind of style. Its subtle, simple, and I can keep my head wherever I want at the same time. And [the color of the Glow] changes exactly when I need to step. It warns me when Im about to take my last step Its very discreet but not distracting. So Ill still be able to see people, and things around me without falling over steps. If my real glasses could do this, it would be good.", "label": "Conclusion", "bboxes": [{"left": 0.0882843137254902, "top": 0.5724823232323233, "width": 0.3942009803921569, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882843137254902, "top": 0.5870290404040405, "width": 0.39417973856209143, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882843137254902, "top": 0.6015757575757575, "width": 0.39416013071895417, "height": 0.01272727272727281, "page": 9}, {"left": 0.0882843137254902, "top": 0.6161237373737374, "width": 0.3942091503267974, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.6306704545454546, "width": 0.39407516339869286, "height": 0.01272727272727281, "page": 9}, {"left": 0.08826797385620916, "top": 0.6452184343434343, "width": 0.3941748366013072, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.6597651515151515, "width": 0.26652777777777775, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1522"}, {"text": "For example, P9 and P15 adjusted Path so that it was in the center of their vision and that they could use it in a similar fashion to a GPS guide.", "label": "Conclusion", "bboxes": [{"left": 0.7446535947712418, "top": 0.37556565656565655, "width": 0.1671601307189542, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.39011237373737373, "width": 0.3941454248366014, "height": 0.012727272727272698, "page": 9}, {"left": 0.517656862745098, "top": 0.4046603535353535, "width": 0.3651830065359477, "height": 0.012727272727272754, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1523"}, {"text": "However, two participants (P6, P14) had difficulty using Glow because of difficulty distinguishing colors.", "label": "Conclusion", "bboxes": [{"left": 0.08826797385620916, "top": 0.681885101010101, "width": 0.3941911764705882, "height": 0.01272727272727281, "page": 9}, {"left": 0.08826797385620916, "top": 0.6961275252525252, "width": 0.31449019607843137, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1524"}, {"text": "Most participants found it difficult to use the Edge Highlights because of the limited vertical FOV.", "label": "Conclusion", "bboxes": [{"left": 0.3063888888888889, "top": 0.17214520202020203, "width": 0.17607843137254903, "height": 0.012727272727272754, "page": 9}, {"left": 0.0882516339869281, "top": 0.1866931818181818, "width": 0.39419117647058827, "height": 0.012727272727272754, "page": 9}, {"left": 0.0882516339869281, "top": 0.201239898989899, "width": 0.08958333333333333, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1525"}, {"text": "Half of the participants indicated that Path could be helpful.", "label": "Conclusion", "bboxes": [{"left": 0.5788937908496732, "top": 0.1358472222222222, "width": 0.3329362745098039, "height": 0.012727272727272754, "page": 9}, {"left": 0.517656862745098, "top": 0.15008964646464645, "width": 0.05040032679738571, "height": 0.012727272727272726, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1526"}, {"text": "P13 described his impression, This is perfect because if Im coming to the stairs, looking at the stairs and I wont have to look down, I immediately know where [the stair] begins and where it ends, as soon as my head turns to the [Path].", "label": "Conclusion", "bboxes": [{"left": 0.5502598039215687, "top": 0.17918308080808082, "width": 0.3615049019607842, "height": 0.012727272727272698, "page": 9}, {"left": 0.517656862745098, "top": 0.1937310606060606, "width": 0.3941405228758169, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.20827777777777776, "width": 0.39422222222222214, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2228257575757576, "width": 0.40147058823529413, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1527"}, {"text": "P8 also felt Path could guide him along the stairs: Its like a reinforced railing but its also like a guide [showing] where Im stepping. Its like a good reference. I kinda like to have the guide.", "label": "Conclusion", "bboxes": [{"left": 0.517640522875817, "top": 0.23737247474747475, "width": 0.3941535947712419, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.25192045454545453, "width": 0.3942565359477125, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2664671717171717, "width": 0.394295751633987, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2810151515151515, "width": 0.07294771241830067, "height": 0.012727272727272754, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1528"}, {"text": "P13 combined all four designs because he used each design for different purposes: Path as a reminder to look for a railing, Edge Highlights to get an overview of the stairs, and Glow when walking on stairs and scanning the environment for people or obstacles.", "label": "Conclusion", "bboxes": [{"left": 0.16658660130718952, "top": 0.5351931818181818, "width": 0.3158725490196078, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.5497411616161616, "width": 0.3942107843137256, "height": 0.01272727272727281, "page": 10}, {"left": 0.08831699346405228, "top": 0.5642878787878788, "width": 0.3941584967320261, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.578834595959596, "width": 0.3942369281045751, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.5933825757575758, "width": 0.1892238562091503, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1529"}, {"text": "However, the real world could be much more complicated, raising all kinds of challenges.", "label": "Conclusion", "bboxes": [{"left": 0.8226421568627451, "top": 0.6067664141414142, "width": 0.08915522875816984, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.6213131313131313, "width": 0.39417320261437905, "height": 0.01272727272727281, "page": 11}, {"left": 0.517640522875817, "top": 0.6355555555555555, "width": 0.09124673202614386, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1530"}, {"text": "For example, AR visualizations could be less visible outdoors, crowded stairs could diminish the accuracy of the stair recognition because the stair edges are blocked, and the projected highlights may also disturb other people.", "label": "Conclusion", "bboxes": [{"left": 0.6134166666666667, "top": 0.6355555555555555, "width": 0.29842973856209143, "height": 0.01272727272727281, "page": 11}, {"left": 0.517640522875817, "top": 0.6501022727272727, "width": 0.3941830065359476, "height": 0.01272727272727281, "page": 11}, {"left": 0.5176241830065359, "top": 0.6646502525252526, "width": 0.394075163398693, "height": 0.012727272727272587, "page": 11}, {"left": 0.5176241830065359, "top": 0.6791969696969696, "width": 0.394140522875817, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1531"}, {"text": "First, the HoloLenss weight strongly diminished PLVs experiences, which may have influenced our results.", "label": "Conclusion", "bboxes": [{"left": 0.8234003267973856, "top": 0.8031477272727272, "width": 0.08831535947712421, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175408496732027, "top": 0.8173901515151516, "width": 0.3941584967320261, "height": 0.012727272727272587, "page": 11}, {"left": 0.5175408496732027, "top": 0.8319368686868687, "width": 0.2527581699346404, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "1532"}, {"text": "Second, because of the extreme head pitch required to view the closest stairs caused by the small vertical FOV of", "label": "Conclusion", "bboxes": [{"left": 0.572388888888889, "top": 0.8610315656565657, "width": 0.3393562091503268, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175571895424836, "top": 0.8755782828282828, "width": 0.3941584967320263, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1533"}, {"text": "Our studies demonstrate the effectiveness of our designs with both projection-based AR and smartglasses.", "label": "Conclusion", "bboxes": [{"left": 0.4569624183006536, "top": 0.35063005050505053, "width": 0.025460784313725504, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.3651767676767677, "width": 0.39417810457516345, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.3797247474747475, "width": 0.29258333333333336, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "1534"}, {"text": "The highlights on projection AR were intuitive to perceive because they directly enhance the stair edges that participants were looking for.", "label": "Conclusion", "bboxes": [{"left": 0.14469607843137255, "top": 0.7288459595959595, "width": 0.33770751633986923, "height": 0.01272727272727281, "page": 11}, {"left": 0.08823529411764706, "top": 0.7433939393939394, "width": 0.3941535947712418, "height": 0.012727272727272587, "page": 11}, {"left": 0.08823529411764706, "top": 0.7579406565656566, "width": 0.205266339869281, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1535"}, {"text": "This new stair perception method increased participants cognitive load, because they had to associate the design with the physical stairs, making them more cautious.", "label": "Conclusion", "bboxes": [{"left": 0.3912450980392157, "top": 0.8161300505050505, "width": 0.09116503267973858, "height": 0.01272727272727281, "page": 11}, {"left": 0.08826797385620916, "top": 0.8306767676767676, "width": 0.39414215686274506, "height": 0.01272727272727281, "page": 11}, {"left": 0.08826797385620916, "top": 0.844919191919192, "width": 0.39414215686274506, "height": 0.012727272727272587, "page": 11}, {"left": 0.08826797385620916, "top": 0.8594659090909091, "width": 0.22991503267973856, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1536"}, {"text": "This could be one major reason why PLVs walking time did not improve when using smartglasses.", "label": "Conclusion", "bboxes": [{"left": 0.32246405228758174, "top": 0.8594659090909091, "width": 0.1600130718954248, "height": 0.01272727272727281, "page": 11}, {"left": 0.0882843137254902, "top": 0.874013888888889, "width": 0.3942009803921569, "height": 0.012727272727272587, "page": 11}, {"left": 0.0882843137254902, "top": 0.888560606060606, "width": 0.08676960784313723, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1537"}, {"text": "The implementation of such a system could be challenging.", "label": "Conclusion", "bboxes": [{"left": 0.8386683006535948, "top": 0.3812891414141414, "width": 0.07317810457516327, "height": 0.012727272727272698, "page": 11}, {"left": 0.5176895424836602, "top": 0.39583712121212117, "width": 0.3220310457516339, "height": 0.012727272727272754, "page": 11}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1538"}, {"text": "For such a dangerous task as stair navigation, the navigation system should be highly accurate and fast since a small error could lead to severe consequences ( e.g ., a slight shift of the edge highlight could make the user fall).", "label": "Conclusion", "bboxes": [{"left": 0.8437745098039215, "top": 0.39583712121212117, "width": 0.068076797385621, "height": 0.012727272727272754, "page": 11}, {"left": 0.5176895424836602, "top": 0.4103838383838384, "width": 0.39422385620915035, "height": 0.012727272727272698, "page": 11}, {"left": 0.5176895424836602, "top": 0.4249318181818182, "width": 0.39426143790849677, "height": 0.012727272727272698, "page": 11}, {"left": 0.5176895424836602, "top": 0.43947853535353537, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 11}, {"left": 0.517673202614379, "top": 0.45402651515151515, "width": 0.22816666666666674, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1539"}, {"text": "While many stair detection methods have been presented in prior research [20, 58], algorithms that locate the exact position of each stair with high speed and accuracy should be investigated and tested to support the stair visualization systems we designed for PLV.", "label": "Conclusion", "bboxes": [{"left": 0.6355196078431372, "top": 0.49736237373737374, "width": 0.2763807189542484, "height": 0.012727272727272754, "page": 11}, {"left": 0.517656862745098, "top": 0.511909090909091, "width": 0.39418954248366, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5264570707070707, "width": 0.3942516339869281, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5410037878787879, "width": 0.39413071895424834, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5555517676767677, "width": 0.28683496732026137, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "1540"}, {"text": "More data could be collected to quantify the head pitch angle to determine an effective vertical FOV that allows PLV to use the stair highlights with a comfortable head pose.", "label": "Conclusion", "bboxes": [{"left": 0.2544035947712418, "top": 0.09928282828282829, "width": 0.22802450980392153, "height": 0.012727272727272726, "page": 12}, {"left": 0.08823202614379085, "top": 0.11383080808080807, "width": 0.39415686274509804, "height": 0.012727272727272712, "page": 12}, {"left": 0.08823202614379085, "top": 0.12837752525252524, "width": 0.39417320261437916, "height": 0.012727272727272726, "page": 12}, {"left": 0.08823202614379085, "top": 0.1426199494949495, "width": 0.1514133986928105, "height": 0.012727272727272726, "page": 12}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1541"}, {"text": "Third, we asked participants to score their feeling of psychological security, but these results could be influenced by a novelty effect.", "label": "Conclusion", "bboxes": [{"left": 0.24340032679738563, "top": 0.1426199494949495, "width": 0.23893954248366014, "height": 0.012727272727272726, "page": 12}, {"left": 0.08823202614379085, "top": 0.15716666666666668, "width": 0.394202614379085, "height": 0.012727272727272754, "page": 12}, {"left": 0.08823202614379085, "top": 0.17171338383838386, "width": 0.23277450980392161, "height": 0.012727272727272698, "page": 12}], "section": "DISCUSSION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "1542"}, {"text": "Landing stages: thin red glow to indicate the flat surface.", "label": "Conclusion", "bboxes": [{"left": 0.517640522875817, "top": 0.13675, "width": 0.38679411764705884, "height": 0.012777777777777777, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1543"}, {"text": "Middle stairs: thin blue steps to indicate the middle stairs.", "label": "Conclusion", "bboxes": [{"left": 0.517640522875817, "top": 0.2409861111111111, "width": 0.39288888888888895, "height": 0.012777777777777805, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1544"}, {"text": "The user can thus observe the start and end of the stairs by looking at the turning points of the Path.", "label": "Conclusion", "bboxes": [{"left": 0.853968954248366, "top": 0.40827272727272723, "width": 0.05784477124183007, "height": 0.012727272727272754, "page": 7}, {"left": 0.517656862745098, "top": 0.4228207070707071, "width": 0.39426143790849677, "height": 0.012727272727272698, "page": 7}, {"left": 0.517656862745098, "top": 0.4373674242424242, "width": 0.1935947712418301, "height": 0.012727272727272754, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1545"}, {"text": "P6 even felt it was misleading to have a virtual railing (Path) in a different place than the real railing because it changed her perception of the width of the staircase: It suggests that there is a railing and then I feel I have a very narrow staircase (P6).", "label": "Conclusion", "bboxes": [{"left": 0.6844705882352942, "top": 0.49921085858585856, "width": 0.2274199346405228, "height": 0.012727272727272698, "page": 9}, {"left": 0.517673202614379, "top": 0.5137575757575757, "width": 0.3942042483660131, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176895424836602, "top": 0.5283055555555556, "width": 0.3941405228758168, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176895424836602, "top": 0.5428522727272728, "width": 0.3941013071895424, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176895424836602, "top": 0.5574002525252525, "width": 0.22959313725490182, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1546"}, {"text": "P6 thought it could reduce cognitive load and enable her to see the surroundings.", "label": "Conclusion", "bboxes": [{"left": 0.5439607843137255, "top": 0.5940669191919191, "width": 0.3679509803921569, "height": 0.01272727272727281, "page": 9}, {"left": 0.5177385620915033, "top": 0.6086148989898991, "width": 0.15678104575163387, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1547"}, {"text": "P14 also felt Beep could be a good compensation when the visualizations are not visible in bright environment.", "label": "Conclusion", "bboxes": [{"left": 0.6763267973856208, "top": 0.6810454545454546, "width": 0.23552777777777778, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176895424836602, "top": 0.6955921717171717, "width": 0.39416013071895417, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176895424836602, "top": 0.7101401515151515, "width": 0.12982026143790848, "height": 0.012727272727272587, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1548"}, {"text": "s study emphasized the need for tools that facilitate stair navigation for PLV.", "label": "Future Work", "bboxes": [{"left": 0.7781062091503268, "top": 0.2443068181818182, "width": 0.1337156862745098, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176241830065359, "top": 0.25885353535353534, "width": 0.3711977124183006, "height": 0.012727272727272754, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1549"}, {"text": "With a growing number of smartphones that have embedded depth sensors (e.g., iPhone XR, Samsung Galaxy S10) and projectors (e.g., Samsung Galaxy Beam [67]), smartphones may support projection-based AR with depth-sensing capabilities in the near future.", "label": "Future Work", "bboxes": [{"left": 0.8096862745098039, "top": 0.5124608585858587, "width": 0.10215196078431366, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.5267032828282828, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.54125, "width": 0.3941617647058824, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.5557979797979798, "width": 0.3941584967320261, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.570344696969697, "width": 0.3941584967320261, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.5848926767676768, "width": 0.043143790849673236, "height": 0.012727272727272698, "page": 2}], "section": "VISUALIZATIONS FOR PROJECTION-BASED AR", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1550"}, {"text": "Beyond these highlights, we sought ways to further emphasize the first and last stairs so that a user will notice them and perceive their exact location from a distance.", "label": "Future Work", "bboxes": [{"left": 0.08820261437908496, "top": 0.49307954545454546, "width": 0.394124183006536, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5076275252525253, "width": 0.3941993464052288, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5218686868686868, "width": 0.3023006535947713, "height": 0.012727272727272698, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1551"}, {"text": "Although we provided different visualizations (flash or movement) to further enhance the end highlights, most participants (seven out of 12) liked the original design.", "label": "Future Work", "bboxes": [{"left": 0.08811928104575163, "top": 0.8230833333333333, "width": 0.3942663398692811, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8376313131313131, "width": 0.3941421568627451, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8521780303030303, "width": 0.331998366013072, "height": 0.01272727272727281, "page": 5}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1552"}, {"text": "They present information only to the user and do not need to project onto a physical surface [88].", "label": "Future Work", "bboxes": [{"left": 0.6089428104575163, "top": 0.4030542929292929, "width": 0.3028790849673202, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.41760101010101014, "width": 0.343032679738562, "height": 0.012727272727272698, "page": 6}], "section": "VISUALIZATIONS FOR SMARTGLASSES", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1553"}, {"text": "Nevertheless, some participants ( e.g ., P6, P10, P13) felt this design was helpful because it provided a preview for future steps, especially when they looked downstairs from the top landing.", "label": "Future Work", "bboxes": [{"left": 0.0882516339869281, "top": 0.29609595959595963, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.3106439393939394, "width": 0.39416993464052286, "height": 0.012727272727272754, "page": 9}, {"left": 0.0882516339869281, "top": 0.32519065656565654, "width": 0.3941650326797385, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882516339869281, "top": 0.3394330808080808, "width": 0.05317647058823531, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1554"}, {"text": "He didnt feel the need to look down all the time because he has good peripheral vision to see the stairs, and he could use the Edge Highlights on the HoloLens to prepare for future steps and verify the last step.", "label": "Future Work", "bboxes": [{"left": 0.20212908496732027, "top": 0.3685277777777778, "width": 0.28036601307189535, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882516339869281, "top": 0.3830744949494949, "width": 0.394140522875817, "height": 0.012727272727272754, "page": 9}, {"left": 0.0882516339869281, "top": 0.3976212121212121, "width": 0.3941846405228758, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882516339869281, "top": 0.4121691919191919, "width": 0.30803267973856213, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1555"}, {"text": "This one is my kind of style. Its subtle, simple, and I can keep my head wherever I want at the same time. And [the color of the Glow] changes exactly when I need to step. It warns me when Im about to take my last step Its very discreet but not distracting. So Ill still be able to see people, and things around me without falling over steps. If my real glasses could do this, it would be good.", "label": "Future Work", "bboxes": [{"left": 0.0882843137254902, "top": 0.5724823232323233, "width": 0.3942009803921569, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882843137254902, "top": 0.5870290404040405, "width": 0.39417973856209143, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882843137254902, "top": 0.6015757575757575, "width": 0.39416013071895417, "height": 0.01272727272727281, "page": 9}, {"left": 0.0882843137254902, "top": 0.6161237373737374, "width": 0.3942091503267974, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.6306704545454546, "width": 0.39407516339869286, "height": 0.01272727272727281, "page": 9}, {"left": 0.08826797385620916, "top": 0.6452184343434343, "width": 0.3941748366013072, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.6597651515151515, "width": 0.26652777777777775, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1556"}, {"text": "that Im at the last step...but I didnt really see that [blue glow in the middle], I need to be reassured that Im still going down the stairs.", "label": "Future Work", "bboxes": [{"left": 0.517673202614379, "top": 0.07008585858585858, "width": 0.3942401960784314, "height": 0.012727272727272712, "page": 9}, {"left": 0.517673202614379, "top": 0.08463257575757575, "width": 0.39417973856209154, "height": 0.012727272727272726, "page": 9}, {"left": 0.5176895424836602, "top": 0.09918055555555556, "width": 0.11300490196078428, "height": 0.012727272727272726, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1557"}, {"text": "In future work, we will consider these real-world challenges when developing AR stair navigation systems.", "label": "Future Work", "bboxes": [{"left": 0.5176078431372549, "top": 0.6937436868686869, "width": 0.39417320261437905, "height": 0.012727272727272587, "page": 11}, {"left": 0.5175898692810458, "top": 0.7082916666666667, "width": 0.30403431372549017, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1558"}, {"text": "For example, besides recognizing stairs with computer vision, we will consider instrumenting the environment ( e.g ., using RFID) to foster accurate and fast stair recognition in a complex environment.", "label": "Future Work", "bboxes": [{"left": 0.8256568627450981, "top": 0.7082916666666667, "width": 0.08623039215686268, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175898692810458, "top": 0.722838383838384, "width": 0.39420751633986917, "height": 0.012727272727272587, "page": 11}, {"left": 0.5175898692810458, "top": 0.7373863636363637, "width": 0.3941519607843137, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175571895424836, "top": 0.7519330808080807, "width": 0.39417483660130725, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175571895424836, "top": 0.7664810606060606, "width": 0.05858333333333332, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1559"}, {"text": "We will also add face detection to avoid projecting in bystanders faces.", "label": "Future Work", "bboxes": [{"left": 0.5800147058823529, "top": 0.7664810606060606, "width": 0.3317500000000001, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175571895424836, "top": 0.7810277777777778, "width": 0.13383986928104574, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1560"}, {"text": "This can potentially set a foundation for future visualization design for stair navigation and more general navigation systems.", "label": "Future Work", "bboxes": [{"left": 0.3805882352941176, "top": 0.6197474747474747, "width": 0.10178758169934643, "height": 0.01272727272727281, "page": 11}, {"left": 0.08821895424836601, "top": 0.6342954545454546, "width": 0.39409803921568626, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.6488421717171717, "width": 0.3194640522875817, "height": 0.012727272727272587, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1561"}, {"text": "Future studies should refine and evaluate the design on more lightweight smartglasses.", "label": "Future Work", "bboxes": [{"left": 0.7735277777777778, "top": 0.8319368686868687, "width": 0.13822875816993463, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175571895424836, "top": 0.8464848484848485, "width": 0.39413562091503274, "height": 0.012727272727272587, "page": 11}, {"left": 0.5175571895424836, "top": 0.8610315656565657, "width": 0.0504003267973856, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1562"}, {"text": "that, he looked much further down to the stairs when not using our visualizations, especially at the beginning and the end of the stairs ( e.g ., preparation area, alert area).", "label": "Future Work", "bboxes": [{"left": 0.08823529411764706, "top": 0.2721426767676768, "width": 0.39410784313725494, "height": 0.012727272727272698, "page": 11}, {"left": 0.08823529411764706, "top": 0.286385101010101, "width": 0.3941846405228758, "height": 0.012727272727272698, "page": 11}, {"left": 0.08823529411764706, "top": 0.3009318181818182, "width": 0.30114869281045753, "height": 0.012727272727272754, "page": 11}], "section": "Evaluation of Smartglasses Visualizations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1563"}, {"text": "Future research should consider more objective measurements ( e.g ., biometrics) to evaluate psychological security.", "label": "Future Work", "bboxes": [{"left": 0.32788562091503265, "top": 0.17171338383838386, "width": 0.15454738562091513, "height": 0.012727272727272698, "page": 12}, {"left": 0.08823202614379085, "top": 0.18626136363636364, "width": 0.3941764705882353, "height": 0.012727272727272698, "page": 12}, {"left": 0.08823202614379085, "top": 0.2008080808080808, "width": 0.20921405228758172, "height": 0.012727272727272754, "page": 12}], "section": "DISCUSSION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1564"}, {"text": "the user can hold their head at a comfortable angle and does not need to look down to see the glow.", "label": "Future Work", "bboxes": [{"left": 0.517640522875817, "top": 0.0700719696969697, "width": 0.39422875816993463, "height": 0.012727272727272726, "page": 7}, {"left": 0.517640522875817, "top": 0.08461868686868687, "width": 0.25990522875817, "height": 0.012727272727272726, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1565"}, {"text": "Our research is the first to explore AR visualizations for people with low vision in the context of stair navigation.", "label": "Objective", "bboxes": [{"left": 0.08823529411764706, "top": 0.33608207070707075, "width": 0.3942238562091503, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.35063005050505053, "width": 0.3627483660130719, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": 0.5734177231788635, "is_author_statement": true, "is_in_expected_section": false, "id": "1566"}, {"text": "In contrast, our work addresses this gap by designing AR visualizations to assist PLV in navigating stairs.", "label": "Objective", "bboxes": [{"left": 0.2446111111111111, "top": 0.1357979797979798, "width": 0.23786437908496727, "height": 0.012727272727272726, "page": 2}, {"left": 0.0883218954248366, "top": 0.15004040404040403, "width": 0.39417320261437905, "height": 0.012727272727272754, "page": 2}, {"left": 0.08833823529411765, "top": 0.1645871212121212, "width": 0.08359477124183005, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for Blind People", "prob": 0.46887704730033875, "is_author_statement": true, "is_in_expected_section": false, "id": "1567"}, {"text": "[58] proposed a method that uses an RGB-D camera to detect stairs.", "label": "Method", "bboxes": [{"left": 0.6922843137254902, "top": 0.8088421717171717, "width": 0.2193725490196078, "height": 0.01272727272727281, "page": 1}, {"left": 0.517531045751634, "top": 0.8233901515151515, "width": 0.22386928104575166, "height": 0.01272727272727281, "page": 1}], "section": "Safe Navigation for Blind People", "prob": 0.6891173720359802, "is_author_statement": false, "is_in_expected_section": true, "id": "1568"}, {"text": "While many stair detection methods have been presented in prior research [20, 58], algorithms that locate the exact position of each stair with high speed and accuracy should be investigated and tested to support the stair visualization systems we designed for PLV.", "label": "Method", "bboxes": [{"left": 0.6355196078431372, "top": 0.49736237373737374, "width": 0.2763807189542484, "height": 0.012727272727272754, "page": 11}, {"left": 0.517656862745098, "top": 0.511909090909091, "width": 0.39418954248366, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5264570707070707, "width": 0.3942516339869281, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5410037878787879, "width": 0.39413071895424834, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5555517676767677, "width": 0.28683496732026137, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": 0.5722301006317139, "is_author_statement": true, "is_in_expected_section": false, "id": "1569"}, {"text": "We added virtual pillars to connect the Path to each stair to help users associate the visualization with the physical stairs.", "label": "Method", "bboxes": [{"left": 0.8890849673202614, "top": 0.4664621212121212, "width": 0.022735294117647076, "height": 0.012727272727272698, "page": 7}, {"left": 0.517640522875817, "top": 0.481010101010101, "width": 0.39421568627451, "height": 0.012727272727272754, "page": 7}, {"left": 0.517640522875817, "top": 0.4955568181818182, "width": 0.3666683006535948, "height": 0.012727272727272698, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.537880539894104, "is_author_statement": true, "is_in_expected_section": true, "id": "1570"}, {"text": "each phase to create a preferred combination.", "label": "Method", "bboxes": [{"left": 0.517656862745098, "top": 0.41733333333333333, "width": 0.31061764705882355, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.48818910121917725, "is_author_statement": false, "is_in_expected_section": false, "id": "1571"}, {"text": "To reduce order effects, we used a simultaneous within-subjects design, switching the task condition after each walking up and down task.", "label": "Method", "bboxes": [{"left": 0.5177222222222222, "top": 0.6725012626262626, "width": 0.3941405228758169, "height": 0.01272727272727281, "page": 4}, {"left": 0.5177058823529412, "top": 0.6870492424242425, "width": 0.3941813725490195, "height": 0.012727272727272587, "page": 4}, {"left": 0.5177058823529412, "top": 0.7015959595959596, "width": 0.1301013071895425, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.4812356233596802, "is_author_statement": true, "is_in_expected_section": false, "id": "1572"}, {"text": "Our designs considered the different characteristics of the two platforms: (1) For projection, which can augment a large physical space, we designed visual highlights with different patterns that are directly projected onto the stairs to enhance their visibility (Figure 1a).", "label": "Method", "bboxes": [{"left": 0.40116666666666667, "top": 0.2082550505050505, "width": 0.08134150326797385, "height": 0.012727272727272754, "page": 1}, {"left": 0.08824183006535948, "top": 0.2228030303030303, "width": 0.3941421568627451, "height": 0.012727272727272698, "page": 1}, {"left": 0.08822549019607843, "top": 0.23734974747474746, "width": 0.39424999999999993, "height": 0.012727272727272754, "page": 1}, {"left": 0.08824183006535948, "top": 0.25189646464646465, "width": 0.3941781045751634, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.26644444444444443, "width": 0.39420751633986933, "height": 0.012727272727272754, "page": 1}, {"left": 0.08822549019607843, "top": 0.2809911616161616, "width": 0.07741176470588236, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION", "prob": 0.4805675148963928, "is_author_statement": true, "is_in_expected_section": true, "id": "1573"}, {"text": "To validate counterbalancing, we added another between-subject factor, Order (two levels: WithWithout, WithoutWith), into our model.", "label": "Method", "bboxes": [{"left": 0.34990359477124183, "top": 0.24183964646464648, "width": 0.1324722222222222, "height": 0.012727272727272726, "page": 5}, {"left": 0.08821895424836601, "top": 0.25638762626262623, "width": 0.3941666666666667, "height": 0.012727272727272754, "page": 5}, {"left": 0.08820261437908496, "top": 0.27093434343434347, "width": 0.3941307189542484, "height": 0.012727272727272698, "page": 5}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.4691787362098694, "is_author_statement": true, "is_in_expected_section": false, "id": "1574"}, {"text": "(2) Path visualization (Figure 7eg): Inspired by the railings, which PLV used as a visual cue to see where the stairs start and end [86], we designed this visualization to show the trend of the stairs.", "label": "Method", "bboxes": [{"left": 0.517640522875817, "top": 0.2631060606060606, "width": 0.39416666666666667, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176241830065359, "top": 0.2776527777777778, "width": 0.3943480392156862, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176241830065359, "top": 0.2922007575757576, "width": 0.394235294117647, "height": 0.012727272727272698, "page": 7}, {"left": 0.517640522875817, "top": 0.3067474747474747, "width": 0.08284640522875819, "height": 0.012727272727272754, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.46090149879455566, "is_author_statement": true, "is_in_expected_section": true, "id": "1575"}, {"text": "We designed visualizations on two AR platforms that can generate immersive virtual content in the physical environment: projection-based AR and smartglasses.", "label": "Method", "bboxes": [{"left": 0.08827450980392157, "top": 0.17916035353535356, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 1}, {"left": 0.08827450980392157, "top": 0.19370833333333334, "width": 0.39412581699346405, "height": 0.012727272727272754, "page": 1}, {"left": 0.08827450980392157, "top": 0.2082550505050505, "width": 0.3062107843137255, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION", "prob": 0.444803923368454, "is_author_statement": true, "is_in_expected_section": true, "id": "1576"}, {"text": "To avoid order effects, we randomized the order of the design alternatives.", "label": "Method", "bboxes": [{"left": 0.3328888888888889, "top": 0.8321780303030304, "width": 0.1494869281045752, "height": 0.012727272727272587, "page": 4}, {"left": 0.08826797385620916, "top": 0.8467260101010101, "width": 0.3348643790849673, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.442716121673584, "is_author_statement": true, "is_in_expected_section": false, "id": "1577"}, {"text": "We designed two middle highlights to support the user in a minimally obtrusive way.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.2945669191919192, "width": 0.39420751633986917, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.30911489898989897, "width": 0.16747385620915034, "height": 0.012727272727272754, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": 0.4378354251384735, "is_author_statement": true, "is_in_expected_section": true, "id": "1578"}, {"text": "To reduce the effect of order on the results, we used a simultaneous within-subjects design by switching the task condition after each round of walking up and down.", "label": "Method", "bboxes": [{"left": 0.5175735294117647, "top": 0.30313005050505054, "width": 0.39399346405228763, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175735294117647, "top": 0.31767676767676767, "width": 0.3942075163398693, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175571895424836, "top": 0.3319191919191919, "width": 0.2976928104575164, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.4353746771812439, "is_author_statement": true, "is_in_expected_section": false, "id": "1579"}, {"text": "the HoloLens, we designed visualizations in the users central vision instead of adding highlights to the stairs in our smartglasses prototype.", "label": "Method", "bboxes": [{"left": 0.08823202614379085, "top": 0.07018813131313131, "width": 0.39417320261437916, "height": 0.012727272727272726, "page": 12}, {"left": 0.0882483660130719, "top": 0.08473611111111112, "width": 0.39414052287581697, "height": 0.012727272727272726, "page": 12}, {"left": 0.0882483660130719, "top": 0.09928282828282829, "width": 0.15778758169934642, "height": 0.012727272727272726, "page": 12}], "section": "DISCUSSION", "prob": 0.4274941384792328, "is_author_statement": true, "is_in_expected_section": false, "id": "1580"}, {"text": "We design two visualizations and one sonification.", "label": "Method", "bboxes": [{"left": 0.3571503267973856, "top": 0.6706717171717173, "width": 0.12527614379084967, "height": 0.012727272727272587, "page": 7}, {"left": 0.08821895424836601, "top": 0.6852184343434343, "width": 0.21148202614379086, "height": 0.01272727272727281, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.42020079493522644, "is_author_statement": true, "is_in_expected_section": true, "id": "1581"}, {"text": "We then gave the HoloLens to the participant and explained how to use it.", "label": "Method", "bboxes": [{"left": 0.33354901960784317, "top": 0.5779368686868687, "width": 0.14881045751633987, "height": 0.012727272727272698, "page": 8}, {"left": 0.08818627450980392, "top": 0.5924848484848485, "width": 0.3545588235294117, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.4201050102710724, "is_author_statement": true, "is_in_expected_section": false, "id": "1582"}, {"text": "We created all visualizations with PowerPoint.", "label": "Method", "bboxes": [{"left": 0.6195343137254902, "top": 0.7879128787878789, "width": 0.2922107843137256, "height": 0.012727272727272587, "page": 3}, {"left": 0.5176143790849673, "top": 0.8024608585858586, "width": 0.03861437908496734, "height": 0.01272727272727281, "page": 3}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.41709038615226746, "is_author_statement": true, "is_in_expected_section": false, "id": "1583"}, {"text": "They conducted each task in three conditions: (1) walking on the stairs as they typically would (they could use a cane if desired, but none chose to use it), (2) walking on the stairs with HoloLens and no visualizations, and (3) walking on the stairs with HoloLens and their chosen designs.", "label": "Method", "bboxes": [{"left": 0.5956274509803922, "top": 0.09917550505050504, "width": 0.31613398692810457, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175898692810458, "top": 0.11372222222222222, "width": 0.39418137254901964, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175735294117647, "top": 0.12827020202020203, "width": 0.39422712418300665, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175898692810458, "top": 0.14251136363636363, "width": 0.3941813725490195, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175898692810458, "top": 0.15705934343434344, "width": 0.3564738562091504, "height": 0.012727272727272726, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.4000074863433838, "is_author_statement": false, "is_in_expected_section": false, "id": "1584"}, {"text": "To minimize the confounding effect of general computer vision accuracy, we marked the position of the stairs with two Vuforia image targets [37] (on the side walls at the top and bottom landing of the stairs) that can be recognized by HoloLens.", "label": "Method", "bboxes": [{"left": 0.08821895424836601, "top": 0.3815555555555556, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 8}, {"left": 0.08821895424836601, "top": 0.3961022727272728, "width": 0.39419444444444446, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.41065025252525256, "width": 0.39425326797385624, "height": 0.012727272727272698, "page": 8}, {"left": 0.08821895424836601, "top": 0.4251969696969697, "width": 0.39415686274509804, "height": 0.012727272727272754, "page": 8}, {"left": 0.08821895424836601, "top": 0.43974368686868687, "width": 0.048620915032679735, "height": 0.012727272727272754, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.39197927713394165, "is_author_statement": true, "is_in_expected_section": false, "id": "1585"}, {"text": "Inspired by the contrast stripes that many PLV used to distinguish stair edges [79], we project highlights on the stair edges to increase their visibility.", "label": "Method", "bboxes": [{"left": 0.5176307189542484, "top": 0.7073030303030302, "width": 0.3943856209150326, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176307189542484, "top": 0.7218510101010102, "width": 0.39407516339869286, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176307189542484, "top": 0.7363977272727272, "width": 0.21194607843137248, "height": 0.01272727272727281, "page": 2}], "section": "Visualization (and Sonification) Design", "prob": 0.38738831877708435, "is_author_statement": true, "is_in_expected_section": true, "id": "1586"}, {"text": "Thus, to better inform the user of their position on the stairs, we distinguish a users position on a set of stairs based on how close she is to a change in her step pattern.", "label": "Method", "bboxes": [{"left": 0.08821895424836601, "top": 0.31457575757575756, "width": 0.39408823529411774, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.32912247474747475, "width": 0.3942924836601307, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.3436691919191919, "width": 0.31083986928104573, "height": 0.012727272727272754, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.38501691818237305, "is_author_statement": true, "is_in_expected_section": true, "id": "1587"}, {"text": "We will also add face detection to avoid projecting in bystanders faces.", "label": "Method", "bboxes": [{"left": 0.5800147058823529, "top": 0.7664810606060606, "width": 0.3317500000000001, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175571895424836, "top": 0.7810277777777778, "width": 0.13383986928104574, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": 0.3805350661277771, "is_author_statement": true, "is_in_expected_section": false, "id": "1588"}, {"text": "After the participant experienced all the design alternatives, we asked for their preferred combination.", "label": "Method", "bboxes": [{"left": 0.3708218954248366, "top": 0.8179608585858587, "width": 0.1115212418300654, "height": 0.012727272727272587, "page": 8}, {"left": 0.08816993464052288, "top": 0.8325075757575757, "width": 0.3941078431372549, "height": 0.01272727272727281, "page": 8}, {"left": 0.08816993464052288, "top": 0.8470555555555557, "width": 0.1837663398692811, "height": 0.012727272727272587, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.37536898255348206, "is_author_statement": true, "is_in_expected_section": false, "id": "1589"}, {"text": "We counterbalanced by randomizing the presentation order of the four designs.", "label": "Method", "bboxes": [{"left": 0.23737581699346405, "top": 0.8034128787878788, "width": 0.24497058823529416, "height": 0.01272727272727281, "page": 8}, {"left": 0.08816993464052288, "top": 0.8179608585858587, "width": 0.27782516339869284, "height": 0.012727272727272587, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.36848801374435425, "is_author_statement": true, "is_in_expected_section": false, "id": "1590"}, {"text": "For each design option, participants were encouraged to walk up and down the stairs.", "label": "Method", "bboxes": [{"left": 0.14981862745098037, "top": 0.8176313131313131, "width": 0.33262418300653596, "height": 0.01272727272727281, "page": 4}, {"left": 0.08826797385620916, "top": 0.8321780303030304, "width": 0.24033986928104578, "height": 0.012727272727272587, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.3538997769355774, "is_author_statement": false, "is_in_expected_section": false, "id": "1591"}, {"text": "With a stable line at the stair edge, we added another line moving towards the edge to generate movement (Figure 2c).", "label": "Method", "bboxes": [{"left": 0.08815359477124184, "top": 0.7046073232323232, "width": 0.3941781045751634, "height": 0.01272727272727281, "page": 3}, {"left": 0.08815359477124184, "top": 0.7191540404040405, "width": 0.394156862745098, "height": 0.012727272727272587, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": 0.34723731875419617, "is_author_statement": true, "is_in_expected_section": true, "id": "1592"}, {"text": "We counterbalanced the starting task (up/down) and condition (with/without the prototype).", "label": "Method", "bboxes": [{"left": 0.6555816993464052, "top": 0.7015959595959596, "width": 0.2562810457516339, "height": 0.01272727272727281, "page": 4}, {"left": 0.5177058823529412, "top": 0.7161439393939394, "width": 0.35490849673202607, "height": 0.012727272727272587, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.34594082832336426, "is_author_statement": true, "is_in_expected_section": false, "id": "1593"}, {"text": "We adjust the glow color and size to inform the user of their current stage on the stairs:", "label": "Method", "bboxes": [{"left": 0.7824869281045752, "top": 0.08461868686868687, "width": 0.1293790849673203, "height": 0.012727272727272726, "page": 7}, {"left": 0.517640522875817, "top": 0.09916666666666668, "width": 0.39417647058823535, "height": 0.012727272727272726, "page": 7}, {"left": 0.517640522875817, "top": 0.11371338383838385, "width": 0.03906699346405229, "height": 0.012727272727272712, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.31381797790527344, "is_author_statement": true, "is_in_expected_section": true, "id": "1594"}, {"text": "Paired Wilcoxon Signed-Rank tests showed that, while wearing HoloLens significantly reduced participants psychological security ( V =8, p =0.031), our visualizations significantly increased participant psychological security compared with not wearing HoloLens ( V =21, p =0.050).", "label": "Result", "bboxes": [{"left": 0.5896470588235294, "top": 0.6803560606060606, "width": 0.3221176470588235, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.6949027777777779, "width": 0.3942859477124183, "height": 0.012727272727272587, "page": 10}, {"left": 0.5176339869281046, "top": 0.7094507575757576, "width": 0.39417320261437894, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.7239974747474747, "width": 0.39426797385620904, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176339869281046, "top": 0.7385454545454545, "width": 0.3647075163398693, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.8254554867744446, "is_author_statement": true, "is_in_expected_section": true, "id": "1595"}, {"text": "The other participants preferred the combination, feeling that the sound and human voice complemented each other: the ding sound was an alert in noisy environment and the human voice reported more concrete information.", "label": "Result", "bboxes": [{"left": 0.302171568627451, "top": 0.6627714646464646, "width": 0.180187908496732, "height": 0.01272727272727281, "page": 5}, {"left": 0.08815359477124184, "top": 0.6773181818181818, "width": 0.3941372549019607, "height": 0.012727272727272587, "page": 5}, {"left": 0.08813725490196078, "top": 0.691864898989899, "width": 0.39417810457516345, "height": 0.01272727272727281, "page": 5}, {"left": 0.08813725490196078, "top": 0.7064128787878788, "width": 0.394202614379085, "height": 0.012727272727272587, "page": 5}, {"left": 0.08815359477124184, "top": 0.7209595959595959, "width": 0.1783267973856209, "height": 0.01272727272727281, "page": 5}], "section": "Results", "prob": 0.8155203461647034, "is_author_statement": false, "is_in_expected_section": true, "id": "1596"}, {"text": "We evaluated our visualizations on each platform with 12 PLV.", "label": "Result", "bboxes": [{"left": 0.08824183006535948, "top": 0.3464482323232323, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 1}, {"left": 0.08822549019607843, "top": 0.36099494949494954, "width": 0.03498366013071895, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": 0.7980832457542419, "is_author_statement": true, "is_in_expected_section": true, "id": "1597"}, {"text": "Our evaluations demonstrated the effectiveness of our visualizations and provide insights for the design of AR visualizations for PLV that support other tasks as well.", "label": "Result", "bboxes": [{"left": 0.40423039215686274, "top": 0.49918813131313133, "width": 0.07813725490196083, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.5137348484848485, "width": 0.39409477124183007, "height": 0.01272727272727281, "page": 1}, {"left": 0.08824183006535948, "top": 0.5282828282828282, "width": 0.3941290849673203, "height": 0.01272727272727281, "page": 1}, {"left": 0.08825816993464053, "top": 0.5428295454545454, "width": 0.2410147058823529, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": 0.7907850742340088, "is_author_statement": true, "is_in_expected_section": true, "id": "1598"}, {"text": "Unlike prior research, which showed that PLV had very different preferences for visual augmentations [84, 85], our study revealed that some common preferences among PLV cross different visual abilities for stair navigation.", "label": "Result", "bboxes": [{"left": 0.45677124183006534, "top": 0.5615593434343434, "width": 0.025457516339869368, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.5761060606060606, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.5906527777777778, "width": 0.39419117647058827, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.6052007575757576, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 11}, {"left": 0.08821895424836601, "top": 0.6197474747474747, "width": 0.28821895424836597, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": 0.7742573618888855, "is_author_statement": true, "is_in_expected_section": true, "id": "1599"}, {"text": "For descending stairs, participants navigation time was reduced by 6.42% when using their preferred visualizations ( mean =6.17s, SD =1.93s) than when not using them ( mean =6.59s, SD =2.03s).", "label": "Result", "bboxes": [{"left": 0.3342892156862745, "top": 0.24488131313131312, "width": 0.1481029411764706, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.2594292929292929, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.2739760101010101, "width": 0.39415849673202613, "height": 0.012727272727272698, "page": 6}, {"left": 0.0882516339869281, "top": 0.2885239898989899, "width": 0.3941421568627451, "height": 0.012727272727272754, "page": 6}, {"left": 0.0882516339869281, "top": 0.3030707070707071, "width": 0.07373039215686274, "height": 0.012727272727272698, "page": 6}], "section": "Results", "prob": 0.7529917359352112, "is_author_statement": false, "is_in_expected_section": true, "id": "1600"}, {"text": "We recruited 12 PLV (6 female, 6 male; mean age=53.9) with different low-vision conditions, as shown in Table 1 (P1  P12).", "label": "Result", "bboxes": [{"left": 0.6077075163398692, "top": 0.5906275252525253, "width": 0.30414869281045764, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.605175505050505, "width": 0.39418627450980404, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.6197222222222222, "width": 0.12820751633986938, "height": 0.012727272727272698, "page": 3}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.7482014298439026, "is_author_statement": true, "is_in_expected_section": true, "id": "1601"}, {"text": "Eleven out of 12 participants found the middle highlights useful.", "label": "Result", "bboxes": [{"left": 0.6618725490196078, "top": 0.5770530303030303, "width": 0.24977941176470597, "height": 0.012727272727272698, "page": 5}, {"left": 0.5174754901960784, "top": 0.5915997474747475, "width": 0.16327287581699346, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": 0.7393662333488464, "is_author_statement": false, "is_in_expected_section": true, "id": "1602"}, {"text": "Moreover, we tracked participants head orientation with HoloLens during the walking tasks, and found that some participants ( e.g ., P6, P9) head orientation changed when using our visualizations.", "label": "Result", "bboxes": [{"left": 0.589093137254902, "top": 0.8043068181818182, "width": 0.32282026143790854, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176503267973857, "top": 0.8188535353535354, "width": 0.3940392156862744, "height": 0.012727272727272587, "page": 10}, {"left": 0.5176503267973857, "top": 0.8334015151515152, "width": 0.39415686274509787, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176503267973857, "top": 0.8476426767676767, "width": 0.19614869281045755, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.7372692823410034, "is_author_statement": true, "is_in_expected_section": true, "id": "1603"}, {"text": "We evaluated the design on each platform with 12 participants, finding that both visualizations increased participants psychological security, making them feel safer and more confident when walking on stairs.", "label": "Result", "bboxes": [{"left": 0.15843300653594772, "top": 0.29396843434343434, "width": 0.3239901960784313, "height": 0.012727272727272698, "page": 12}, {"left": 0.08820261437908496, "top": 0.3085151515151515, "width": 0.3942107843137255, "height": 0.012727272727272754, "page": 12}, {"left": 0.08820261437908496, "top": 0.32306313131313136, "width": 0.3940751633986928, "height": 0.012727272727272754, "page": 12}, {"left": 0.08820261437908496, "top": 0.3376098484848485, "width": 0.2662287581699347, "height": 0.012727272727272698, "page": 12}], "section": "CONCLUSIONS", "prob": 0.7344146966934204, "is_author_statement": true, "is_in_expected_section": true, "id": "1604"}, {"text": "We found that theres no significant effect of Condition (HoloLens with visualizations vs. HoloLens without visualizations) on participants walking time for both ascending ( F (1,10) =0.466, p =0.511) and descending stairs ( F (1,10) =0.114, p =0.742).", "label": "Result", "bboxes": [{"left": 0.7176797385620916, "top": 0.2733623737373737, "width": 0.1942173202614379, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176437908496732, "top": 0.2879090909090909, "width": 0.3982598039215687, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176437908496732, "top": 0.3024558080808081, "width": 0.39418137254901964, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176437908496732, "top": 0.31668813131313134, "width": 0.39422875816993475, "height": 0.012737373737373658, "page": 10}, {"left": 0.5176486928104574, "top": 0.33123358585858587, "width": 0.2712467320261438, "height": 0.012729797979797952, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.7266348004341125, "is_author_statement": true, "is_in_expected_section": true, "id": "1605"}, {"text": "For ascending stairs, participants navigation time was reduced by 5.78% when using their preferred visualizations ( mean =5.84s, SD =1.59s) than when not using them ( mean =6.20s, SD =1.81s).", "label": "Result", "bboxes": [{"left": 0.08822712418300653, "top": 0.4121489898989899, "width": 0.3941748366013072, "height": 0.012727272727272698, "page": 6}, {"left": 0.08824509803921568, "top": 0.42669696969696974, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 6}, {"left": 0.08824509803921568, "top": 0.4412436868686868, "width": 0.39421895424836595, "height": 0.012727272727272754, "page": 6}, {"left": 0.08824509803921568, "top": 0.45579040404040405, "width": 0.16658660130718952, "height": 0.012727272727272698, "page": 6}], "section": "Results", "prob": 0.72309809923172, "is_author_statement": false, "is_in_expected_section": true, "id": "1606"}, {"text": "Interestingly, we found that participants had different preferences for Paths position in their visual field.", "label": "Result", "bboxes": [{"left": 0.517640522875817, "top": 0.3176818181818182, "width": 0.3941078431372549, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.33192424242424245, "width": 0.28760947712418305, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.7129396200180054, "is_author_statement": true, "is_in_expected_section": true, "id": "1607"}, {"text": "However, when walking upstairs, there was no significant effect of Condition on participants walking time ( F (2,10) =2.924, p =0.092).", "label": "Result", "bboxes": [{"left": 0.37454738562091505, "top": 0.7827487373737374, "width": 0.10796405228758171, "height": 0.01272727272727281, "page": 10}, {"left": 0.08823856209150327, "top": 0.7972967171717171, "width": 0.3941748366013072, "height": 0.01272727272727281, "page": 10}, {"left": 0.08823856209150327, "top": 0.8118396464646465, "width": 0.3539950980392157, "height": 0.01273106060606044, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.712212860584259, "is_author_statement": false, "is_in_expected_section": true, "id": "1608"}, {"text": "Half of the participants indicated that Path could be helpful.", "label": "Result", "bboxes": [{"left": 0.5788937908496732, "top": 0.1358472222222222, "width": 0.3329362745098039, "height": 0.012727272727272754, "page": 9}, {"left": 0.517656862745098, "top": 0.15008964646464645, "width": 0.05040032679738571, "height": 0.012727272727272726, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.7081584334373474, "is_author_statement": false, "is_in_expected_section": true, "id": "1609"}, {"text": "First, the HoloLenss weight strongly diminished PLVs experiences, which may have influenced our results.", "label": "Result", "bboxes": [{"left": 0.8234003267973856, "top": 0.8031477272727272, "width": 0.08831535947712421, "height": 0.01272727272727281, "page": 11}, {"left": 0.5175408496732027, "top": 0.8173901515151516, "width": 0.3941584967320261, "height": 0.012727272727272587, "page": 11}, {"left": 0.5175408496732027, "top": 0.8319368686868687, "width": 0.2527581699346404, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "prob": 0.705638587474823, "is_author_statement": true, "is_in_expected_section": true, "id": "1610"}, {"text": "While they all mentioned that visualizations were more effective than audio feedback and used the visualization as a primary guide, participants also appreciated the beep and used it as a secondary complement to the visualizations.", "label": "Result", "bboxes": [{"left": 0.35291830065359475, "top": 0.3388118686868687, "width": 0.12949183006535953, "height": 0.012727272727272698, "page": 10}, {"left": 0.08826797385620916, "top": 0.35335858585858587, "width": 0.3941846405228759, "height": 0.012727272727272698, "page": 10}, {"left": 0.08826797385620916, "top": 0.36790656565656565, "width": 0.39415849673202613, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882843137254902, "top": 0.38245328282828284, "width": 0.39422058823529416, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882843137254902, "top": 0.39699999999999996, "width": 0.22379901960784312, "height": 0.012727272727272754, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.7015767693519592, "is_author_statement": false, "is_in_expected_section": true, "id": "1611"}, {"text": "P10 also changed how he balanced his body when using our prototype: without our design, he walked down leaning his left shoulder forward instead of facing forward.", "label": "Result", "bboxes": [{"left": 0.21679084967320258, "top": 0.7467083333333334, "width": 0.2655375816993464, "height": 0.012727272727272587, "page": 6}, {"left": 0.08818790849673203, "top": 0.761256313131313, "width": 0.3942075163398693, "height": 0.01272727272727281, "page": 6}, {"left": 0.08818790849673203, "top": 0.7758030303030303, "width": 0.3941748366013072, "height": 0.012727272727272587, "page": 6}, {"left": 0.08820424836601308, "top": 0.7903497474747475, "width": 0.08078104575163399, "height": 0.01272727272727281, "page": 6}], "section": "Results", "prob": 0.6956032514572144, "is_author_statement": true, "is_in_expected_section": true, "id": "1612"}, {"text": "Meanwhile, four participants felt that the middle highlights should be a different color from the end highlights.", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.07002146464646465, "width": 0.3941372549019607, "height": 0.01272727272727274, "page": 6}, {"left": 0.08823529411764706, "top": 0.08456944444444445, "width": 0.34987418300653594, "height": 0.012727272727272726, "page": 6}], "section": "Results", "prob": 0.6891882419586182, "is_author_statement": false, "is_in_expected_section": true, "id": "1613"}, {"text": "Our evaluation was conducted indoors, with no other people around.", "label": "Result", "bboxes": [{"left": 0.7362549019607844, "top": 0.5922184343434344, "width": 0.17555882352941177, "height": 0.01272727272727281, "page": 11}, {"left": 0.517640522875817, "top": 0.6067664141414142, "width": 0.2998464052287583, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": 0.6884004473686218, "is_author_statement": true, "is_in_expected_section": true, "id": "1614"}, {"text": "Our visualizations improved participants psychological security when walking on stairs.", "label": "Result", "bboxes": [{"left": 0.6788235294117647, "top": 0.07005050505050504, "width": 0.2329934640522877, "height": 0.01272727272727274, "page": 6}, {"left": 0.517625816993464, "top": 0.08459722222222223, "width": 0.3616274509803922, "height": 0.012727272727272726, "page": 6}], "section": "Results", "prob": 0.6834399700164795, "is_author_statement": true, "is_in_expected_section": true, "id": "1615"}, {"text": "We determined Time from the video we recorded during the study.", "label": "Result", "bboxes": [{"left": 0.5525343137254902, "top": 0.5358737373737373, "width": 0.3592238562091503, "height": 0.01272727272727281, "page": 8}, {"left": 0.5175245098039216, "top": 0.5504204545454545, "width": 0.08653267973856205, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.6808173656463623, "is_author_statement": true, "is_in_expected_section": true, "id": "1616"}, {"text": "While there is no significant improvement in walking speed when using the visualizations, participants reported feeling safer and more confident when using our design.", "label": "Result", "bboxes": [{"left": 0.6847941176470588, "top": 0.48427904040404035, "width": 0.2270800653594771, "height": 0.012727272727272754, "page": 10}, {"left": 0.5176666666666667, "top": 0.49852146464646463, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176666666666667, "top": 0.5130681818181818, "width": 0.39417156862745095, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176830065359477, "top": 0.5276161616161616, "width": 0.1139101307189544, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.6712138056755066, "is_author_statement": true, "is_in_expected_section": true, "id": "1617"}, {"text": "Although we provided different visualizations (flash or movement) to further enhance the end highlights, most participants (seven out of 12) liked the original design.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.8230833333333333, "width": 0.3942663398692811, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8376313131313131, "width": 0.3941421568627451, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8521780303030303, "width": 0.331998366013072, "height": 0.01272727272727281, "page": 5}], "section": "Results", "prob": 0.6712120175361633, "is_author_statement": true, "is_in_expected_section": true, "id": "1618"}, {"text": "Our studies demonstrate the effectiveness of our designs with both projection-based AR and smartglasses.", "label": "Result", "bboxes": [{"left": 0.4569624183006536, "top": 0.35063005050505053, "width": 0.025460784313725504, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.3651767676767677, "width": 0.39417810457516345, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.3797247474747475, "width": 0.29258333333333336, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": 0.670737624168396, "is_author_statement": true, "is_in_expected_section": true, "id": "1619"}, {"text": "Most participants found Glow helpful and easy to understand.", "label": "Result", "bboxes": [{"left": 0.15563235294117647, "top": 0.43428914141414143, "width": 0.32681045751633986, "height": 0.012727272727272698, "page": 9}, {"left": 0.08826797385620916, "top": 0.4488358585858586, "width": 0.07579738562091504, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.6511512994766235, "is_author_statement": false, "is_in_expected_section": true, "id": "1620"}, {"text": "We compared users experiences with the visualizations on both platforms given that seven participated in both studies.", "label": "Result", "bboxes": [{"left": 0.08821895424836601, "top": 0.6709621212121213, "width": 0.39417320261437905, "height": 0.012727272727272587, "page": 11}, {"left": 0.08821895424836601, "top": 0.6852045454545455, "width": 0.39422385620915035, "height": 0.012727272727272587, "page": 11}], "section": "DISCUSSION", "prob": 0.6470201015472412, "is_author_statement": true, "is_in_expected_section": true, "id": "1621"}, {"text": "The middle highlights distracted her from seeing her surroundings.", "label": "Result", "bboxes": [{"left": 0.2328970588235294, "top": 0.19366666666666665, "width": 0.2494950980392157, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.20821464646464646, "width": 0.19608496732026137, "height": 0.012727272727272698, "page": 6}], "section": "Results", "prob": 0.641943633556366, "is_author_statement": false, "is_in_expected_section": true, "id": "1622"}, {"text": "We recorded the time for each task.", "label": "Result", "bboxes": [{"left": 0.657843137254902, "top": 0.281010101010101, "width": 0.2327516339869281, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.6399437785148621, "is_author_statement": true, "is_in_expected_section": true, "id": "1623"}, {"text": "They conducted each task in two conditions: (1) walking in their original way (participants could use a cane if desired, but nobody chose to use it); (2) walking using our prototype with their preferred combinations.", "label": "Result", "bboxes": [{"left": 0.6529133986928105, "top": 0.483094696969697, "width": 0.25903267973856203, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.49764141414141416, "width": 0.3941977124183007, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.5121893939393939, "width": 0.3941944444444444, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.5267361111111111, "width": 0.3563349673202614, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.6363961100578308, "is_author_statement": true, "is_in_expected_section": true, "id": "1624"}, {"text": "We conducted a user study to evaluate the visualizations we designed for commercial smartglasses.", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.7388093434343433, "width": 0.3941911764705881, "height": 0.01272727272727281, "page": 7}, {"left": 0.5176633986928104, "top": 0.7533573232323232, "width": 0.25916503267973856, "height": 0.01272727272727281, "page": 7}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.6353594660758972, "is_author_statement": true, "is_in_expected_section": true, "id": "1625"}, {"text": "To determine what platforms would be appropriate, we began by conducting a formative study with 11 PLV (7 female, 4 male; age: 2870, mean = 40) to evaluate prototype visualizations for a smartphone.", "label": "Result", "bboxes": [{"left": 0.08820261437908496, "top": 0.7803396464646465, "width": 0.39422385620915035, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.7948863636363636, "width": 0.3942107843137255, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.8094343434343434, "width": 0.3942336601307189, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.8239810606060606, "width": 0.18408823529411766, "height": 0.01272727272727281, "page": 2}], "section": "INITIAL EXPLORATION", "prob": 0.6251866817474365, "is_author_statement": true, "is_in_expected_section": false, "id": "1626"}, {"text": "Third, we asked participants to score their feeling of psychological security, but these results could be influenced by a novelty effect.", "label": "Result", "bboxes": [{"left": 0.24340032679738563, "top": 0.1426199494949495, "width": 0.23893954248366014, "height": 0.012727272727272726, "page": 12}, {"left": 0.08823202614379085, "top": 0.15716666666666668, "width": 0.394202614379085, "height": 0.012727272727272754, "page": 12}, {"left": 0.08823202614379085, "top": 0.17171338383838386, "width": 0.23277450980392161, "height": 0.012727272727272698, "page": 12}], "section": "DISCUSSION", "prob": 0.6188192367553711, "is_author_statement": true, "is_in_expected_section": true, "id": "1627"}, {"text": "All participants felt that the end highlights were an important aspect of the design.", "label": "Result", "bboxes": [{"left": 0.22225653594771239, "top": 0.7430795454545455, "width": 0.2600212418300654, "height": 0.01272727272727281, "page": 5}, {"left": 0.08813725490196078, "top": 0.7576275252525253, "width": 0.3093496732026144, "height": 0.012727272727272587, "page": 5}], "section": "Results", "prob": 0.617854118347168, "is_author_statement": false, "is_in_expected_section": true, "id": "1628"}, {"text": "We followed the same recruitment procedures as in the previous study.", "label": "Result", "bboxes": [{"left": 0.1336111111111111, "top": 0.20669570707070709, "width": 0.34882516339869274, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.22124242424242424, "width": 0.12385620915032682, "height": 0.012727272727272754, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.6155742406845093, "is_author_statement": true, "is_in_expected_section": true, "id": "1629"}, {"text": "Three participants (P6, P4, P11) felt the flash effect grabbed their attention more and alerted them.", "label": "Result", "bboxes": [{"left": 0.5174918300653595, "top": 0.3076414141414141, "width": 0.3941911764705883, "height": 0.012727272727272754, "page": 5}, {"left": 0.5175081699346404, "top": 0.32218939393939394, "width": 0.2536633986928106, "height": 0.012727272727272754, "page": 5}], "section": "Results", "prob": 0.6154895424842834, "is_author_statement": false, "is_in_expected_section": true, "id": "1630"}, {"text": "All participants except for P17 felt Beep was helpful.", "label": "Result", "bboxes": [{"left": 0.5815457516339869, "top": 0.579520202020202, "width": 0.3304166666666667, "height": 0.012727272727272698, "page": 9}, {"left": 0.5177385620915033, "top": 0.5940669191919191, "width": 0.022271241830065347, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.6128525137901306, "is_author_statement": false, "is_in_expected_section": true, "id": "1631"}, {"text": "All participants mentioned the heaviness of the hardware, which potentially impacted their experience negatively.", "label": "Result", "bboxes": [{"left": 0.2831650326797386, "top": 0.08456944444444445, "width": 0.1992614379084967, "height": 0.012727272727272726, "page": 9}, {"left": 0.08823529411764706, "top": 0.09911616161616162, "width": 0.3941241830065359, "height": 0.012727272727272726, "page": 9}, {"left": 0.08823529411764706, "top": 0.11366414141414143, "width": 0.14702124183006532, "height": 0.012727272727272712, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.6097465753555298, "is_author_statement": false, "is_in_expected_section": true, "id": "1632"}, {"text": "Although no participants chose the Moving Edge in the study, P6 felt it could be helpful since it indicated direction.", "label": "Result", "bboxes": [{"left": 0.5175245098039216, "top": 0.4970492424242424, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 5}, {"left": 0.5175081699346404, "top": 0.5115959595959596, "width": 0.39425980392156856, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": 0.6024670004844666, "is_author_statement": false, "is_in_expected_section": true, "id": "1633"}, {"text": "(4) Moving Horizontal Zebra: Since movement can be distracting [84], we design a more subtle movement effect with a yellow and black zebra pattern moving back and forth at a frequency of 1Hz (Figure 2d).", "label": "Result", "bboxes": [{"left": 0.08815359477124184, "top": 0.740969696969697, "width": 0.39414052287581697, "height": 0.01272727272727281, "page": 3}, {"left": 0.08813725490196078, "top": 0.7555164141414141, "width": 0.3941764705882353, "height": 0.012727272727272587, "page": 3}, {"left": 0.08813725490196078, "top": 0.7700631313131313, "width": 0.39417156862745095, "height": 0.01272727272727281, "page": 3}, {"left": 0.08813725490196078, "top": 0.7846111111111111, "width": 0.19653921568627453, "height": 0.012727272727272587, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": 0.5968042612075806, "is_author_statement": true, "is_in_expected_section": false, "id": "1634"}, {"text": "The second platform we explored was optical see-through smartglasses.", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.3885063131313131, "width": 0.3941503267973855, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.4030542929292929, "width": 0.08672058823529405, "height": 0.012727272727272698, "page": 6}], "section": "VISUALIZATIONS FOR SMARTGLASSES", "prob": 0.5941872000694275, "is_author_statement": true, "is_in_expected_section": false, "id": "1635"}, {"text": "The most commonly chosen visualization was Glow, which was preferred by eight participants.", "label": "Result", "bboxes": [{"left": 0.0882843137254902, "top": 0.49185732323232323, "width": 0.3941911764705882, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882843137254902, "top": 0.5060984848484849, "width": 0.24099836601307187, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.590134859085083, "is_author_statement": false, "is_in_expected_section": true, "id": "1636"}, {"text": "The participant experienced our design in three phases: (1) Auditory feedback when approaching the stairs, with three alternatives: sound, human voice, and the combination of them; (2) End highlights on the first and last stairs with six design alternatives (Figure 2); and (3) Middle highlights on the middle stairs with three design alternatives (Figure 3).", "label": "Result", "bboxes": [{"left": 0.4569624183006536, "top": 0.6503434343434343, "width": 0.025464052287581695, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882516339869281, "top": 0.6648914141414142, "width": 0.39412418300653596, "height": 0.012727272727272587, "page": 4}, {"left": 0.0882516339869281, "top": 0.6794381313131314, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 4}, {"left": 0.08826797385620916, "top": 0.6939848484848484, "width": 0.39414869281045745, "height": 0.01272727272727281, "page": 4}, {"left": 0.08826797385620916, "top": 0.7085328282828283, "width": 0.3942173202614379, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882843137254902, "top": 0.7230795454545454, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882843137254902, "top": 0.7376275252525253, "width": 0.32853921568627453, "height": 0.012727272727272587, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.5868438482284546, "is_author_statement": true, "is_in_expected_section": true, "id": "1637"}, {"text": "s study emphasized the need for tools that facilitate stair navigation for PLV.", "label": "Result", "bboxes": [{"left": 0.7781062091503268, "top": 0.2443068181818182, "width": 0.1337156862745098, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176241830065359, "top": 0.25885353535353534, "width": 0.3711977124183006, "height": 0.012727272727272754, "page": 1}], "section": "RELATED WORK", "prob": 0.5848885178565979, "is_author_statement": false, "is_in_expected_section": false, "id": "1638"}, {"text": "They also felt the visualizations were comfortable to see ( mean =5.6, SD =1.73), as shown in Figure 9.", "label": "Result", "bboxes": [{"left": 0.29765849673202616, "top": 0.6591439393939394, "width": 0.18481699346405223, "height": 0.012727272727272587, "page": 10}, {"left": 0.0882843137254902, "top": 0.6733863636363636, "width": 0.3941584967320261, "height": 0.01272727272727281, "page": 10}, {"left": 0.0882843137254902, "top": 0.6879330808080808, "width": 0.12263888888888891, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5807610750198364, "is_author_statement": false, "is_in_expected_section": true, "id": "1639"}, {"text": "Next, we report participants responses on all design alternatives in the three design phases.", "label": "Result", "bboxes": [{"left": 0.08818627450980392, "top": 0.5536729797979798, "width": 0.39420261437908494, "height": 0.012727272727272698, "page": 5}, {"left": 0.08816993464052288, "top": 0.568219696969697, "width": 0.20695261437908502, "height": 0.01272727272727281, "page": 5}], "section": "Results", "prob": 0.5716547966003418, "is_author_statement": true, "is_in_expected_section": true, "id": "1640"}, {"text": "Each task in each condition was repeated five times.", "label": "Result", "bboxes": [{"left": 0.8790539215686275, "top": 0.15705934343434344, "width": 0.03272058823529411, "height": 0.012727272727272726, "page": 8}, {"left": 0.5175898692810458, "top": 0.17160606060606062, "width": 0.30493790849673197, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.569905698299408, "is_author_statement": false, "is_in_expected_section": true, "id": "1641"}, {"text": "We continued the study with a design exploration session and a stair navigation session.", "label": "Result", "bboxes": [{"left": 0.42598039215686273, "top": 0.6361262626262626, "width": 0.0563790849673203, "height": 0.01272727272727281, "page": 8}, {"left": 0.08818627450980392, "top": 0.6506729797979798, "width": 0.394140522875817, "height": 0.01272727272727281, "page": 8}, {"left": 0.08818627450980392, "top": 0.6652209595959596, "width": 0.12438562091503268, "height": 0.012727272727272587, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5658233165740967, "is_author_statement": true, "is_in_expected_section": true, "id": "1642"}, {"text": "Effectiveness of the Visualizations (and Sonification).", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.386385101010101, "width": 0.36806535947712415, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": 0.5656393766403198, "is_author_statement": false, "is_in_expected_section": true, "id": "1643"}, {"text": "Effectiveness of the visualizations (and sonification) .", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.1357840909090909, "width": 0.3654820261437909, "height": 0.012727272727272726, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5656393766403198, "is_author_statement": false, "is_in_expected_section": true, "id": "1644"}, {"text": "Two participants (P2, P3) liked the Moving Vertical Zebra the most.", "label": "Result", "bboxes": [{"left": 0.5175081699346404, "top": 0.4021931818181818, "width": 0.39417483660130737, "height": 0.012727272727272754, "page": 5}, {"left": 0.5175081699346404, "top": 0.416739898989899, "width": 0.06097058823529411, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": 0.5646823644638062, "is_author_statement": false, "is_in_expected_section": true, "id": "1645"}, {"text": "Based on our observations of the walking tasks, some participants ( e.g ., P9, P4) looked down less when using our design since they could use their lower peripheral vision to notice the highlights.", "label": "Result", "bboxes": [{"left": 0.21390849673202617, "top": 0.5066843434343434, "width": 0.26850326797385615, "height": 0.01272727272727281, "page": 6}, {"left": 0.08823692810457516, "top": 0.5212323232323232, "width": 0.3941781045751634, "height": 0.01272727272727281, "page": 6}, {"left": 0.08822058823529412, "top": 0.5357790404040403, "width": 0.39414542483660137, "height": 0.012727272727272698, "page": 6}, {"left": 0.08822058823529412, "top": 0.5503270202020202, "width": 0.20637745098039217, "height": 0.01272727272727281, "page": 6}], "section": "Results", "prob": 0.5588145852088928, "is_author_statement": true, "is_in_expected_section": true, "id": "1646"}, {"text": "With the condition of wearing HoloLens without visualizations as the baseline, we analyzed the effect of our visualiza-", "label": "Result", "bboxes": [{"left": 0.08822222222222222, "top": 0.8627487373737374, "width": 0.39427450980392165, "height": 0.012727272727272587, "page": 10}, {"left": 0.08822222222222222, "top": 0.8772967171717171, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5575814843177795, "is_author_statement": true, "is_in_expected_section": true, "id": "1647"}, {"text": "We first report the effect of the HoloLens on participants visual abilities.", "label": "Result", "bboxes": [{"left": 0.7512826797385621, "top": 0.7748699494949495, "width": 0.16049673202614378, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176307189542484, "top": 0.7894179292929293, "width": 0.3207320261437907, "height": 0.012727272727272587, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5559591054916382, "is_author_statement": true, "is_in_expected_section": true, "id": "1648"}, {"text": "report participants feedback on each design alternative.", "label": "Result", "bboxes": [{"left": 0.0882516339869281, "top": 0.15033080808080806, "width": 0.36614052287581705, "height": 0.012727272727272726, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5528454184532166, "is_author_statement": false, "is_in_expected_section": true, "id": "1649"}, {"text": "Our visualizations inform PLV of the different stair stages via different design.", "label": "Result", "bboxes": [{"left": 0.23304411764705882, "top": 0.6561237373737373, "width": 0.2493480392156863, "height": 0.01272727272727281, "page": 7}, {"left": 0.08821895424836601, "top": 0.6706717171717173, "width": 0.26529084967320266, "height": 0.012727272727272587, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.5498331785202026, "is_author_statement": true, "is_in_expected_section": false, "id": "1650"}, {"text": "We analyzed participants qualitative responses with the same method we used in the previous study.", "label": "Result", "bboxes": [{"left": 0.517656862745098, "top": 0.7251717171717171, "width": 0.39416993464052286, "height": 0.01272727272727281, "page": 8}, {"left": 0.517640522875817, "top": 0.7397196969696969, "width": 0.28866339869281055, "height": 0.01272727272727281, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.5475605130195618, "is_author_statement": true, "is_in_expected_section": true, "id": "1651"}, {"text": "In terms of color, most participants preferred the bright yellow (seven out of 12), wanting to be alert on each step.", "label": "Result", "bboxes": [{"left": 0.5174264705882353, "top": 0.8176742424242424, "width": 0.3942892156862746, "height": 0.01272727272727281, "page": 5}, {"left": 0.5174264705882353, "top": 0.8322222222222222, "width": 0.3575098039215686, "height": 0.01272727272727281, "page": 5}], "section": "Results", "prob": 0.5426410436630249, "is_author_statement": false, "is_in_expected_section": true, "id": "1652"}, {"text": "Since locating the first and last stairs was most important but challenging for PLV [86], we distinguish the first and last stairs from the rest by projecting thick highlights on them (Figure 2a), while projecting thin highlights on the middle stairs (Figure 3a).", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.3545820707070707, "width": 0.3941388888888889, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3691287878787879, "width": 0.39412745098039215, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.38367676767676767, "width": 0.39422058823529404, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3982234848484848, "width": 0.3942075163398692, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.4127714646464647, "width": 0.11595261437908495, "height": 0.012727272727272698, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": 0.5230543613433838, "is_author_statement": true, "is_in_expected_section": false, "id": "1653"}, {"text": "The task ended when both their feet first touched the landing.", "label": "Result", "bboxes": [{"left": 0.8555130718954248, "top": 0.606739898989899, "width": 0.056364379084967275, "height": 0.012727272727272698, "page": 4}, {"left": 0.5177058823529412, "top": 0.6212878787878788, "width": 0.3419754901960784, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.5122244954109192, "is_author_statement": false, "is_in_expected_section": true, "id": "1654"}, {"text": "We ended the study with an exit interview, asking about the participants general experience with the prototype.", "label": "Result", "bboxes": [{"left": 0.5177058823529412, "top": 0.7382638888888889, "width": 0.3941830065359476, "height": 0.012727272727272587, "page": 4}, {"left": 0.5176895424836602, "top": 0.752810606060606, "width": 0.3531781045751634, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.5020940899848938, "is_author_statement": true, "is_in_expected_section": true, "id": "1655"}, {"text": "To simulate the limited projection area of a handheld projector, we projected visualizations only on the three stairs in front of the participant (Figure 1a).", "label": "Result", "bboxes": [{"left": 0.8331748366013072, "top": 0.8315555555555555, "width": 0.07861274509803928, "height": 0.01272727272727281, "page": 3}, {"left": 0.5175980392156863, "top": 0.8461022727272728, "width": 0.3941895424836601, "height": 0.012727272727272587, "page": 3}, {"left": 0.5175980392156863, "top": 0.8606502525252525, "width": 0.39417973856209154, "height": 0.01272727272727281, "page": 3}, {"left": 0.5175980392156863, "top": 0.8751969696969696, "width": 0.15113398692810454, "height": 0.01272727272727281, "page": 3}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.5014510154724121, "is_author_statement": true, "is_in_expected_section": true, "id": "1656"}, {"text": "We built our prototype on Microsoft HoloLens v1.", "label": "Result", "bboxes": [{"left": 0.16765686274509803, "top": 0.24336237373737374, "width": 0.3147483660130719, "height": 0.012727272727272754, "page": 8}, {"left": 0.08823529411764706, "top": 0.2579090909090909, "width": 0.02052287581699347, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.4957635700702667, "is_author_statement": true, "is_in_expected_section": true, "id": "1657"}, {"text": "Participants gave high scores to the usefulness and comfort level of the visualizations, as shown in Figure 4.", "label": "Result", "bboxes": [{"left": 0.08820261437908496, "top": 0.5170050505050505, "width": 0.394140522875817, "height": 0.012727272727272698, "page": 5}, {"left": 0.08820261437908496, "top": 0.5315530303030304, "width": 0.31593627450980394, "height": 0.012727272727272698, "page": 5}], "section": "Results", "prob": 0.48626115918159485, "is_author_statement": false, "is_in_expected_section": true, "id": "1658"}, {"text": "Participants combined different visualizations and sonification based on their preferences, as shown in Figure 8.", "label": "Result", "bboxes": [{"left": 0.4342663398692811, "top": 0.273354797979798, "width": 0.04816013071895425, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882516339869281, "top": 0.2879027777777778, "width": 0.39415849673202613, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882516339869281, "top": 0.3024494949494949, "width": 0.31948039215686275, "height": 0.012727272727272754, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.48260214924812317, "is_author_statement": false, "is_in_expected_section": true, "id": "1659"}, {"text": "[86] found that they looked at contrast stripes ( i.e ., contrasting marking stripes on stair treads) to perceive the exact location of stair edges; some also observed the trend of the railing to understand the overall structure of a staircase.", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.5324545454545455, "width": 0.3942499999999999, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176633986928104, "top": 0.5470025252525252, "width": 0.39417156862745095, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176797385620915, "top": 0.5615492424242424, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176797385620915, "top": 0.5760972222222223, "width": 0.3941584967320262, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION", "prob": 0.47991108894348145, "is_author_statement": false, "is_in_expected_section": true, "id": "1660"}, {"text": "An ANOVA found no significant effect of Order on walking time (downstairs: F (1,10) =0.108, p =0.749; upstairs: F (1,10) =0.007, p =0.937) for  = 0.05.", "label": "Result", "bboxes": [{"left": 0.08820261437908496, "top": 0.28548232323232325, "width": 0.3942728758169935, "height": 0.012727272727272698, "page": 5}, {"left": 0.08820261437908496, "top": 0.3000214646464646, "width": 0.3942696078431373, "height": 0.012734848484848516, "page": 5}, {"left": 0.0882483660130719, "top": 0.3145669191919192, "width": 0.2352516339869281, "height": 0.012729797979797952, "page": 5}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.4787900149822235, "is_author_statement": false, "is_in_expected_section": true, "id": "1661"}, {"text": "Participants used this combination for the stair navigation portion.", "label": "Result", "bboxes": [{"left": 0.8347156862745098, "top": 0.41733333333333333, "width": 0.07719771241830065, "height": 0.012727272727272698, "page": 4}, {"left": 0.517673202614379, "top": 0.43188005050505046, "width": 0.35134967320261445, "height": 0.012727272727272754, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.4738747179508209, "is_author_statement": false, "is_in_expected_section": true, "id": "1662"}, {"text": "Most participants found it difficult to use the Edge Highlights because of the limited vertical FOV.", "label": "Result", "bboxes": [{"left": 0.3063888888888889, "top": 0.17214520202020203, "width": 0.17607843137254903, "height": 0.012727272727272754, "page": 9}, {"left": 0.0882516339869281, "top": 0.1866931818181818, "width": 0.39419117647058827, "height": 0.012727272727272754, "page": 9}, {"left": 0.0882516339869281, "top": 0.201239898989899, "width": 0.08958333333333333, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.4677228331565857, "is_author_statement": false, "is_in_expected_section": true, "id": "1663"}, {"text": "The study ended with a final interview asking about the participants general experience with the prototype.", "label": "Result", "bboxes": [{"left": 0.5175571895424836, "top": 0.36858585858585863, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175408496732027, "top": 0.3831338383838384, "width": 0.32315032679738565, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.4671858549118042, "is_author_statement": false, "is_in_expected_section": true, "id": "1664"}, {"text": "We designed five animations to achieve this:", "label": "Result", "bboxes": [{"left": 0.3960212418300654, "top": 0.5218686868686868, "width": 0.0863888888888889, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5364166666666667, "width": 0.20429084967320266, "height": 0.01272727272727281, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": 0.46282291412353516, "is_author_statement": true, "is_in_expected_section": false, "id": "1665"}, {"text": "Nevertheless, some participants ( e.g ., P6, P10, P13) felt this design was helpful because it provided a preview for future steps, especially when they looked downstairs from the top landing.", "label": "Result", "bboxes": [{"left": 0.0882516339869281, "top": 0.29609595959595963, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.3106439393939394, "width": 0.39416993464052286, "height": 0.012727272727272754, "page": 9}, {"left": 0.0882516339869281, "top": 0.32519065656565654, "width": 0.3941650326797385, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882516339869281, "top": 0.3394330808080808, "width": 0.05317647058823531, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.44805797934532166, "is_author_statement": false, "is_in_expected_section": true, "id": "1666"}, {"text": "To better distinguish the landing and the stairs, we colored the straight part of the visualization (over the landing) yellow and the slope blue.", "label": "Result", "bboxes": [{"left": 0.7153300653594771, "top": 0.4373674242424242, "width": 0.19646732026143798, "height": 0.012727272727272754, "page": 7}, {"left": 0.517640522875817, "top": 0.45191540404040403, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 7}, {"left": 0.517640522875817, "top": 0.4664621212121212, "width": 0.3639166666666668, "height": 0.012727272727272698, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.4389118552207947, "is_author_statement": true, "is_in_expected_section": false, "id": "1667"}, {"text": "We analyzed the participants qualitative feedback by coding the interview transcripts based on grounded theory [66].", "label": "Result", "bboxes": [{"left": 0.08823366013071895, "top": 0.3366868686868687, "width": 0.3941911764705883, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823366013071895, "top": 0.3512348484848485, "width": 0.3666683006535948, "height": 0.012727272727272698, "page": 5}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.4370916485786438, "is_author_statement": true, "is_in_expected_section": true, "id": "1668"}, {"text": "[80] measured 782 older adults visual abilities and collected self-reported mobility limitations.", "label": "Result", "bboxes": [{"left": 0.2741062091503268, "top": 0.765794191919192, "width": 0.20828594771241832, "height": 0.01272727272727281, "page": 1}, {"left": 0.08821895424836601, "top": 0.780340909090909, "width": 0.39415686274509804, "height": 0.01272727272727281, "page": 1}, {"left": 0.08821895424836601, "top": 0.7948888888888889, "width": 0.03587254901960785, "height": 0.012727272727272587, "page": 1}], "section": "RELATED WORK", "prob": 0.4296902120113373, "is_author_statement": false, "is_in_expected_section": false, "id": "1669"}, {"text": "(1) Glow visualization (Figure 7ad): We generate a glow effect at the bottom of the display to simulate the experience of seeing the edge highlights on the stairs with peripheral vision.", "label": "Result", "bboxes": [{"left": 0.08821895424836601, "top": 0.7073383838383839, "width": 0.39422385620915035, "height": 0.01272727272727281, "page": 7}, {"left": 0.08823529411764706, "top": 0.7218863636363637, "width": 0.3941633986928104, "height": 0.012727272727272587, "page": 7}, {"left": 0.08823529411764706, "top": 0.7364330808080808, "width": 0.39417483660130714, "height": 0.01272727272727281, "page": 7}, {"left": 0.08823529411764706, "top": 0.7506755050505051, "width": 0.03136601307189542, "height": 0.01272727272727281, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.4273926317691803, "is_author_statement": true, "is_in_expected_section": false, "id": "1670"}, {"text": "We also counterbalanced the starting task (up/down) and the conditions.", "label": "Result", "bboxes": [{"left": 0.8186732026143791, "top": 0.3319191919191919, "width": 0.09317320261437889, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175571895424836, "top": 0.34646590909090913, "width": 0.38311764705882345, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.42715010046958923, "is_author_statement": true, "is_in_expected_section": true, "id": "1671"}, {"text": "platforms, The first experience [projection-based AR] gave me a better sense of a direction as to where this was goingBut the [glow] was like floating over the steps, and they didnt stay fixed in place. That was one big difference. I like the light fixed on the step.", "label": "Result", "bboxes": [{"left": 0.5177058823529412, "top": 0.2721919191919192, "width": 0.39415686274509787, "height": 0.012727272727272754, "page": 11}, {"left": 0.5176895424836602, "top": 0.2864330808080808, "width": 0.39418137254901964, "height": 0.012727272727272754, "page": 11}, {"left": 0.5176895424836602, "top": 0.3009810606060606, "width": 0.39412745098039215, "height": 0.012727272727272698, "page": 11}, {"left": 0.5176895424836602, "top": 0.3155277777777778, "width": 0.39417320261437905, "height": 0.012727272727272754, "page": 11}, {"left": 0.517673202614379, "top": 0.3300757575757576, "width": 0.17794771241830087, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "prob": 0.4218125343322754, "is_author_statement": false, "is_in_expected_section": true, "id": "1672"}, {"text": "To validate counterbalancing, we added another betweensubject factor, Order (six levels based on the three conditions), into our model.", "label": "Result", "bboxes": [{"left": 0.5175245098039216, "top": 0.601635101010101, "width": 0.3941584967320262, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175245098039216, "top": 0.6161818181818182, "width": 0.3941078431372549, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175081699346404, "top": 0.630729797979798, "width": 0.1468709150326798, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "prob": 0.4197796583175659, "is_author_statement": true, "is_in_expected_section": true, "id": "1673"}, {"text": "We also implemented the auditory feedback on the smartphone.", "label": "Result", "bboxes": [{"left": 0.31862581699346404, "top": 0.4463888888888889, "width": 0.16376307189542483, "height": 0.012727272727272698, "page": 4}, {"left": 0.0882516339869281, "top": 0.46093686868686873, "width": 0.2534068627450981, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "prob": 0.4146425127983093, "is_author_statement": true, "is_in_expected_section": true, "id": "1674"}, {"text": "Similar to glow, we adjusted the sound based on the different stages of the stairs:", "label": "Result", "bboxes": [{"left": 0.7188055555555556, "top": 0.5319191919191919, "width": 0.19301307189542483, "height": 0.01272727272727281, "page": 7}, {"left": 0.517640522875817, "top": 0.5464659090909091, "width": 0.3379395424836602, "height": 0.012727272727272698, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.4044635593891144, "is_author_statement": true, "is_in_expected_section": false, "id": "1675"}, {"text": "We provide three different auditory feedback choices: (1) Sonification that indicates stair direction: one ding sound for going up and two ding sounds for going down, adapted from the sonic alerts for some elevators; (2) a human voice that verbally reported stair direction and number of stairs: Approaching upstairs, 14 stairs going up; and (3) a combined sonification and human voice: ding, approaching upstairs, 14 stairs going up.", "label": "Result", "bboxes": [{"left": 0.40587091503267975, "top": 0.21638888888888888, "width": 0.0765049019607843, "height": 0.012727272727272754, "page": 3}, {"left": 0.08820261437908496, "top": 0.23093686868686866, "width": 0.39420261437908505, "height": 0.012727272727272754, "page": 3}, {"left": 0.08821895424836601, "top": 0.24548358585858587, "width": 0.3941993464052288, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.260030303030303, "width": 0.3941764705882353, "height": 0.012727272727272754, "page": 3}, {"left": 0.08821895424836601, "top": 0.27457828282828284, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.289125, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.0882516339869281, "top": 0.3036729797979798, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.318219696969697, "width": 0.3941748366013072, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.33276767676767677, "width": 0.12806862745098035, "height": 0.012727272727272698, "page": 3}], "section": "Visualization (and Sonification) Design", "prob": 0.40030258893966675, "is_author_statement": true, "is_in_expected_section": false, "id": "1676"}, {"text": "From an interaction perspective, we aimed to simulate use of a flashlight, which is commonly used by PLV in dark places [79]: when a user points the projection-based AR phone at the stairs, it recognizes several stairs in front of her and projects visualizations on those stairs in real time (Figure 1a).", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.6345669191919192, "width": 0.39411274509803906, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.649114898989899, "width": 0.394142156862745, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.6636616161616161, "width": 0.3941454248366014, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176307189542484, "top": 0.6782095959595961, "width": 0.3941078431372549, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176307189542484, "top": 0.6927563131313131, "width": 0.39412581699346394, "height": 0.01272727272727281, "page": 2}], "section": "Visualization (and Sonification) Design", "prob": 0.3787771463394165, "is_author_statement": true, "is_in_expected_section": false, "id": "1677"}, {"text": "We designed and evaluated visualizations for both platforms, given that each platform has its own strength: projectionbased AR can augment large physical surfaces but projects content publicly, which may be better suited to private places with few people ( e.g ., home, workspace); meanwhile, smartglasses present information only to the user, which may be better for crowded public places ( e.g ., subway stations).", "label": "Result", "bboxes": [{"left": 0.8890522875816994, "top": 0.22278282828282828, "width": 0.022735294117647076, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.2373308080808081, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.25187752525252527, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.26642424242424245, "width": 0.3940915032679738, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.28097222222222223, "width": 0.3941584967320261, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.29551893939393936, "width": 0.39412418300653596, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.3100669191919192, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.3246136363636364, "width": 0.3652679738562091, "height": 0.012727272727272698, "page": 2}], "section": "INITIAL EXPLORATION", "prob": 0.3765772879123688, "is_author_statement": true, "is_in_expected_section": false, "id": "1678"}, {"text": "Most participants were not concerned about projecting highlights on stairs.", "label": "Result", "bboxes": [{"left": 0.6544460784313726, "top": 0.20824242424242423, "width": 0.2573839869281044, "height": 0.012727272727272754, "page": 6}, {"left": 0.517625816993464, "top": 0.22279040404040404, "width": 0.24226797385620924, "height": 0.012727272727272754, "page": 6}], "section": "Results", "prob": 0.3719455897808075, "is_author_statement": false, "is_in_expected_section": true, "id": "1679"}, {"text": "Middle stairs: thin blue steps to indicate the middle stairs.", "label": "Result", "bboxes": [{"left": 0.517640522875817, "top": 0.2409861111111111, "width": 0.39288888888888895, "height": 0.012777777777777805, "page": 7}], "section": "Visualization (and Sonification) Design", "prob": 0.36088037490844727, "is_author_statement": false, "is_in_expected_section": false, "id": "1680"}], "uist-2": [{"text": "While we acknowledge that not all deaf/hard-of-hearing (DHH) individuals want to use sound or captioning technologies, prior work has demonstrated that many people would find such technologies desirable and useful in everyday activities [16].", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.7672941919191919, "width": 0.39440522875816986, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.7818421717171717, "width": 0.3944705882352941, "height": 0.012727272727272587, "page": 1}, {"left": 0.08823529411764706, "top": 0.7963888888888889, "width": 0.3943055555555556, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.8109368686868687, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.8251780303030304, "width": 0.1660392156862745, "height": 0.012727272727272587, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1681"}, {"text": "Our HWD prototype augments the users perception of speech and sounds in the environment (Figure 1).", "label": "Author", "bboxes": [{"left": 0.08821895424836601, "top": 0.2976287878787879, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 1}, {"left": 0.08821895424836601, "top": 0.31217550505050506, "width": 0.3327058823529412, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1682"}, {"text": "In the following sections, we review related work suggesting initial design requirements for this project.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.6097247474747475, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.6242714646464647, "width": 0.279406862745098, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1683"}, {"text": "Next, we discuss a survey regarding the situations where a captioning HWD might be most useful for people who are deaf or hard of hearing.", "label": "Author", "bboxes": [{"left": 0.3721552287581699, "top": 0.6242714646464647, "width": 0.11033660130718964, "height": 0.012727272727272698, "page": 1}, {"left": 0.08823529411764706, "top": 0.6388181818181818, "width": 0.3945866013071896, "height": 0.012727272727272698, "page": 1}, {"left": 0.08823529411764706, "top": 0.6533661616161617, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.6679128787878787, "width": 0.053117647058823506, "height": 0.01272727272727281, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1684"}, {"text": "We conclude with current limitations and future avenues for this work.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.6970075757575758, "width": 0.3944052287581699, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.7115555555555556, "width": 0.0656846405228758, "height": 0.012727272727272587, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1685"}, {"text": "The major contributions of this work include", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.4197184343434343, "width": 0.2931764705882353, "height": 0.012727272727272754, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1686"}, {"text": "Here, we focus on a form factor that is virtually unnoticeable to bystanders.", "label": "Author", "bboxes": [{"left": 0.3646944444444445, "top": 0.10820707070707071, "width": 0.11797875816993464, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.12275505050505049, "width": 0.3942892156862745, "height": 0.01272727272727274, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1687"}, {"text": "For these devices, speech recognition could be relegated to the smartphone and cloud as in our approach, but our informal experiments suggest that these systems only provide a few hours of battery life with the display on and otherwise idle.", "label": "Author", "bboxes": [{"left": 0.41720261437908496, "top": 0.4558042929292929, "width": 0.06535457516339865, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.47035227272727276, "width": 0.3943888888888889, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.48489898989898994, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.4994469696969697, "width": 0.3945686274509804, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.5139936868686868, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1688"}, {"text": "To learn more about challenges in mobile contexts, we conducted a brief large-scale online survey with participants who used hearing aids, TDD/TTY (telecommunications device for the deaf/teletypewriter) [13], CART [31], and cochlear implants.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.8039608585858585, "width": 0.3943218954248366, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8185088383838384, "width": 0.39453594771241834, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8330555555555555, "width": 0.3946192810457516, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8476035353535354, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.8621502525252525, "width": 0.11901633986928102, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1689"}, {"text": "We aimed to expand upon existing studies which have already identified interest in real-time captioning", "label": "Author", "bboxes": [{"left": 0.210578431372549, "top": 0.8621502525252525, "width": 0.2722761437908497, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8766969696969698, "width": 0.39440522875816986, "height": 0.012727272727272587, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1690"}, {"text": "We used Google Surveys [7] to deploy a short, ten-question survey.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.49803535353535355, "width": 0.39449509803921556, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5125782828282828, "width": 0.04764215686274509, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1691"}, {"text": "Due to restrictions from our institution, we did not ask participants to self-report their hearing levels or inquire about the use of signed languages for this survey, which may have resulted in an underrepresentation of Deaf participants.", "label": "Author", "bboxes": [{"left": 0.8630931372549019, "top": 0.5416641414141414, "width": 0.0490245098039217, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5562070707070708, "width": 0.39459313725490197, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.57075, "width": 0.3944428104575163, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5852929292929293, "width": 0.3943349673202614, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5998358585858585, "width": 0.2919248366013072, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1692"}, {"text": "We received 501 responses indicating usage of assistive technologies such as a hearing aid, cochlear implant, or realtime captioning/transcription services (i.e., CART).", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.815530303030303, "width": 0.3931895424836601, "height": 0.012626262626262652, "page": 2}, {"left": 0.5176470588235295, "top": 0.830050505050505, "width": 0.3926225490196078, "height": 0.012626262626262652, "page": 2}, {"left": 0.5176470588235295, "top": 0.844570707070707, "width": 0.379656862745098, "height": 0.012626262626262652, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1693"}, {"text": "Our eyewear prototype weighs 54 g with 30 g on the nose.", "label": "Author", "bboxes": [{"left": 0.08821895424836601, "top": 0.725215909090909, "width": 0.3942075163398693, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1694"}, {"text": "With a total power consumption of ~266 mW, our device does not exceed body temperature (37C), and we have not observed thermal discomfort in any of our user studies.", "label": "Author", "bboxes": [{"left": 0.08821895424836601, "top": 0.7397626262626263, "width": 0.39432189542483664, "height": 0.012727272727272587, "page": 2}, {"left": 0.08821895424836601, "top": 0.7543106060606061, "width": 0.3943055555555556, "height": 0.01272727272727281, "page": 2}, {"left": 0.08821895424836601, "top": 0.7688573232323231, "width": 0.36093627450980387, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1695"}, {"text": "For graphics effects that require animation, such as our smoothly-scrolling transcript text, we implement interpolation primitives which are executed on the device to drive position and scale parameters of other primitives.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.6842601010101009, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.6988080808080809, "width": 0.394388888888889, "height": 0.012727272727272587, "page": 3}, {"left": 0.5176797385620915, "top": 0.713354797979798, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.7279027777777778, "width": 0.36096895424836595, "height": 0.012727272727272587, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1696"}, {"text": "However, we acknowledge limitations in our survey.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.7451906565656566, "width": 0.3675898692810457, "height": 0.012727272727272587, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1697"}, {"text": "By asking only about assistive technology to identify DHH users, as per our institutions allowance, Deaf participants who are less likely to use assistive technologies may have been underrepresented.", "label": "Author", "bboxes": [{"left": 0.463468954248366, "top": 0.7451906565656566, "width": 0.019220588235294156, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7597386363636364, "width": 0.3946356209150327, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.7742853535353535, "width": 0.3945866013071896, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7888333333333333, "width": 0.39438888888888884, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.8033800505050506, "width": 0.1519084967320261, "height": 0.012727272727272587, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1698"}, {"text": "For this case, we reference Findlater et al.", "label": "Author", "bboxes": [{"left": 0.24426143790849672, "top": 0.8033800505050506, "width": 0.23837908496732022, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.8179267676767676, "width": 0.03351797385620915, "height": 0.01272727272727281, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1699"}, {"text": "To overcome the limitations of existing platforms, we developed a hybrid approach adapted to our specific application that consists of a thin-client low-power eyewear prototype coupled with a mobile phone over a wireless connection (Figure 3).", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.27850631313131313, "width": 0.39435457516339867, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.2930542929292929, "width": 0.39430555555555546, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.3076010101010101, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.3221489898989899, "width": 0.3941911764705881, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.33669570707070706, "width": 0.15742647058823522, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1700"}, {"text": "We implemented our prototype using a MediaTek MT2523D System-in-Package (SiP) [27], which is a single-chip CortexM4F with integrated Bluetooth (BT) 4.0 EDR and Bluetooth Low Energy (BLE) transceiver, power management, MIPIDSI display controller, and memory subsystem.", "label": "Author", "bboxes": [{"left": 0.8889771241830065, "top": 0.38003156565656565, "width": 0.022991830065359542, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.3945795454545455, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.40912626262626267, "width": 0.3944869281045752, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.42367424242424245, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.43822095959595964, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.4527689393939394, "width": 0.3582189542483659, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1701"}, {"text": "We optimized our electronics layout for a mostly single-sided design with a compact footprint of 14.5  60 mm (Figure 4).", "label": "Author", "bboxes": [{"left": 0.8890098039215686, "top": 0.4527689393939394, "width": 0.022991830065359542, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.46731565656565655, "width": 0.39450326797385615, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.4818636363636364, "width": 0.39431045751633986, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1702"}, {"text": "Our large-scale survey enabled us to identify common mobile experiences, scenarios that frequently exacerbate hearing difficulties, and attitudes toward assistive technologies.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.5345669191919192, "width": 0.39405882352941174, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.549114898989899, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.5636616161616161, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.5779040404040404, "width": 0.08686601307189544, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1703"}, {"text": "Our scenarios present both specific and openended situations for future user evaluations which can focus on issues of visual dispersion, facial or text visibility, and ambient noise.", "label": "Author", "bboxes": [{"left": 0.18000980392156862, "top": 0.5779040404040404, "width": 0.3026960784313726, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.5924507575757576, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.6069987373737373, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.6215454545454545, "width": 0.09587581699346405, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1704"}, {"text": "Our prototype system uses a 1-lane MIPI-DSI compatible microdisplay engine with 30 fps graphics rendering.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.25093055555555555, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.2654785353535353, "width": 0.34133660130718957, "height": 0.012727272727272754, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1705"}, {"text": "For our proof-of-concept, we use an optical combiner similar to the 15 diagonal example in Cakmakci et al.", "label": "Author", "bboxes": [{"left": 0.4340849673202615, "top": 0.2654785353535353, "width": 0.048620915032679735, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.2800252525252525, "width": 0.39433823529411755, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.2945732323232323, "width": 0.3021192810457516, "height": 0.012727272727272754, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1706"}, {"text": "Since the optical see-through display can only add light, we render the text in white to maximize contrast and visibility.", "label": "Author", "bboxes": [{"left": 0.19998856209150326, "top": 0.3527613636363636, "width": 0.282733660130719, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.3673093434343434, "width": 0.3943545751633987, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.3818560606060606, "width": 0.14568300653594768, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1707"}, {"text": "All studies used our eyewear prototype with transcribed speech sent wirelessly from an Android Pixel 3 phone.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.36183964646464645, "width": 0.3944379084967321, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.3760820707070707, "width": 0.3742941176470589, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1708"}, {"text": "In cases where the eyewear experience was compared to a mobile device, we used an updated version of Live Transcribe [9] running on the same phone.", "label": "Author", "bboxes": [{"left": 0.8984133986928103, "top": 0.3760820707070707, "width": 0.013720588235294207, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.39062878787878785, "width": 0.39427287581699355, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.40517676767676764, "width": 0.39432189542483653, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.4197234848484848, "width": 0.28642320261437904, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1709"}, {"text": "All participants were fitted in individual in-person sessions with our HWD, which included mechanical adjustments (nose pads, nose bridge) to optimize fit.", "label": "Author", "bboxes": [{"left": 0.8093578431372549, "top": 0.4197234848484848, "width": 0.10285784313725488, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.4342714646464646, "width": 0.39427287581699355, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.44881818181818184, "width": 0.39430555555555546, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.463364898989899, "width": 0.1511993464052287, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1710"}, {"text": "Our first prototype frame included the board in the right temple while the battery was embedded in the left temple (Figure 5 left).", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.4824482323232323, "width": 0.394421568627451, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4969949494949495, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5115429292929293, "width": 0.09823039215686273, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1711"}, {"text": "After studies 1 and 2 below, we redesigned the prototypes to include two 180 mAh 1-cell LiPo batteries symmetrically placed at the back of left and right temples, which creates a more balanced design and reduced the nose weight from 78% (42g) to 56% (30g).", "label": "Author", "bboxes": [{"left": 0.19214869281045752, "top": 0.5115429292929293, "width": 0.2904918300653594, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5260896464646465, "width": 0.3946519607843137, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5406363636363637, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5551843434343434, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5697310606060606, "width": 0.25819281045751635, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1712"}, {"text": "Our prototypes weigh 54 g, which is below the suggested 75 g maximum weight for all-day use HWDs [38].", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.5842790404040404, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5988257575757575, "width": 0.31035457516339865, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1713"}, {"text": "We ran formative in-lab pilot studies with 14 participants in the U.S. ( New York, NY =6, Mountain View, CA =8), whose self-reported hearing loss ranged from moderate (41-70 dB) to profound (>95 dB).", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.5130517676767676, "width": 0.3945359477124182, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.5275997474747475, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.5421464646464647, "width": 0.3944215686274509, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.5566944444444444, "width": 0.1425196078431371, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1714"}, {"text": "Our plastic parts are 3D printed with a biocompatible material and use a multi-step finishing process.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.4169911616161616, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4315391414141414, "width": 0.30447549019607845, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1715"}, {"text": "To maximize material strength and robustness in our rapid prototyping for all-day use, we use SLS (selective laser sintering).", "label": "Author", "bboxes": [{"left": 0.3964330065359477, "top": 0.4315391414141414, "width": 0.08612418300653596, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.445780303030303, "width": 0.3944705882352941, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4603282828282828, "width": 0.3290294117647059, "height": 0.012727272727272754, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1716"}, {"text": "We are particularly interested in our proof-of-concepts potential to augment communication and perception for DHH individuals.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.8588093434343435, "width": 0.39456862745098037, "height": 0.012727272727272587, "page": 4}, {"left": 0.08823529411764706, "top": 0.8733573232323232, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 4}, {"left": 0.08823529411764706, "top": 0.8875984848484848, "width": 0.11745098039215686, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1717"}, {"text": "To improve the prototype iteratively, we", "label": "Author", "bboxes": [{"left": 0.21136928104575164, "top": 0.8875984848484848, "width": 0.2712205882352941, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1718"}, {"text": "To provide visual feedback to the wearer about when the prototype was connected to the phone, we implemented a real-time audio level meter in the lower right corner of the display (Figure 1).", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.8127487373737374, "width": 0.39430555555555546, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8272967171717173, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.8418434343434343, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8563914141414141, "width": 0.12137254901960781, "height": 0.01272727272727281, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1719"}, {"text": "A total of five DHH participants were recruited from our institution.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.5136578282828282, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5282058080808081, "width": 0.07041176470588235, "height": 0.012727272727272698, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1720"}, {"text": "We did not collect hearing loss data due to restrictions about data collection on participants from our institution.", "label": "Author", "bboxes": [{"left": 0.434843137254902, "top": 0.5427525252525253, "width": 0.04784640522875816, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5573005050505051, "width": 0.39437254901960783, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5718472222222223, "width": 0.3578398692810457, "height": 0.012727272727272698, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1721"}, {"text": "We received feedback that the eyewear temple overlapped with cochlear implants and all four cochlear implant users experienced discomfort.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.29305176767676766, "width": 0.39442156862745104, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.3075997474747475, "width": 0.39460294117647066, "height": 0.012727272727272754, "page": 5}, {"left": 0.5176470588235295, "top": 0.3221464646464646, "width": 0.1593856209150326, "height": 0.012727272727272754, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1722"}, {"text": "To gather feedback from in-the-wild scenarios, we conducted a study where participants could use Version 1 of our prototype over three days.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.44941540404040403, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.46396338383838387, "width": 0.39440522875816986, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.47851010101010105, "width": 0.19660947712418297, "height": 0.012727272727272698, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1723"}, {"text": "Study 1 identified several important issues that we focused on addressing before Study 2.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.7630517676767676, "width": 0.3944379084967321, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.7775997474747475, "width": 0.1946830065359476, "height": 0.012727272727272587, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1724"}, {"text": "Based on the feedback from Study 1, we adjusted the wrap and width of the nose bridges to provide a more central display location (See Figure 7, B1/B2).", "label": "Author", "bboxes": [{"left": 0.872078431372549, "top": 0.656415404040404, "width": 0.03999019607843146, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.6709621212121213, "width": 0.3944869281045751, "height": 0.012727272727272587, "page": 6}, {"left": 0.5176470588235295, "top": 0.685510101010101, "width": 0.394470588235294, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.7000568181818182, "width": 0.20528921568627445, "height": 0.012727272727272587, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1725"}, {"text": "Minimal in-lens displays with small eyeboxes, such as North Focals 1.0 and our prototype, require alignment of the optics to the users face and eye geometry.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.3651767676767677, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.3797247474747475, "width": 0.39460294117647055, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.3942714646464646, "width": 0.24409477124183, "height": 0.012727272727272754, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1726"}, {"text": "To offset the weight of the optics and the display engine in the front, which can lead to slipping and nose pressure, we redesigned the frame to place the batteries at the back of the temple (Figure 5, right), similar to other head-worn systems such as Vuzix Blade and Google Glass.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.09214267676767678, "width": 0.39445424836601306, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.10669065656565656, "width": 0.3942892156862744, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.12123737373737373, "width": 0.39432189542483653, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.13578535353535354, "width": 0.39422385620915046, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.15033207070707072, "width": 0.2668235294117647, "height": 0.012727272727272726, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1727"}, {"text": "With the revisions to the Version 1 prototype discussed in the previous section, we conducted an additional study to understand the physical and social comfort of Version 2 and associated software improvements.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.8297184343434343, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.8442664141414141, "width": 0.3945359477124182, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.8588131313131312, "width": 0.394470588235294, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.8733611111111111, "width": 0.22836437908496732, "height": 0.01272727272727281, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1728"}, {"text": "To accommodate different and/or asymmetric interpupillary distances (IPDs), we prototyped a more adaptable nose bridge and nose piece.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.2851729797979798, "width": 0.39447058823529413, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.29972095959595957, "width": 0.39435457516339856, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.31426767676767675, "width": 0.14919117647058822, "height": 0.012727272727272754, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1729"}, {"text": "We also investigated shifting the UI towards participants line-of-sight and display contrast improvements.", "label": "Author", "bboxes": [{"left": 0.6717450980392157, "top": 0.31426767676767675, "width": 0.2403725490196078, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.3288156565656566, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.3433623737373737, "width": 0.09783496732026142, "height": 0.012727272727272754, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1730"}, {"text": "We improved the RSSI from -70 to -40 dBm, which eliminated unwanted disconnects with the phone anywhere in an approximately 5 m radius (free space).", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.09366035353535354, "width": 0.39448692810457514, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.10820707070707071, "width": 0.3943055555555555, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.12275505050505049, "width": 0.2887957516339869, "height": 0.01272727272727274, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1731"}, {"text": "Interacting with our participants also highlighted the importance of smooth scrolling in order to mitigate eye fatigue.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.15790025252525253, "width": 0.39435457516339867, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.17244823232323234, "width": 0.39438888888888884, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.1869949494949495, "width": 0.049609477124183024, "height": 0.012727272727272754, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1732"}, {"text": "In Version 2 of the eyewear prototype, we minimized this abrupt motion by preserving line breaks for the high-confidence portion of the ASR result, such that only the text subject to modification as new audio is collected would reflow.", "label": "Author", "bboxes": [{"left": 0.20547222222222222, "top": 0.317614898989899, "width": 0.27706862745098043, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.33216287878787876, "width": 0.39445424836601306, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.34670959595959594, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.3609520202020202, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.37549873737373735, "width": 0.09508496732026141, "height": 0.012727272727272754, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1733"}, {"text": "We also implemented scrolling animations that would smoothly roll the text upwards as new lines are added, helping to guide the user's eye along as the transcript moves.", "label": "Author", "bboxes": [{"left": 0.19018790849673203, "top": 0.37549873737373735, "width": 0.29274836601307197, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.39004545454545453, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.4045934343434343, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.4191401515151515, "width": 0.046859477124182994, "height": 0.012727272727272698, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1734"}, {"text": "We generated 12 nose bridges, which we 3D-printed to cover interlens distances from 39 mm and 0, 5, and 15 wrap.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.7218712121212121, "width": 0.3943888888888889, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.7364191919191919, "width": 0.39420751633986917, "height": 0.012727272727272587, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1735"}, {"text": "We did not vary cyclo-rotation in these studies.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.7509659090909091, "width": 0.3181781045751634, "height": 0.01272727272727281, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1736"}, {"text": "Our fitting process takes about 10 minutes and captures IPD and head width, followed by selection of an optimal nose bridge.", "label": "Author", "bboxes": [{"left": 0.8415081699346406, "top": 0.7509659090909091, "width": 0.0705931372549019, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.7655138888888889, "width": 0.39442156862745104, "height": 0.012727272727272587, "page": 6}, {"left": 0.5176470588235295, "top": 0.780060606060606, "width": 0.3645098039215686, "height": 0.01272727272727281, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1737"}, {"text": "To enable personalization in a single frame design without the need for 3D scanning, we developed an interchangeable nose bridge, which provides independent adjustments of IPD, wrap, and cyclo-rotation.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.5324646464646464, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5470113636363636, "width": 0.39435457516339856, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5615593434343434, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5761060606060606, "width": 0.1989803921568628, "height": 0.012727272727272698, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1738"}, {"text": "Since our Study 1 participants mostly spent time at work, we decided to emphasize more varied scenarios in our continued experiments, including controlled environments with structured tasks.", "label": "Author", "bboxes": [{"left": 0.358781045751634, "top": 0.07911237373737373, "width": 0.12394117647058822, "height": 0.012727272727272726, "page": 7}, {"left": 0.08823529411764706, "top": 0.09366035353535354, "width": 0.39448692810457514, "height": 0.012727272727272726, "page": 7}, {"left": 0.08823529411764706, "top": 0.10820707070707071, "width": 0.39445424836601306, "height": 0.012727272727272726, "page": 7}, {"left": 0.08823529411764706, "top": 0.12275505050505049, "width": 0.39448692810457514, "height": 0.01272727272727274, "page": 7}, {"left": 0.08823529411764706, "top": 0.13730176767676766, "width": 0.10644934640522875, "height": 0.012727272727272726, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1739"}, {"text": "We wanted to collect feedback on the more central position of the transcriptions and the updated mechanical design in Version 2 of our prototype to see if it would increase perceived physical and social comfort, better accommodating different head sizes, IPDs, and face geometries.", "label": "Author", "bboxes": [{"left": 0.19880228758169935, "top": 0.13730176767676766, "width": 0.2837385620915033, "height": 0.012727272727272726, "page": 7}, {"left": 0.08823529411764706, "top": 0.1515441919191919, "width": 0.39447058823529413, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.1660909090909091, "width": 0.39438398692810456, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.18063762626262625, "width": 0.39438888888888884, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.19518560606060606, "width": 0.3943055555555555, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.20973232323232324, "width": 0.07586274509803921, "height": 0.012727272727272698, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1740"}, {"text": "Specifically, participants found our eyewear prototype to be more discreet than captions on a phone in a mobile context, and also felt that it allowed them to be more aware of their surroundings.", "label": "Author", "bboxes": [{"left": 0.6043316993464052, "top": 0.30214267676767675, "width": 0.3078676470588235, "height": 0.012727272727272754, "page": 8}, {"left": 0.5176470588235295, "top": 0.31669065656565654, "width": 0.39432189542483664, "height": 0.012727272727272754, "page": 8}, {"left": 0.5176470588235295, "top": 0.3312373737373737, "width": 0.3943725490196077, "height": 0.012727272727272754, "page": 8}, {"left": 0.5176470588235295, "top": 0.345479797979798, "width": 0.18605228758169923, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1741"}, {"text": "Our previous studies identified group conversations as an area of priority, and Study 2 results suggest that head-worn captions also bring benefits through better speaker awareness, which enabled better participation in the conversations.", "label": "Author", "bboxes": [{"left": 0.7086078431372549, "top": 0.345479797979798, "width": 0.20352614379084966, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.3600265151515152, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.37457449494949496, "width": 0.39450326797385604, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.38912121212121215, "width": 0.39442156862745104, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.4036679292929293, "width": 0.13510784313725488, "height": 0.012727272727272754, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1742"}, {"text": "Our power consumption modeling from device specifications maps well to measured values.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.7069911616161616, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.7215391414141413, "width": 0.2951029411764705, "height": 0.01272727272727281, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1743"}, {"text": "At full display brightness, we estimated 266 mW and measured 258 mW.", "label": "Author", "bboxes": [{"left": 0.8172630718954249, "top": 0.7215391414141413, "width": 0.09490359477124166, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.7360858585858586, "width": 0.3942401960784313, "height": 0.012727272727272587, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1744"}, {"text": "With a 360 mAh LiPo battery (3.7V) and 90% power efficiency, we estimate 4.5 hours in sunlight (100% display), 8 hours indoors (50% display), and approximately 15 hours in a dark room (5%) (Figure 10, right).", "label": "Author", "bboxes": [{"left": 0.6336993464052287, "top": 0.7506338383838383, "width": 0.2783856209150327, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.7651805555555556, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 8}, {"left": 0.5176470588235295, "top": 0.7794229797979798, "width": 0.394437908496732, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.7939696969696969, "width": 0.39427287581699344, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.8085164141414142, "width": 0.040583333333333305, "height": 0.012727272727272587, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1745"}, {"text": "Note that our LCOS display backlight accounts for >70% of the power consumption at maximum brightness.", "label": "Author", "bboxes": [{"left": 0.5647042483660131, "top": 0.8085164141414142, "width": 0.34744607843137243, "height": 0.012727272727272587, "page": 8}, {"left": 0.5176470588235295, "top": 0.8230643939393939, "width": 0.39427287581699344, "height": 0.01272727272727281, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1746"}, {"text": "More efficient displays would greatly extend our battery life.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.8376111111111112, "width": 0.39425653594771237, "height": 0.012727272727272587, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1747"}, {"text": "We use high-efficiency (>90%) switched-mode power", "label": "Author", "bboxes": [{"left": 0.5588235294117647, "top": 0.8812537878787878, "width": 0.3532450980392158, "height": 0.01272727272727281, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1748"}, {"text": "Study 2 incorporated both a mobile phone and our prototype eyewear in walking and multi-speaker interactions to bring further insights into the potential for a HWD for captions.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.6060820707070707, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.6206300505050505, "width": 0.39443790849673205, "height": 0.01272727272727281, "page": 8}, {"left": 0.08823529411764706, "top": 0.6351767676767677, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1749"}, {"text": "Our results align with the work from Jain et al.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.6497247474747475, "width": 0.3260359477124183, "height": 0.01272727272727281, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1750"}, {"text": "In this section, we evaluate the power/performance relationship of our proof-of-concept system across power, latency, and bandwidth metrics.", "label": "Author", "bboxes": [{"left": 0.7313692810457516, "top": 0.6427525252525252, "width": 0.18066666666666675, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176633986928104, "top": 0.6573005050505051, "width": 0.39437254901960805, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176633986928104, "top": 0.6718472222222222, "width": 0.35192647058823534, "height": 0.01272727272727281, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1751"}, {"text": "With these improvements and positive feedback, we are excited about opportunities to further validate the potential through quantitative methods for attention, as well as through more extended usage in continued diary studies.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.5348863636363637, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.5494343434343434, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.5639810606060606, "width": 0.3946290849673202, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.5785290404040404, "width": 0.3695669934640523, "height": 0.012727272727272698, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1752"}, {"text": "We thank our many collaborators for contributing their expertise in hearing accessibility, in particular Dimitri Kanevsky, who is continuously helping us improve this vision while evangelizing the DHH perspective, and also Chet Gnegy, Pascal Getreuer and Dick Lyon; software development, Eric Bouchard; optics, human factors and hardware, especially Omar Negrete, Kiet Tang, Ozan Cakmakci, David Hoffman and Ella Zhang; mobile perception, especially Eunyoung Kim, Jeff Gilbert and Alec Go; user experience and product thinking, Robin Dua, Chelsey Fleming, JD Velasquez, Jin Kim, Leo Szrejter, and many others.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.6727487373737374, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.6872967171717173, "width": 0.39441176470588224, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7018434343434343, "width": 0.3946356209150327, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.716391414141414, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7309381313131313, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7451805555555555, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7597272727272727, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7742739898989899, "width": 0.39432189542483653, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7888219696969696, "width": 0.3943725490196077, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.8033686868686869, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.8179166666666666, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.8324633838383839, "width": 0.09076960784313715, "height": 0.012727272727272587, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1753"}, {"text": "We thank Mathieu Le Goc for valuable discussions and suggestions, and our reviewers and study participants for their useful feedback and perspectives.", "label": "Author", "bboxes": [{"left": 0.6184133986928104, "top": 0.8324633838383839, "width": 0.2935392156862745, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.8470113636363636, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.8615580808080808, "width": 0.36602614379084963, "height": 0.012727272727272587, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1754"}, {"text": "We thank Mike Burns for additional footage and feedback.", "label": "Author", "bboxes": [{"left": 0.8890261437908495, "top": 0.8615580808080808, "width": 0.02300000000000002, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.8761060606060606, "width": 0.358549019607843, "height": 0.01272727272727281, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1755"}, {"text": "In this paper, we introduce Wearable Subtitles, a lightweight headworn prototype system for all-day hearing accessibility.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.3975972222222222, "width": 0.3943382352941175, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.412145202020202, "width": 0.39430555555555546, "height": 0.012727272727272754, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1756"}, {"text": "By integrating a real-time pipeline that uses cloud speech recognition technologies with our low-power embedded system and a near-eye display, we enable continuous speech transcription, translation, and sound awareness in the users private field-of-view.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.4266919191919192, "width": 0.3945359477124183, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.44123989898989896, "width": 0.394437908496732, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.45578661616161614, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.4703333333333334, "width": 0.3945866013071895, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.48488131313131316, "width": 0.13978594771241815, "height": 0.012727272727272698, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1757"}, {"text": "We describe our technical architecture and system evaluation which explain the strategies that enable up to 15 hours of active use, 54 g weight and compact electronics packaged into 3D-printed frames.", "label": "Author", "bboxes": [{"left": 0.6611552287581699, "top": 0.48488131313131316, "width": 0.2508137254901962, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.49942803030303035, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 9}, {"left": 0.5176470588235295, "top": 0.5139760101010101, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.5285227272727272, "width": 0.29470751633986925, "height": 0.01272727272727281, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1758"}, {"text": "Our recent offline privacy-preserving ASR implementation on the phone still results in a 3% longer battery life when combined with the eyewear, compared to cloud-computed transcription displayed on the phone.", "label": "Author", "bboxes": [{"left": 0.517640522875817, "top": 0.26098737373737374, "width": 0.39441993464052294, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2755353535353535, "width": 0.3945196078431372, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2900820707070707, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.3046300505050505, "width": 0.2703970588235294, "height": 0.012727272727272754, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1759"}, {"text": "We evaluated BLE power consumption under various usage scenarios.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.12880934343434344, "width": 0.3943545751633987, "height": 0.012727272727272726, "page": 9}, {"left": 0.08823529411764706, "top": 0.14335732323232322, "width": 0.06489379084967321, "height": 0.012727272727272726, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1760"}, {"text": "For latency-critical components (such as audio streaming), we do not perform this buffering.", "label": "Author", "bboxes": [{"left": 0.35678758169934643, "top": 0.21578787878787878, "width": 0.1257173202614379, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.23033459595959596, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.24488257575757577, "width": 0.09314052287581699, "height": 0.012727272727272698, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1761"}, {"text": "We evaluated peak transmit bandwidth using a dummy firmware image to keep the transmit queue full.", "label": "Author", "bboxes": [{"left": 0.18588888888888888, "top": 0.24488257575757577, "width": 0.2968660130718955, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.2594292929292929, "width": 0.3683970588235294, "height": 0.012727272727272754, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1762"}, {"text": "We achieve an efficiency of 2.2 KiB/mJ at a 645 kbps peak data rate (65% of the 1 Mbps PHY signaling rate).", "label": "Author", "bboxes": [{"left": 0.45958169934640525, "top": 0.2594292929292929, "width": 0.02299183006535943, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.27397727272727274, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.2885239898989899, "width": 0.32914869281045744, "height": 0.012727272727272754, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1763"}, {"text": "We also evaluated system latency by measuring the round-trip time from a test script to the firmware and back over BLE.", "label": "Author", "bboxes": [{"left": 0.425421568627451, "top": 0.2885239898989899, "width": 0.05726797385620913, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.3030719696969697, "width": 0.39437091503267974, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.3176186868686869, "width": 0.3600964052287582, "height": 0.012727272727272754, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1764"}, {"text": "This latency is largely controlled by the connection interval and slave latency of the BLE connection, which we configured to their minimum possible values (7.5 ms and 0 ms, respectively).", "label": "Author", "bboxes": [{"left": 0.45324019607843136, "top": 0.3176186868686869, "width": 0.02954738562091508, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.3321666666666667, "width": 0.39443790849673205, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.3467133838383838, "width": 0.39445424836601306, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.3609545454545454, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.3755025252525253, "width": 0.0888251633986928, "height": 0.012727272727272698, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1765"}, {"text": "We observed a mean latency of 21.0 ms with a standard deviation of 7.7 ms (N=200).", "label": "Author", "bboxes": [{"left": 0.18157352941176472, "top": 0.3755025252525253, "width": 0.3011650326797385, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.3900492424242424, "width": 0.2746633986928104, "height": 0.012727272727272754, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1766"}, {"text": "Our user studies included 24 DHH participants, 19 from outside our institution, and five from within.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.45426388888888886, "width": 0.3943218954248366, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.46881186868686864, "width": 0.30177450980392156, "height": 0.012727272727272754, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1767"}, {"text": "We recruited DHH individuals who used a breadth of communication methods and assistive technologies to include diverse perspectives.", "label": "Author", "bboxes": [{"left": 0.39569117647058827, "top": 0.46881186868686864, "width": 0.0870310457516339, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.4833585858585859, "width": 0.3944705882352941, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.49790656565656566, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.5124532828282828, "width": 0.08488888888888889, "height": 0.012727272727272698, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1768"}, {"text": "In future work, we would like to validate our technology with both an increased number of participants and extended usage time to facilitate statistical analysis.", "label": "Author", "bboxes": [{"left": 0.17880718954248367, "top": 0.5124532828282828, "width": 0.3038823529411765, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.527, "width": 0.3946356209150327, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.5415479797979798, "width": 0.3672450980392157, "height": 0.012727272727272698, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1769"}, {"text": "We plan to conduct more in-depth human factors experiments to quantify ergonomics and comfort over longer periods.", "label": "Author", "bboxes": [{"left": 0.45959803921568626, "top": 0.5415479797979798, "width": 0.022991830065359542, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.556094696969697, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.5706426767676768, "width": 0.35345915032679737, "height": 0.012727272727272698, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1770"}, {"text": "We have also enabled translation between different languages, which is another feature that could unlock benefits to a larger population.", "label": "Author", "bboxes": [{"left": 0.853343137254902, "top": 0.07915277777777778, "width": 0.0586846405228757, "height": 0.01272727272727274, "page": 9}, {"left": 0.517640522875817, "top": 0.09370075757575758, "width": 0.39441993464052294, "height": 0.012727272727272726, "page": 9}, {"left": 0.517640522875817, "top": 0.10824747474747475, "width": 0.39445424836601306, "height": 0.012727272727272726, "page": 9}, {"left": 0.517640522875817, "top": 0.12279545454545455, "width": 0.0743153594771242, "height": 0.01272727272727274, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1771"}, {"text": "We hope to explore display architectures without active backlights to further reduce power consumption.", "label": "Author", "bboxes": [{"left": 0.5949035947712419, "top": 0.12279545454545455, "width": 0.3170751633986927, "height": 0.01272727272727274, "page": 9}, {"left": 0.517640522875817, "top": 0.13734217171717172, "width": 0.3664199346405228, "height": 0.012727272727272726, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1772"}, {"text": "We also plan to quantify the impact on the phones battery life.", "label": "Author", "bboxes": [{"left": 0.888968954248366, "top": 0.13734217171717172, "width": 0.022993464052287638, "height": 0.012727272727272726, "page": 9}, {"left": 0.517640522875817, "top": 0.15158459595959597, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1773"}, {"text": "In future work, we are investigating whether a beam forming microphone on the HWD might help the user focus attention on one speaker when necessary by turning their head to that speaker.", "label": "Author", "bboxes": [{"left": 0.3634264705882353, "top": 0.6651931818181818, "width": 0.11903267973856207, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.6797411616161616, "width": 0.3943055555555555, "height": 0.012727272727272587, "page": 9}, {"left": 0.08823529411764706, "top": 0.6942878787878788, "width": 0.3944705882352941, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.708834595959596, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1774"}, {"text": "To validate our proposed approach for hearing accessibility, we conducted a pilot and two studies with 24 DHH participants who provided feedback on our prototypes in various scenarios and tasks.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.5503371212121212, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.564885101010101, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.5794318181818182, "width": 0.394437908496732, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.593979797979798, "width": 0.1836797385620914, "height": 0.012727272727272698, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1775"}, {"text": "Our user research suggests that HWDs could greatly improve hearing accessibility through privately transcribed text, which can be used hands-free, in mobile contexts, and in socially acceptable interactions.", "label": "Author", "bboxes": [{"left": 0.7062352941176471, "top": 0.593979797979798, "width": 0.2057009803921569, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.6085265151515151, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.623074494949495, "width": 0.39440686274509795, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.6376212121212121, "width": 0.36492156862745084, "height": 0.01272727272727281, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1776"}, {"text": "In future work, we are interested in evaluating text placement with our prototype system and strategies to improve legibility and comprehension [4, 5].", "label": "Author", "bboxes": [{"left": 0.4262287581699346, "top": 0.8033863636363637, "width": 0.05631209150326799, "height": 0.012727272727272587, "page": 9}, {"left": 0.08823529411764706, "top": 0.8179330808080808, "width": 0.39443790849673205, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.8324810606060606, "width": 0.3945196078431373, "height": 0.012727272727272587, "page": 9}, {"left": 0.08823529411764706, "top": 0.8470277777777778, "width": 0.14332679738562093, "height": 0.01272727272727281, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1777"}, {"text": "We have recently implemented sound awareness, inspired by suggestions from related work and our studies.", "label": "Author", "bboxes": [{"left": 0.23567973856209148, "top": 0.8470277777777778, "width": 0.24700980392156865, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.8615757575757577, "width": 0.39443790849673205, "height": 0.012727272727272587, "page": 9}, {"left": 0.08823529411764706, "top": 0.8761224747474747, "width": 0.07588071895424835, "height": 0.01272727272727281, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1778"}, {"text": "Our first diary study pilot participant, who used our device over four weeks, mentioned how useful it was to", "label": "Author", "bboxes": [{"left": 0.16862745098039217, "top": 0.8761224747474747, "width": 0.31409477124183, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.8906691919191919, "width": 0.39447058823529413, "height": 0.01272727272727281, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1779"}, {"text": "We launched a formative in-lab pilot study using our prototypes with external participants (outside of our institution) with a wide range of hearing loss.", "label": "Author", "bboxes": [{"left": 0.5250849673202614, "top": 0.11577904040404041, "width": 0.3870588235294117, "height": 0.012727272727272726, "page": 4}, {"left": 0.5470392156862746, "top": 0.13032575757575757, "width": 0.36505555555555547, "height": 0.012727272727272726, "page": 4}, {"left": 0.5470392156862746, "top": 0.14487373737373738, "width": 0.2970147058823529, "height": 0.012727272727272726, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1780"}, {"text": "We recruited a group of DHH participants from our institution for a usability study to use the prototype over three days and provide feedback on the contexts of use and the challenges encountered.", "label": "Author", "bboxes": [{"left": 0.5250849673202614, "top": 0.16306060606060607, "width": 0.3870098039215686, "height": 0.012727272727272698, "page": 4}, {"left": 0.5470392156862746, "top": 0.17760732323232323, "width": 0.36510457516339867, "height": 0.012727272727272754, "page": 4}, {"left": 0.5470392156862746, "top": 0.19215530303030304, "width": 0.36493954248366, "height": 0.012727272727272698, "page": 4}, {"left": 0.5470392156862746, "top": 0.2067020202020202, "width": 0.20879901960784308, "height": 0.012727272727272754, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1781"}, {"text": "Based on these studies, we incorporated many changes including software improvements, re-engineering the nose bridge, and placing the batteries behind the ears.", "label": "Author", "bboxes": [{"left": 0.5250849673202614, "top": 0.22489015151515152, "width": 0.387156862745098, "height": 0.012727272727272754, "page": 4}, {"left": 0.5470392156862746, "top": 0.23943686868686867, "width": 0.36492320261437894, "height": 0.012727272727272726, "page": 4}, {"left": 0.5470392156862746, "top": 0.2539848484848485, "width": 0.3648905228758169, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1782"}, {"text": "The major contributions of this work include", "label": "Contribution", "bboxes": [{"left": 0.08823529411764706, "top": 0.4197184343434343, "width": 0.2931764705882353, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1783"}, {"text": "CONTRIBUTIONS", "label": "Contribution", "bboxes": [{"left": 0.08823529411764706, "top": 0.4063800505050505, "width": 0.12288235294117648, "height": 0.011515151515151534, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1784"}, {"text": "While applications such as Live Transcribe [9] and Microsoft Translator [28] are available across desktop and mobile devices, they do not enable captioning within the line of sight of interlocutors [16].", "label": "Novelty", "bboxes": [{"left": 0.59725, "top": 0.5463888888888889, "width": 0.3148349673202615, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5609356060606061, "width": 0.394470588235294, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5754835858585858, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.590030303030303, "width": 0.22411601307189544, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1785"}, {"text": "While subtle prototype HWD eyeglasses have existed since 1997 [20, 37], the public a vailability of HWDs has increased interest in head-up captioning in the past decade.", "label": "Novelty", "bboxes": [{"left": 0.5493954248366013, "top": 0.23186868686868686, "width": 0.36164052287581694, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.24641540404040405, "width": 0.3908709150326798, "height": 0.012727272727272726, "page": 1}, {"left": 0.517640522875817, "top": 0.2609621212121212, "width": 0.3942565359477125, "height": 0.012727272727272754, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1786"}, {"text": "However, many people do not wish to call attention to their disability for fear of exploitation or discrimination [36].", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.8192133838383838, "width": 0.39446895424836603, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176470588235295, "top": 0.8337537878787878, "width": 0.3942434640522876, "height": 0.01272727272727281, "page": 1}], "section": "Social Acceptability of Wearable Assistive Devices", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1787"}, {"text": "While we acknowledge that not all deaf/hard-of-hearing (DHH) individuals want to use sound or captioning technologies, prior work has demonstrated that many people would find such technologies desirable and useful in everyday activities [16].", "label": "Novelty", "bboxes": [{"left": 0.08823529411764706, "top": 0.7672941919191919, "width": 0.39440522875816986, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.7818421717171717, "width": 0.3944705882352941, "height": 0.012727272727272587, "page": 1}, {"left": 0.08823529411764706, "top": 0.7963888888888889, "width": 0.3943055555555556, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.8109368686868687, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.8251780303030304, "width": 0.1660392156862745, "height": 0.012727272727272587, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1788"}, {"text": "While Glass can be worn comfortably for a full day (though battery life is <2h with display lit), it mounts the display high in the visual field, which is designed for short, glanceable interactions as opposed to extended reading.", "label": "Novelty", "bboxes": [{"left": 0.517640522875817, "top": 0.6391022727272727, "width": 0.394437908496732, "height": 0.01272727272727281, "page": 1}, {"left": 0.517640522875817, "top": 0.653614898989899, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 1}, {"left": 0.517640522875817, "top": 0.6681275252525253, "width": 0.3922990196078432, "height": 0.012727272727272587, "page": 1}, {"left": 0.517640522875817, "top": 0.6826401515151514, "width": 0.2939101307189542, "height": 0.01272727272727281, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1789"}, {"text": "[29] switched from Glass to the Epson BT-200 but discovered that its weight on the nose was too uncomfortable.", "label": "Novelty", "bboxes": [{"left": 0.7224509803921568, "top": 0.6971527777777777, "width": 0.1881715686274511, "height": 0.01272727272727281, "page": 1}, {"left": 0.517640522875817, "top": 0.711665404040404, "width": 0.39435457516339867, "height": 0.01272727272727281, "page": 1}, {"left": 0.517640522875817, "top": 0.7261780303030303, "width": 0.15652777777777782, "height": 0.012727272727272587, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1790"}, {"text": "Although early co-design sessions suggested that voice transcriptions could further social participation, the final usability ratings of the captioning designs were overshadowed by the limitations of the form factor and delay in the voice recognition software.", "label": "Novelty", "bboxes": [{"left": 0.846390522875817, "top": 0.3990795454545455, "width": 0.06196241830065363, "height": 0.012727272727272698, "page": 1}, {"left": 0.517640522875817, "top": 0.41360858585858584, "width": 0.39299836601307203, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.42813762626262625, "width": 0.3923496732026144, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.44266666666666665, "width": 0.38794607843137263, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.45719570707070706, "width": 0.38832352941176473, "height": 0.012727272727272698, "page": 1}, {"left": 0.517640522875817, "top": 0.47172474747474746, "width": 0.13820424836601308, "height": 0.012727272727272698, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1791"}, {"text": "Despite the benefits of HWDs for captioning, current commercially available solutions inhibit mobility and are not socially acceptable due to their poor fit or large form factor [16, 29].", "label": "Novelty", "bboxes": [{"left": 0.08821895424836601, "top": 0.1303409090909091, "width": 0.3945032679738562, "height": 0.012727272727272726, "page": 1}, {"left": 0.08821895424836601, "top": 0.14488888888888887, "width": 0.394421568627451, "height": 0.012727272727272726, "page": 1}, {"left": 0.08821895424836601, "top": 0.15913005050505052, "width": 0.3944542483660131, "height": 0.012727272727272698, "page": 1}, {"left": 0.08821895424836601, "top": 0.1736780303030303, "width": 0.108031045751634, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1792"}, {"text": "Thus, a major challenge is developing a HWD with interactions and human factors that are compatible with the social contexts of everyday life, while also minimizing visual dispersion.", "label": "Novelty", "bboxes": [{"left": 0.2034967320261438, "top": 0.1736780303030303, "width": 0.279062091503268, "height": 0.012727272727272754, "page": 1}, {"left": 0.08821895424836601, "top": 0.1882247474747475, "width": 0.3943398692810458, "height": 0.012727272727272698, "page": 1}, {"left": 0.08821895424836601, "top": 0.20277272727272727, "width": 0.3943218954248366, "height": 0.012727272727272698, "page": 1}, {"left": 0.08821895424836601, "top": 0.21731944444444443, "width": 0.24177287581699347, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1793"}, {"text": "While lack of noticeability does not necessarily mean improved social acceptance [22], it does reduce the social weight [40] with naive spectators.", "label": "Novelty", "bboxes": [{"left": 0.08823529411764706, "top": 0.13730176767676766, "width": 0.3944869281045751, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.1515441919191919, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 2}, {"left": 0.08823529411764706, "top": 0.1660909090909091, "width": 0.22217156862745097, "height": 0.012727272727272698, "page": 2}], "section": "Social Acceptability of Wearable Assistive Devices", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1794"}, {"text": "However, existing commercially available HWD systems were not designed to support these continuous usage scenarios as they run high level operating systems to support generic applications and drivers.", "label": "Novelty", "bboxes": [{"left": 0.08821895424836601, "top": 0.3542790404040404, "width": 0.39419117647058827, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.3685214646464647, "width": 0.39432189542483664, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.3830681818181818, "width": 0.3943218954248366, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.3976161616161616, "width": 0.21233823529411766, "height": 0.012727272727272754, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1795"}, {"text": "For these devices, speech recognition could be relegated to the smartphone and cloud as in our approach, but our informal experiments suggest that these systems only provide a few hours of battery life with the display on and otherwise idle.", "label": "Novelty", "bboxes": [{"left": 0.41720261437908496, "top": 0.4558042929292929, "width": 0.06535457516339865, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.47035227272727276, "width": 0.3943888888888889, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.48489898989898994, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.4994469696969697, "width": 0.3945686274509804, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.5139936868686868, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1796"}, {"text": "However, we acknowledge limitations in our survey.", "label": "Novelty", "bboxes": [{"left": 0.08823529411764706, "top": 0.7451906565656566, "width": 0.3675898692810457, "height": 0.012727272727272587, "page": 3}], "section": "Discussion and Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1797"}, {"text": "Participants positive social acceptability ratings (70%) of their existing technology may also indicate a favorable outlook towards assistive technologies, while there is an opportunity to also improve the solution for less satisfied participants (30%).", "label": "Novelty", "bboxes": [{"left": 0.2654918300653595, "top": 0.6651868686868686, "width": 0.21713235294117644, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.6797348484848486, "width": 0.39448692810457514, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.6942815656565656, "width": 0.3943545751633987, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.7088295454545455, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7233762626262626, "width": 0.31623529411764706, "height": 0.01272727272727281, "page": 3}], "section": "Discussion and Limitations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1798"}, {"text": "Overall, these scenarios represent moments of visual dispersion (e.g., multiple speakers, while playing video games ), low facial visibility (e.g., phone conversations, situations in rain/snow ), and ambient noise ( eating in public with background noise, conversations with children present ).", "label": "Novelty", "bboxes": [{"left": 0.26430555555555557, "top": 0.2594280303030303, "width": 0.2182679738562091, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.2739747474747475, "width": 0.3944052287581699, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.28852272727272726, "width": 0.3943545751633987, "height": 0.012727272727272754, "page": 3}, {"left": 0.08821895424836601, "top": 0.30306944444444445, "width": 0.3943709150326798, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.31761742424242423, "width": 0.39453594771241834, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.3321641414141414, "width": 0.1241062091503268, "height": 0.012727272727272698, "page": 3}], "section": "Survey Design", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1799"}, {"text": "These scenarios not only introduce communication challenges, but they can occur in a mobile context or while engaging in an activity with a dynamic environment.", "label": "Novelty", "bboxes": [{"left": 0.2238872549019608, "top": 0.3321641414141414, "width": 0.25865359477124183, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.3467121212121212, "width": 0.3943709150326798, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.3609532828282828, "width": 0.39433823529411766, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.3755, "width": 0.08684967320261439, "height": 0.012727272727272698, "page": 3}], "section": "Survey Design", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1800"}, {"text": "It is a monocular right-eyed display and is horizontally offset from the central vision for all users, though the position in the visual field varies depending on the mechanical fit for the individual user.", "label": "Novelty", "bboxes": [{"left": 0.43412091503267974, "top": 0.2945732323232323, "width": 0.04849999999999999, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.30911994949494953, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.32366666666666666, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.33821464646464644, "width": 0.39433823529411755, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.3527613636363636, "width": 0.10488398692810456, "height": 0.012727272727272698, "page": 4}], "section": "Display System", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1801"}, {"text": "Since the optical see-through display can only add light, we render the text in white to maximize contrast and visibility.", "label": "Novelty", "bboxes": [{"left": 0.19998856209150326, "top": 0.3527613636363636, "width": 0.282733660130719, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.3673093434343434, "width": 0.3943545751633987, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.3818560606060606, "width": 0.14568300653594768, "height": 0.012727272727272698, "page": 4}], "section": "Display System", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1802"}, {"text": "Participants were asked to wear the prototype throughout various pre-planned activities, which included single and multi-party conversations (locally and over video call), communication while working on manual tasks (card sorting, assembly tasks on a computer), watching a movie, and mingling with other participants in a happy hour.", "label": "Novelty", "bboxes": [{"left": 0.5176633986928104, "top": 0.6221502525252525, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176633986928104, "top": 0.6366969696969698, "width": 0.3944379084967321, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176633986928104, "top": 0.6512449494949494, "width": 0.3942565359477125, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176633986928104, "top": 0.6657916666666667, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 4}, {"left": 0.5176633986928104, "top": 0.6803396464646464, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176633986928104, "top": 0.6948863636363637, "width": 0.3452728758169935, "height": 0.012727272727272587, "page": 4}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1803"}, {"text": "Our first prototype frame included the board in the right temple while the battery was embedded in the left temple (Figure 5 left).", "label": "Novelty", "bboxes": [{"left": 0.08823529411764706, "top": 0.4824482323232323, "width": 0.394421568627451, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4969949494949495, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5115429292929293, "width": 0.09823039215686273, "height": 0.012727272727272698, "page": 4}], "section": "MECHANICAL DESIGN FOR RAPID PROTOTYPING", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1804"}, {"text": "Same weight, but two batteries behind the ears create a more balanced design (56% nose weight, 30 g).", "label": "Novelty", "bboxes": [{"left": 0.14823039215686273, "top": 0.7985530303030303, "width": 0.3344509803921569, "height": 0.011515151515151478, "page": 4}, {"left": 0.08823529411764706, "top": 0.8115883838383838, "width": 0.30408986928104575, "height": 0.011515151515151478, "page": 4}], "section": "MECHANICAL DESIGN FOR RAPID PROTOTYPING", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1805"}, {"text": "Participants were instructed to try different types of conversations including one-on-one and group conversations, conversing while a car passenger, and in situations with mixed signing and vocal communication.", "label": "Novelty", "bboxes": [{"left": 0.40542647058823533, "top": 0.693965909090909, "width": 0.07737745098039217, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.7085138888888889, "width": 0.39458660130718953, "height": 0.012727272727272587, "page": 5}, {"left": 0.08823529411764706, "top": 0.7230606060606061, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.7376073232323231, "width": 0.3945196078431373, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.752155303030303, "width": 0.18962581699346404, "height": 0.01272727272727281, "page": 5}], "section": "Task and Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1806"}, {"text": "Lastly, all users requested a UI closer to their central vision, in order to maintain better focus on the text while talking to peers, although some suggested that it may be a matter of adaptation: Getting more used to placement of screen. (P4) .", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.6118636363636364, "width": 0.39427287581699344, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.6264103535353536, "width": 0.3945359477124182, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.6409583333333333, "width": 0.394437908496732, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.6551994949494949, "width": 0.3946356209150327, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.6697474747474749, "width": 0.03313888888888883, "height": 0.012727272727272587, "page": 5}], "section": "User Experience", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1807"}, {"text": "The comments from participants signaled that the work was premature, but were encouraging and showed interest in the device: Good idea but execution needs some improvement. Please keep trying and happy to try again later! (P5).", "label": "Novelty", "bboxes": [{"left": 0.5545081699346405, "top": 0.6697474747474749, "width": 0.3576094771241831, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.6842941919191919, "width": 0.394437908496732, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.6988421717171717, "width": 0.3943725490196077, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.713388888888889, "width": 0.3944869281045751, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.7279368686868686, "width": 0.0829297385620914, "height": 0.01272727272727281, "page": 5}], "section": "User Experience", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1808"}, {"text": "For text visibility, two participants found that bright backgrounds diminished visibility, leading them to take the device off: I took it off [when] working on my computer. Too bright to read it even though I want[ed] to see what other [people] around my desk [were saying]. (P2) .", "label": "Novelty", "bboxes": [{"left": 0.7952745098039216, "top": 0.41548232323232326, "width": 0.11669444444444443, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.43002904040404044, "width": 0.394470588235294, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.44457575757575757, "width": 0.3943888888888889, "height": 0.012727272727272754, "page": 5}, {"left": 0.5176470588235295, "top": 0.45912373737373735, "width": 0.3945196078431372, "height": 0.012727272727272754, "page": 5}, {"left": 0.5176470588235295, "top": 0.473364898989899, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 5}, {"left": 0.5176470588235295, "top": 0.4879128787878788, "width": 0.14372222222222208, "height": 0.012727272727272698, "page": 5}], "section": "User Experience", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1809"}, {"text": "Thus, with different face geometries and small eyeboxes, mechanical adjustments are needed to align the pupil inside the eyebox.", "label": "Novelty", "bboxes": [{"left": 0.5780604575163399, "top": 0.43791287878787877, "width": 0.33395915032679724, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.45246085858585855, "width": 0.39435457516339856, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.4670075757575758, "width": 0.17743790849673202, "height": 0.012727272727272698, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1810"}, {"text": "To accommodate different and/or asymmetric interpupillary distances (IPDs), we prototyped a more adaptable nose bridge and nose piece.", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.2851729797979798, "width": 0.39447058823529413, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.29972095959595957, "width": 0.39435457516339856, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.31426767676767675, "width": 0.14919117647058822, "height": 0.012727272727272754, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1811"}, {"text": "We also investigated shifting the UI towards participants line-of-sight and display contrast improvements.", "label": "Novelty", "bboxes": [{"left": 0.6717450980392157, "top": 0.31426767676767675, "width": 0.2403725490196078, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.3288156565656566, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.3433623737373737, "width": 0.09783496732026142, "height": 0.012727272727272754, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1812"}, {"text": "We wanted to collect feedback on the more central position of the transcriptions and the updated mechanical design in Version 2 of our prototype to see if it would increase perceived physical and social comfort, better accommodating different head sizes, IPDs, and face geometries.", "label": "Novelty", "bboxes": [{"left": 0.19880228758169935, "top": 0.13730176767676766, "width": 0.2837385620915033, "height": 0.012727272727272726, "page": 7}, {"left": 0.08823529411764706, "top": 0.1515441919191919, "width": 0.39447058823529413, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.1660909090909091, "width": 0.39438398692810456, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.18063762626262625, "width": 0.39438888888888884, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.19518560606060606, "width": 0.3943055555555555, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.20973232323232324, "width": 0.07586274509803921, "height": 0.012727272727272698, "page": 7}], "section": "STUDY 2: MOBILE AND GROUP CONVERSATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1813"}, {"text": "An additional sixth participant was recruited for the study, but was excluded from analysis as they completed the tasks but not any of the feedback questionnaires.", "label": "Novelty", "bboxes": [{"left": 0.13372712418300653, "top": 0.2594179292929293, "width": 0.3488464052287581, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.27396464646464647, "width": 0.39432189542483653, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.28851262626262625, "width": 0.3189852941176471, "height": 0.012727272727272698, "page": 7}], "section": "Participants", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1814"}, {"text": "Participants rated the text rate on the phone while on-the-go slightly more positive (x  =4, IQR =1) compared to the neutral ratings for the eyewear (x  =3, IQR =1).", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.5842638888888889, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.5988118686868688, "width": 0.39432189542483653, "height": 0.014241161616161513, "page": 7}, {"left": 0.5176307189542484, "top": 0.6130530303030303, "width": 0.2656699346405228, "height": 0.014242424242424279, "page": 7}], "section": "Comprehension of Presented Contents", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1815"}, {"text": "Participants were, however, neutral about the text rate for the group activity in both conditions, and overall rated their ability to understand the spoken communication as Extremely well in both conditions (x  =5; IQR phone =0, IQR eyewear =1).", "label": "Novelty", "bboxes": [{"left": 0.7913382352941176, "top": 0.6130530303030303, "width": 0.12054901960784326, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176307189542484, "top": 0.6276010101010101, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176307189542484, "top": 0.6421477272727273, "width": 0.39447058823529413, "height": 0.012727272727272587, "page": 7}, {"left": 0.5176307189542484, "top": 0.656695707070707, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 7}, {"left": 0.5176307189542484, "top": 0.6712335858585858, "width": 0.2801960784313725, "height": 0.01425126262626264, "page": 7}], "section": "Comprehension of Presented Contents", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1816"}, {"text": "However, participants did express concern that the apparent use of transcriptions on a phone may be misinterpreted as ignoring other bystanders, which may happen if they needed to look away: With [this eyewear], I could just be a person getting support versus Im ignoring you and you dont know if I am reading my Facebook feed [on a mobile phone] (P1).", "label": "Novelty", "bboxes": [{"left": 0.6870147058823529, "top": 0.2654810606060606, "width": 0.22478921568627452, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176470588235295, "top": 0.28002904040404036, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176470588235295, "top": 0.2945757575757576, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.30912247474747473, "width": 0.3946683006535947, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176470588235295, "top": 0.3236704545454545, "width": 0.3942565359477125, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176470588235295, "top": 0.3382171717171717, "width": 0.3942892156862744, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.3527651515151515, "width": 0.2844967320261438, "height": 0.012727272727272754, "page": 7}], "section": "Social D ynamics: Discreetness and Awareness", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1817"}, {"text": "Participants ratings also suggest that the eyewear prototypes helped participants become more aware of their surroundings (x   eyewear =4 vs. x phone =3), and who was currently speaking (x   eyewear =4 vs. x phone =2) while using the eyewear, whereas awareness of body language was similar for both conditions (Figure 9, center).", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.374885101010101, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.3894318181818182, "width": 0.39460294117647066, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.4036578282828283, "width": 0.39441503267973854, "height": 0.014257575757575691, "page": 7}, {"left": 0.5176421568627452, "top": 0.4182032828282828, "width": 0.39459803921568626, "height": 0.014243686868686878, "page": 7}, {"left": 0.5176535947712417, "top": 0.4327512626262626, "width": 0.3945866013071897, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176535947712417, "top": 0.4472979797979798, "width": 0.12291993464052298, "height": 0.012727272727272698, "page": 7}], "section": "Social D ynamics: Discreetness and Awareness", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1818"}, {"text": "Participants explained that the eyewear kept them engaged in conversations:  The capacity to have other people talking and have me actually know whats going on, instead of me going off [of] other peoples body movements, that feels freeing I miss a lot of cues, a lot of laughs, but I miss a lot Im already discreet and faking [that I can hear], this gives me another tool in my pocket so that I can fake less (P3).", "label": "Novelty", "bboxes": [{"left": 0.647047385620915, "top": 0.4472979797979798, "width": 0.26506045751634, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176535947712417, "top": 0.4618459595959596, "width": 0.3943562091503269, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176535947712417, "top": 0.47639267676767677, "width": 0.3944869281045753, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176535947712417, "top": 0.49093939393939395, "width": 0.39437254901960783, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176535947712417, "top": 0.5054873737373737, "width": 0.39437254901960794, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176535947712417, "top": 0.5200340909090909, "width": 0.3945032679738564, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176535947712417, "top": 0.5345820707070708, "width": 0.3945032679738564, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176535947712417, "top": 0.5491287878787879, "width": 0.16803267973856206, "height": 0.012727272727272698, "page": 7}], "section": "Social D ynamics: Discreetness and Awareness", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1819"}, {"text": "More efficient displays would greatly extend our battery life.", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.8376111111111112, "width": 0.39425653594771237, "height": 0.012727272727272587, "page": 8}], "section": "System Power Consumption", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1820"}, {"text": "They felt that their field-of-view was clear while using the eyewear (x  =3, IQR =1) but had a neutral rating on the visibility of the captions (x  =3, IQR =2).", "label": "Novelty", "bboxes": [{"left": 0.3592254901960784, "top": 0.3312373737373737, "width": 0.12352941176470594, "height": 0.012727272727272754, "page": 8}, {"left": 0.0882516339869281, "top": 0.345479797979798, "width": 0.3943218954248366, "height": 0.014241161616161568, "page": 8}, {"left": 0.0882516339869281, "top": 0.3600265151515152, "width": 0.39433823529411766, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.37457449494949496, "width": 0.15194117647058825, "height": 0.014241161616161624, "page": 8}], "section": "Prototype Eyewear Experience and Desired Use", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1821"}, {"text": "For instance, P2 mentioned the desire for ongoing transcription throughout their work day,  I dont know the [range of the eyewear transcription] but like, if youre sitting over here and your boss is over there having a conversation, and you really wish you could be a part of that [conversation] but you cant hear it, [this tool could let you] know if you should get out of your seat and go be a part of it.", "label": "Novelty", "bboxes": [{"left": 0.27845424836601307, "top": 0.4694305555555555, "width": 0.20420261437908505, "height": 0.012727272727272754, "page": 8}, {"left": 0.0882516339869281, "top": 0.48397727272727276, "width": 0.3942565359477125, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.49852525252525254, "width": 0.3943562091503268, "height": 0.012727272727272754, "page": 8}, {"left": 0.0882516339869281, "top": 0.5130719696969697, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.5273143939393939, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 8}, {"left": 0.0882516339869281, "top": 0.5418611111111111, "width": 0.3943382352941177, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.5564090909090909, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.570955808080808, "width": 0.10249673202614377, "height": 0.012727272727272698, "page": 8}], "section": "Prototype Eyewear Experience and Desired Use", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1822"}, {"text": "We thank our many collaborators for contributing their expertise in hearing accessibility, in particular Dimitri Kanevsky, who is continuously helping us improve this vision while evangelizing the DHH perspective, and also Chet Gnegy, Pascal Getreuer and Dick Lyon; software development, Eric Bouchard; optics, human factors and hardware, especially Omar Negrete, Kiet Tang, Ozan Cakmakci, David Hoffman and Ella Zhang; mobile perception, especially Eunyoung Kim, Jeff Gilbert and Alec Go; user experience and product thinking, Robin Dua, Chelsey Fleming, JD Velasquez, Jin Kim, Leo Szrejter, and many others.", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.6727487373737374, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.6872967171717173, "width": 0.39441176470588224, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7018434343434343, "width": 0.3946356209150327, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.716391414141414, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7309381313131313, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7451805555555555, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7597272727272727, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7742739898989899, "width": 0.39432189542483653, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7888219696969696, "width": 0.3943725490196077, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.8033686868686869, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.8179166666666666, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.8324633838383839, "width": 0.09076960784313715, "height": 0.012727272727272587, "page": 9}], "section": "CONCLUSIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1823"}, {"text": "Future technical privacy opportunities, such as beamforming, could constrain microphone direction and distance to match human perception, while UI and industrial design could improve transparency and conformity to social norms.", "label": "Novelty", "bboxes": [{"left": 0.7992205882352941, "top": 0.3046300505050505, "width": 0.11282352941176488, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.31917676767676767, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.3334191919191919, "width": 0.3944869281045752, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.3479659090909091, "width": 0.39437091503267974, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.36251388888888886, "width": 0.29586111111111113, "height": 0.012727272727272698, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1824"}, {"text": "Figure 11 shows the energy efficiency of communication (KiB/mJ) for different transmission intervals and transmission sizes (block size).", "label": "Novelty", "bboxes": [{"left": 0.16587745098039214, "top": 0.14335732323232322, "width": 0.316812091503268, "height": 0.012727272727272726, "page": 9}, {"left": 0.08823529411764706, "top": 0.1579040404040404, "width": 0.3946029411764706, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.1724520202020202, "width": 0.23979575163398692, "height": 0.012727272727272754, "page": 9}], "section": "System Power Consumption", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1825"}, {"text": "We have also enabled translation between different languages, which is another feature that could unlock benefits to a larger population.", "label": "Novelty", "bboxes": [{"left": 0.853343137254902, "top": 0.07915277777777778, "width": 0.0586846405228757, "height": 0.01272727272727274, "page": 9}, {"left": 0.517640522875817, "top": 0.09370075757575758, "width": 0.39441993464052294, "height": 0.012727272727272726, "page": 9}, {"left": 0.517640522875817, "top": 0.10824747474747475, "width": 0.39445424836601306, "height": 0.012727272727272726, "page": 9}, {"left": 0.517640522875817, "top": 0.12279545454545455, "width": 0.0743153594771242, "height": 0.01272727272727274, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1826"}, {"text": "An informal experiment suggests that cloud-based transcription in the eyewear (phone in ambient display mode) could extend the battery by 62% (suggesting battery life on par with the eyewear), compared to only using the phone with its display at full brightness.", "label": "Novelty", "bboxes": [{"left": 0.517640522875817, "top": 0.16613131313131313, "width": 0.39445098039215687, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.18067929292929294, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.1952260101010101, "width": 0.39445424836601317, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.20977272727272725, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2243207070707071, "width": 0.21785620915032688, "height": 0.012727272727272698, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1827"}, {"text": "While providing each speaker with a wireless microphone and displaying each speakers captions in separate colors can improve the problem, the extra hardware is inconvenient to carry and keep charged.", "label": "Novelty", "bboxes": [{"left": 0.13489705882352943, "top": 0.6215517676767677, "width": 0.34752941176470586, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.6360984848484849, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.6506464646464647, "width": 0.3942401960784313, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.6651931818181818, "width": 0.27146895424836603, "height": 0.01272727272727281, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1828"}, {"text": "Combining this feature with an ambient microphone and different colors for the different conversational streams, might help disambiguate the attended speaker from others.", "label": "Novelty", "bboxes": [{"left": 0.08823529411764706, "top": 0.7233825757575757, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.7376237373737373, "width": 0.3942238562091503, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.7521717171717172, "width": 0.3828921568627451, "height": 0.012727272727272587, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1829"}, {"text": "The second usability study evaluated the improved prototype in more challenging scenarios, such as conversation while walking, interactions in a cafeteria, and a social setting that incorporated group dynamics, multi-party conversation and turn-taking.", "label": "Novelty", "bboxes": [{"left": 0.5470392156862746, "top": 0.26853156565656566, "width": 0.36507189542483653, "height": 0.012727272727272754, "page": 4}, {"left": 0.5470392156862746, "top": 0.28307828282828285, "width": 0.3651862745098039, "height": 0.012727272727272698, "page": 4}, {"left": 0.5470392156862746, "top": 0.2976262626262626, "width": 0.36487418300653596, "height": 0.012727272727272754, "page": 4}, {"left": 0.5470392156862746, "top": 0.3121729797979798, "width": 0.36487418300653596, "height": 0.012727272727272754, "page": 4}, {"left": 0.5470392156862746, "top": 0.3267209595959596, "width": 0.2695915032679739, "height": 0.012727272727272754, "page": 4}], "section": "USABILITY EVALUATIONS: THREE STUDIES WITH DEAF/HARD-OF-HEARING PARTICIPANTS", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1830"}, {"text": "We used Google Surveys [7] to deploy a short, ten-question survey.", "label": "Objective", "bboxes": [{"left": 0.5176470588235295, "top": 0.49803535353535355, "width": 0.39449509803921556, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5125782828282828, "width": 0.04764215686274509, "height": 0.012727272727272698, "page": 2}], "section": "Survey Design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1831"}, {"text": "The first four questions included the informed consent and background questions (assistive technology usage, gender, and communication preferences).", "label": "Objective", "bboxes": [{"left": 0.5752892156862746, "top": 0.5125782828282828, "width": 0.33681699346405214, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5271212121212121, "width": 0.3944820261437908, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5416641414141414, "width": 0.3362303921568627, "height": 0.012727272727272698, "page": 2}], "section": "Survey Design", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1832"}, {"text": "The remaining six questions asked about scenarios that participants experienced on a daily basis and if it was difficult to hear in any of those scenarios.", "label": "Objective", "bboxes": [{"left": 0.8148660130718954, "top": 0.5998358585858585, "width": 0.09725326797385625, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.6143787878787879, "width": 0.3946290849673202, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.6289217171717172, "width": 0.39446078431372544, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.6434646464646465, "width": 0.1574395424836601, "height": 0.012727272727272587, "page": 2}], "section": "Survey Design", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1833"}, {"text": "In the single-party conversations, the researcher asked the participant questions about themselves.", "label": "Objective", "bboxes": [{"left": 0.8709738562091502, "top": 0.6948863636363637, "width": 0.041027777777777774, "height": 0.012727272727272587, "page": 4}, {"left": 0.5176633986928104, "top": 0.7094343434343434, "width": 0.39432189542483653, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176633986928104, "top": 0.7239810606060606, "width": 0.2660490196078432, "height": 0.012727272727272587, "page": 4}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1834"}, {"text": "While we acknowledge that not all deaf/hard-of-hearing (DHH) individuals want to use sound or captioning technologies, prior work has demonstrated that many people would find such technologies desirable and useful in everyday activities [16].", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.7672941919191919, "width": 0.39440522875816986, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.7818421717171717, "width": 0.3944705882352941, "height": 0.012727272727272587, "page": 1}, {"left": 0.08823529411764706, "top": 0.7963888888888889, "width": 0.3943055555555556, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.8109368686868687, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.8251780303030304, "width": 0.1660392156862745, "height": 0.012727272727272587, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1835"}, {"text": "Our HWD prototype augments the users perception of speech and sounds in the environment (Figure 1).", "label": "Method", "bboxes": [{"left": 0.08821895424836601, "top": 0.2976287878787879, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 1}, {"left": 0.08821895424836601, "top": 0.31217550505050506, "width": 0.3327058823529412, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1836"}, {"text": "In the following sections, we review related work suggesting initial design requirements for this project.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.6097247474747475, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.6242714646464647, "width": 0.279406862745098, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1837"}, {"text": "The major contributions of this work include", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.4197184343434343, "width": 0.2931764705882353, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1838"}, {"text": "Here, we focus on a form factor that is virtually unnoticeable to bystanders.", "label": "Method", "bboxes": [{"left": 0.3646944444444445, "top": 0.10820707070707071, "width": 0.11797875816993464, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.12275505050505049, "width": 0.3942892156862745, "height": 0.01272727272727274, "page": 2}], "section": "Social Acceptability of Wearable Assistive Devices", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1839"}, {"text": "For these devices, speech recognition could be relegated to the smartphone and cloud as in our approach, but our informal experiments suggest that these systems only provide a few hours of battery life with the display on and otherwise idle.", "label": "Method", "bboxes": [{"left": 0.41720261437908496, "top": 0.4558042929292929, "width": 0.06535457516339865, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.47035227272727276, "width": 0.3943888888888889, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.48489898989898994, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.4994469696969697, "width": 0.3945686274509804, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.5139936868686868, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1840"}, {"text": "To learn more about challenges in mobile contexts, we conducted a brief large-scale online survey with participants who used hearing aids, TDD/TTY (telecommunications device for the deaf/teletypewriter) [13], CART [31], and cochlear implants.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.8039608585858585, "width": 0.3943218954248366, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8185088383838384, "width": 0.39453594771241834, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8330555555555555, "width": 0.3946192810457516, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8476035353535354, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.8621502525252525, "width": 0.11901633986928102, "height": 0.01272727272727281, "page": 2}], "section": "MOBILE SCENARIOS SURVEY: 501 RESPONDENTS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1841"}, {"text": "We used Google Surveys [7] to deploy a short, ten-question survey.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.49803535353535355, "width": 0.39449509803921556, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5125782828282828, "width": 0.04764215686274509, "height": 0.012727272727272698, "page": 2}], "section": "Survey Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1842"}, {"text": "We received 501 responses indicating usage of assistive technologies such as a hearing aid, cochlear implant, or realtime captioning/transcription services (i.e., CART).", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.815530303030303, "width": 0.3931895424836601, "height": 0.012626262626262652, "page": 2}, {"left": 0.5176470588235295, "top": 0.830050505050505, "width": 0.3926225490196078, "height": 0.012626262626262652, "page": 2}, {"left": 0.5176470588235295, "top": 0.844570707070707, "width": 0.379656862745098, "height": 0.012626262626262652, "page": 2}], "section": "Survey Design", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1843"}, {"text": "Our eyewear prototype weighs 54 g with 30 g on the nose.", "label": "Method", "bboxes": [{"left": 0.08821895424836601, "top": 0.725215909090909, "width": 0.3942075163398693, "height": 0.01272727272727281, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1844"}, {"text": "For graphics effects that require animation, such as our smoothly-scrolling transcript text, we implement interpolation primitives which are executed on the device to drive position and scale parameters of other primitives.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.6842601010101009, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.6988080808080809, "width": 0.394388888888889, "height": 0.012727272727272587, "page": 3}, {"left": 0.5176797385620915, "top": 0.713354797979798, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.7279027777777778, "width": 0.36096895424836595, "height": 0.012727272727272587, "page": 3}], "section": "Embedded System and Communication protocol", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1845"}, {"text": "However, we acknowledge limitations in our survey.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.7451906565656566, "width": 0.3675898692810457, "height": 0.012727272727272587, "page": 3}], "section": "Discussion and Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1846"}, {"text": "To overcome the limitations of existing platforms, we developed a hybrid approach adapted to our specific application that consists of a thin-client low-power eyewear prototype coupled with a mobile phone over a wireless connection (Figure 3).", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.27850631313131313, "width": 0.39435457516339867, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.2930542929292929, "width": 0.39430555555555546, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.3076010101010101, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.3221489898989899, "width": 0.3941911764705881, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.33669570707070706, "width": 0.15742647058823522, "height": 0.012727272727272698, "page": 3}], "section": "SYSTEM ARCHITECTURE AND IMPLEMENTATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1847"}, {"text": "Our large-scale survey enabled us to identify common mobile experiences, scenarios that frequently exacerbate hearing difficulties, and attitudes toward assistive technologies.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.5345669191919192, "width": 0.39405882352941174, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.549114898989899, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.5636616161616161, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.5779040404040404, "width": 0.08686601307189544, "height": 0.012727272727272698, "page": 3}], "section": "Discussion and Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1848"}, {"text": "Our prototype system uses a 1-lane MIPI-DSI compatible microdisplay engine with 30 fps graphics rendering.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.25093055555555555, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.2654785353535353, "width": 0.34133660130718957, "height": 0.012727272727272754, "page": 4}], "section": "Display System", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1849"}, {"text": "All studies used our eyewear prototype with transcribed speech sent wirelessly from an Android Pixel 3 phone.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.36183964646464645, "width": 0.3944379084967321, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.3760820707070707, "width": 0.3742941176470589, "height": 0.012727272727272698, "page": 4}], "section": "Prototype Apparatus and Fitting", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1850"}, {"text": "Our first prototype frame included the board in the right temple while the battery was embedded in the left temple (Figure 5 left).", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.4824482323232323, "width": 0.394421568627451, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4969949494949495, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5115429292929293, "width": 0.09823039215686273, "height": 0.012727272727272698, "page": 4}], "section": "MECHANICAL DESIGN FOR RAPID PROTOTYPING", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1851"}, {"text": "We ran formative in-lab pilot studies with 14 participants in the U.S. ( New York, NY =6, Mountain View, CA =8), whose self-reported hearing loss ranged from moderate (41-70 dB) to profound (>95 dB).", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.5130517676767676, "width": 0.3945359477124182, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.5275997474747475, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.5421464646464647, "width": 0.3944215686274509, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.5566944444444444, "width": 0.1425196078431371, "height": 0.01272727272727281, "page": 4}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1852"}, {"text": "Our plastic parts are 3D printed with a biocompatible material and use a multi-step finishing process.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.4169911616161616, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4315391414141414, "width": 0.30447549019607845, "height": 0.012727272727272698, "page": 4}], "section": "MECHANICAL DESIGN FOR RAPID PROTOTYPING", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1853"}, {"text": "We are particularly interested in our proof-of-concepts potential to augment communication and perception for DHH individuals.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.8588093434343435, "width": 0.39456862745098037, "height": 0.012727272727272587, "page": 4}, {"left": 0.08823529411764706, "top": 0.8733573232323232, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 4}, {"left": 0.08823529411764706, "top": 0.8875984848484848, "width": 0.11745098039215686, "height": 0.01272727272727281, "page": 4}], "section": "USABILITY EVALUATIONS: THREE STUDIES WITH DEAF/HARD-OF-HEARING PARTICIPANTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1854"}, {"text": "To provide visual feedback to the wearer about when the prototype was connected to the phone, we implemented a real-time audio level meter in the lower right corner of the display (Figure 1).", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.8127487373737374, "width": 0.39430555555555546, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8272967171717173, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.8418434343434343, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8563914141414141, "width": 0.12137254901960781, "height": 0.01272727272727281, "page": 5}], "section": "Connectivity and Pairing", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1855"}, {"text": "A total of five DHH participants were recruited from our institution.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.5136578282828282, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5282058080808081, "width": 0.07041176470588235, "height": 0.012727272727272698, "page": 5}], "section": "Participants", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1856"}, {"text": "We received feedback that the eyewear temple overlapped with cochlear implants and all four cochlear implant users experienced discomfort.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.29305176767676766, "width": 0.39442156862745104, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.3075997474747475, "width": 0.39460294117647066, "height": 0.012727272727272754, "page": 5}, {"left": 0.5176470588235295, "top": 0.3221464646464646, "width": 0.1593856209150326, "height": 0.012727272727272754, "page": 5}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1857"}, {"text": "To gather feedback from in-the-wild scenarios, we conducted a study where participants could use Version 1 of our prototype over three days.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.44941540404040403, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.46396338383838387, "width": 0.39440522875816986, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.47851010101010105, "width": 0.19660947712418297, "height": 0.012727272727272698, "page": 5}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1858"}, {"text": "Study 1 identified several important issues that we focused on addressing before Study 2.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.7630517676767676, "width": 0.3944379084967321, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.7775997474747475, "width": 0.1946830065359476, "height": 0.012727272727272587, "page": 5}], "section": "DISCUSSION AND DESIGN IMPROVEMENTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1859"}, {"text": "Based on the feedback from Study 1, we adjusted the wrap and width of the nose bridges to provide a more central display location (See Figure 7, B1/B2).", "label": "Method", "bboxes": [{"left": 0.872078431372549, "top": 0.656415404040404, "width": 0.03999019607843146, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.6709621212121213, "width": 0.3944869281045751, "height": 0.012727272727272587, "page": 6}, {"left": 0.5176470588235295, "top": 0.685510101010101, "width": 0.394470588235294, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.7000568181818182, "width": 0.20528921568627445, "height": 0.012727272727272587, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1860"}, {"text": "Minimal in-lens displays with small eyeboxes, such as North Focals 1.0 and our prototype, require alignment of the optics to the users face and eye geometry.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.3651767676767677, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.3797247474747475, "width": 0.39460294117647055, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.3942714646464646, "width": 0.24409477124183, "height": 0.012727272727272754, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1861"}, {"text": "To offset the weight of the optics and the display engine in the front, which can lead to slipping and nose pressure, we redesigned the frame to place the batteries at the back of the temple (Figure 5, right), similar to other head-worn systems such as Vuzix Blade and Google Glass.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.09214267676767678, "width": 0.39445424836601306, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.10669065656565656, "width": 0.3942892156862744, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.12123737373737373, "width": 0.39432189542483653, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.13578535353535354, "width": 0.39422385620915046, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.15033207070707072, "width": 0.2668235294117647, "height": 0.012727272727272726, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1862"}, {"text": "With the revisions to the Version 1 prototype discussed in the previous section, we conducted an additional study to understand the physical and social comfort of Version 2 and associated software improvements.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.8297184343434343, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.8442664141414141, "width": 0.3945359477124182, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.8588131313131312, "width": 0.394470588235294, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.8733611111111111, "width": 0.22836437908496732, "height": 0.01272727272727281, "page": 6}], "section": "STUDY 2: MOBILE AND GROUP CONVERSATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1863"}, {"text": "To accommodate different and/or asymmetric interpupillary distances (IPDs), we prototyped a more adaptable nose bridge and nose piece.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.2851729797979798, "width": 0.39447058823529413, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.29972095959595957, "width": 0.39435457516339856, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.31426767676767675, "width": 0.14919117647058822, "height": 0.012727272727272754, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1864"}, {"text": "We improved the RSSI from -70 to -40 dBm, which eliminated unwanted disconnects with the phone anywhere in an approximately 5 m radius (free space).", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.09366035353535354, "width": 0.39448692810457514, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.10820707070707071, "width": 0.3943055555555555, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.12275505050505049, "width": 0.2887957516339869, "height": 0.01272727272727274, "page": 6}], "section": "Connectivity and Pairing", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1865"}, {"text": "Interacting with our participants also highlighted the importance of smooth scrolling in order to mitigate eye fatigue.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.15790025252525253, "width": 0.39435457516339867, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.17244823232323234, "width": 0.39438888888888884, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.1869949494949495, "width": 0.049609477124183024, "height": 0.012727272727272754, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1866"}, {"text": "We generated 12 nose bridges, which we 3D-printed to cover interlens distances from 39 mm and 0, 5, and 15 wrap.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.7218712121212121, "width": 0.3943888888888889, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.7364191919191919, "width": 0.39420751633986917, "height": 0.012727272727272587, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1867"}, {"text": "To enable personalization in a single frame design without the need for 3D scanning, we developed an interchangeable nose bridge, which provides independent adjustments of IPD, wrap, and cyclo-rotation.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.5324646464646464, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5470113636363636, "width": 0.39435457516339856, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5615593434343434, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5761060606060606, "width": 0.1989803921568628, "height": 0.012727272727272698, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1868"}, {"text": "Since our Study 1 participants mostly spent time at work, we decided to emphasize more varied scenarios in our continued experiments, including controlled environments with structured tasks.", "label": "Method", "bboxes": [{"left": 0.358781045751634, "top": 0.07911237373737373, "width": 0.12394117647058822, "height": 0.012727272727272726, "page": 7}, {"left": 0.08823529411764706, "top": 0.09366035353535354, "width": 0.39448692810457514, "height": 0.012727272727272726, "page": 7}, {"left": 0.08823529411764706, "top": 0.10820707070707071, "width": 0.39445424836601306, "height": 0.012727272727272726, "page": 7}, {"left": 0.08823529411764706, "top": 0.12275505050505049, "width": 0.39448692810457514, "height": 0.01272727272727274, "page": 7}, {"left": 0.08823529411764706, "top": 0.13730176767676766, "width": 0.10644934640522875, "height": 0.012727272727272726, "page": 7}], "section": "STUDY 2: MOBILE AND GROUP CONVERSATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1869"}, {"text": "Specifically, participants found our eyewear prototype to be more discreet than captions on a phone in a mobile context, and also felt that it allowed them to be more aware of their surroundings.", "label": "Method", "bboxes": [{"left": 0.6043316993464052, "top": 0.30214267676767675, "width": 0.3078676470588235, "height": 0.012727272727272754, "page": 8}, {"left": 0.5176470588235295, "top": 0.31669065656565654, "width": 0.39432189542483664, "height": 0.012727272727272754, "page": 8}, {"left": 0.5176470588235295, "top": 0.3312373737373737, "width": 0.3943725490196077, "height": 0.012727272727272754, "page": 8}, {"left": 0.5176470588235295, "top": 0.345479797979798, "width": 0.18605228758169923, "height": 0.012727272727272698, "page": 8}], "section": "Discussion", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1870"}, {"text": "Our power consumption modeling from device specifications maps well to measured values.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.7069911616161616, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.7215391414141413, "width": 0.2951029411764705, "height": 0.01272727272727281, "page": 8}], "section": "System Power Consumption", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1871"}, {"text": "Study 2 incorporated both a mobile phone and our prototype eyewear in walking and multi-speaker interactions to bring further insights into the potential for a HWD for captions.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.6060820707070707, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.6206300505050505, "width": 0.39443790849673205, "height": 0.01272727272727281, "page": 8}, {"left": 0.08823529411764706, "top": 0.6351767676767677, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 8}], "section": "Discussion", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1872"}, {"text": "In this section, we evaluate the power/performance relationship of our proof-of-concept system across power, latency, and bandwidth metrics.", "label": "Method", "bboxes": [{"left": 0.7313692810457516, "top": 0.6427525252525252, "width": 0.18066666666666675, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176633986928104, "top": 0.6573005050505051, "width": 0.39437254901960805, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176633986928104, "top": 0.6718472222222222, "width": 0.35192647058823534, "height": 0.01272727272727281, "page": 8}], "section": "TECHNICAL EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1873"}, {"text": "With these improvements and positive feedback, we are excited about opportunities to further validate the potential through quantitative methods for attention, as well as through more extended usage in continued diary studies.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.5348863636363637, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.5494343434343434, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.5639810606060606, "width": 0.3946290849673202, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.5785290404040404, "width": 0.3695669934640523, "height": 0.012727272727272698, "page": 8}], "section": "Discussion", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1874"}, {"text": "We thank our many collaborators for contributing their expertise in hearing accessibility, in particular Dimitri Kanevsky, who is continuously helping us improve this vision while evangelizing the DHH perspective, and also Chet Gnegy, Pascal Getreuer and Dick Lyon; software development, Eric Bouchard; optics, human factors and hardware, especially Omar Negrete, Kiet Tang, Ozan Cakmakci, David Hoffman and Ella Zhang; mobile perception, especially Eunyoung Kim, Jeff Gilbert and Alec Go; user experience and product thinking, Robin Dua, Chelsey Fleming, JD Velasquez, Jin Kim, Leo Szrejter, and many others.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.6727487373737374, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.6872967171717173, "width": 0.39441176470588224, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7018434343434343, "width": 0.3946356209150327, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.716391414141414, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7309381313131313, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7451805555555555, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7597272727272727, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7742739898989899, "width": 0.39432189542483653, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7888219696969696, "width": 0.3943725490196077, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.8033686868686869, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.8179166666666666, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.8324633838383839, "width": 0.09076960784313715, "height": 0.012727272727272587, "page": 9}], "section": "CONCLUSIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1875"}, {"text": "In this paper, we introduce Wearable Subtitles, a lightweight headworn prototype system for all-day hearing accessibility.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.3975972222222222, "width": 0.3943382352941175, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.412145202020202, "width": 0.39430555555555546, "height": 0.012727272727272754, "page": 9}], "section": "CONCLUSIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1876"}, {"text": "Our recent offline privacy-preserving ASR implementation on the phone still results in a 3% longer battery life when combined with the eyewear, compared to cloud-computed transcription displayed on the phone.", "label": "Method", "bboxes": [{"left": 0.517640522875817, "top": 0.26098737373737374, "width": 0.39441993464052294, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2755353535353535, "width": 0.3945196078431372, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2900820707070707, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.3046300505050505, "width": 0.2703970588235294, "height": 0.012727272727272754, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1877"}, {"text": "We evaluated BLE power consumption under various usage scenarios.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.12880934343434344, "width": 0.3943545751633987, "height": 0.012727272727272726, "page": 9}, {"left": 0.08823529411764706, "top": 0.14335732323232322, "width": 0.06489379084967321, "height": 0.012727272727272726, "page": 9}], "section": "System Power Consumption", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1878"}, {"text": "Our user studies included 24 DHH participants, 19 from outside our institution, and five from within.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.45426388888888886, "width": 0.3943218954248366, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.46881186868686864, "width": 0.30177450980392156, "height": 0.012727272727272754, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1879"}, {"text": "We have also enabled translation between different languages, which is another feature that could unlock benefits to a larger population.", "label": "Method", "bboxes": [{"left": 0.853343137254902, "top": 0.07915277777777778, "width": 0.0586846405228757, "height": 0.01272727272727274, "page": 9}, {"left": 0.517640522875817, "top": 0.09370075757575758, "width": 0.39441993464052294, "height": 0.012727272727272726, "page": 9}, {"left": 0.517640522875817, "top": 0.10824747474747475, "width": 0.39445424836601306, "height": 0.012727272727272726, "page": 9}, {"left": 0.517640522875817, "top": 0.12279545454545455, "width": 0.0743153594771242, "height": 0.01272727272727274, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1880"}, {"text": "In future work, we are investigating whether a beam forming microphone on the HWD might help the user focus attention on one speaker when necessary by turning their head to that speaker.", "label": "Method", "bboxes": [{"left": 0.3634264705882353, "top": 0.6651931818181818, "width": 0.11903267973856207, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.6797411616161616, "width": 0.3943055555555555, "height": 0.012727272727272587, "page": 9}, {"left": 0.08823529411764706, "top": 0.6942878787878788, "width": 0.3944705882352941, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.708834595959596, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1881"}, {"text": "To validate our proposed approach for hearing accessibility, we conducted a pilot and two studies with 24 DHH participants who provided feedback on our prototypes in various scenarios and tasks.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.5503371212121212, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.564885101010101, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.5794318181818182, "width": 0.394437908496732, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.593979797979798, "width": 0.1836797385620914, "height": 0.012727272727272698, "page": 9}], "section": "CONCLUSIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1882"}, {"text": "In future work, we are interested in evaluating text placement with our prototype system and strategies to improve legibility and comprehension [4, 5].", "label": "Method", "bboxes": [{"left": 0.4262287581699346, "top": 0.8033863636363637, "width": 0.05631209150326799, "height": 0.012727272727272587, "page": 9}, {"left": 0.08823529411764706, "top": 0.8179330808080808, "width": 0.39443790849673205, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.8324810606060606, "width": 0.3945196078431373, "height": 0.012727272727272587, "page": 9}, {"left": 0.08823529411764706, "top": 0.8470277777777778, "width": 0.14332679738562093, "height": 0.01272727272727281, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1883"}, {"text": "We launched a formative in-lab pilot study using our prototypes with external participants (outside of our institution) with a wide range of hearing loss.", "label": "Method", "bboxes": [{"left": 0.5250849673202614, "top": 0.11577904040404041, "width": 0.3870588235294117, "height": 0.012727272727272726, "page": 4}, {"left": 0.5470392156862746, "top": 0.13032575757575757, "width": 0.36505555555555547, "height": 0.012727272727272726, "page": 4}, {"left": 0.5470392156862746, "top": 0.14487373737373738, "width": 0.2970147058823529, "height": 0.012727272727272726, "page": 4}], "section": "USABILITY EVALUATIONS: THREE STUDIES WITH DEAF/HARD-OF-HEARING PARTICIPANTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1884"}, {"text": "While we acknowledge that not all deaf/hard-of-hearing (DHH) individuals want to use sound or captioning technologies, prior work has demonstrated that many people would find such technologies desirable and useful in everyday activities [16].", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.7672941919191919, "width": 0.39440522875816986, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.7818421717171717, "width": 0.3944705882352941, "height": 0.012727272727272587, "page": 1}, {"left": 0.08823529411764706, "top": 0.7963888888888889, "width": 0.3943055555555556, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.8109368686868687, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.8251780303030304, "width": 0.1660392156862745, "height": 0.012727272727272587, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1885"}, {"text": "For the happy hour, participants were given a card which directed them to find out information about other participants and start a conversation (e.g., what is your secret talent?).", "label": "Result", "bboxes": [{"left": 0.7054771241830066, "top": 0.752770202020202, "width": 0.20677287581699344, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176633986928104, "top": 0.7673181818181818, "width": 0.3943055555555556, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176633986928104, "top": 0.781864898989899, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176633986928104, "top": 0.7964116161616163, "width": 0.23151143790849682, "height": 0.012727272727272587, "page": 4}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1886"}, {"text": "For example, eyeglasses may be more desirable than hearing aids due to the perception that hearing", "label": "Conclusion", "bboxes": [{"left": 0.6529248366013072, "top": 0.877375, "width": 0.25906699346405226, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176454248366013, "top": 0.8919154040404041, "width": 0.39445588235294127, "height": 0.012727272727272587, "page": 1}], "section": "Social Acceptability of Wearable Assistive Devices", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1887"}, {"text": "We conclude with current limitations and future avenues for this work.", "label": "Conclusion", "bboxes": [{"left": 0.08823529411764706, "top": 0.6970075757575758, "width": 0.3944052287581699, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.7115555555555556, "width": 0.0656846405228758, "height": 0.012727272727272587, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": 1, "is_author_statement": true, "is_in_expected_section": false, "id": "1888"}, {"text": "Academic evaluations of HWDs suggest that commercially available solutions do not have a suitable form factor for sustained captioning.", "label": "Conclusion", "bboxes": [{"left": 0.517640522875817, "top": 0.34096338383838387, "width": 0.39322058823529416, "height": 0.012727272727272698, "page": 1}, {"left": 0.517640522875817, "top": 0.3554924242424243, "width": 0.39202614379084966, "height": 0.012727272727272698, "page": 1}, {"left": 0.517640522875817, "top": 0.3700214646464647, "width": 0.1383398692810457, "height": 0.012727272727272698, "page": 1}], "section": "RELATED WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1889"}, {"text": "Although early co-design sessions suggested that voice transcriptions could further social participation, the final usability ratings of the captioning designs were overshadowed by the limitations of the form factor and delay in the voice recognition software.", "label": "Conclusion", "bboxes": [{"left": 0.846390522875817, "top": 0.3990795454545455, "width": 0.06196241830065363, "height": 0.012727272727272698, "page": 1}, {"left": 0.517640522875817, "top": 0.41360858585858584, "width": 0.39299836601307203, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.42813762626262625, "width": 0.3923496732026144, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.44266666666666665, "width": 0.38794607843137263, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.45719570707070706, "width": 0.38832352941176473, "height": 0.012727272727272698, "page": 1}, {"left": 0.517640522875817, "top": 0.47172474747474746, "width": 0.13820424836601308, "height": 0.012727272727272698, "page": 1}], "section": "RELATED WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1890"}, {"text": "Smartphone presence may lessen conversational quality and reduce empathic exchange [30].", "label": "Conclusion", "bboxes": [{"left": 0.08821895424836601, "top": 0.09367424242424242, "width": 0.39443790849673205, "height": 0.012727272727272726, "page": 1}, {"left": 0.08821895424836601, "top": 0.10822095959595959, "width": 0.20879738562091504, "height": 0.012727272727272726, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1891"}, {"text": "Thus, a major challenge is developing a HWD with interactions and human factors that are compatible with the social contexts of everyday life, while also minimizing visual dispersion.", "label": "Conclusion", "bboxes": [{"left": 0.2034967320261438, "top": 0.1736780303030303, "width": 0.279062091503268, "height": 0.012727272727272754, "page": 1}, {"left": 0.08821895424836601, "top": 0.1882247474747475, "width": 0.3943398692810458, "height": 0.012727272727272698, "page": 1}, {"left": 0.08821895424836601, "top": 0.20277272727272727, "width": 0.3943218954248366, "height": 0.012727272727272698, "page": 1}, {"left": 0.08821895424836601, "top": 0.21731944444444443, "width": 0.24177287581699347, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1892"}, {"text": "Thus, assistive technology that resembles mainstream devices is more accepted, especially by those with invisible disabilities [36].", "label": "Conclusion", "bboxes": [{"left": 0.2815343137254902, "top": 0.07911237373737373, "width": 0.2010228758169934, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.09366035353535354, "width": 0.3944705882352941, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.10820707070707071, "width": 0.27234150326797385, "height": 0.012727272727272726, "page": 2}], "section": "Social Acceptability of Wearable Assistive Devices", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1893"}, {"text": "However, existing commercially available HWD systems were not designed to support these continuous usage scenarios as they run high level operating systems to support generic applications and drivers.", "label": "Conclusion", "bboxes": [{"left": 0.08821895424836601, "top": 0.3542790404040404, "width": 0.39419117647058827, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.3685214646464647, "width": 0.39432189542483664, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.3830681818181818, "width": 0.3943218954248366, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.3976161616161616, "width": 0.21233823529411766, "height": 0.012727272727272754, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1894"}, {"text": "Current lightweight HWDs thus use power-intensive mobile processors that are intended for intermittent engagement, such as Snapdragon XR1 (Google Glass EE2), quad-core Cortex-A53 (Vuzix Blade), and quad-core Cortex-A7 (North Focals 1.0).", "label": "Conclusion", "bboxes": [{"left": 0.30467483660130723, "top": 0.3976161616161616, "width": 0.17814705882352944, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.4121628787878788, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.42671085858585855, "width": 0.39447058823529413, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.44125757575757574, "width": 0.39425653594771237, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.4558042929292929, "width": 0.32015522875817, "height": 0.012727272727272698, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1895"}, {"text": "For these devices, speech recognition could be relegated to the smartphone and cloud as in our approach, but our informal experiments suggest that these systems only provide a few hours of battery life with the display on and otherwise idle.", "label": "Conclusion", "bboxes": [{"left": 0.41720261437908496, "top": 0.4558042929292929, "width": 0.06535457516339865, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.47035227272727276, "width": 0.3943888888888889, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.48489898989898994, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.4994469696969697, "width": 0.3945686274509804, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.5139936868686868, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1896"}, {"text": "Due to restrictions from our institution, we did not ask participants to self-report their hearing levels or inquire about the use of signed languages for this survey, which may have resulted in an underrepresentation of Deaf participants.", "label": "Conclusion", "bboxes": [{"left": 0.8630931372549019, "top": 0.5416641414141414, "width": 0.0490245098039217, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5562070707070708, "width": 0.39459313725490197, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.57075, "width": 0.3944428104575163, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5852929292929293, "width": 0.3943349673202614, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5998358585858585, "width": 0.2919248366013072, "height": 0.012727272727272698, "page": 2}], "section": "Survey Design", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1897"}, {"text": "The weights of the lightest devices with a discrete, line-of-sight display, Vuzix Blade (97 g; 56 g nose weight) and Focals 1.0 (69 g; 49 g nose weight) illustrate the challenges of meeting the suggested 75 g weight with a maximum 40% nose weight (30 g) [38].", "label": "Conclusion", "bboxes": [{"left": 0.24742320261437908, "top": 0.5794507575757576, "width": 0.2354150326797386, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.5939974747474748, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.6085454545454546, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.6230921717171717, "width": 0.3943888888888889, "height": 0.01272727272727281, "page": 2}, {"left": 0.08821895424836601, "top": 0.6376388888888889, "width": 0.3064673202614379, "height": 0.01272727272727281, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1898"}, {"text": "By asking only about assistive technology to identify DHH users, as per our institutions allowance, Deaf participants who are less likely to use assistive technologies may have been underrepresented.", "label": "Conclusion", "bboxes": [{"left": 0.463468954248366, "top": 0.7451906565656566, "width": 0.019220588235294156, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7597386363636364, "width": 0.3946356209150327, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.7742853535353535, "width": 0.3945866013071896, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7888333333333333, "width": 0.39438888888888884, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.8033800505050506, "width": 0.1519084967320261, "height": 0.012727272727272587, "page": 3}], "section": "Discussion and Limitations", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "1899"}, {"text": "Participants positive social acceptability ratings (70%) of their existing technology may also indicate a favorable outlook towards assistive technologies, while there is an opportunity to also improve the solution for less satisfied participants (30%).", "label": "Conclusion", "bboxes": [{"left": 0.2654918300653595, "top": 0.6651868686868686, "width": 0.21713235294117644, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.6797348484848486, "width": 0.39448692810457514, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.6942815656565656, "width": 0.3943545751633987, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.7088295454545455, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7233762626262626, "width": 0.31623529411764706, "height": 0.01272727272727281, "page": 3}], "section": "Discussion and Limitations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1900"}, {"text": "The frequency with which participants experience these scenarios and have difficulties with them suggest that they may serve as realistic opportunities for mobile user evaluations.", "label": "Conclusion", "bboxes": [{"left": 0.18938071895424835, "top": 0.3755, "width": 0.2933905228758171, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.3900479797979798, "width": 0.3943218954248366, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.404594696969697, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.4191426767676768, "width": 0.15943464052287581, "height": 0.012727272727272698, "page": 3}], "section": "Survey Design", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1901"}, {"text": "At the end of each day, participants completed a questionnaire detailing their usage time, contexts where they used the prototype, and feedback on how it could be improved.", "label": "Conclusion", "bboxes": [{"left": 0.2902140522875817, "top": 0.752155303030303, "width": 0.19234313725490199, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.7667020202020202, "width": 0.3944052287581699, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.78125, "width": 0.3945196078431373, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.7957967171717171, "width": 0.19272222222222224, "height": 0.01272727272727281, "page": 5}], "section": "Task and Procedure", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1902"}, {"text": "Participants suggested that testing in group conversations and more challenging auditory environments could be helpful to fully understand the benefits of the HWD.", "label": "Conclusion", "bboxes": [{"left": 0.08823529411764706, "top": 0.3127487373737374, "width": 0.3942565359477125, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.32729671717171716, "width": 0.39437254901960783, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.34184343434343434, "width": 0.36487254901960786, "height": 0.012727272727272698, "page": 5}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1903"}, {"text": "To gather feedback from in-the-wild scenarios, we conducted a study where participants could use Version 1 of our prototype over three days.", "label": "Conclusion", "bboxes": [{"left": 0.08823529411764706, "top": 0.44941540404040403, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.46396338383838387, "width": 0.39440522875816986, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.47851010101010105, "width": 0.19660947712418297, "height": 0.012727272727272698, "page": 5}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1904"}, {"text": "Lastly, all users requested a UI closer to their central vision, in order to maintain better focus on the text while talking to peers, although some suggested that it may be a matter of adaptation: Getting more used to placement of screen. (P4) .", "label": "Conclusion", "bboxes": [{"left": 0.5176470588235295, "top": 0.6118636363636364, "width": 0.39427287581699344, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.6264103535353536, "width": 0.3945359477124182, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.6409583333333333, "width": 0.394437908496732, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.6551994949494949, "width": 0.3946356209150327, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.6697474747474749, "width": 0.03313888888888883, "height": 0.012727272727272587, "page": 5}], "section": "User Experience", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1905"}, {"text": "Thus, with different face geometries and small eyeboxes, mechanical adjustments are needed to align the pupil inside the eyebox.", "label": "Conclusion", "bboxes": [{"left": 0.5780604575163399, "top": 0.43791287878787877, "width": 0.33395915032679724, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.45246085858585855, "width": 0.39435457516339856, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.4670075757575758, "width": 0.17743790849673202, "height": 0.012727272727272698, "page": 6}], "section": "Display of Transcriptions", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1906"}, {"text": "However, participants did express concern that the apparent use of transcriptions on a phone may be misinterpreted as ignoring other bystanders, which may happen if they needed to look away: With [this eyewear], I could just be a person getting support versus Im ignoring you and you dont know if I am reading my Facebook feed [on a mobile phone] (P1).", "label": "Conclusion", "bboxes": [{"left": 0.6870147058823529, "top": 0.2654810606060606, "width": 0.22478921568627452, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176470588235295, "top": 0.28002904040404036, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176470588235295, "top": 0.2945757575757576, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.30912247474747473, "width": 0.3946683006535947, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176470588235295, "top": 0.3236704545454545, "width": 0.3942565359477125, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176470588235295, "top": 0.3382171717171717, "width": 0.3942892156862744, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.3527651515151515, "width": 0.2844967320261438, "height": 0.012727272727272754, "page": 7}], "section": "Social D ynamics: Discreetness and Awareness", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1907"}, {"text": "Participants ratings also suggest that the eyewear prototypes helped participants become more aware of their surroundings (x   eyewear =4 vs. x phone =3), and who was currently speaking (x   eyewear =4 vs. x phone =2) while using the eyewear, whereas awareness of body language was similar for both conditions (Figure 9, center).", "label": "Conclusion", "bboxes": [{"left": 0.5176470588235295, "top": 0.374885101010101, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.3894318181818182, "width": 0.39460294117647066, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.4036578282828283, "width": 0.39441503267973854, "height": 0.014257575757575691, "page": 7}, {"left": 0.5176421568627452, "top": 0.4182032828282828, "width": 0.39459803921568626, "height": 0.014243686868686878, "page": 7}, {"left": 0.5176535947712417, "top": 0.4327512626262626, "width": 0.3945866013071897, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176535947712417, "top": 0.4472979797979798, "width": 0.12291993464052298, "height": 0.012727272727272698, "page": 7}], "section": "Social D ynamics: Discreetness and Awareness", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1908"}, {"text": "Our previous studies identified group conversations as an area of priority, and Study 2 results suggest that head-worn captions also bring benefits through better speaker awareness, which enabled better participation in the conversations.", "label": "Conclusion", "bboxes": [{"left": 0.7086078431372549, "top": 0.345479797979798, "width": 0.20352614379084966, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.3600265151515152, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.37457449494949496, "width": 0.39450326797385604, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.38912121212121215, "width": 0.39442156862745104, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.4036679292929293, "width": 0.13510784313725488, "height": 0.012727272727272754, "page": 8}], "section": "Discussion", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "1909"}, {"text": "These preliminary results also suggest that Version 2 of the prototype addresses some of the main technical challenges identified in the previous studies.", "label": "Conclusion", "bboxes": [{"left": 0.5176470588235295, "top": 0.42578787878787877, "width": 0.39435457516339856, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.44033585858585855, "width": 0.3946356209150327, "height": 0.012727272727272754, "page": 8}, {"left": 0.5176470588235295, "top": 0.45488257575757574, "width": 0.21902614379084961, "height": 0.012727272727272698, "page": 8}], "section": "Discussion", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "1910"}, {"text": "For instance, P2 mentioned the desire for ongoing transcription throughout their work day,  I dont know the [range of the eyewear transcription] but like, if youre sitting over here and your boss is over there having a conversation, and you really wish you could be a part of that [conversation] but you cant hear it, [this tool could let you] know if you should get out of your seat and go be a part of it.", "label": "Conclusion", "bboxes": [{"left": 0.27845424836601307, "top": 0.4694305555555555, "width": 0.20420261437908505, "height": 0.012727272727272754, "page": 8}, {"left": 0.0882516339869281, "top": 0.48397727272727276, "width": 0.3942565359477125, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.49852525252525254, "width": 0.3943562091503268, "height": 0.012727272727272754, "page": 8}, {"left": 0.0882516339869281, "top": 0.5130719696969697, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.5273143939393939, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 8}, {"left": 0.0882516339869281, "top": 0.5418611111111111, "width": 0.3943382352941177, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.5564090909090909, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.570955808080808, "width": 0.10249673202614377, "height": 0.012727272727272698, "page": 8}], "section": "Prototype Eyewear Experience and Desired Use", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1911"}, {"text": "Future technical privacy opportunities, such as beamforming, could constrain microphone direction and distance to match human perception, while UI and industrial design could improve transparency and conformity to social norms.", "label": "Conclusion", "bboxes": [{"left": 0.7992205882352941, "top": 0.3046300505050505, "width": 0.11282352941176488, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.31917676767676767, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.3334191919191919, "width": 0.3944869281045752, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.3479659090909091, "width": 0.39437091503267974, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.36251388888888886, "width": 0.29586111111111113, "height": 0.012727272727272698, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1912"}, {"text": "This latency is largely controlled by the connection interval and slave latency of the BLE connection, which we configured to their minimum possible values (7.5 ms and 0 ms, respectively).", "label": "Conclusion", "bboxes": [{"left": 0.45324019607843136, "top": 0.3176186868686869, "width": 0.02954738562091508, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.3321666666666667, "width": 0.39443790849673205, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.3467133838383838, "width": 0.39445424836601306, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.3609545454545454, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.3755025252525253, "width": 0.0888251633986928, "height": 0.012727272727272698, "page": 9}], "section": "System Power Consumption", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1913"}, {"text": "We have also enabled translation between different languages, which is another feature that could unlock benefits to a larger population.", "label": "Conclusion", "bboxes": [{"left": 0.853343137254902, "top": 0.07915277777777778, "width": 0.0586846405228757, "height": 0.01272727272727274, "page": 9}, {"left": 0.517640522875817, "top": 0.09370075757575758, "width": 0.39441993464052294, "height": 0.012727272727272726, "page": 9}, {"left": 0.517640522875817, "top": 0.10824747474747475, "width": 0.39445424836601306, "height": 0.012727272727272726, "page": 9}, {"left": 0.517640522875817, "top": 0.12279545454545455, "width": 0.0743153594771242, "height": 0.01272727272727274, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "1914"}, {"text": "An informal experiment suggests that cloud-based transcription in the eyewear (phone in ambient display mode) could extend the battery by 62% (suggesting battery life on par with the eyewear), compared to only using the phone with its display at full brightness.", "label": "Conclusion", "bboxes": [{"left": 0.517640522875817, "top": 0.16613131313131313, "width": 0.39445098039215687, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.18067929292929294, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.1952260101010101, "width": 0.39445424836601317, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.20977272727272725, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2243207070707071, "width": 0.21785620915032688, "height": 0.012727272727272698, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "1915"}, {"text": "Our user research suggests that HWDs could greatly improve hearing accessibility through privately transcribed text, which can be used hands-free, in mobile contexts, and in socially acceptable interactions.", "label": "Conclusion", "bboxes": [{"left": 0.7062352941176471, "top": 0.593979797979798, "width": 0.2057009803921569, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.6085265151515151, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.623074494949495, "width": 0.39440686274509795, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.6376212121212121, "width": 0.36492156862745084, "height": 0.01272727272727281, "page": 9}], "section": "CONCLUSIONS", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "1916"}, {"text": "HWDs reduce the need to realign captions to the main point of visual attention [29], increase the perceived emotional connection to the interlocutor [16], and enable environmental awareness [16].", "label": "Future Work", "bboxes": [{"left": 0.7379901960784313, "top": 0.742770202020202, "width": 0.17407843137254908, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.7573181818181819, "width": 0.39432189542483664, "height": 0.012727272727272587, "page": 0}, {"left": 0.5176470588235295, "top": 0.771864898989899, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.7864116161616161, "width": 0.39420751633986917, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1917"}, {"text": "We conclude with current limitations and future avenues for this work.", "label": "Future Work", "bboxes": [{"left": 0.08823529411764706, "top": 0.6970075757575758, "width": 0.3944052287581699, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.7115555555555556, "width": 0.0656846405228758, "height": 0.012727272727272587, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1918"}, {"text": "Although early co-design sessions suggested that voice transcriptions could further social participation, the final usability ratings of the captioning designs were overshadowed by the limitations of the form factor and delay in the voice recognition software.", "label": "Future Work", "bboxes": [{"left": 0.846390522875817, "top": 0.3990795454545455, "width": 0.06196241830065363, "height": 0.012727272727272698, "page": 1}, {"left": 0.517640522875817, "top": 0.41360858585858584, "width": 0.39299836601307203, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.42813762626262625, "width": 0.3923496732026144, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.44266666666666665, "width": 0.38794607843137263, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.45719570707070706, "width": 0.38832352941176473, "height": 0.012727272727272698, "page": 1}, {"left": 0.517640522875817, "top": 0.47172474747474746, "width": 0.13820424836601308, "height": 0.012727272727272698, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1919"}, {"text": "[16] also identified the need for transcription during recreational or mobile activities, such as when exercising (yoga, hiking, kayaking), in transit (bus, car), and walking.", "label": "Future Work", "bboxes": [{"left": 0.29489215686274506, "top": 0.28851767676767676, "width": 0.1878137254901961, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.30306439393939394, "width": 0.3943055555555556, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.3176123737373737, "width": 0.3944052287581699, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.3321590909090909, "width": 0.15429575163398696, "height": 0.012727272727272698, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1920"}, {"text": "Our scenarios present both specific and openended situations for future user evaluations which can focus on issues of visual dispersion, facial or text visibility, and ambient noise.", "label": "Future Work", "bboxes": [{"left": 0.18000980392156862, "top": 0.5779040404040404, "width": 0.3026960784313726, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.5924507575757576, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.6069987373737373, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.6215454545454545, "width": 0.09587581699346405, "height": 0.012727272727272698, "page": 3}], "section": "Discussion and Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1921"}, {"text": "This provides an opportunity to include more batteries in the future, if the electronics can be rebalanced to avoid adding nose weight.", "label": "Future Work", "bboxes": [{"left": 0.7815392156862745, "top": 0.21578787878787878, "width": 0.130578431372549, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.23033585858585856, "width": 0.3943888888888889, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.24488257575757577, "width": 0.38445588235294115, "height": 0.012727272727272698, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1922"}, {"text": "To enable personalization in a single frame design without the need for 3D scanning, we developed an interchangeable nose bridge, which provides independent adjustments of IPD, wrap, and cyclo-rotation.", "label": "Future Work", "bboxes": [{"left": 0.5176470588235295, "top": 0.5324646464646464, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5470113636363636, "width": 0.39435457516339856, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5615593434343434, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5761060606060606, "width": 0.1989803921568628, "height": 0.012727272727272698, "page": 6}], "section": "Display of Transcriptions", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "1923"}, {"text": "Study 2 incorporated both a mobile phone and our prototype eyewear in walking and multi-speaker interactions to bring further insights into the potential for a HWD for captions.", "label": "Future Work", "bboxes": [{"left": 0.08823529411764706, "top": 0.6060820707070707, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.6206300505050505, "width": 0.39443790849673205, "height": 0.01272727272727281, "page": 8}, {"left": 0.08823529411764706, "top": 0.6351767676767677, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 8}], "section": "Discussion", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1924"}, {"text": "As discussed in the Related Work, prior research indicated a need for reliable transcription and sufficient battery throughout daily tasks [16, 25].", "label": "Future Work", "bboxes": [{"left": 0.5176470588235295, "top": 0.6136578282828283, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.6282058080808081, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.6427525252525252, "width": 0.20843464052287586, "height": 0.012727272727272698, "page": 8}], "section": "TECHNICAL EVALUATION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "1925"}, {"text": "With these improvements and positive feedback, we are excited about opportunities to further validate the potential through quantitative methods for attention, as well as through more extended usage in continued diary studies.", "label": "Future Work", "bboxes": [{"left": 0.5176470588235295, "top": 0.5348863636363637, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.5494343434343434, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.5639810606060606, "width": 0.3946290849673202, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.5785290404040404, "width": 0.3695669934640523, "height": 0.012727272727272698, "page": 8}], "section": "Discussion", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1926"}, {"text": "Future technical privacy opportunities, such as beamforming, could constrain microphone direction and distance to match human perception, while UI and industrial design could improve transparency and conformity to social norms.", "label": "Future Work", "bboxes": [{"left": 0.7992205882352941, "top": 0.3046300505050505, "width": 0.11282352941176488, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.31917676767676767, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.3334191919191919, "width": 0.3944869281045752, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.3479659090909091, "width": 0.39437091503267974, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.36251388888888886, "width": 0.29586111111111113, "height": 0.012727272727272698, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "1927"}, {"text": "In future work, we would like to validate our technology with both an increased number of participants and extended usage time to facilitate statistical analysis.", "label": "Future Work", "bboxes": [{"left": 0.17880718954248367, "top": 0.5124532828282828, "width": 0.3038823529411765, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.527, "width": 0.3946356209150327, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.5415479797979798, "width": 0.3672450980392157, "height": 0.012727272727272698, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1928"}, {"text": "We hope to explore display architectures without active backlights to further reduce power consumption.", "label": "Future Work", "bboxes": [{"left": 0.5949035947712419, "top": 0.12279545454545455, "width": 0.3170751633986927, "height": 0.01272727272727274, "page": 9}, {"left": 0.517640522875817, "top": 0.13734217171717172, "width": 0.3664199346405228, "height": 0.012727272727272726, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1929"}, {"text": "In future work, we are investigating whether a beam forming microphone on the HWD might help the user focus attention on one speaker when necessary by turning their head to that speaker.", "label": "Future Work", "bboxes": [{"left": 0.3634264705882353, "top": 0.6651931818181818, "width": 0.11903267973856207, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.6797411616161616, "width": 0.3943055555555555, "height": 0.012727272727272587, "page": 9}, {"left": 0.08823529411764706, "top": 0.6942878787878788, "width": 0.3944705882352941, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.708834595959596, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1930"}, {"text": "In future work, we are interested in evaluating text placement with our prototype system and strategies to improve legibility and comprehension [4, 5].", "label": "Future Work", "bboxes": [{"left": 0.4262287581699346, "top": 0.8033863636363637, "width": 0.05631209150326799, "height": 0.012727272727272587, "page": 9}, {"left": 0.08823529411764706, "top": 0.8179330808080808, "width": 0.39443790849673205, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.8324810606060606, "width": 0.3945196078431373, "height": 0.012727272727272587, "page": 9}, {"left": 0.08823529411764706, "top": 0.8470277777777778, "width": 0.14332679738562093, "height": 0.01272727272727281, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "1931"}, {"text": "We aimed to expand upon existing studies which have already identified interest in real-time captioning", "label": "Objective", "bboxes": [{"left": 0.210578431372549, "top": 0.8621502525252525, "width": 0.2722761437908497, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8766969696969698, "width": 0.39440522875816986, "height": 0.012727272727272587, "page": 2}], "section": "MOBILE SCENARIOS SURVEY: 501 RESPONDENTS", "prob": 0.5126603841781616, "is_author_statement": true, "is_in_expected_section": false, "id": "1932"}, {"text": "The scenarios aimed to represent these difficulties at varying levels of specificity to inform potential scenarios for user evaluations.", "label": "Objective", "bboxes": [{"left": 0.7615490196078432, "top": 0.6870934343434344, "width": 0.1505784313725489, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.7016363636363637, "width": 0.3944346405228757, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.716179292929293, "width": 0.3075898692810457, "height": 0.012727272727272587, "page": 2}], "section": "Survey Design", "prob": 0.5106127262115479, "is_author_statement": false, "is_in_expected_section": false, "id": "1933"}, {"text": "For graphics effects that require animation, such as our smoothly-scrolling transcript text, we implement interpolation primitives which are executed on the device to drive position and scale parameters of other primitives.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.6842601010101009, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.6988080808080809, "width": 0.394388888888889, "height": 0.012727272727272587, "page": 3}, {"left": 0.5176797385620915, "top": 0.713354797979798, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.7279027777777778, "width": 0.36096895424836595, "height": 0.012727272727272587, "page": 3}], "section": "Embedded System and Communication protocol", "prob": 0.6552963256835938, "is_author_statement": true, "is_in_expected_section": true, "id": "1934"}, {"text": "We implemented our prototype using a MediaTek MT2523D System-in-Package (SiP) [27], which is a single-chip CortexM4F with integrated Bluetooth (BT) 4.0 EDR and Bluetooth Low Energy (BLE) transceiver, power management, MIPIDSI display controller, and memory subsystem.", "label": "Method", "bboxes": [{"left": 0.8889771241830065, "top": 0.38003156565656565, "width": 0.022991830065359542, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.3945795454545455, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.40912626262626267, "width": 0.3944869281045752, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.42367424242424245, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.43822095959595964, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.4527689393939394, "width": 0.3582189542483659, "height": 0.012727272727272698, "page": 3}], "section": "SYSTEM ARCHITECTURE AND IMPLEMENTATION", "prob": 0.618439793586731, "is_author_statement": true, "is_in_expected_section": true, "id": "1935"}, {"text": "To improve the prototype iteratively, we", "label": "Method", "bboxes": [{"left": 0.21136928104575164, "top": 0.8875984848484848, "width": 0.2712205882352941, "height": 0.01272727272727281, "page": 4}], "section": "USABILITY EVALUATIONS: THREE STUDIES WITH DEAF/HARD-OF-HEARING PARTICIPANTS", "prob": 0.5970529913902283, "is_author_statement": true, "is_in_expected_section": false, "id": "1936"}, {"text": "To provide visual feedback to the wearer about when the prototype was connected to the phone, we implemented a real-time audio level meter in the lower right corner of the display (Figure 1).", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.8127487373737374, "width": 0.39430555555555546, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8272967171717173, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.8418434343434343, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8563914141414141, "width": 0.12137254901960781, "height": 0.01272727272727281, "page": 5}], "section": "Connectivity and Pairing", "prob": 0.5855739712715149, "is_author_statement": true, "is_in_expected_section": true, "id": "1937"}, {"text": "To enable personalization in a single frame design without the need for 3D scanning, we developed an interchangeable nose bridge, which provides independent adjustments of IPD, wrap, and cyclo-rotation.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.5324646464646464, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5470113636363636, "width": 0.39435457516339856, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5615593434343434, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5761060606060606, "width": 0.1989803921568628, "height": 0.012727272727272698, "page": 6}], "section": "Display of Transcriptions", "prob": 0.5811709761619568, "is_author_statement": true, "is_in_expected_section": true, "id": "1938"}, {"text": "We also implemented scrolling animations that would smoothly roll the text upwards as new lines are added, helping to guide the user's eye along as the transcript moves.", "label": "Method", "bboxes": [{"left": 0.19018790849673203, "top": 0.37549873737373735, "width": 0.29274836601307197, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.39004545454545453, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.4045934343434343, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.4191401515151515, "width": 0.046859477124182994, "height": 0.012727272727272698, "page": 6}], "section": "Display of Transcriptions", "prob": 0.5347535014152527, "is_author_statement": true, "is_in_expected_section": true, "id": "1939"}, {"text": "To offset the weight of the optics and the display engine in the front, which can lead to slipping and nose pressure, we redesigned the frame to place the batteries at the back of the temple (Figure 5, right), similar to other head-worn systems such as Vuzix Blade and Google Glass.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.09214267676767678, "width": 0.39445424836601306, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.10669065656565656, "width": 0.3942892156862744, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.12123737373737373, "width": 0.39432189542483653, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.13578535353535354, "width": 0.39422385620915046, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.15033207070707072, "width": 0.2668235294117647, "height": 0.012727272727272726, "page": 6}], "section": "Display of Transcriptions", "prob": 0.49184146523475647, "is_author_statement": true, "is_in_expected_section": true, "id": "1940"}, {"text": "To overcome the limitations of existing platforms, we developed a hybrid approach adapted to our specific application that consists of a thin-client low-power eyewear prototype coupled with a mobile phone over a wireless connection (Figure 3).", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.27850631313131313, "width": 0.39435457516339867, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.2930542929292929, "width": 0.39430555555555546, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.3076010101010101, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.3221489898989899, "width": 0.3941911764705881, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.33669570707070706, "width": 0.15742647058823522, "height": 0.012727272727272698, "page": 3}], "section": "SYSTEM ARCHITECTURE AND IMPLEMENTATION", "prob": 0.465814471244812, "is_author_statement": true, "is_in_expected_section": true, "id": "1941"}, {"text": "To maximize material strength and robustness in our rapid prototyping for all-day use, we use SLS (selective laser sintering).", "label": "Method", "bboxes": [{"left": 0.3964330065359477, "top": 0.4315391414141414, "width": 0.08612418300653596, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.445780303030303, "width": 0.3944705882352941, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4603282828282828, "width": 0.3290294117647059, "height": 0.012727272727272754, "page": 4}], "section": "MECHANICAL DESIGN FOR RAPID PROTOTYPING", "prob": 0.4635617136955261, "is_author_statement": true, "is_in_expected_section": true, "id": "1942"}, {"text": "2. Uses fixed nose bridge modules.", "label": "Method", "bboxes": [{"left": 0.32587745098039217, "top": 0.7855176767676768, "width": 0.156797385620915, "height": 0.011515151515151478, "page": 4}, {"left": 0.08823529411764706, "top": 0.7985530303030303, "width": 0.05626960784313724, "height": 0.011515151515151478, "page": 4}], "section": "MECHANICAL DESIGN FOR RAPID PROTOTYPING", "prob": 0.40011921525001526, "is_author_statement": false, "is_in_expected_section": true, "id": "1943"}, {"text": "In cases where the eyewear experience was compared to a mobile device, we used an updated version of Live Transcribe [9] running on the same phone.", "label": "Method", "bboxes": [{"left": 0.8984133986928103, "top": 0.3760820707070707, "width": 0.013720588235294207, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.39062878787878785, "width": 0.39427287581699355, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.40517676767676764, "width": 0.39432189542483653, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.4197234848484848, "width": 0.28642320261437904, "height": 0.012727272727272698, "page": 4}], "section": "Prototype Apparatus and Fitting", "prob": 0.37224721908569336, "is_author_statement": true, "is_in_expected_section": true, "id": "1944"}, {"text": "To learn more about challenges in mobile contexts, we conducted a brief large-scale online survey with participants who used hearing aids, TDD/TTY (telecommunications device for the deaf/teletypewriter) [13], CART [31], and cochlear implants.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.8039608585858585, "width": 0.3943218954248366, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8185088383838384, "width": 0.39453594771241834, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8330555555555555, "width": 0.3946192810457516, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8476035353535354, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.8621502525252525, "width": 0.11901633986928102, "height": 0.01272727272727281, "page": 2}], "section": "MOBILE SCENARIOS SURVEY: 501 RESPONDENTS", "prob": 0.35571423172950745, "is_author_statement": true, "is_in_expected_section": true, "id": "1945"}, {"text": "To validate our proposed approach for hearing accessibility, we conducted a pilot and two studies with 24 DHH participants who provided feedback on our prototypes in various scenarios and tasks.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.5503371212121212, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.564885101010101, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.5794318181818182, "width": 0.394437908496732, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.593979797979798, "width": 0.1836797385620914, "height": 0.012727272727272698, "page": 9}], "section": "CONCLUSIONS", "prob": 0.32505935430526733, "is_author_statement": true, "is_in_expected_section": true, "id": "1946"}, {"text": "The subsequent sections describe the iterative development of the prototype and three related user studies.", "label": "Method", "bboxes": [{"left": 0.15174509803921568, "top": 0.6679128787878787, "width": 0.3307957516339869, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.6824608585858586, "width": 0.3942565359477124, "height": 0.01272727272727281, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": 0.3029843866825104, "is_author_statement": false, "is_in_expected_section": true, "id": "1947"}, {"text": "Our previous studies identified group conversations as an area of priority, and Study 2 results suggest that head-worn captions also bring benefits through better speaker awareness, which enabled better participation in the conversations.", "label": "Result", "bboxes": [{"left": 0.7086078431372549, "top": 0.345479797979798, "width": 0.20352614379084966, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.3600265151515152, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.37457449494949496, "width": 0.39450326797385604, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.38912121212121215, "width": 0.39442156862745104, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.4036679292929293, "width": 0.13510784313725488, "height": 0.012727272727272754, "page": 8}], "section": "Discussion", "prob": 0.883140504360199, "is_author_statement": true, "is_in_expected_section": true, "id": "1948"}, {"text": "An informal experiment suggests that cloud-based transcription in the eyewear (phone in ambient display mode) could extend the battery by 62% (suggesting battery life on par with the eyewear), compared to only using the phone with its display at full brightness.", "label": "Result", "bboxes": [{"left": 0.517640522875817, "top": 0.16613131313131313, "width": 0.39445098039215687, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.18067929292929294, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.1952260101010101, "width": 0.39445424836601317, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.20977272727272725, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2243207070707071, "width": 0.21785620915032688, "height": 0.012727272727272698, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": 0.8228884339332581, "is_author_statement": false, "is_in_expected_section": false, "id": "1949"}, {"text": "With a 360 mAh LiPo battery (3.7V) and 90% power efficiency, we estimate 4.5 hours in sunlight (100% display), 8 hours indoors (50% display), and approximately 15 hours in a dark room (5%) (Figure 10, right).", "label": "Result", "bboxes": [{"left": 0.6336993464052287, "top": 0.7506338383838383, "width": 0.2783856209150327, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.7651805555555556, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 8}, {"left": 0.5176470588235295, "top": 0.7794229797979798, "width": 0.394437908496732, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.7939696969696969, "width": 0.39427287581699344, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.8085164141414142, "width": 0.040583333333333305, "height": 0.012727272727272587, "page": 8}], "section": "System Power Consumption", "prob": 0.8081368803977966, "is_author_statement": true, "is_in_expected_section": false, "id": "1950"}, {"text": "We evaluated BLE power consumption under various usage scenarios.", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.12880934343434344, "width": 0.3943545751633987, "height": 0.012727272727272726, "page": 9}, {"left": 0.08823529411764706, "top": 0.14335732323232322, "width": 0.06489379084967321, "height": 0.012727272727272726, "page": 9}], "section": "System Power Consumption", "prob": 0.788404643535614, "is_author_statement": true, "is_in_expected_section": false, "id": "1951"}, {"text": "This study shows positive ratings for fit, as well as display ergonomics, such as UI placement, clarity of their field-of-view, and overall comprehension.", "label": "Result", "bboxes": [{"left": 0.7411862745098039, "top": 0.45488257575757574, "width": 0.17086601307189553, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.4694305555555555, "width": 0.39450326797385604, "height": 0.012727272727272754, "page": 8}, {"left": 0.5176470588235295, "top": 0.48397727272727276, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.49852525252525254, "width": 0.10409477124182998, "height": 0.012727272727272754, "page": 8}], "section": "Discussion", "prob": 0.7762364745140076, "is_author_statement": false, "is_in_expected_section": true, "id": "1952"}, {"text": "After studies 1 and 2 below, we redesigned the prototypes to include two 180 mAh 1-cell LiPo batteries symmetrically placed at the back of left and right temples, which creates a more balanced design and reduced the nose weight from 78% (42g) to 56% (30g).", "label": "Result", "bboxes": [{"left": 0.19214869281045752, "top": 0.5115429292929293, "width": 0.2904918300653594, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5260896464646465, "width": 0.3946519607843137, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5406363636363637, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5551843434343434, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5697310606060606, "width": 0.25819281045751635, "height": 0.01272727272727281, "page": 4}], "section": "MECHANICAL DESIGN FOR RAPID PROTOTYPING", "prob": 0.7750939726829529, "is_author_statement": true, "is_in_expected_section": false, "id": "1953"}, {"text": "In Version 2 of the eyewear prototype, we minimized this abrupt motion by preserving line breaks for the high-confidence portion of the ASR result, such that only the text subject to modification as new audio is collected would reflow.", "label": "Result", "bboxes": [{"left": 0.20547222222222222, "top": 0.317614898989899, "width": 0.27706862745098043, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.33216287878787876, "width": 0.39445424836601306, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.34670959595959594, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.3609520202020202, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.37549873737373735, "width": 0.09508496732026141, "height": 0.012727272727272754, "page": 6}], "section": "Display of Transcriptions", "prob": 0.7739335298538208, "is_author_statement": true, "is_in_expected_section": false, "id": "1954"}, {"text": "Our recent offline privacy-preserving ASR implementation on the phone still results in a 3% longer battery life when combined with the eyewear, compared to cloud-computed transcription displayed on the phone.", "label": "Result", "bboxes": [{"left": 0.517640522875817, "top": 0.26098737373737374, "width": 0.39441993464052294, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2755353535353535, "width": 0.3945196078431372, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.2900820707070707, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 9}, {"left": 0.517640522875817, "top": 0.3046300505050505, "width": 0.2703970588235294, "height": 0.012727272727272754, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": 0.7561562657356262, "is_author_statement": true, "is_in_expected_section": false, "id": "1955"}, {"text": "All participants experienced issues with Version 1 of the prototype.", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.18547601010101009, "width": 0.39435457516339856, "height": 0.012727272727272754, "page": 5}, {"left": 0.5176470588235295, "top": 0.20002398989898992, "width": 0.06687091503267972, "height": 0.012727272727272698, "page": 5}], "section": "Display of Transcriptions", "prob": 0.7190526723861694, "is_author_statement": false, "is_in_expected_section": false, "id": "1956"}, {"text": "Participants ratings also suggest that the eyewear prototypes helped participants become more aware of their surroundings (x   eyewear =4 vs. x phone =3), and who was currently speaking (x   eyewear =4 vs. x phone =2) while using the eyewear, whereas awareness of body language was similar for both conditions (Figure 9, center).", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.374885101010101, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.3894318181818182, "width": 0.39460294117647066, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.4036578282828283, "width": 0.39441503267973854, "height": 0.014257575757575691, "page": 7}, {"left": 0.5176421568627452, "top": 0.4182032828282828, "width": 0.39459803921568626, "height": 0.014243686868686878, "page": 7}, {"left": 0.5176535947712417, "top": 0.4327512626262626, "width": 0.3945866013071897, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176535947712417, "top": 0.4472979797979798, "width": 0.12291993464052298, "height": 0.012727272727272698, "page": 7}], "section": "Social D ynamics: Discreetness and Awareness", "prob": 0.719000518321991, "is_author_statement": false, "is_in_expected_section": false, "id": "1957"}, {"text": "For group conversation, the level of perceived discreetness was similar.", "label": "Result", "bboxes": [{"left": 0.5799722222222222, "top": 0.25093434343434345, "width": 0.33214542483660126, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.2654810606060606, "width": 0.16408006535947706, "height": 0.012727272727272754, "page": 7}], "section": "Social D ynamics: Discreetness and Awareness", "prob": 0.7077237963676453, "is_author_statement": false, "is_in_expected_section": false, "id": "1958"}, {"text": "Two out of the five participants indicated using the prototype in sessions longer than two hours.", "label": "Result", "bboxes": [{"left": 0.3874232026143791, "top": 0.859719696969697, "width": 0.0951503267973855, "height": 0.012727272727272587, "page": 5}, {"left": 0.08823529411764706, "top": 0.8742676767676767, "width": 0.3946519607843137, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.888814393939394, "width": 0.1472467320261438, "height": 0.012727272727272587, "page": 5}], "section": "Results", "prob": 0.7031517624855042, "is_author_statement": false, "is_in_expected_section": true, "id": "1959"}, {"text": "The feedback questionnaires revealed the benefits of the eyewear prototype in comparison to the experience on the mobile phone.", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.12880934343434344, "width": 0.39430555555555546, "height": 0.012727272727272726, "page": 7}, {"left": 0.5176470588235295, "top": 0.14335732323232322, "width": 0.39435457516339856, "height": 0.012727272727272726, "page": 7}, {"left": 0.5176470588235295, "top": 0.1579040404040404, "width": 0.0927450980392156, "height": 0.012727272727272698, "page": 7}], "section": "Task and Procedure", "prob": 0.7006250023841858, "is_author_statement": false, "is_in_expected_section": false, "id": "1960"}, {"text": "Participants also indicated the perceived social acceptability of their assistive technologies.", "label": "Result", "bboxes": [{"left": 0.08821895424836601, "top": 0.4412626262626263, "width": 0.3945196078431372, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.45580934343434343, "width": 0.2021764705882353, "height": 0.012727272727272754, "page": 3}], "section": "Survey Design", "prob": 0.6863115429878235, "is_author_statement": false, "is_in_expected_section": false, "id": "1961"}, {"text": "Our eyewear prototype weighs 54 g with 30 g on the nose.", "label": "Result", "bboxes": [{"left": 0.08821895424836601, "top": 0.725215909090909, "width": 0.3942075163398693, "height": 0.01272727272727281, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "prob": 0.6786578893661499, "is_author_statement": true, "is_in_expected_section": false, "id": "1962"}, {"text": "Participant ages ranged between 1865, with 36 % identifying as female.", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.859090909090909, "width": 0.3936241830065359, "height": 0.01377525252525258, "page": 2}, {"left": 0.5176470588235295, "top": 0.8736111111111111, "width": 0.1473856209150326, "height": 0.012626262626262652, "page": 2}], "section": "Survey Design", "prob": 0.6774283051490784, "is_author_statement": false, "is_in_expected_section": false, "id": "1963"}, {"text": "This improvement reduced the nose weight from 42 g (78%) to 30 g (56%).", "label": "Result", "bboxes": [{"left": 0.7901535947712418, "top": 0.15033207070707072, "width": 0.12181535947712419, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.16487878787878787, "width": 0.3703251633986927, "height": 0.012727272727272698, "page": 6}], "section": "Display of Transcriptions", "prob": 0.6721805930137634, "is_author_statement": false, "is_in_expected_section": false, "id": "1964"}, {"text": "participants indicated the use of a hearing aid, 7% used a cochlear implant, 53% used transcription services such as CART, and 83% used TDD/TTY.", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.07911237373737373, "width": 0.39433823529411755, "height": 0.012727272727272726, "page": 3}, {"left": 0.08823529411764706, "top": 0.09366035353535354, "width": 0.39461928104575167, "height": 0.012727272727272726, "page": 3}, {"left": 0.08823529411764706, "top": 0.10820707070707071, "width": 0.2217271241830065, "height": 0.012727272727272726, "page": 3}], "section": "Survey Design", "prob": 0.6382911205291748, "is_author_statement": false, "is_in_expected_section": false, "id": "1965"}, {"text": "Five newly-recruited participants completed all parts of the study.", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.2448699494949495, "width": 0.39437254901960783, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.2594179292929293, "width": 0.03980882352941177, "height": 0.012727272727272698, "page": 7}], "section": "Participants", "prob": 0.6378591656684875, "is_author_statement": false, "is_in_expected_section": false, "id": "1966"}, {"text": "The result maintains the subtle eyewear form factor (Figure 8).", "label": "Result", "bboxes": [{"left": 0.886669934640523, "top": 0.780060606060606, "width": 0.025349673202614165, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.7946073232323232, "width": 0.3817058823529411, "height": 0.01272727272727281, "page": 6}], "section": "Display of Transcriptions", "prob": 0.6354719400405884, "is_author_statement": false, "is_in_expected_section": false, "id": "1967"}, {"text": "HWDs also yield promising results when used to assess training comprehension [43].", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.800959595959596, "width": 0.3941911764705881, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.815506313131313, "width": 0.19035130718954252, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": 0.6226154565811157, "is_author_statement": false, "is_in_expected_section": true, "id": "1968"}, {"text": "A total of five DHH participants were recruited from our institution.", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.5136578282828282, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5282058080808081, "width": 0.07041176470588235, "height": 0.012727272727272698, "page": 5}], "section": "Participants", "prob": 0.6176249384880066, "is_author_statement": true, "is_in_expected_section": false, "id": "1969"}, {"text": "Participants ranked both fit and the", "label": "Result", "bboxes": [{"left": 0.677625816993464, "top": 0.7209330808080808, "width": 0.23431045751633994, "height": 0.012727272727272587, "page": 7}], "section": "Prototype Eyewear Experience and Desired Use", "prob": 0.6152290105819702, "is_author_statement": false, "is_in_expected_section": false, "id": "1970"}, {"text": "Our results align with the work from Jain et al.", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.6497247474747475, "width": 0.3260359477124183, "height": 0.01272727272727281, "page": 8}], "section": "Discussion", "prob": 0.6051910519599915, "is_author_statement": true, "is_in_expected_section": true, "id": "1971"}, {"text": "The scenarios where the most participants indicated difficulty with communication were eating in public (49%), conversations with three or more speakers (43%), and split attention situations (43%).", "label": "Result", "bboxes": [{"left": 0.45722549019607844, "top": 0.20123863636363637, "width": 0.025348039215686236, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.21578661616161615, "width": 0.3944705882352941, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.23033333333333333, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.2448800505050505, "width": 0.3943055555555555, "height": 0.012727272727272726, "page": 3}, {"left": 0.08823529411764706, "top": 0.2594280303030303, "width": 0.17234803921568626, "height": 0.012727272727272698, "page": 3}], "section": "Survey Design", "prob": 0.5919587016105652, "is_author_statement": false, "is_in_expected_section": false, "id": "1972"}, {"text": "Based on the feedback from Study 1, we adjusted the wrap and width of the nose bridges to provide a more central display location (See Figure 7, B1/B2).", "label": "Result", "bboxes": [{"left": 0.872078431372549, "top": 0.656415404040404, "width": 0.03999019607843146, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.6709621212121213, "width": 0.3944869281045751, "height": 0.012727272727272587, "page": 6}, {"left": 0.5176470588235295, "top": 0.685510101010101, "width": 0.394470588235294, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.7000568181818182, "width": 0.20528921568627445, "height": 0.012727272727272587, "page": 6}], "section": "Display of Transcriptions", "prob": 0.5793301463127136, "is_author_statement": true, "is_in_expected_section": false, "id": "1973"}, {"text": "Participants suggested that testing in group conversations and more challenging auditory environments could be helpful to fully understand the benefits of the HWD.", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.3127487373737374, "width": 0.3942565359477125, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.32729671717171716, "width": 0.39437254901960783, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.34184343434343434, "width": 0.36487254901960786, "height": 0.012727272727272698, "page": 5}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "prob": 0.5704503655433655, "is_author_statement": false, "is_in_expected_section": false, "id": "1974"}, {"text": "Technical evaluation and characterization of power, bandwidth and latency.", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.49305303030303027, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 1}, {"left": 0.11765196078431372, "top": 0.5076010101010101, "width": 0.15235294117647058, "height": 0.01272727272727281, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": 0.5460796356201172, "is_author_statement": false, "is_in_expected_section": true, "id": "1975"}, {"text": "Participants were, however, neutral about the text rate for the group activity in both conditions, and overall rated their ability to understand the spoken communication as Extremely well in both conditions (x  =5; IQR phone =0, IQR eyewear =1).", "label": "Result", "bboxes": [{"left": 0.7913382352941176, "top": 0.6130530303030303, "width": 0.12054901960784326, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176307189542484, "top": 0.6276010101010101, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176307189542484, "top": 0.6421477272727273, "width": 0.39447058823529413, "height": 0.012727272727272587, "page": 7}, {"left": 0.5176307189542484, "top": 0.656695707070707, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 7}, {"left": 0.5176307189542484, "top": 0.6712335858585858, "width": 0.2801960784313725, "height": 0.01425126262626264, "page": 7}], "section": "Comprehension of Presented Contents", "prob": 0.5393132567405701, "is_author_statement": false, "is_in_expected_section": false, "id": "1976"}, {"text": "Our first diary study pilot participant, who used our device over four weeks, mentioned how useful it was to", "label": "Result", "bboxes": [{"left": 0.16862745098039217, "top": 0.8761224747474747, "width": 0.31409477124183, "height": 0.01272727272727281, "page": 9}, {"left": 0.08823529411764706, "top": 0.8906691919191919, "width": 0.39447058823529413, "height": 0.01272727272727281, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "prob": 0.5303794145584106, "is_author_statement": true, "is_in_expected_section": false, "id": "1977"}, {"text": "[3], who have shown that 70.5% of their participants who preferred sign language as their form of communication showed interest in captions.", "label": "Result", "bboxes": [{"left": 0.12745098039215685, "top": 0.8179267676767676, "width": 0.3553709150326798, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.8324747474747476, "width": 0.39445424836601306, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.8470214646464647, "width": 0.18175326797385621, "height": 0.01272727272727281, "page": 3}], "section": "Discussion and Limitations", "prob": 0.5110629796981812, "is_author_statement": false, "is_in_expected_section": true, "id": "1978"}, {"text": "Study 1 also helped us discover and address antenna issues.", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.07911237373737373, "width": 0.3942892156862745, "height": 0.012727272727272726, "page": 6}], "section": "Connectivity and Pairing", "prob": 0.5053318738937378, "is_author_statement": false, "is_in_expected_section": false, "id": "1979"}, {"text": "The final fit can be adjusted manually by shaping the nose pads.", "label": "Result", "bboxes": [{"left": 0.8529362745098038, "top": 0.49610227272727275, "width": 0.05906535947712421, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.5106502525252525, "width": 0.35588071895424844, "height": 0.012727272727272698, "page": 6}], "section": "Display of Transcriptions", "prob": 0.5025209188461304, "is_author_statement": false, "is_in_expected_section": false, "id": "1980"}, {"text": "Two participants expressed that they would want to use it for 10 hours or more (the highest option), whereas the other three participants chose 3, 4, and 6 hours, respectively.", "label": "Result", "bboxes": [{"left": 0.3019901960784314, "top": 0.42578787878787877, "width": 0.18074836601307182, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.44033585858585855, "width": 0.39437254901960783, "height": 0.012727272727272754, "page": 8}, {"left": 0.0882516339869281, "top": 0.45488257575757574, "width": 0.3942565359477124, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.4694305555555555, "width": 0.18529411764705883, "height": 0.012727272727272754, "page": 8}], "section": "Prototype Eyewear Experience and Desired Use", "prob": 0.4961967468261719, "is_author_statement": false, "is_in_expected_section": false, "id": "1981"}, {"text": "We optimized our electronics layout for a mostly single-sided design with a compact footprint of 14.5  60 mm (Figure 4).", "label": "Result", "bboxes": [{"left": 0.8890098039215686, "top": 0.4527689393939394, "width": 0.022991830065359542, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.46731565656565655, "width": 0.39450326797385615, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.4818636363636364, "width": 0.39431045751633986, "height": 0.012727272727272698, "page": 3}], "section": "SYSTEM ARCHITECTURE AND IMPLEMENTATION", "prob": 0.49104028940200806, "is_author_statement": true, "is_in_expected_section": false, "id": "1982"}, {"text": "Participants positive social acceptability ratings (70%) of their existing technology may also indicate a favorable outlook towards assistive technologies, while there is an opportunity to also improve the solution for less satisfied participants (30%).", "label": "Result", "bboxes": [{"left": 0.2654918300653595, "top": 0.6651868686868686, "width": 0.21713235294117644, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.6797348484848486, "width": 0.39448692810457514, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.6942815656565656, "width": 0.3943545751633987, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.7088295454545455, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7233762626262626, "width": 0.31623529411764706, "height": 0.01272727272727281, "page": 3}], "section": "Discussion and Limitations", "prob": 0.47638562321662903, "is_author_statement": false, "is_in_expected_section": true, "id": "1983"}, {"text": "With these improvements and positive feedback, we are excited about opportunities to further validate the potential through quantitative methods for attention, as well as through more extended usage in continued diary studies.", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.5348863636363637, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.5494343434343434, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.5639810606060606, "width": 0.3946290849673202, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.5785290404040404, "width": 0.3695669934640523, "height": 0.012727272727272698, "page": 8}], "section": "Discussion", "prob": 0.4629824161529541, "is_author_statement": true, "is_in_expected_section": true, "id": "1984"}, {"text": "We conclude with current limitations and future avenues for this work.", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.6970075757575758, "width": 0.3944052287581699, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.7115555555555556, "width": 0.0656846405228758, "height": 0.012727272727272587, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "prob": 0.45975732803344727, "is_author_statement": true, "is_in_expected_section": true, "id": "1985"}, {"text": "We ran formative in-lab pilot studies with 14 participants in the U.S. ( New York, NY =6, Mountain View, CA =8), whose self-reported hearing loss ranged from moderate (41-70 dB) to profound (>95 dB).", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.5130517676767676, "width": 0.3945359477124182, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.5275997474747475, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.5421464646464647, "width": 0.3944215686274509, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.5566944444444444, "width": 0.1425196078431371, "height": 0.01272727272727281, "page": 4}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "prob": 0.4561498761177063, "is_author_statement": true, "is_in_expected_section": false, "id": "1986"}, {"text": "After each round, roles rotated.", "label": "Result", "bboxes": [{"left": 0.44819934640522874, "top": 0.6215517676767677, "width": 0.0344901960784314, "height": 0.012727272727272698, "page": 7}, {"left": 0.11765196078431372, "top": 0.6360984848484849, "width": 0.16643464052287582, "height": 0.012727272727272698, "page": 7}], "section": "Task and Procedure", "prob": 0.450370192527771, "is_author_statement": false, "is_in_expected_section": false, "id": "1987"}, {"text": "In this section, we evaluate the power/performance relationship of our proof-of-concept system across power, latency, and bandwidth metrics.", "label": "Result", "bboxes": [{"left": 0.7313692810457516, "top": 0.6427525252525252, "width": 0.18066666666666675, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176633986928104, "top": 0.6573005050505051, "width": 0.39437254901960805, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176633986928104, "top": 0.6718472222222222, "width": 0.35192647058823534, "height": 0.01272727272727281, "page": 8}], "section": "TECHNICAL EVALUATION", "prob": 0.4486546516418457, "is_author_statement": true, "is_in_expected_section": true, "id": "1988"}, {"text": "The visualization showed that the system was listening and ready for transcription, also helping to communicate any latency in transcription.", "label": "Result", "bboxes": [{"left": 0.6431372549019608, "top": 0.8563914141414141, "width": 0.2688643790849672, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8709381313131314, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8854848484848484, "width": 0.2746797385620915, "height": 0.01272727272727281, "page": 5}], "section": "Connectivity and Pairing", "prob": 0.43910884857177734, "is_author_statement": false, "is_in_expected_section": false, "id": "1989"}, {"text": "One participant watched TV at home.", "label": "Result", "bboxes": [{"left": 0.6376192810457516, "top": 0.12123737373737373, "width": 0.27426797385620927, "height": 0.012727272727272726, "page": 5}], "section": "Connectivity and Pairing", "prob": 0.4373500645160675, "is_author_statement": false, "is_in_expected_section": false, "id": "1990"}, {"text": "Blade ran 2.5h with 50% brightness; Focals 1.0 ran 2h at the mid brightness setting.", "label": "Result", "bboxes": [{"left": 0.08821895424836601, "top": 0.5282361111111111, "width": 0.3943218954248366, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.5427828282828283, "width": 0.14844934640522878, "height": 0.012727272727272698, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "prob": 0.4245363473892212, "is_author_statement": false, "is_in_expected_section": false, "id": "1991"}, {"text": "Same weight, but two batteries behind the ears create a more balanced design (56% nose weight, 30 g).", "label": "Result", "bboxes": [{"left": 0.14823039215686273, "top": 0.7985530303030303, "width": 0.3344509803921569, "height": 0.011515151515151478, "page": 4}, {"left": 0.08823529411764706, "top": 0.8115883838383838, "width": 0.30408986928104575, "height": 0.011515151515151478, "page": 4}], "section": "MECHANICAL DESIGN FOR RAPID PROTOTYPING", "prob": 0.42283356189727783, "is_author_statement": false, "is_in_expected_section": false, "id": "1992"}, {"text": "We also investigated shifting the UI towards participants line-of-sight and display contrast improvements.", "label": "Result", "bboxes": [{"left": 0.6717450980392157, "top": 0.31426767676767675, "width": 0.2403725490196078, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.3288156565656566, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.3433623737373737, "width": 0.09783496732026142, "height": 0.012727272727272754, "page": 6}], "section": "Display of Transcriptions", "prob": 0.411396324634552, "is_author_statement": true, "is_in_expected_section": false, "id": "1993"}, {"text": "We describe our technical architecture and system evaluation which explain the strategies that enable up to 15 hours of active use, 54 g weight and compact electronics packaged into 3D-printed frames.", "label": "Result", "bboxes": [{"left": 0.6611552287581699, "top": 0.48488131313131316, "width": 0.2508137254901962, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.49942803030303035, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 9}, {"left": 0.5176470588235295, "top": 0.5139760101010101, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.5285227272727272, "width": 0.29470751633986925, "height": 0.01272727272727281, "page": 9}], "section": "CONCLUSIONS", "prob": 0.40191948413848877, "is_author_statement": true, "is_in_expected_section": true, "id": "1994"}, {"text": "P1 was unable to attend the happy hour and movie.", "label": "Result", "bboxes": [{"left": 0.7540816993464052, "top": 0.7964116161616163, "width": 0.15806862745098038, "height": 0.012727272727272587, "page": 4}, {"left": 0.5176633986928104, "top": 0.8109595959595959, "width": 0.1711617647058824, "height": 0.01272727272727281, "page": 4}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "prob": 0.40073779225349426, "is_author_statement": false, "is_in_expected_section": false, "id": "1995"}, {"text": "conducted a brief pilot study and rapidly iterated with two usability studies:", "label": "Result", "bboxes": [{"left": 0.517640522875817, "top": 0.0791111111111111, "width": 0.394437908496732, "height": 0.01272727272727274, "page": 4}, {"left": 0.517640522875817, "top": 0.0936590909090909, "width": 0.1104673202614379, "height": 0.012727272727272726, "page": 4}], "section": "USABILITY EVALUATIONS: THREE STUDIES WITH DEAF/HARD-OF-HEARING PARTICIPANTS", "prob": 0.3852882981300354, "is_author_statement": false, "is_in_expected_section": true, "id": "1996"}, {"text": "Participants provided feedback after completing the following activities:", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.45426388888888886, "width": 0.39433823529411755, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.46881186868686864, "width": 0.13125490196078432, "height": 0.012727272727272754, "page": 7}], "section": "Task and Procedure", "prob": 0.3721780478954315, "is_author_statement": false, "is_in_expected_section": false, "id": "1997"}, {"text": "At the end of each day, participants completed a questionnaire detailing their usage time, contexts where they used the prototype, and feedback on how it could be improved.", "label": "Result", "bboxes": [{"left": 0.2902140522875817, "top": 0.752155303030303, "width": 0.19234313725490199, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.7667020202020202, "width": 0.3944052287581699, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.78125, "width": 0.3945196078431373, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.7957967171717171, "width": 0.19272222222222224, "height": 0.01272727272727281, "page": 5}], "section": "Task and Procedure", "prob": 0.35280299186706543, "is_author_statement": false, "is_in_expected_section": false, "id": "1998"}, {"text": "We wanted to collect feedback on the more central position of the transcriptions and the updated mechanical design in Version 2 of our prototype to see if it would increase perceived physical and social comfort, better accommodating different head sizes, IPDs, and face geometries.", "label": "Result", "bboxes": [{"left": 0.19880228758169935, "top": 0.13730176767676766, "width": 0.2837385620915033, "height": 0.012727272727272726, "page": 7}, {"left": 0.08823529411764706, "top": 0.1515441919191919, "width": 0.39447058823529413, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.1660909090909091, "width": 0.39438398692810456, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.18063762626262625, "width": 0.39438888888888884, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.19518560606060606, "width": 0.3943055555555555, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.20973232323232324, "width": 0.07586274509803921, "height": 0.012727272727272698, "page": 7}], "section": "STUDY 2: MOBILE AND GROUP CONVERSATIONS", "prob": 0.2911496162414551, "is_author_statement": true, "is_in_expected_section": true, "id": "1999"}], "uist-3": [{"text": "For example, we created a glue gun that transmits its live temperature (Figure 1).", "label": "Author", "bboxes": [{"left": 0.7077843137254902, "top": 0.6267007575757576, "width": 0.20411928104575172, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.6412487373737374, "width": 0.3158725490196077, "height": 0.012727272727272587, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2000"}, {"text": "As smartphones are the most pervasive AR platform at present, we created a proof-of-concept LightAnchors implementation for iOS.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.7212525252525253, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.7357992424242424, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 0}, {"left": 0.5176470588235295, "top": 0.7503472222222222, "width": 0.12451797385620911, "height": 0.01272727272727281, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2001"}, {"text": "In addition to describing our algorithm, we also report the findings of a transmission performance study, which tested accuracy at different distances, with two light sizes, and while held still and in motion.", "label": "Author", "bboxes": [{"left": 0.7258349673202614, "top": 0.8085353535353536, "width": 0.1862663398692811, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8230833333333333, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8376300505050506, "width": 0.3942565359477125, "height": 0.012727272727272587, "page": 0}, {"left": 0.5176307189542484, "top": 0.8518724747474747, "width": 0.3668986928104575, "height": 0.01272727272727281, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2002"}, {"text": "We conclude by describing ten example applications we built to illustrate the potential of LightAnchors.", "label": "Author", "bboxes": [{"left": 0.8890424836601308, "top": 0.8518724747474747, "width": 0.022993464052287638, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8664191919191918, "width": 0.3945147058823528, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8809671717171718, "width": 0.25941176470588245, "height": 0.01272727272727281, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2003"}, {"text": "In this paper, we present LightAnchors, a new method to display spatially-anchored data in augmented reality applications.", "label": "Author", "bboxes": [{"left": 0.08821895424836601, "top": 0.6173030303030304, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.6318510101010101, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.6463977272727273, "width": 0.03588888888888887, "height": 0.01272727272727281, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2004"}, {"text": "Unlike most prior tracking methods, which instrument objects with markers (often large and/or obtrusive), we take advantage of point lights already found in many objects and environments.", "label": "Author", "bboxes": [{"left": 0.12822549019607843, "top": 0.6463977272727273, "width": 0.3543643790849674, "height": 0.01272727272727281, "page": 0}, {"left": 0.08821895424836601, "top": 0.6609457070707071, "width": 0.39433823529411766, "height": 0.012727272727272587, "page": 0}, {"left": 0.08821895424836601, "top": 0.6754924242424243, "width": 0.39453594771241834, "height": 0.01272727272727281, "page": 0}, {"left": 0.08821895424836601, "top": 0.690040404040404, "width": 0.09312418300653594, "height": 0.01272727272727281, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2005"}, {"text": "information and interfaces to specific objects), we also coopt these lights for data transmission, blinking them rapidly to encode binary data.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.4739608585858586, "width": 0.3944869281045752, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.4885088383838384, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5030555555555556, "width": 0.1444803921568627, "height": 0.012727272727272698, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2006"}, {"text": "At a high level, for every incoming frame of video, our algorithm creates an image pyramid, such that lights  big or small, close or far  are guaranteed to be contained within a single pixel at least one level.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.38699116161616165, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.40153914141414143, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.4160858585858586, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.4306338383838384, "width": 0.20018300653594767, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2007"}, {"text": "Our algorithm then searches for candidate light anchors using a max-pooling template that finds bright pixels surrounded by darker pixels.", "label": "Author", "bboxes": [{"left": 0.7235130718954249, "top": 0.4306338383838384, "width": 0.18872058823529414, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.4451805555555556, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.4597272727272727, "width": 0.32876960784313713, "height": 0.012727272727272754, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2008"}, {"text": "We then track candidate anchors over frames, decoding a blinked binary pattern using an adaptive threshold.", "label": "Author", "bboxes": [{"left": 0.8536633986928105, "top": 0.4597272727272727, "width": 0.05842156862745107, "height": 0.012727272727272754, "page": 1}, {"left": 0.5176470588235295, "top": 0.4742752525252525, "width": 0.39450326797385615, "height": 0.012727272727272754, "page": 1}, {"left": 0.5176470588235295, "top": 0.48882196969696967, "width": 0.26603267973856215, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2009"}, {"text": "We encode all data as a binary sequence, prefixed with a known pattern.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.5821426767676768, "width": 0.3943807189542482, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.596385101010101, "width": 0.09902124183006533, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2010"}, {"text": "Since we repeatedly transmit the same message, the prefix appears at both the beginning and end of every transmission, which makes payload segmentation straightforward.", "label": "Author", "bboxes": [{"left": 0.6215767973856209, "top": 0.596385101010101, "width": 0.2905898692810458, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176633986928104, "top": 0.6109318181818182, "width": 0.39445424836601317, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176633986928104, "top": 0.625479797979798, "width": 0.3944379084967321, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176633986928104, "top": 0.6400265151515152, "width": 0.10490196078431369, "height": 0.012727272727272698, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2011"}, {"text": "We modulate lights with this pattern between high and low intensities at 120 FPS using a microcontroller (Teensy 3.6 or Arduino Mega) and its digital-to-analog converter (DAC).", "label": "Author", "bboxes": [{"left": 0.629812091503268, "top": 0.6400265151515152, "width": 0.2823545751633987, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176633986928104, "top": 0.6545744949494949, "width": 0.39447058823529424, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176633986928104, "top": 0.6691212121212121, "width": 0.39450326797385626, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176633986928104, "top": 0.6836679292929293, "width": 0.1393937908496733, "height": 0.012727272727272587, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2012"}, {"text": "Unlike prior approaches that synchronized light modulation with e.g., RF triggers [12], our lights and smartphones are totally unsynchronized.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.7348825757575758, "width": 0.394470588235294, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176470588235295, "top": 0.7494305555555555, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176470588235295, "top": 0.7639772727272728, "width": 0.15274836601307185, "height": 0.012727272727272587, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2013"}, {"text": "This means it is possible for the camera shutter to align with transitions in our blinked pattern, which at best reduces SNR, and at worse, means the pattern is unresolvable.", "label": "Author", "bboxes": [{"left": 0.673326797385621, "top": 0.7639772727272728, "width": 0.2388071895424837, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176307189542484, "top": 0.778219696969697, "width": 0.3942892156862745, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176307189542484, "top": 0.7927664141414141, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176307189542484, "top": 0.807314393939394, "width": 0.10605392156862736, "height": 0.012727272727272587, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2014"}, {"text": "To recover from this type of failure, we phase shift our transmitted signal by 36 after each transmission.", "label": "Author", "bboxes": [{"left": 0.6309313725490197, "top": 0.807314393939394, "width": 0.28098856209150325, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176307189542484, "top": 0.821861111111111, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176307189542484, "top": 0.836409090909091, "width": 0.03157352941176472, "height": 0.012727272727272587, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2015"}, {"text": "We used basic binary transmission as a proof of concept, but LightAnchors could also be extended to use multiple illumination levels and colors.", "label": "Author", "bboxes": [{"left": 0.5544918300653595, "top": 0.836409090909091, "width": 0.35759313725490194, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176307189542484, "top": 0.8509558080808081, "width": 0.39450326797385626, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176307189542484, "top": 0.8655025252525252, "width": 0.22215522875816995, "height": 0.01272727272727281, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2016"}, {"text": "We now briefly review these research areas.", "label": "Author", "bboxes": [{"left": 0.12196732026143792, "top": 0.12669444444444444, "width": 0.2886960784313725, "height": 0.012727272727272726, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2017"}, {"text": "After each frame is tracked, we attempt to decode all candidate anchors.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.7112335858585859, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.7257815656565656, "width": 0.08683333333333333, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2018"}, {"text": "As noted above, our tracker keeps a history of candidate anchors over time, which provides a sequence of intensity values.", "label": "Author", "bboxes": [{"left": 0.17958169934640522, "top": 0.7257815656565656, "width": 0.3030915032679739, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.7403282828282829, "width": 0.39443790849673205, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.7548762626262626, "width": 0.10646568627450982, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2019"}, {"text": "Rather than use only the center pixel value, we average over a small region, which we found to be less sensitive to camera noise and sub-pixel aliasing during motion.", "label": "Author", "bboxes": [{"left": 0.1988186274509804, "top": 0.7548762626262626, "width": 0.28372222222222226, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.7694229797979798, "width": 0.39425163398692803, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.783969696969697, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.7982121212121212, "width": 0.02961437908496732, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2020"}, {"text": "To convert the analog light intensity signal into a binary sequence, we use a dynamic threshold.", "label": "Author", "bboxes": [{"left": 0.12117647058823529, "top": 0.7982121212121212, "width": 0.36154575163398694, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8127588383838384, "width": 0.24877124183006533, "height": 0.012727272727272587, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2021"}, {"text": "We purposely employ preambles that contain both 1s and 0s (i.e., high and low brightness), which allows us to find the midpoint of the min and max intensity values at both the beginning and end of a transmission.", "label": "Author", "bboxes": [{"left": 0.33995588235294116, "top": 0.8127588383838384, "width": 0.14275000000000004, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.8273068181818182, "width": 0.3946519607843137, "height": 0.01272727272727281, "page": 2}, {"left": 0.0882516339869281, "top": 0.8418535353535355, "width": 0.3945196078431373, "height": 0.012727272727272587, "page": 2}, {"left": 0.0882516339869281, "top": 0.8564015151515151, "width": 0.39440522875817, "height": 0.01272727272727281, "page": 2}, {"left": 0.0882516339869281, "top": 0.8709482323232324, "width": 0.08686601307189544, "height": 0.012727272727272587, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2022"}, {"text": "We linearly interpolate between these two midpoints (Figure 2) to produce a binary string.", "label": "Author", "bboxes": [{"left": 0.17806535947712418, "top": 0.8709482323232324, "width": 0.3046895424836602, "height": 0.012727272727272587, "page": 2}, {"left": 0.0882516339869281, "top": 0.8854962121212121, "width": 0.3515490196078432, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2023"}, {"text": "We profiled our implementation using Xcode on both a iPhone 7 and iPhone X. We tested different base resolutions (i.e., largest pyramid size), and for each, ran three trials of 500 frames each.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.6260820707070708, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.6406300505050505, "width": 0.3945686274509803, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.6548712121212121, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.6694191919191919, "width": 0.11157189542483659, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2024"}, {"text": "Although reducing the image resolution greatly improves processing time, we found that scaling the image becomes a major bottleck (often making up 50% of the processing).", "label": "Author", "bboxes": [{"left": 0.7192140522875817, "top": 0.6985138888888889, "width": 0.19282189542483663, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.7130606060606061, "width": 0.39435457516339867, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.7276073232323232, "width": 0.394470588235294, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.7421553030303031, "width": 0.17743790849673202, "height": 0.012727272727272587, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2025"}, {"text": "Even though we used the highly optimized iOS CoreGraphics API, we suspect that this bottleneck could be greatly mitigated with better GPU acceleration, which could allow LightAnchor detection to run at 240 FPS or more.", "label": "Author", "bboxes": [{"left": 0.6995980392156863, "top": 0.7421553030303031, "width": 0.2125522875816993, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.7567020202020202, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.7712500000000001, "width": 0.394470588235294, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176307189542484, "top": 0.7857967171717172, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.800344696969697, "width": 0.08686601307189534, "height": 0.012727272727272587, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2026"}, {"text": "To evaluate the robustness of our approach, we tested point lights of different size, across varying rooms, lighting conditions, and sensing distances.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.8354760101010101, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.85002398989899, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.8642651515151516, "width": 0.18759967320261428, "height": 0.012727272727272587, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2027"}, {"text": "We also tested accuracy while the device was still and held by a user while walking.", "label": "Author", "bboxes": [{"left": 0.7101552287581699, "top": 0.8642651515151516, "width": 0.20178104575163403, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.8788131313131313, "width": 0.36567973856209146, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2028"}, {"text": "We now describe this procedure and results in detail.", "label": "Author", "bboxes": [{"left": 0.8890098039215686, "top": 0.8788131313131313, "width": 0.022991830065359542, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.8933598484848485, "width": 0.3193970588235293, "height": 0.012727272727272587, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2029"}, {"text": "Our detection process passes all candidate anchors to our tracker on every frame, which must be computationally inexpensive in order to maintain a high frame rate.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.4582032828282828, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.4727512626262626, "width": 0.3944379084967321, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.4872979797979798, "width": 0.33000490196078436, "height": 0.012727272727272754, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2030"}, {"text": "First, we merge proximate candidate anchors  ones too close to be separate LightAnchors (this often happens when a LightAnchor is detected at multiple pyramid levels).", "label": "Author", "bboxes": [{"left": 0.42390522875816994, "top": 0.4872979797979798, "width": 0.05868137254901967, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.5018459595959596, "width": 0.39433823529411766, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.5163926767676768, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.5309393939393939, "width": 0.2841013071895425, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2031"}, {"text": "We then attempt to pair all current candidates with candidates from the previous frame using a greedy Euclidean distance matcher with a threshold to discard unlikely pairings.", "label": "Author", "bboxes": [{"left": 0.37566339869281046, "top": 0.5309393939393939, "width": 0.10692647058823535, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.5454873737373738, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.560034090909091, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.5745820707070707, "width": 0.25428921568627455, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2032"}, {"text": "Our tracker also uses a timeto-live of five frames to compensate for momentary losses in tracking (e.g., image noise, momentary occlusion, loss of focus).", "label": "Author", "bboxes": [{"left": 0.29489215686274506, "top": 0.6033712121212121, "width": 0.18784640522875817, "height": 0.012727272727272698, "page": 2}, {"left": 0.08823529411764706, "top": 0.6179179292929293, "width": 0.3945130718954248, "height": 0.012727272727272698, "page": 2}, {"left": 0.08823529411764706, "top": 0.632465909090909, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 2}, {"left": 0.08823529411764706, "top": 0.6470126262626262, "width": 0.03155718954248367, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2033"}, {"text": "Although this algorithm is basic, it is computationally inexpensive and works well in practice due to our high frame rate.", "label": "Author", "bboxes": [{"left": 0.12470098039215685, "top": 0.6470126262626262, "width": 0.3580866013071896, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.6615606060606061, "width": 0.3943218954248366, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.6761073232323233, "width": 0.02882679738562091, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2034"}, {"text": "Our LightAnchor detection algorithm is designed to have high recall.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.263354797979798, "width": 0.39438888888888884, "height": 0.012727272727272754, "page": 2}, {"left": 0.08823529411764706, "top": 0.2779027777777778, "width": 0.07508986928104573, "height": 0.012727272727272754, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2035"}, {"text": "Given a raw camera image, we first convert to grayscale and build an image pyramid (five layers, scaling by half).", "label": "Author", "bboxes": [{"left": 0.16900653594771242, "top": 0.2779027777777778, "width": 0.3136993464052288, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.29244949494949496, "width": 0.3944869281045752, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.30699747474747474, "width": 0.055076797385620935, "height": 0.012727272727272754, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2036"}, {"text": "We model LightAnchors as bright spots surrounded by darker regions.", "label": "Author", "bboxes": [{"left": 0.14662418300653593, "top": 0.30699747474747474, "width": 0.33606535947712424, "height": 0.012727272727272754, "page": 2}, {"left": 0.08820261437908496, "top": 0.3215441919191919, "width": 0.12058169934640524, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2037"}, {"text": "Specifically, for each pixel, we compute the difference between the center pixel value and the maximum value of all pixels in a 7  7 diamond perimeter.", "label": "Author", "bboxes": [{"left": 0.2140702614379085, "top": 0.3215441919191919, "width": 0.2684705882352941, "height": 0.012727272727272698, "page": 2}, {"left": 0.08818627450980392, "top": 0.3360909090909091, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 2}, {"left": 0.08818627450980392, "top": 0.3506388888888889, "width": 0.33700490196078425, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2038"}, {"text": "We then threshold this result at every pixel and at every pyramid level, which produces an array of candidate anchors for each incoming frame of video.", "label": "Author", "bboxes": [{"left": 0.428125816993464, "top": 0.3506388888888889, "width": 0.05449509803921565, "height": 0.012727272727272698, "page": 2}, {"left": 0.08816993464052288, "top": 0.365185606060606, "width": 0.3943055555555556, "height": 0.012727272727272754, "page": 2}, {"left": 0.08816993464052288, "top": 0.3794280303030303, "width": 0.3944035947712418, "height": 0.012727272727272698, "page": 2}, {"left": 0.08816993464052288, "top": 0.3939747474747475, "width": 0.15821568627450983, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2039"}, {"text": "Finally, we flatten results from all pyramid layers so that candidate anchors are in the coordinate space of the highest resolution pyramid.", "label": "Author", "bboxes": [{"left": 0.25246405228758173, "top": 0.3939747474747475, "width": 0.23006045751633986, "height": 0.012727272727272698, "page": 2}, {"left": 0.08816993464052288, "top": 0.40852272727272726, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 2}, {"left": 0.08816993464052288, "top": 0.4230694444444445, "width": 0.29269771241830067, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2040"}, {"text": "Like actual LightAnchors, these blink valid sequences and are decoded correctly by our pipeline.", "label": "Author", "bboxes": [{"left": 0.7411862745098039, "top": 0.489125, "width": 0.1707826797385621, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176633986928104, "top": 0.5036729797979798, "width": 0.3944379084967321, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.5182196969696969, "width": 0.08609150326797388, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2041"}, {"text": "However, they almost always have a lower range of intensities (as they are secondary sources of light) and we use this fact to filter them.", "label": "Author", "bboxes": [{"left": 0.6113970588235295, "top": 0.5182196969696969, "width": 0.30070424836601306, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.5327676767676768, "width": 0.39440522875816997, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.547314393939394, "width": 0.226467320261438, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2042"}, {"text": "More specifically, if two candidate anchors are found to have valid, but identical sequences in the same frame, we only accept the one with higher signal variance.", "label": "Author", "bboxes": [{"left": 0.7490294117647059, "top": 0.547314393939394, "width": 0.16314379084967312, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.5618623737373737, "width": 0.39450326797385626, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.576409090909091, "width": 0.39451143790849674, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.590955808080808, "width": 0.1484330065359477, "height": 0.01272727272727281, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2043"}, {"text": "In our proof-of-concept iOS app, we use the AVCaptureSession API to grab video frames and OpenCV for image processing.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.08305176767676768, "width": 0.3945196078431373, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.09759974747474748, "width": 0.39443790849673205, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.11214646464646463, "width": 0.052343137254901986, "height": 0.01272727272727274, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2044"}, {"text": "We enqueue all video frames, which are consumed asynchronously by our detection-tracking-decoding thread (described in subsequent sections).", "label": "Author", "bboxes": [{"left": 0.14509150326797388, "top": 0.11214646464646463, "width": 0.33764705882352936, "height": 0.01272727272727274, "page": 2}, {"left": 0.08823529411764706, "top": 0.12669444444444444, "width": 0.3944705882352941, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.14124116161616163, "width": 0.22801960784313724, "height": 0.012727272727272726, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2045"}, {"text": "Our software runs on the iPhone 7 and X, which can capture video frames at 240 FPS.", "label": "Author", "bboxes": [{"left": 0.3203888888888889, "top": 0.14124116161616163, "width": 0.16216830065359472, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.15578787878787878, "width": 0.3943218954248366, "height": 0.012727272727272726, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2046"}, {"text": "In general, this is too much pixel data for our current implementation to process in real time at high video resolutions, and so we generally use 720p video.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.1703358585858586, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 2}, {"left": 0.08823529411764706, "top": 0.18457702020202021, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 2}, {"left": 0.08823529411764706, "top": 0.199125, "width": 0.2335359477124183, "height": 0.012727272727272754, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2047"}, {"text": "Only at 320x180 can we process a 240 FPS image stream.", "label": "Author", "bboxes": [{"left": 0.3254934640522876, "top": 0.199125, "width": 0.15709640522875817, "height": 0.012727272727272754, "page": 2}, {"left": 0.08823529411764706, "top": 0.21367171717171718, "width": 0.2237042483660131, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2048"}, {"text": "When frames are scaled, we use iOSs optimized CoreGraphics API.", "label": "Author", "bboxes": [{"left": 0.31722549019607843, "top": 0.21367171717171718, "width": 0.16534803921568625, "height": 0.012727272727272698, "page": 2}, {"left": 0.08826797385620916, "top": 0.22821969696969696, "width": 0.2852875816993464, "height": 0.012727272727272754, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2049"}, {"text": "We then test for the presence of our known pre/postamble.", "label": "Author", "bboxes": [{"left": 0.6333202614379084, "top": 0.36547979797979796, "width": 0.2787810457516341, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176470588235295, "top": 0.3800277777777778, "width": 0.09666666666666657, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2050"}, {"text": "If this is missing, the candidate is not decoded (i.e., we might be too early or late, or the tracked point is a static light and not a modulated light anchor).", "label": "Author", "bboxes": [{"left": 0.6176405228758169, "top": 0.3800277777777778, "width": 0.2944934640522877, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.3945744949494949, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176470588235295, "top": 0.4091212121212121, "width": 0.3088235294117646, "height": 0.012727272727272698, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2051"}, {"text": "We captured study data using an iPhone 7 (720p at 120 FPS) in three environments: workshop, classroom and office.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.29153661616161614, "width": 0.39407516339869275, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.3057790404040404, "width": 0.37464052287581695, "height": 0.012727272727272754, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2052"}, {"text": "In each of these settings, we varied the lighting condition: artificial light only (mean 321 lux), artificial light + diffuse sunlight (mean 428 lux) and lights off (mean 98 lux).", "label": "Author", "bboxes": [{"left": 0.46895261437908503, "top": 0.3057790404040404, "width": 0.013720588235294096, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.3203257575757576, "width": 0.39450326797385615, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.33487373737373743, "width": 0.39443790849673205, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3494204545454545, "width": 0.3334967320261438, "height": 0.012727272727272754, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2053"}, {"text": "We captured data using a tripod (i.e., smartphone held still) and also while walking slowly (~1 m/s, to induce some motion blur).", "label": "Author", "bboxes": [{"left": 0.426640522875817, "top": 0.3494204545454545, "width": 0.05608169934640522, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.3639684343434343, "width": 0.3945196078431373, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.3785151515151515, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2054"}, {"text": "We recorded approximately one second of video data at 2, 4, 6, 8, 10 and 12 meters.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.3930618686868687, "width": 0.39428921568627445, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.4076098484848485, "width": 0.14762581699346405, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2055"}, {"text": "For our still condition, we used a surveyors rope to mark distances, and for our walking condition, we used a 50cm printed ArUco tag for continuous distance tracking (accepting frames within 0.5m).", "label": "Author", "bboxes": [{"left": 0.23958333333333334, "top": 0.4076098484848485, "width": 0.24308986928104573, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.4221565656565657, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.43670454545454546, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.45125126262626264, "width": 0.3119035947712418, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2056"}, {"text": "Within each setting, we used two exemplar point lights: a standard 3mm LED and a larger 100x100mm LED matrix.", "label": "Author", "bboxes": [{"left": 0.4034656862745098, "top": 0.45125126262626264, "width": 0.07920751633986928, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.46549368686868686, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.48004040404040405, "width": 0.308016339869281, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2057"}, {"text": "The likelihood of any random pixel in the environment matching our pre/postamble is fairly low.", "label": "Author", "bboxes": [{"left": 0.5835228758169935, "top": 0.2782058080808081, "width": 0.3285882352941176, "height": 0.012727272727272754, "page": 3}, {"left": 0.517640522875817, "top": 0.29275378787878786, "width": 0.31000816993464064, "height": 0.012727272727272754, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2058"}, {"text": "Upon closer analysis of the video data, it appears most of these were actually small reflections of our actual LightAnchors, and thus transmitting correct patterns (an effect discussed in our Decoding section above).", "label": "Author", "bboxes": [{"left": 0.8321617647058823, "top": 0.29275378787878786, "width": 0.07999836601307186, "height": 0.012727272727272754, "page": 3}, {"left": 0.517640522875817, "top": 0.30730050505050505, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.517640522875817, "top": 0.32184848484848483, "width": 0.3946339869281047, "height": 0.012727272727272698, "page": 3}, {"left": 0.517640522875817, "top": 0.336395202020202, "width": 0.39450326797385626, "height": 0.012727272727272698, "page": 3}, {"left": 0.517640522875817, "top": 0.35094191919191925, "width": 0.14564869281045756, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2059"}, {"text": "If we apply our variance filter, accepting only the best signal, it reduces false positives to 0.4% and true positives to 99.6%.", "label": "Author", "bboxes": [{"left": 0.6666160130718954, "top": 0.35094191919191925, "width": 0.24552777777777768, "height": 0.012727272727272698, "page": 3}, {"left": 0.517640522875817, "top": 0.36548989898989903, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 3}, {"left": 0.517640522875817, "top": 0.38003661616161616, "width": 0.15508660130718954, "height": 0.012727272727272754, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2060"}, {"text": "Across all conditions and distances, we found a mean bit error rate (BER) of 5.2%, or roughly 1 error in every 20 transmitted bits.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.4151729797979798, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.4297209595959596, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.4442676767676768, "width": 0.07512254901960769, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2061"}, {"text": "Note that this figure includes the 0.4% of false positives that made it through our various filtering stages.", "label": "Author", "bboxes": [{"left": 0.5980571895424837, "top": 0.4442676767676768, "width": 0.3139624183006535, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.4588156565656566, "width": 0.39427287581699355, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2062"}, {"text": "We also computed BER across the different base resolutions used in our performance analysis (Figure 3), and it is clear that high resolution (at least 720p) is needed to detect, track and decode LightAnchors accurately.", "label": "Author", "bboxes": [{"left": 0.7839428104575163, "top": 0.5748876262626262, "width": 0.1282401960784314, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.5894356060606061, "width": 0.3944379084967321, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176633986928104, "top": 0.6039823232323233, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.618530303030303, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.6330770202020202, "width": 0.11074836601307203, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2063"}, {"text": "Our detection rate did not change substantially across study conditions, and so we combine detection results for brevity.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.6024457070707071, "width": 0.3945196078431373, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.6169936868686868, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2064"}, {"text": "On average, our system found 50.8 candidate anchors (9.0 SD) in each frame.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.631540404040404, "width": 0.39443790849673205, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.6460883838383839, "width": 0.12412254901960786, "height": 0.01272727272727281, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2065"}, {"text": "Of course, only two of these were actual LightAnchors, and our system detected these in all cases (i.e., a true positive rate of 100%).", "label": "Author", "bboxes": [{"left": 0.21687091503267972, "top": 0.6460883838383839, "width": 0.26568627450980387, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.6606351010101009, "width": 0.3946356209150326, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.6751818181818182, "width": 0.22404901960784307, "height": 0.012727272727272587, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2066"}, {"text": "After the pre/postamble filtering process, the true positive rate was still 100%, but our system found 3.1% false", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.6969974747474748, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7115441919191919, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2067"}, {"text": "Across all conditions, we found a mean recognition time of 464 ms. As our test LightAnchors were 22 bits long (6 bit preamble, 10 bit payload, 6 bit postamble), they take a minimum of 183ms to transmit at 120 FPS.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.4436666666666667, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4582146464646465, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4727613636363637, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.48730934343434346, "width": 0.25980718954248366, "height": 0.012727272727272754, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2068"}, {"text": "We note that this latency varies across conditions, for example, mean recognition latency is 312 ms when the camera was held still (i.e., the first full transmission is often successful) vs. 615 ms when the user was walking (~3 transmissions before recognition).", "label": "Author", "bboxes": [{"left": 0.17055555555555554, "top": 0.5742878787878788, "width": 0.31218300653594766, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.588834595959596, "width": 0.3945179738562091, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.6033813131313132, "width": 0.3946307189542484, "height": 0.01272727272727281, "page": 4}, {"left": 0.08821895424836601, "top": 0.6179292929292929, "width": 0.39447058823529413, "height": 0.01272727272727281, "page": 4}, {"left": 0.08821895424836601, "top": 0.6324760101010101, "width": 0.222921568627451, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2069"}, {"text": "To illustrate these different options, as well as to highlight the potential utility of LightAnchors, we created eleven demo applications.", "label": "Author", "bboxes": [{"left": 0.6129297385620915, "top": 0.36214646464646466, "width": 0.2993039215686275, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.37669444444444444, "width": 0.3944869281045751, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.3912411616161616, "width": 0.1798251633986928, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2070"}, {"text": "We note that these examples would require no a priori setup of devices and smartphones, and would allow anyone with the LightAnchors App on their phone to begin instantly interacting with objects in AR.", "label": "Author", "bboxes": [{"left": 0.7063006535947712, "top": 0.3912411616161616, "width": 0.20599836601307198, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.4057878787878788, "width": 0.39427287581699344, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176633986928104, "top": 0.4203358585858586, "width": 0.3944869281045752, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176633986928104, "top": 0.4348825757575758, "width": 0.36255065359477134, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2071"}, {"text": "Instead, we envision transmission of an ID, which permits lookup through a cloud service, after which larger payloads (e.g., restaurant name, opening hours, menu, coupons, etc.) could be fetched over a faster connection (e.g., cellular, WiFi).", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.5121489898989899, "width": 0.39460294117647055, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.526695707070707, "width": 0.3946356209150327, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.5412424242424242, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.555790404040404, "width": 0.3942401960784314, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.5703371212121212, "width": 0.04318954248366014, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2072"}, {"text": "As a demonstration of a fixed payload, we instrumented a street parking meter (Figure 5, left) with a light that transmits its enforcement zone ID (from which the rate schedule could", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.6070050505050505, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.6212462121212121, "width": 0.39460294117647055, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.6357941919191918, "width": 0.39440522875816997, "height": 0.01272727272727281, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2073"}, {"text": "for our system to completely miss a visible LightAnchor, but it is common for a LightAnchors to have to transmit several times before being recognized.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.32002146464646464, "width": 0.39440522875816986, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.3345694444444444, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.3491161616161616, "width": 0.20095751633986925, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2074"}, {"text": "To quantity this, we used our collected data to compute the average time required to detect, track, and decode a LightAnchor.", "label": "Author", "bboxes": [{"left": 0.29291503267973856, "top": 0.3491161616161616, "width": 0.18979084967320264, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.3636641414141414, "width": 0.3943088235294117, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.37821085858585857, "width": 0.2205408496732026, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2075"}, {"text": "To do this, we started our detection pipeline at random offsets in our video data and recorded how long it took until LightAnchors were successfully decoded.", "label": "Author", "bboxes": [{"left": 0.3132892156862745, "top": 0.37821085858585857, "width": 0.16940032679738565, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.3924532828282828, "width": 0.3944705882352941, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.407, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.42154671717171716, "width": 0.0935032679738562, "height": 0.012727272727272698, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2076"}, {"text": "As a final demo, we created a mock payment terminal (Figure 7, right) that could allow a", "label": "Author", "bboxes": [{"left": 0.3011339869281046, "top": 0.8576161616161616, "width": 0.18150653594771232, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.8718573232323232, "width": 0.3943055555555555, "height": 0.01272727272727281, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2077"}, {"text": "We have presented our work on LightAnchors, a new approach for overlaying information and interfaces onto everyday objects in mobile AR.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.8215366161616162, "width": 0.3943888888888889, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.8360845959595959, "width": 0.39440522875816997, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8506313131313132, "width": 0.1707843137254902, "height": 0.012727272727272587, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2078"}, {"text": "We take advantage of point lights (e.g., LEDs) that already exist in a wide range of products (or could be added for a few dollars).", "label": "Author", "bboxes": [{"left": 0.6925490196078431, "top": 0.8506313131313132, "width": 0.21973366013071904, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.8651792929292929, "width": 0.39445424836601295, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8794204545454545, "width": 0.2248398692810457, "height": 0.01272727272727281, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2079"}, {"text": "We described our implementation and results from an evaluation, which shows that", "label": "Author", "bboxes": [{"left": 0.7473954248366013, "top": 0.8794204545454545, "width": 0.16475490196078435, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8939684343434344, "width": 0.39435457516339867, "height": 0.012727272727272587, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2080"}, {"text": "The biggest drawback of our method is limited bitrate, which is chiefly set by smartphone processors and camera FPS.", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.36456691919191925, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.37911489898989903, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2081"}, {"text": "This limits our practical payload size and makes our system prone to security issues found in schemes such as QR codes [16] .", "label": "Author", "bboxes": [{"left": 0.5176470588235295, "top": 0.3936616161616161, "width": 0.394388888888889, "height": 0.012727272727272754, "page": 5}, {"left": 0.5176470588235295, "top": 0.408209595959596, "width": 0.3945866013071896, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.4227563131313131, "width": 0.034704248366012935, "height": 0.012727272727272754, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2082"}, {"text": "We found the automatic camera settings were not ideal for LightAnchors (i.e., clipping in dark scenes), and so we had to lock settings such as exposure.", "label": "Author", "bboxes": [{"left": 0.8195196078431373, "top": 0.5754962121212122, "width": 0.09240032679738563, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.5900441919191919, "width": 0.3946192810457517, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.6045909090909091, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.6191376262626263, "width": 0.12565359477124183, "height": 0.012727272727272698, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2083"}, {"text": "However, as our user interface is a passthrough AR experience, settings that are ideal for LightAnchors are not always ideal for a human user.", "label": "Author", "bboxes": [{"left": 0.6532826797385621, "top": 0.6191376262626263, "width": 0.25873856209150325, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.6336856060606061, "width": 0.3945359477124183, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.6482323232323232, "width": 0.34372875816993476, "height": 0.01272727272727281, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2084"}, {"text": "We also created two other demo devices that generally lack compute: a fire alarm that reports its operating status and an electrical strip that reports its power draw (Figure 6, left and center).", "label": "Author", "bboxes": [{"left": 0.3435130718954248, "top": 0.5242727272727272, "width": 0.13923202614379088, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5388194444444444, "width": 0.3943545751633987, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5533674242424242, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5679141414141414, "width": 0.3096062091503268, "height": 0.012727272727272698, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2085"}, {"text": "Finally, at present, our LightAnchor widgets are flat with respect to the smartphone screen, as a single LightAnchor cannot provide 3D orientation.", "label": "Author", "bboxes": [{"left": 0.5176307189542484, "top": 0.6700467171717172, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176307189542484, "top": 0.6845946969696969, "width": 0.39450326797385626, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176307189542484, "top": 0.6991414141414142, "width": 0.17979248366013068, "height": 0.012727272727272587, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2086"}, {"text": "Similarly, we modified an outdoor entrance light to output a fixed UID (Figure 5, center); the summoned LightAnchor displays the building name (Department of Motor Vehicles), its current status (open), and closing time (5pm).", "label": "Author", "bboxes": [{"left": 0.18509803921568627, "top": 0.3148699494949495, "width": 0.29747549019607844, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.3294179292929293, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.3439646464646465, "width": 0.3945196078431373, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.35851262626262625, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.37305934343434344, "width": 0.044107843137254896, "height": 0.012727272727272698, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2087"}, {"text": "Lastly, we modified a conference room phone demo that conveniently displays its call-in number (Figure 5, right).", "label": "Author", "bboxes": [{"left": 0.1372516339869281, "top": 0.37305934343434344, "width": 0.34543790849673206, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.38760606060606057, "width": 0.3941895424836601, "height": 0.012727272727272754, "page": 5}, {"left": 0.08823529411764706, "top": 0.40215404040404035, "width": 0.04059150326797385, "height": 0.012727272727272754, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2088"}, {"text": "our approach can be rapid and accurate.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.07002146464646465, "width": 0.2652745098039216, "height": 0.01272727272727274, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2089"}, {"text": "We plan to release our LightAnchors app on the Apple App Store, as it can run on recent iOS devices.", "label": "Author", "bboxes": [{"left": 0.35841830065359476, "top": 0.07002146464646465, "width": 0.12418954248366015, "height": 0.01272727272727274, "page": 6}, {"left": 0.08823529411764706, "top": 0.08456944444444445, "width": 0.39450326797385615, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.09911616161616162, "width": 0.1472467320261438, "height": 0.012727272727272726, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2090"}, {"text": "This research was generously supported with funds from the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.", "label": "Author", "bboxes": [{"left": 0.08823529411764706, "top": 0.1342638888888889, "width": 0.39428921568627445, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.1488118686868687, "width": 0.3945196078431373, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.16335858585858584, "width": 0.3944705882352941, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.17790656565656565, "width": 0.08057352941176472, "height": 0.012727272727272754, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2091"}, {"text": "We are also grateful to Anthony Rowe and his lab for early brainstorming on this effort.", "label": "Author", "bboxes": [{"left": 0.1737173202614379, "top": 0.17790656565656565, "width": 0.3091209150326798, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.19245328282828283, "width": 0.26838888888888884, "height": 0.012727272727272698, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2092"}, {"text": "Another difference from conventional markers is that LightAnchors can transmit dynamic payloads, without the need for WiFi, Bluetooth or indeed, any connectivity.", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.5248699494949495, "width": 0.394375816993464, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5394179292929293, "width": 0.3943382352941175, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5539646464646465, "width": 0.36368627450980384, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2093"}, {"text": "In addition to describing our algorithm, we also report the findings of a transmission performance study, which tested accuracy at different distances, with two light sizes, and while held still and in motion.", "label": "Novelty", "bboxes": [{"left": 0.7258349673202614, "top": 0.8085353535353536, "width": 0.1862663398692811, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8230833333333333, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8376300505050506, "width": 0.3942565359477125, "height": 0.012727272727272587, "page": 0}, {"left": 0.5176307189542484, "top": 0.8518724747474747, "width": 0.3668986928104575, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2094"}, {"text": "Unlike most prior tracking methods, which instrument objects with markers (often large and/or obtrusive), we take advantage of point lights already found in many objects and environments.", "label": "Novelty", "bboxes": [{"left": 0.12822549019607843, "top": 0.6463977272727273, "width": 0.3543643790849674, "height": 0.01272727272727281, "page": 0}, {"left": 0.08821895424836601, "top": 0.6609457070707071, "width": 0.39433823529411766, "height": 0.012727272727272587, "page": 0}, {"left": 0.08821895424836601, "top": 0.6754924242424243, "width": 0.39453594771241834, "height": 0.01272727272727281, "page": 0}, {"left": 0.08821895424836601, "top": 0.690040404040404, "width": 0.09312418300653594, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2095"}, {"text": "This blinking speed is right at humans flicker fusion threshold, and the flashing is generally imperceptible, but depends on the particular payload.", "label": "Novelty", "bboxes": [{"left": 0.6603970588235294, "top": 0.6836679292929293, "width": 0.2517532679738562, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176470588235295, "top": 0.698215909090909, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176470588235295, "top": 0.7127626262626263, "width": 0.30956535947712416, "height": 0.012727272727272587, "page": 1}], "section": "Encoding & Point Lights", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2096"}, {"text": "However, this approach does not work as well for point light sources (which only cover a small portion of the image at typical distances), nor when many lights are active in a scene.", "label": "Novelty", "bboxes": [{"left": 0.6948872549019608, "top": 0.30852020202020203, "width": 0.21709803921568616, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176307189542484, "top": 0.3230681818181818, "width": 0.3944052287581701, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176307189542484, "top": 0.337614898989899, "width": 0.3944869281045752, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176307189542484, "top": 0.3521628787878788, "width": 0.17626797385620907, "height": 0.012727272727272698, "page": 1}], "section": "Visible Light Communication with Commodity Devices", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2097"}, {"text": "However, these systems require some form of preregistration of the to-be-recognized object or scene, and the object itself cannot transfer information beyond its identify.", "label": "Novelty", "bboxes": [{"left": 0.8749199346405229, "top": 0.09912247474747474, "width": 0.03725653594771228, "height": 0.012727272727272726, "page": 1}, {"left": 0.517673202614379, "top": 0.11366919191919192, "width": 0.3944379084967321, "height": 0.012727272727272726, "page": 1}, {"left": 0.517673202614379, "top": 0.12821717171717173, "width": 0.39447058823529413, "height": 0.012727272727272726, "page": 1}, {"left": 0.517673202614379, "top": 0.14276388888888888, "width": 0.3095637254901962, "height": 0.012727272727272726, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2098"}, {"text": "Unlike prior approaches that synchronized light modulation with e.g., RF triggers [12], our lights and smartphones are totally unsynchronized.", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.7348825757575758, "width": 0.394470588235294, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176470588235295, "top": 0.7494305555555555, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176470588235295, "top": 0.7639772727272728, "width": 0.15274836601307185, "height": 0.012727272727272587, "page": 1}], "section": "Encoding & Point Lights", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2099"}, {"text": "We used basic binary transmission as a proof of concept, but LightAnchors could also be extended to use multiple illumination levels and colors.", "label": "Novelty", "bboxes": [{"left": 0.5544918300653595, "top": 0.836409090909091, "width": 0.35759313725490194, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176307189542484, "top": 0.8509558080808081, "width": 0.39450326797385626, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176307189542484, "top": 0.8655025252525252, "width": 0.22215522875816995, "height": 0.01272727272727281, "page": 1}], "section": "Encoding & Point Lights", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2100"}, {"text": "We profiled our implementation using Xcode on both a iPhone 7 and iPhone X. We tested different base resolutions (i.e., largest pyramid size), and for each, ran three trials of 500 frames each.", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.6260820707070708, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.6406300505050505, "width": 0.3945686274509803, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.6548712121212121, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.6694191919191919, "width": 0.11157189542483659, "height": 0.01272727272727281, "page": 2}], "section": "Performance Analysis", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2101"}, {"text": "Although reducing the image resolution greatly improves processing time, we found that scaling the image becomes a major bottleck (often making up 50% of the processing).", "label": "Novelty", "bboxes": [{"left": 0.7192140522875817, "top": 0.6985138888888889, "width": 0.19282189542483663, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.7130606060606061, "width": 0.39435457516339867, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.7276073232323232, "width": 0.394470588235294, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.7421553030303031, "width": 0.17743790849673202, "height": 0.012727272727272587, "page": 2}], "section": "Performance Analysis", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2102"}, {"text": "Even though we used the highly optimized iOS CoreGraphics API, we suspect that this bottleneck could be greatly mitigated with better GPU acceleration, which could allow LightAnchor detection to run at 240 FPS or more.", "label": "Novelty", "bboxes": [{"left": 0.6995980392156863, "top": 0.7421553030303031, "width": 0.2125522875816993, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.7567020202020202, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.7712500000000001, "width": 0.394470588235294, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176307189542484, "top": 0.7857967171717172, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.800344696969697, "width": 0.08686601307189534, "height": 0.012727272727272587, "page": 2}], "section": "Performance Analysis", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2103"}, {"text": "To evaluate the robustness of our approach, we tested point lights of different size, across varying rooms, lighting conditions, and sensing distances.", "label": "Novelty", "bboxes": [{"left": 0.5176470588235295, "top": 0.8354760101010101, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.85002398989899, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.8642651515151516, "width": 0.18759967320261428, "height": 0.012727272727272587, "page": 2}], "section": "EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2104"}, {"text": "We also tested accuracy while the device was still and held by a user while walking.", "label": "Novelty", "bboxes": [{"left": 0.7101552287581699, "top": 0.8642651515151516, "width": 0.20178104575163403, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.8788131313131313, "width": 0.36567973856209146, "height": 0.01272727272727281, "page": 2}], "section": "EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2105"}, {"text": "Although this algorithm is basic, it is computationally inexpensive and works well in practice due to our high frame rate.", "label": "Novelty", "bboxes": [{"left": 0.12470098039215685, "top": 0.6470126262626262, "width": 0.3580866013071896, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.6615606060606061, "width": 0.3943218954248366, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.6761073232323233, "width": 0.02882679738562091, "height": 0.01272727272727281, "page": 2}], "section": "Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2106"}, {"text": "Specifically, for each pixel, we compute the difference between the center pixel value and the maximum value of all pixels in a 7  7 diamond perimeter.", "label": "Novelty", "bboxes": [{"left": 0.2140702614379085, "top": 0.3215441919191919, "width": 0.2684705882352941, "height": 0.012727272727272698, "page": 2}, {"left": 0.08818627450980392, "top": 0.3360909090909091, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 2}, {"left": 0.08818627450980392, "top": 0.3506388888888889, "width": 0.33700490196078425, "height": 0.012727272727272698, "page": 2}], "section": "Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2107"}, {"text": "However, they almost always have a lower range of intensities (as they are secondary sources of light) and we use this fact to filter them.", "label": "Novelty", "bboxes": [{"left": 0.6113970588235295, "top": 0.5182196969696969, "width": 0.30070424836601306, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.5327676767676768, "width": 0.39440522875816997, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.547314393939394, "width": 0.226467320261438, "height": 0.012727272727272698, "page": 2}], "section": "Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2108"}, {"text": "More specifically, if two candidate anchors are found to have valid, but identical sequences in the same frame, we only accept the one with higher signal variance.", "label": "Novelty", "bboxes": [{"left": 0.7490294117647059, "top": 0.547314393939394, "width": 0.16314379084967312, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.5618623737373737, "width": 0.39450326797385626, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.576409090909091, "width": 0.39451143790849674, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.590955808080808, "width": 0.1484330065359477, "height": 0.01272727272727281, "page": 2}], "section": "Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2109"}, {"text": "However, if the pre/postamble is correct, the data payload is saved to the anchor.", "label": "Novelty", "bboxes": [{"left": 0.8325490196078431, "top": 0.4091212121212121, "width": 0.07956862745098048, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.4236691919191919, "width": 0.3943888888888889, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.4382159090909091, "width": 0.048802287581699355, "height": 0.012727272727272698, "page": 2}], "section": "Detection", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2110"}, {"text": "We captured data using a tripod (i.e., smartphone held still) and also while walking slowly (~1 m/s, to induce some motion blur).", "label": "Novelty", "bboxes": [{"left": 0.426640522875817, "top": 0.3494204545454545, "width": 0.05608169934640522, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.3639684343434343, "width": 0.3945196078431373, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.3785151515151515, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 3}], "section": "Apparatus & Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2111"}, {"text": "These two lights simultaneously emitted different (but known) 16-bit LightAnchors, driven by a single Arduino Mega.", "label": "Novelty", "bboxes": [{"left": 0.14392156862745098, "top": 0.5091351010101011, "width": 0.33868627450980393, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.5236818181818181, "width": 0.39447058823529413, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.538229797979798, "width": 0.04137418300653596, "height": 0.012727272727272698, "page": 3}], "section": "Apparatus & Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2112"}, {"text": "With respect to light size, the small LED had 6.5% BER, while the larger LED had 3.8% (Figure 4, left).", "label": "Novelty", "bboxes": [{"left": 0.6694068627450981, "top": 0.5021515151515151, "width": 0.2426127450980391, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.5166982323232323, "width": 0.3943725490196077, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.5312462121212121, "width": 0.052359477124182985, "height": 0.012727272727272698, "page": 3}], "section": "Bit Error Rate", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2113"}, {"text": "Unsurprisingly, BER was higher while walking (mean 7.7%) than when the camera was still (2.7%), as seen in Figure 4 middle.", "label": "Novelty", "bboxes": [{"left": 0.5788349673202614, "top": 0.5312462121212121, "width": 0.33329901960784325, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176633986928104, "top": 0.5457929292929292, "width": 0.39437581699346413, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.5603409090909092, "width": 0.13235784313725496, "height": 0.012727272727272698, "page": 3}], "section": "Bit Error Rate", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2114"}, {"text": "We also computed BER across the different base resolutions used in our performance analysis (Figure 3), and it is clear that high resolution (at least 720p) is needed to detect, track and decode LightAnchors accurately.", "label": "Novelty", "bboxes": [{"left": 0.7839428104575163, "top": 0.5748876262626262, "width": 0.1282401960784314, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.5894356060606061, "width": 0.3944379084967321, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176633986928104, "top": 0.6039823232323233, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.618530303030303, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.6330770202020202, "width": 0.11074836601307203, "height": 0.012727272727272698, "page": 3}], "section": "Bit Error Rate", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2115"}, {"text": "After the pre/postamble filtering process, the true positive rate was still 100%, but our system found 3.1% false", "label": "Novelty", "bboxes": [{"left": 0.08823529411764706, "top": 0.6969974747474748, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7115441919191919, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 3}], "section": "LightAnchor Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2116"}, {"text": "To illustrate these different options, as well as to highlight the potential utility of LightAnchors, we created eleven demo applications.", "label": "Novelty", "bboxes": [{"left": 0.6129297385620915, "top": 0.36214646464646466, "width": 0.2993039215686275, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.37669444444444444, "width": 0.3944869281045751, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.3912411616161616, "width": 0.1798251633986928, "height": 0.012727272727272698, "page": 4}], "section": "PAYLOAD TYPES & EXAMPLE USES", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2117"}, {"text": "Although this could contain plain text, the limited bitrate of LightAnchors makes this impractical.", "label": "Novelty", "bboxes": [{"left": 0.657202614379085, "top": 0.4830542929292929, "width": 0.25473366013071896, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.49760101010101015, "width": 0.39432189542483653, "height": 0.012727272727272754, "page": 4}], "section": "Fixed Payloads", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2118"}, {"text": "for our system to completely miss a visible LightAnchor, but it is common for a LightAnchors to have to transmit several times before being recognized.", "label": "Novelty", "bboxes": [{"left": 0.08823529411764706, "top": 0.32002146464646464, "width": 0.39440522875816986, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.3345694444444444, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.3491161616161616, "width": 0.20095751633986925, "height": 0.012727272727272698, "page": 4}], "section": "Recognition Latency", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2119"}, {"text": "However, as our user interface is a passthrough AR experience, settings that are ideal for LightAnchors are not always ideal for a human user.", "label": "Novelty", "bboxes": [{"left": 0.6532826797385621, "top": 0.6191376262626263, "width": 0.25873856209150325, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.6336856060606061, "width": 0.3945359477124183, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.6482323232323232, "width": 0.34372875816993476, "height": 0.01272727272727281, "page": 5}], "section": "LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2120"}, {"text": "However, a known geometry of at least three non-planar LightAnchors (e.g., status lights on a microwave or WiFi router) could allow for recovery of 6DOF position in the future.", "label": "Novelty", "bboxes": [{"left": 0.7023153594771242, "top": 0.6991414141414142, "width": 0.20976960784313725, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176307189542484, "top": 0.7136893939393939, "width": 0.3945359477124183, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176307189542484, "top": 0.7282361111111112, "width": 0.39445424836601306, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176307189542484, "top": 0.7427840909090909, "width": 0.18960947712418308, "height": 0.01272727272727281, "page": 5}], "section": "LIMITATIONS", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2121"}, {"text": "For example, we created a glue gun that transmits its live temperature (Figure 1).", "label": "Method", "bboxes": [{"left": 0.7077843137254902, "top": 0.6267007575757576, "width": 0.20411928104575172, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.6412487373737374, "width": 0.3158725490196077, "height": 0.012727272727272587, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2122"}, {"text": "As smartphones are the most pervasive AR platform at present, we created a proof-of-concept LightAnchors implementation for iOS.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.7212525252525253, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.7357992424242424, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 0}, {"left": 0.5176470588235295, "top": 0.7503472222222222, "width": 0.12451797385620911, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2123"}, {"text": "In this paper, we present LightAnchors, a new method to display spatially-anchored data in augmented reality applications.", "label": "Method", "bboxes": [{"left": 0.08821895424836601, "top": 0.6173030303030304, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.6318510101010101, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.6463977272727273, "width": 0.03588888888888887, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2124"}, {"text": "information and interfaces to specific objects), we also coopt these lights for data transmission, blinking them rapidly to encode binary data.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.4739608585858586, "width": 0.3944869281045752, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.4885088383838384, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5030555555555556, "width": 0.1444803921568627, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2125"}, {"text": "At a high level, for every incoming frame of video, our algorithm creates an image pyramid, such that lights  big or small, close or far  are guaranteed to be contained within a single pixel at least one level.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.38699116161616165, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.40153914141414143, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.4160858585858586, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.4306338383838384, "width": 0.20018300653594767, "height": 0.012727272727272698, "page": 1}], "section": "IMPLEMENTATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2126"}, {"text": "We encode all data as a binary sequence, prefixed with a known pattern.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.5821426767676768, "width": 0.3943807189542482, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.596385101010101, "width": 0.09902124183006533, "height": 0.012727272727272698, "page": 1}], "section": "Encoding & Point Lights", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2127"}, {"text": "Unlike prior approaches that synchronized light modulation with e.g., RF triggers [12], our lights and smartphones are totally unsynchronized.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.7348825757575758, "width": 0.394470588235294, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176470588235295, "top": 0.7494305555555555, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176470588235295, "top": 0.7639772727272728, "width": 0.15274836601307185, "height": 0.012727272727272587, "page": 1}], "section": "Encoding & Point Lights", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2128"}, {"text": "We now briefly review these research areas.", "label": "Method", "bboxes": [{"left": 0.12196732026143792, "top": 0.12669444444444444, "width": 0.2886960784313725, "height": 0.012727272727272726, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2129"}, {"text": "After each frame is tracked, we attempt to decode all candidate anchors.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.7112335858585859, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.7257815656565656, "width": 0.08683333333333333, "height": 0.01272727272727281, "page": 2}], "section": "Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2130"}, {"text": "We profiled our implementation using Xcode on both a iPhone 7 and iPhone X. We tested different base resolutions (i.e., largest pyramid size), and for each, ran three trials of 500 frames each.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.6260820707070708, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.6406300505050505, "width": 0.3945686274509803, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.6548712121212121, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.6694191919191919, "width": 0.11157189542483659, "height": 0.01272727272727281, "page": 2}], "section": "Performance Analysis", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2131"}, {"text": "To evaluate the robustness of our approach, we tested point lights of different size, across varying rooms, lighting conditions, and sensing distances.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.8354760101010101, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.85002398989899, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.8642651515151516, "width": 0.18759967320261428, "height": 0.012727272727272587, "page": 2}], "section": "EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2132"}, {"text": "Our detection process passes all candidate anchors to our tracker on every frame, which must be computationally inexpensive in order to maintain a high frame rate.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.4582032828282828, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.4727512626262626, "width": 0.3944379084967321, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.4872979797979798, "width": 0.33000490196078436, "height": 0.012727272727272754, "page": 2}], "section": "Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2133"}, {"text": "Our LightAnchor detection algorithm is designed to have high recall.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.263354797979798, "width": 0.39438888888888884, "height": 0.012727272727272754, "page": 2}, {"left": 0.08823529411764706, "top": 0.2779027777777778, "width": 0.07508986928104573, "height": 0.012727272727272754, "page": 2}], "section": "Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2134"}, {"text": "Like actual LightAnchors, these blink valid sequences and are decoded correctly by our pipeline.", "label": "Method", "bboxes": [{"left": 0.7411862745098039, "top": 0.489125, "width": 0.1707826797385621, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176633986928104, "top": 0.5036729797979798, "width": 0.3944379084967321, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.5182196969696969, "width": 0.08609150326797388, "height": 0.012727272727272698, "page": 2}], "section": "Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2135"}, {"text": "In our proof-of-concept iOS app, we use the AVCaptureSession API to grab video frames and OpenCV for image processing.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.08305176767676768, "width": 0.3945196078431373, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.09759974747474748, "width": 0.39443790849673205, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.11214646464646463, "width": 0.052343137254901986, "height": 0.01272727272727274, "page": 2}], "section": "Encoding & Point Lights", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2136"}, {"text": "We then test for the presence of our known pre/postamble.", "label": "Method", "bboxes": [{"left": 0.6333202614379084, "top": 0.36547979797979796, "width": 0.2787810457516341, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176470588235295, "top": 0.3800277777777778, "width": 0.09666666666666657, "height": 0.012727272727272698, "page": 2}], "section": "Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2137"}, {"text": "We captured study data using an iPhone 7 (720p at 120 FPS) in three environments: workshop, classroom and office.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.29153661616161614, "width": 0.39407516339869275, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.3057790404040404, "width": 0.37464052287581695, "height": 0.012727272727272754, "page": 3}], "section": "Apparatus & Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2138"}, {"text": "The likelihood of any random pixel in the environment matching our pre/postamble is fairly low.", "label": "Method", "bboxes": [{"left": 0.5835228758169935, "top": 0.2782058080808081, "width": 0.3285882352941176, "height": 0.012727272727272754, "page": 3}, {"left": 0.517640522875817, "top": 0.29275378787878786, "width": 0.31000816993464064, "height": 0.012727272727272754, "page": 3}], "section": "LightAnchor Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2139"}, {"text": "Across all conditions and distances, we found a mean bit error rate (BER) of 5.2%, or roughly 1 error in every 20 transmitted bits.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.4151729797979798, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.4297209595959596, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.4442676767676768, "width": 0.07512254901960769, "height": 0.012727272727272698, "page": 3}], "section": "Bit Error Rate", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2140"}, {"text": "Our detection rate did not change substantially across study conditions, and so we combine detection results for brevity.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.6024457070707071, "width": 0.3945196078431373, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.6169936868686868, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 3}], "section": "LightAnchor Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2141"}, {"text": "After the pre/postamble filtering process, the true positive rate was still 100%, but our system found 3.1% false", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.6969974747474748, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7115441919191919, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 3}], "section": "LightAnchor Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2142"}, {"text": "Across all conditions, we found a mean recognition time of 464 ms. As our test LightAnchors were 22 bits long (6 bit preamble, 10 bit payload, 6 bit postamble), they take a minimum of 183ms to transmit at 120 FPS.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.4436666666666667, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4582146464646465, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4727613636363637, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.48730934343434346, "width": 0.25980718954248366, "height": 0.012727272727272754, "page": 4}], "section": "Recognition Latency", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2143"}, {"text": "To illustrate these different options, as well as to highlight the potential utility of LightAnchors, we created eleven demo applications.", "label": "Method", "bboxes": [{"left": 0.6129297385620915, "top": 0.36214646464646466, "width": 0.2993039215686275, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.37669444444444444, "width": 0.3944869281045751, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.3912411616161616, "width": 0.1798251633986928, "height": 0.012727272727272698, "page": 4}], "section": "PAYLOAD TYPES & EXAMPLE USES", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2144"}, {"text": "Instead, we envision transmission of an ID, which permits lookup through a cloud service, after which larger payloads (e.g., restaurant name, opening hours, menu, coupons, etc.) could be fetched over a faster connection (e.g., cellular, WiFi).", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.5121489898989899, "width": 0.39460294117647055, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.526695707070707, "width": 0.3946356209150327, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.5412424242424242, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.555790404040404, "width": 0.3942401960784314, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.5703371212121212, "width": 0.04318954248366014, "height": 0.012727272727272698, "page": 4}], "section": "Fixed Payloads", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2145"}, {"text": "As a demonstration of a fixed payload, we instrumented a street parking meter (Figure 5, left) with a light that transmits its enforcement zone ID (from which the rate schedule could", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.6070050505050505, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.6212462121212121, "width": 0.39460294117647055, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.6357941919191918, "width": 0.39440522875816997, "height": 0.01272727272727281, "page": 4}], "section": "Fixed Payloads", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2146"}, {"text": "for our system to completely miss a visible LightAnchor, but it is common for a LightAnchors to have to transmit several times before being recognized.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.32002146464646464, "width": 0.39440522875816986, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.3345694444444444, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.3491161616161616, "width": 0.20095751633986925, "height": 0.012727272727272698, "page": 4}], "section": "Recognition Latency", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2147"}, {"text": "As a final demo, we created a mock payment terminal (Figure 7, right) that could allow a", "label": "Method", "bboxes": [{"left": 0.3011339869281046, "top": 0.8576161616161616, "width": 0.18150653594771232, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.8718573232323232, "width": 0.3943055555555555, "height": 0.01272727272727281, "page": 5}], "section": "Dynamic Payloads", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2148"}, {"text": "We have presented our work on LightAnchors, a new approach for overlaying information and interfaces onto everyday objects in mobile AR.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.8215366161616162, "width": 0.3943888888888889, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.8360845959595959, "width": 0.39440522875816997, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8506313131313132, "width": 0.1707843137254902, "height": 0.012727272727272587, "page": 5}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2149"}, {"text": "The biggest drawback of our method is limited bitrate, which is chiefly set by smartphone processors and camera FPS.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.36456691919191925, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.37911489898989903, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 5}], "section": "LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2150"}, {"text": "We found the automatic camera settings were not ideal for LightAnchors (i.e., clipping in dark scenes), and so we had to lock settings such as exposure.", "label": "Method", "bboxes": [{"left": 0.8195196078431373, "top": 0.5754962121212122, "width": 0.09240032679738563, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.5900441919191919, "width": 0.3946192810457517, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.6045909090909091, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.6191376262626263, "width": 0.12565359477124183, "height": 0.012727272727272698, "page": 5}], "section": "LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2151"}, {"text": "We also created two other demo devices that generally lack compute: a fire alarm that reports its operating status and an electrical strip that reports its power draw (Figure 6, left and center).", "label": "Method", "bboxes": [{"left": 0.3435130718954248, "top": 0.5242727272727272, "width": 0.13923202614379088, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5388194444444444, "width": 0.3943545751633987, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5533674242424242, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5679141414141414, "width": 0.3096062091503268, "height": 0.012727272727272698, "page": 5}], "section": "Dynamic Payloads", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2152"}, {"text": "Finally, at present, our LightAnchor widgets are flat with respect to the smartphone screen, as a single LightAnchor cannot provide 3D orientation.", "label": "Method", "bboxes": [{"left": 0.5176307189542484, "top": 0.6700467171717172, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176307189542484, "top": 0.6845946969696969, "width": 0.39450326797385626, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176307189542484, "top": 0.6991414141414142, "width": 0.17979248366013068, "height": 0.012727272727272587, "page": 5}], "section": "LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2153"}, {"text": "Similarly, we modified an outdoor entrance light to output a fixed UID (Figure 5, center); the summoned LightAnchor displays the building name (Department of Motor Vehicles), its current status (open), and closing time (5pm).", "label": "Method", "bboxes": [{"left": 0.18509803921568627, "top": 0.3148699494949495, "width": 0.29747549019607844, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.3294179292929293, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.3439646464646465, "width": 0.3945196078431373, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.35851262626262625, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.37305934343434344, "width": 0.044107843137254896, "height": 0.012727272727272698, "page": 5}], "section": "Fixed Payloads", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2154"}, {"text": "our approach can be rapid and accurate.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.07002146464646465, "width": 0.2652745098039216, "height": 0.01272727272727274, "page": 6}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2155"}, {"text": "This research was generously supported with funds from the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.1342638888888889, "width": 0.39428921568627445, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.1488118686868687, "width": 0.3945196078431373, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.16335858585858584, "width": 0.3944705882352941, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.17790656565656565, "width": 0.08057352941176472, "height": 0.012727272727272754, "page": 6}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2156"}, {"text": "We purposely employ preambles that contain both 1s and 0s (i.e., high and low brightness), which allows us to find the midpoint of the min and max intensity values at both the beginning and end of a transmission.", "label": "Result", "bboxes": [{"left": 0.33995588235294116, "top": 0.8127588383838384, "width": 0.14275000000000004, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.8273068181818182, "width": 0.3946519607843137, "height": 0.01272727272727281, "page": 2}, {"left": 0.0882516339869281, "top": 0.8418535353535355, "width": 0.3945196078431373, "height": 0.012727272727272587, "page": 2}, {"left": 0.0882516339869281, "top": 0.8564015151515151, "width": 0.39440522875817, "height": 0.01272727272727281, "page": 2}, {"left": 0.0882516339869281, "top": 0.8709482323232324, "width": 0.08686601307189544, "height": 0.012727272727272587, "page": 2}], "section": "Detection", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2157"}, {"text": "This could allow dumb devices to become smarter through AR with minimal extra cost (much less than e.g., adding a screen to a device).", "label": "Conclusion", "bboxes": [{"left": 0.5592189542483661, "top": 0.5976060606060606, "width": 0.35291503267973845, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.6121540404040404, "width": 0.39431209150326796, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.6267007575757576, "width": 0.1860196078431372, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2158"}, {"text": "For devices that already contain a microprocessor, LightAnchors opens a new information outlet in AR, for example, the LED found in many security cameras could be used to share the devices privacy policy (Figure 1).", "label": "Conclusion", "bboxes": [{"left": 0.8372418300653596, "top": 0.6412487373737374, "width": 0.07502450980392139, "height": 0.012727272727272587, "page": 0}, {"left": 0.5176470588235295, "top": 0.6557954545454545, "width": 0.3943055555555556, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.6700378787878787, "width": 0.39447058823529413, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.684584595959596, "width": 0.39455228758169925, "height": 0.012727272727272587, "page": 0}, {"left": 0.5176470588235295, "top": 0.6991325757575757, "width": 0.1680571895424836, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2159"}, {"text": "We conclude by describing ten example applications we built to illustrate the potential of LightAnchors.", "label": "Conclusion", "bboxes": [{"left": 0.8890424836601308, "top": 0.8518724747474747, "width": 0.022993464052287638, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8664191919191918, "width": 0.3945147058823528, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8809671717171718, "width": 0.25941176470588245, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "prob": 1, "is_author_statement": true, "is_in_expected_section": false, "id": "2160"}, {"text": "With specialized equipment, it is possible to transmit data at high speeds with visible light (VLC).", "label": "Conclusion", "bboxes": [{"left": 0.5176470588235295, "top": 0.17790025252525252, "width": 0.394388888888889, "height": 0.012727272727272754, "page": 1}, {"left": 0.5176470588235295, "top": 0.19214267676767677, "width": 0.24292810457516334, "height": 0.012727272727272754, "page": 1}], "section": "Visible Light Communication with Commodity Devices", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2161"}, {"text": "These printed tags are highly visible, and thus often obtrusive to the visual design of objects.", "label": "Conclusion", "bboxes": [{"left": 0.2905767973856209, "top": 0.23427146464646464, "width": 0.1921617647058823, "height": 0.012727272727272698, "page": 1}, {"left": 0.08821895424836601, "top": 0.2488181818181818, "width": 0.3945032679738562, "height": 0.012727272727272726, "page": 1}, {"left": 0.08821895424836601, "top": 0.263364898989899, "width": 0.03431699346405229, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2162"}, {"text": "All of these approaches require special hardware and do not demonstrate dynamic data payloads.", "label": "Conclusion", "bboxes": [{"left": 0.44409640522875815, "top": 0.4436666666666667, "width": 0.038426470588235284, "height": 0.012727272727272698, "page": 1}, {"left": 0.08821895424836601, "top": 0.4582133838383838, "width": 0.3943055555555556, "height": 0.012727272727272754, "page": 1}, {"left": 0.08821895424836601, "top": 0.4727613636363637, "width": 0.23780228758169936, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2163"}, {"text": "Rarer are systems that support dynamic payloads.", "label": "Conclusion", "bboxes": [{"left": 0.08821895424836601, "top": 0.49488131313131317, "width": 0.32290522875816996, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2164"}, {"text": "For example, SideBySide [33] digitally projected infrared ARTags onto the environment, which could e.g., identify users in multiplayer projected AR games.", "label": "Conclusion", "bboxes": [{"left": 0.4152418300653595, "top": 0.49488131313131317, "width": 0.06748039215686275, "height": 0.012727272727272754, "page": 1}, {"left": 0.08821895424836601, "top": 0.5094280303030303, "width": 0.39465196078431375, "height": 0.01272727272727281, "page": 1}, {"left": 0.08821895424836601, "top": 0.52397601010101, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 1}, {"left": 0.08821895424836601, "top": 0.5385227272727272, "width": 0.24840849673202614, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2165"}, {"text": "To support tracking of many tags in a scene, [12] synchronized LEDs with RF communication.", "label": "Conclusion", "bboxes": [{"left": 0.4643055555555555, "top": 0.734905303030303, "width": 0.018428104575163418, "height": 0.01272727272727281, "page": 1}, {"left": 0.08821895424836601, "top": 0.7494520202020203, "width": 0.39448692810457514, "height": 0.012727272727272587, "page": 1}, {"left": 0.08821895424836601, "top": 0.7639999999999999, "width": 0.22765686274509805, "height": 0.01272727272727281, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2166"}, {"text": "This means it is possible for the camera shutter to align with transitions in our blinked pattern, which at best reduces SNR, and at worse, means the pattern is unresolvable.", "label": "Conclusion", "bboxes": [{"left": 0.673326797385621, "top": 0.7639772727272728, "width": 0.2388071895424837, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176307189542484, "top": 0.778219696969697, "width": 0.3942892156862745, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176307189542484, "top": 0.7927664141414141, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176307189542484, "top": 0.807314393939394, "width": 0.10605392156862736, "height": 0.012727272727272587, "page": 1}], "section": "Encoding & Point Lights", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2167"}, {"text": "We used basic binary transmission as a proof of concept, but LightAnchors could also be extended to use multiple illumination levels and colors.", "label": "Conclusion", "bboxes": [{"left": 0.5544918300653595, "top": 0.836409090909091, "width": 0.35759313725490194, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176307189542484, "top": 0.8509558080808081, "width": 0.39450326797385626, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176307189542484, "top": 0.8655025252525252, "width": 0.22215522875816995, "height": 0.01272727272727281, "page": 1}], "section": "Encoding & Point Lights", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2168"}, {"text": "Even though we used the highly optimized iOS CoreGraphics API, we suspect that this bottleneck could be greatly mitigated with better GPU acceleration, which could allow LightAnchor detection to run at 240 FPS or more.", "label": "Conclusion", "bboxes": [{"left": 0.6995980392156863, "top": 0.7421553030303031, "width": 0.2125522875816993, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.7567020202020202, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.7712500000000001, "width": 0.394470588235294, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176307189542484, "top": 0.7857967171717172, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.800344696969697, "width": 0.08686601307189534, "height": 0.012727272727272587, "page": 2}], "section": "Performance Analysis", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2169"}, {"text": "Upon closer analysis of the video data, it appears most of these were actually small reflections of our actual LightAnchors, and thus transmitting correct patterns (an effect discussed in our Decoding section above).", "label": "Conclusion", "bboxes": [{"left": 0.8321617647058823, "top": 0.29275378787878786, "width": 0.07999836601307186, "height": 0.012727272727272754, "page": 3}, {"left": 0.517640522875817, "top": 0.30730050505050505, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.517640522875817, "top": 0.32184848484848483, "width": 0.3946339869281047, "height": 0.012727272727272698, "page": 3}, {"left": 0.517640522875817, "top": 0.336395202020202, "width": 0.39450326797385626, "height": 0.012727272727272698, "page": 3}, {"left": 0.517640522875817, "top": 0.35094191919191925, "width": 0.14564869281045756, "height": 0.012727272727272698, "page": 3}], "section": "LightAnchor Detection", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2170"}, {"text": "Because there is no synchronization, detection of a LightAnchor is almost certainly going to start somewhere in the middle of a transmission, meaning the system will have to wait on average 92 ms for the start of a sequence.", "label": "Conclusion", "bboxes": [{"left": 0.35333006535947714, "top": 0.48730934343434346, "width": 0.12939215686274502, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.5018560606060607, "width": 0.3945196078431373, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5164040404040404, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5309507575757575, "width": 0.3942549019607843, "height": 0.01272727272727281, "page": 4}, {"left": 0.08823529411764706, "top": 0.5454987373737374, "width": 0.16842810457516338, "height": 0.012727272727272698, "page": 4}], "section": "Recognition Latency", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2171"}, {"text": "To illustrate these different options, as well as to highlight the potential utility of LightAnchors, we created eleven demo applications.", "label": "Conclusion", "bboxes": [{"left": 0.6129297385620915, "top": 0.36214646464646466, "width": 0.2993039215686275, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.37669444444444444, "width": 0.3944869281045751, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.3912411616161616, "width": 0.1798251633986928, "height": 0.012727272727272698, "page": 4}], "section": "PAYLOAD TYPES & EXAMPLE USES", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2172"}, {"text": "Although this could contain plain text, the limited bitrate of LightAnchors makes this impractical.", "label": "Conclusion", "bboxes": [{"left": 0.657202614379085, "top": 0.4830542929292929, "width": 0.25473366013071896, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.49760101010101015, "width": 0.39432189542483653, "height": 0.012727272727272754, "page": 4}], "section": "Fixed Payloads", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2173"}, {"text": "Instead, we envision transmission of an ID, which permits lookup through a cloud service, after which larger payloads (e.g., restaurant name, opening hours, menu, coupons, etc.) could be fetched over a faster connection (e.g., cellular, WiFi).", "label": "Conclusion", "bboxes": [{"left": 0.5176470588235295, "top": 0.5121489898989899, "width": 0.39460294117647055, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.526695707070707, "width": 0.3946356209150327, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.5412424242424242, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.555790404040404, "width": 0.3942401960784314, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.5703371212121212, "width": 0.04318954248366014, "height": 0.012727272727272698, "page": 4}], "section": "Fixed Payloads", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2174"}, {"text": "By utilizing geolocation in the lookup, it may be possible to use fairly small IDs (e.g., 16 bits).", "label": "Conclusion", "bboxes": [{"left": 0.5645588235294118, "top": 0.5703371212121212, "width": 0.34759150326797383, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.5848851010101009, "width": 0.2750588235294118, "height": 0.01272727272727281, "page": 4}], "section": "Fixed Payloads", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2175"}, {"text": "As a demonstration of a fixed payload, we instrumented a street parking meter (Figure 5, left) with a light that transmits its enforcement zone ID (from which the rate schedule could", "label": "Conclusion", "bboxes": [{"left": 0.5176470588235295, "top": 0.6070050505050505, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.6212462121212121, "width": 0.39460294117647055, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.6357941919191918, "width": 0.39440522875816997, "height": 0.01272727272727281, "page": 4}], "section": "Fixed Payloads", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2176"}, {"text": "Finally, LightAnchor payloads could be used to provide connection information.", "label": "Conclusion", "bboxes": [{"left": 0.08823529411764706, "top": 0.7121426767676768, "width": 0.39447058823529413, "height": 0.012727272727272587, "page": 5}, {"left": 0.08823529411764706, "top": 0.7266906565656566, "width": 0.13313235294117648, "height": 0.01272727272727281, "page": 5}], "section": "Dynamic Payloads", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2177"}, {"text": "For example, a smart light switch could provide an IP address, allowing smartphones to open a socket and take control (Figure 7, left).", "label": "Conclusion", "bboxes": [{"left": 0.2254852941176471, "top": 0.7266906565656566, "width": 0.2572369281045751, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.7412373737373736, "width": 0.3943055555555555, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.7557853535353535, "width": 0.2162745098039216, "height": 0.01272727272727281, "page": 5}], "section": "Dynamic Payloads", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2178"}, {"text": "To mitigate malicious behavior, a token with a short time-to-live could also be transmitted to ensure that users have at least line-of-sight.", "label": "Conclusion", "bboxes": [{"left": 0.309797385620915, "top": 0.7557853535353535, "width": 0.1729411764705882, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.7703320707070707, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.784878787878788, "width": 0.3476454248366013, "height": 0.012727272727272587, "page": 5}], "section": "Dynamic Payloads", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2179"}, {"text": "An internet connection could also allow devices to transmit a custom control interface, for example, a small HTML/CSS app.", "label": "Conclusion", "bboxes": [{"left": 0.4403921568627451, "top": 0.784878787878788, "width": 0.042346405228758155, "height": 0.012727272727272587, "page": 5}, {"left": 0.08823529411764706, "top": 0.7994267676767676, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.8139734848484849, "width": 0.3943218954248366, "height": 0.012727272727272587, "page": 5}], "section": "Dynamic Payloads", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2180"}, {"text": "For instance, upon connecting to a smart thermostat, a simple temperature control widget could be downloaded and displayed in AR (Figure 7, center).", "label": "Conclusion", "bboxes": [{"left": 0.08823529411764706, "top": 0.8285214646464646, "width": 0.39437254901960783, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.8430681818181819, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 5}, {"left": 0.08823529411764706, "top": 0.8576161616161616, "width": 0.20878104575163398, "height": 0.01272727272727281, "page": 5}], "section": "Dynamic Payloads", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2181"}, {"text": "As a final demo, we created a mock payment terminal (Figure 7, right) that could allow a", "label": "Conclusion", "bboxes": [{"left": 0.3011339869281046, "top": 0.8576161616161616, "width": 0.18150653594771232, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.8718573232323232, "width": 0.3943055555555555, "height": 0.01272727272727281, "page": 5}], "section": "Dynamic Payloads", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2182"}, {"text": "We take advantage of point lights (e.g., LEDs) that already exist in a wide range of products (or could be added for a few dollars).", "label": "Conclusion", "bboxes": [{"left": 0.6925490196078431, "top": 0.8506313131313132, "width": 0.21973366013071904, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.8651792929292929, "width": 0.39445424836601295, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8794204545454545, "width": 0.2248398692810457, "height": 0.01272727272727281, "page": 5}], "section": "CONCLUSION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "2183"}, {"text": "A typical glue gun contains no digital components, and thus in practice, would require the addition of a microcontroller and thermistor, costing as little as $1 USD [22].", "label": "Conclusion", "bboxes": [{"left": 0.3921176470588235, "top": 0.4806313131313132, "width": 0.09045588235294116, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.49517803030303026, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 5}, {"left": 0.08823529411764706, "top": 0.50972601010101, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.5242727272727272, "width": 0.24959477124183005, "height": 0.012727272727272698, "page": 5}], "section": "Dynamic Payloads", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2184"}, {"text": "However, a known geometry of at least three non-planar LightAnchors (e.g., status lights on a microwave or WiFi router) could allow for recovery of 6DOF position in the future.", "label": "Conclusion", "bboxes": [{"left": 0.7023153594771242, "top": 0.6991414141414142, "width": 0.20976960784313725, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176307189542484, "top": 0.7136893939393939, "width": 0.3945359477124183, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176307189542484, "top": 0.7282361111111112, "width": 0.39445424836601306, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176307189542484, "top": 0.7427840909090909, "width": 0.18960947712418308, "height": 0.01272727272727281, "page": 5}], "section": "LIMITATIONS", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2185"}, {"text": "Of course, many devices already contain microprocessors that can control status lights and could be LightAnchor-enabled with a firmware update.", "label": "Conclusion", "bboxes": [{"left": 0.08823529411764706, "top": 0.590034090909091, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.6045820707070707, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.6191287878787879, "width": 0.19194771241830066, "height": 0.012727272727272698, "page": 5}], "section": "Dynamic Payloads", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2186"}, {"text": "For example, a networked security camera could be updated to use its recording light to share its privacy policy (Figure 1), and a router could shares its SSID and a randomly generated guest password via its status lights (Figure 6, right).", "label": "Conclusion", "bboxes": [{"left": 0.28547058823529414, "top": 0.6191287878787879, "width": 0.1972679738562091, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.6336767676767677, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.6482234848484849, "width": 0.3946192810457516, "height": 0.012727272727272587, "page": 5}, {"left": 0.08823529411764706, "top": 0.6624659090909091, "width": 0.39467156862745095, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.6770126262626263, "width": 0.1907875816993464, "height": 0.01272727272727281, "page": 5}], "section": "Dynamic Payloads", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2187"}, {"text": "Another difference from conventional markers is that LightAnchors can transmit dynamic payloads, without the need for WiFi, Bluetooth or indeed, any connectivity.", "label": "Future Work", "bboxes": [{"left": 0.5176470588235295, "top": 0.5248699494949495, "width": 0.394375816993464, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5394179292929293, "width": 0.3943382352941175, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5539646464646465, "width": 0.36368627450980384, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2188"}, {"text": "Devices need only an inexpensive microcontroller (e.g., [22], which costs less than $0.50 USD) with the ability to blink a LED.", "label": "Future Work", "bboxes": [{"left": 0.8874117647058823, "top": 0.5539646464646465, "width": 0.02473856209150327, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5685126262626262, "width": 0.39430555555555546, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5830593434343434, "width": 0.3943888888888889, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5976060606060606, "width": 0.03588888888888886, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2189"}, {"text": "Because there is no synchronization, detection of a LightAnchor is almost certainly going to start somewhere in the middle of a transmission, meaning the system will have to wait on average 92 ms for the start of a sequence.", "label": "Future Work", "bboxes": [{"left": 0.35333006535947714, "top": 0.48730934343434346, "width": 0.12939215686274502, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.5018560606060607, "width": 0.3945196078431373, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5164040404040404, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5309507575757575, "width": 0.3942549019607843, "height": 0.01272727272727281, "page": 4}, {"left": 0.08823529411764706, "top": 0.5454987373737374, "width": 0.16842810457516338, "height": 0.012727272727272698, "page": 4}], "section": "Recognition Latency", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2190"}, {"text": "However, a known geometry of at least three non-planar LightAnchors (e.g., status lights on a microwave or WiFi router) could allow for recovery of 6DOF position in the future.", "label": "Future Work", "bboxes": [{"left": 0.7023153594771242, "top": 0.6991414141414142, "width": 0.20976960784313725, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176307189542484, "top": 0.7136893939393939, "width": 0.3945359477124183, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176307189542484, "top": 0.7282361111111112, "width": 0.39445424836601306, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176307189542484, "top": 0.7427840909090909, "width": 0.18960947712418308, "height": 0.01272727272727281, "page": 5}], "section": "LIMITATIONS", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2191"}, {"text": "We have presented our work on LightAnchors, a new approach for overlaying information and interfaces onto everyday objects in mobile AR.", "label": "Objective", "bboxes": [{"left": 0.5176470588235295, "top": 0.8215366161616162, "width": 0.3943888888888889, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.8360845959595959, "width": 0.39440522875816997, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8506313131313132, "width": 0.1707843137254902, "height": 0.012727272727272587, "page": 5}], "section": "CONCLUSION", "prob": 0.4837046265602112, "is_author_statement": true, "is_in_expected_section": true, "id": "2192"}, {"text": "Given a raw camera image, we first convert to grayscale and build an image pyramid (five layers, scaling by half).", "label": "Method", "bboxes": [{"left": 0.16900653594771242, "top": 0.2779027777777778, "width": 0.3136993464052288, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.29244949494949496, "width": 0.3944869281045752, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.30699747474747474, "width": 0.055076797385620935, "height": 0.012727272727272754, "page": 2}], "section": "Detection", "prob": 0.6738670468330383, "is_author_statement": true, "is_in_expected_section": true, "id": "2193"}, {"text": "To convert the analog light intensity signal into a binary sequence, we use a dynamic threshold.", "label": "Method", "bboxes": [{"left": 0.12117647058823529, "top": 0.7982121212121212, "width": 0.36154575163398694, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8127588383838384, "width": 0.24877124183006533, "height": 0.012727272727272587, "page": 2}], "section": "Detection", "prob": 0.5849134922027588, "is_author_statement": true, "is_in_expected_section": true, "id": "2194"}, {"text": "Our algorithm then searches for candidate light anchors using a max-pooling template that finds bright pixels surrounded by darker pixels.", "label": "Method", "bboxes": [{"left": 0.7235130718954249, "top": 0.4306338383838384, "width": 0.18872058823529414, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.4451805555555556, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.4597272727272727, "width": 0.32876960784313713, "height": 0.012727272727272754, "page": 1}], "section": "IMPLEMENTATION", "prob": 0.546614408493042, "is_author_statement": true, "is_in_expected_section": true, "id": "2195"}, {"text": "In our proof-of-concept iOS app, we use the AVCaptureSession API to grab video frames and OpenCV for image processing.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.08305176767676768, "width": 0.3945196078431373, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.09759974747474748, "width": 0.39443790849673205, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.11214646464646463, "width": 0.052343137254901986, "height": 0.01272727272727274, "page": 2}], "section": "Encoding & Point Lights", "prob": 0.5099578499794006, "is_author_statement": true, "is_in_expected_section": true, "id": "2196"}, {"text": "We then attempt to pair all current candidates with candidates from the previous frame using a greedy Euclidean distance matcher with a threshold to discard unlikely pairings.", "label": "Method", "bboxes": [{"left": 0.37566339869281046, "top": 0.5309393939393939, "width": 0.10692647058823535, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.5454873737373738, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.560034090909091, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.5745820707070707, "width": 0.25428921568627455, "height": 0.012727272727272698, "page": 2}], "section": "Detection", "prob": 0.48269563913345337, "is_author_statement": true, "is_in_expected_section": true, "id": "2197"}, {"text": "We then track candidate anchors over frames, decoding a blinked binary pattern using an adaptive threshold.", "label": "Method", "bboxes": [{"left": 0.8536633986928105, "top": 0.4597272727272727, "width": 0.05842156862745107, "height": 0.012727272727272754, "page": 1}, {"left": 0.5176470588235295, "top": 0.4742752525252525, "width": 0.39450326797385615, "height": 0.012727272727272754, "page": 1}, {"left": 0.5176470588235295, "top": 0.48882196969696967, "width": 0.26603267973856215, "height": 0.012727272727272698, "page": 1}], "section": "IMPLEMENTATION", "prob": 0.48194247484207153, "is_author_statement": true, "is_in_expected_section": true, "id": "2198"}, {"text": "For our still condition, we used a surveyors rope to mark distances, and for our walking condition, we used a 50cm printed ArUco tag for continuous distance tracking (accepting frames within 0.5m).", "label": "Method", "bboxes": [{"left": 0.23958333333333334, "top": 0.4076098484848485, "width": 0.24308986928104573, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.4221565656565657, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.43670454545454546, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.45125126262626264, "width": 0.3119035947712418, "height": 0.012727272727272698, "page": 3}], "section": "Apparatus & Procedure", "prob": 0.4234379529953003, "is_author_statement": true, "is_in_expected_section": true, "id": "2199"}, {"text": "After each frame is tracked, we attempt to decode all candidate anchors.", "label": "Method", "bboxes": [{"left": 0.08823529411764706, "top": 0.7112335858585859, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.7257815656565656, "width": 0.08683333333333333, "height": 0.01272727272727281, "page": 2}], "section": "Detection", "prob": 0.4012656509876251, "is_author_statement": true, "is_in_expected_section": true, "id": "2200"}, {"text": "To illustrate these different options, as well as to highlight the potential utility of LightAnchors, we created eleven demo applications.", "label": "Method", "bboxes": [{"left": 0.6129297385620915, "top": 0.36214646464646466, "width": 0.2993039215686275, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.37669444444444444, "width": 0.3944869281045751, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.3912411616161616, "width": 0.1798251633986928, "height": 0.012727272727272698, "page": 4}], "section": "PAYLOAD TYPES & EXAMPLE USES", "prob": 0.3793506324291229, "is_author_statement": true, "is_in_expected_section": true, "id": "2201"}, {"text": "In this paper, we present LightAnchors, a new method to display spatially-anchored data in augmented reality applications.", "label": "Method", "bboxes": [{"left": 0.08821895424836601, "top": 0.6173030303030304, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.6318510101010101, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.6463977272727273, "width": 0.03588888888888887, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "prob": 0.370817631483078, "is_author_statement": true, "is_in_expected_section": true, "id": "2202"}, {"text": "Within each setting, we used two exemplar point lights: a standard 3mm LED and a larger 100x100mm LED matrix.", "label": "Method", "bboxes": [{"left": 0.4034656862745098, "top": 0.45125126262626264, "width": 0.07920751633986928, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.46549368686868686, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.48004040404040405, "width": 0.308016339869281, "height": 0.012727272727272698, "page": 3}], "section": "Apparatus & Procedure", "prob": 0.3634554147720337, "is_author_statement": true, "is_in_expected_section": true, "id": "2203"}, {"text": "The biggest drawback of our method is limited bitrate, which is chiefly set by smartphone processors and camera FPS.", "label": "Method", "bboxes": [{"left": 0.5176470588235295, "top": 0.36456691919191925, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.37911489898989903, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 5}], "section": "LIMITATIONS", "prob": 0.3601846992969513, "is_author_statement": true, "is_in_expected_section": true, "id": "2204"}, {"text": "When frames are scaled, we use iOSs optimized CoreGraphics API.", "label": "Method", "bboxes": [{"left": 0.31722549019607843, "top": 0.21367171717171718, "width": 0.16534803921568625, "height": 0.012727272727272698, "page": 2}, {"left": 0.08826797385620916, "top": 0.22821969696969696, "width": 0.2852875816993464, "height": 0.012727272727272754, "page": 2}], "section": "Encoding & Point Lights", "prob": 0.3594832122325897, "is_author_statement": true, "is_in_expected_section": true, "id": "2205"}, {"text": "To quantity this, we used our collected data to compute the average time required to detect, track, and decode a LightAnchor.", "label": "Method", "bboxes": [{"left": 0.29291503267973856, "top": 0.3491161616161616, "width": 0.18979084967320264, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.3636641414141414, "width": 0.3943088235294117, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.37821085858585857, "width": 0.2205408496732026, "height": 0.012727272727272698, "page": 4}], "section": "Recognition Latency", "prob": 0.3565177321434021, "is_author_statement": true, "is_in_expected_section": true, "id": "2206"}, {"text": "For example, we created a glue gun that transmits its live temperature (Figure 1).", "label": "Method", "bboxes": [{"left": 0.7077843137254902, "top": 0.6267007575757576, "width": 0.20411928104575172, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.6412487373737374, "width": 0.3158725490196077, "height": 0.012727272727272587, "page": 0}], "section": "INTRODUCTION", "prob": 0.31956085562705994, "is_author_statement": true, "is_in_expected_section": true, "id": "2207"}, {"text": "The combined results are shown in Figure 3.", "label": "Result", "bboxes": [{"left": 0.8164558823529412, "top": 0.6839659090909092, "width": 0.09567810457516335, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.6985138888888889, "width": 0.19665849673202607, "height": 0.01272727272727281, "page": 2}], "section": "Performance Analysis", "prob": 0.8077055215835571, "is_author_statement": false, "is_in_expected_section": false, "id": "2208"}, {"text": "We described our implementation and results from an evaluation, which shows that", "label": "Result", "bboxes": [{"left": 0.7473954248366013, "top": 0.8794204545454545, "width": 0.16475490196078435, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8939684343434344, "width": 0.39435457516339867, "height": 0.012727272727272587, "page": 5}], "section": "CONCLUSION", "prob": 0.807327389717102, "is_author_statement": true, "is_in_expected_section": true, "id": "2209"}, {"text": "Finally, we flatten results from all pyramid layers so that candidate anchors are in the coordinate space of the highest resolution pyramid.", "label": "Result", "bboxes": [{"left": 0.25246405228758173, "top": 0.3939747474747475, "width": 0.23006045751633986, "height": 0.012727272727272698, "page": 2}, {"left": 0.08816993464052288, "top": 0.40852272727272726, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 2}, {"left": 0.08816993464052288, "top": 0.4230694444444445, "width": 0.29269771241830067, "height": 0.012727272727272698, "page": 2}], "section": "Detection", "prob": 0.7856106758117676, "is_author_statement": true, "is_in_expected_section": false, "id": "2210"}, {"text": "Of course, only two of these were actual LightAnchors, and our system detected these in all cases (i.e., a true positive rate of 100%).", "label": "Result", "bboxes": [{"left": 0.21687091503267972, "top": 0.6460883838383839, "width": 0.26568627450980387, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.6606351010101009, "width": 0.3946356209150326, "height": 0.01272727272727281, "page": 3}, {"left": 0.08823529411764706, "top": 0.6751818181818182, "width": 0.22404901960784307, "height": 0.012727272727272587, "page": 3}], "section": "LightAnchor Detection", "prob": 0.7762699723243713, "is_author_statement": true, "is_in_expected_section": false, "id": "2211"}, {"text": "Across all conditions and distances, we found a mean bit error rate (BER) of 5.2%, or roughly 1 error in every 20 transmitted bits.", "label": "Result", "bboxes": [{"left": 0.5176470588235295, "top": 0.4151729797979798, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.4297209595959596, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.4442676767676768, "width": 0.07512254901960769, "height": 0.012727272727272698, "page": 3}], "section": "Bit Error Rate", "prob": 0.7456056475639343, "is_author_statement": true, "is_in_expected_section": false, "id": "2212"}, {"text": "After the pre/postamble filtering process, the true positive rate was still 100%, but our system found 3.1% false", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.6969974747474748, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7115441919191919, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 3}], "section": "LightAnchor Detection", "prob": 0.7188700437545776, "is_author_statement": true, "is_in_expected_section": false, "id": "2213"}, {"text": "If we apply our variance filter, accepting only the best signal, it reduces false positives to 0.4% and true positives to 99.6%.", "label": "Result", "bboxes": [{"left": 0.6666160130718954, "top": 0.35094191919191925, "width": 0.24552777777777768, "height": 0.012727272727272698, "page": 3}, {"left": 0.517640522875817, "top": 0.36548989898989903, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 3}, {"left": 0.517640522875817, "top": 0.38003661616161616, "width": 0.15508660130718954, "height": 0.012727272727272754, "page": 3}], "section": "LightAnchor Detection", "prob": 0.6925808787345886, "is_author_statement": true, "is_in_expected_section": false, "id": "2214"}, {"text": "We now describe this procedure and results in detail.", "label": "Result", "bboxes": [{"left": 0.8890098039215686, "top": 0.8788131313131313, "width": 0.022991830065359542, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.8933598484848485, "width": 0.3193970588235293, "height": 0.012727272727272587, "page": 2}], "section": "EVALUATION", "prob": 0.6684032082557678, "is_author_statement": true, "is_in_expected_section": true, "id": "2215"}, {"text": "We found the automatic camera settings were not ideal for LightAnchors (i.e., clipping in dark scenes), and so we had to lock settings such as exposure.", "label": "Result", "bboxes": [{"left": 0.8195196078431373, "top": 0.5754962121212122, "width": 0.09240032679738563, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.5900441919191919, "width": 0.3946192810457517, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.6045909090909091, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.6191376262626263, "width": 0.12565359477124183, "height": 0.012727272727272698, "page": 5}], "section": "LIMITATIONS", "prob": 0.5907381772994995, "is_author_statement": true, "is_in_expected_section": false, "id": "2216"}, {"text": "Only at 320x180 can we process a 240 FPS image stream.", "label": "Result", "bboxes": [{"left": 0.3254934640522876, "top": 0.199125, "width": 0.15709640522875817, "height": 0.012727272727272754, "page": 2}, {"left": 0.08823529411764706, "top": 0.21367171717171718, "width": 0.2237042483660131, "height": 0.012727272727272698, "page": 2}], "section": "Encoding & Point Lights", "prob": 0.5732557773590088, "is_author_statement": true, "is_in_expected_section": false, "id": "2217"}, {"text": "Similarly, we modified an outdoor entrance light to output a fixed UID (Figure 5, center); the summoned LightAnchor displays the building name (Department of Motor Vehicles), its current status (open), and closing time (5pm).", "label": "Result", "bboxes": [{"left": 0.18509803921568627, "top": 0.3148699494949495, "width": 0.29747549019607844, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.3294179292929293, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.3439646464646465, "width": 0.3945196078431373, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.35851262626262625, "width": 0.3943055555555555, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.37305934343434344, "width": 0.044107843137254896, "height": 0.012727272727272698, "page": 5}], "section": "Fixed Payloads", "prob": 0.5353716611862183, "is_author_statement": true, "is_in_expected_section": false, "id": "2218"}, {"text": "We note that this latency varies across conditions, for example, mean recognition latency is 312 ms when the camera was held still (i.e., the first full transmission is often successful) vs. 615 ms when the user was walking (~3 transmissions before recognition).", "label": "Result", "bboxes": [{"left": 0.17055555555555554, "top": 0.5742878787878788, "width": 0.31218300653594766, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.588834595959596, "width": 0.3945179738562091, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.6033813131313132, "width": 0.3946307189542484, "height": 0.01272727272727281, "page": 4}, {"left": 0.08821895424836601, "top": 0.6179292929292929, "width": 0.39447058823529413, "height": 0.01272727272727281, "page": 4}, {"left": 0.08821895424836601, "top": 0.6324760101010101, "width": 0.222921568627451, "height": 0.012727272727272698, "page": 4}], "section": "Recognition Latency", "prob": 0.532991886138916, "is_author_statement": true, "is_in_expected_section": false, "id": "2219"}, {"text": "our approach can be rapid and accurate.", "label": "Result", "bboxes": [{"left": 0.08823529411764706, "top": 0.07002146464646465, "width": 0.2652745098039216, "height": 0.01272727272727274, "page": 6}], "section": "CONCLUSION", "prob": 0.5249853730201721, "is_author_statement": true, "is_in_expected_section": true, "id": "2220"}, {"text": "We then test for the presence of our known pre/postamble.", "label": "Result", "bboxes": [{"left": 0.6333202614379084, "top": 0.36547979797979796, "width": 0.2787810457516341, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176470588235295, "top": 0.3800277777777778, "width": 0.09666666666666657, "height": 0.012727272727272698, "page": 2}], "section": "Detection", "prob": 0.5100246667861938, "is_author_statement": true, "is_in_expected_section": false, "id": "2221"}, {"text": "We conclude by describing ten example applications we built to illustrate the potential of LightAnchors.", "label": "Result", "bboxes": [{"left": 0.8890424836601308, "top": 0.8518724747474747, "width": 0.022993464052287638, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8664191919191918, "width": 0.3945147058823528, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8809671717171718, "width": 0.25941176470588245, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "prob": 0.5038172006607056, "is_author_statement": true, "is_in_expected_section": true, "id": "2222"}, {"text": "Rather than use only the center pixel value, we average over a small region, which we found to be less sensitive to camera noise and sub-pixel aliasing during motion.", "label": "Result", "bboxes": [{"left": 0.1988186274509804, "top": 0.7548762626262626, "width": 0.28372222222222226, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.7694229797979798, "width": 0.39425163398692803, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.783969696969697, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.7982121212121212, "width": 0.02961437908496732, "height": 0.01272727272727281, "page": 2}], "section": "Detection", "prob": 0.4657265543937683, "is_author_statement": true, "is_in_expected_section": false, "id": "2223"}, {"text": "In each of these settings, we varied the lighting condition: artificial light only (mean 321 lux), artificial light + diffuse sunlight (mean 428 lux) and lights off (mean 98 lux).", "label": "Result", "bboxes": [{"left": 0.46895261437908503, "top": 0.3057790404040404, "width": 0.013720588235294096, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.3203257575757576, "width": 0.39450326797385615, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.33487373737373743, "width": 0.39443790849673205, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3494204545454545, "width": 0.3334967320261438, "height": 0.012727272727272754, "page": 3}], "section": "Apparatus & Procedure", "prob": 0.38555437326431274, "is_author_statement": true, "is_in_expected_section": false, "id": "2224"}, {"text": "For example, a networked security camera could be updated to use its recording light to share its privacy policy (Figure 1), and a router could shares its SSID and a randomly generated guest password via its status lights (Figure 6, right).", "label": "Result", "bboxes": [{"left": 0.28547058823529414, "top": 0.6191287878787879, "width": 0.1972679738562091, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.6336767676767677, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.6482234848484849, "width": 0.3946192810457516, "height": 0.012727272727272587, "page": 5}, {"left": 0.08823529411764706, "top": 0.6624659090909091, "width": 0.39467156862745095, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.6770126262626263, "width": 0.1907875816993464, "height": 0.01272727272727281, "page": 5}], "section": "Dynamic Payloads", "prob": 0.3459721803665161, "is_author_statement": false, "is_in_expected_section": false, "id": "2225"}, {"text": "We also created two other demo devices that generally lack compute: a fire alarm that reports its operating status and an electrical strip that reports its power draw (Figure 6, left and center).", "label": "Result", "bboxes": [{"left": 0.3435130718954248, "top": 0.5242727272727272, "width": 0.13923202614379088, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5388194444444444, "width": 0.3943545751633987, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5533674242424242, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5679141414141414, "width": 0.3096062091503268, "height": 0.012727272727272698, "page": 5}], "section": "Dynamic Payloads", "prob": 0.32538002729415894, "is_author_statement": true, "is_in_expected_section": false, "id": "2226"}], "2104.03820": [{"text": "Our research focuses on understanding the extent to which generative models are still useful to human stakeholders despite their potential to produce imperfect output.", "label": "Author", "bboxes": [{"left": 0.196281045751634, "top": 0.2942222222222222, "width": 0.6837222222222222, "height": 0.013969696969696965, "page": 1}, {"left": 0.18000163398692812, "top": 0.31151893939393943, "width": 0.3034836601307189, "height": 0.011320707070707037, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2227"}, {"text": "Are there ways that we can leverage generative model state to aid in the detection and correction of errors that do occur?", "label": "Author", "bboxes": [{"left": 0.6676977124183006, "top": 0.32881439393939393, "width": 0.21229738562091516, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.3461111111111111, "width": 0.515, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2228"}, {"text": "Given the probabilistic nature of deep learning models, we posit that there will always be some amount of noise in their output; correspondingly, deep generative models that produce code as output may exhibit some amount of deviation from a programmers intentions.", "label": "Author", "bboxes": [{"left": 0.18000163398692812, "top": 0.20774116161616163, "width": 0.7002810457516339, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.2250366161616162, "width": 0.6999934640522876, "height": 0.011320707070707037, "page": 1}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.19113398692810454, "height": 0.011320707070707065, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2229"}, {"text": "We address these questions by considering the use of an NMT model within the context of application modernization [27, 64, 72].", "label": "Author", "bboxes": [{"left": 0.196281045751634, "top": 0.3807032828282828, "width": 0.6861993464052287, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.398, "width": 0.11356862745098037, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2230"}, {"text": "To explore the utility of NMT models, we conducted a series of scenario-based design interviews with 11 professional software engineers who work across a variety of technology areas.", "label": "Author", "bboxes": [{"left": 0.196281045751634, "top": 0.4671843434343434, "width": 0.6837124183006537, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.4844810606060606, "width": 0.3955375816993464, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2231"}, {"text": "Through a thematic analysis [12, 22], we identified four themes when considering the design of user experiences that leverage generative models in software engineering: acceptance through verification, human-AI patterns of interaction, the utility of imperfect AI, and future opportunities for generative AI in application modernization.", "label": "Author", "bboxes": [{"left": 0.5792075163398693, "top": 0.4844810606060606, "width": 0.30079575163398686, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.5017765151515151, "width": 0.7017254901960783, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.5190732323232323, "width": 0.7000032679738561, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.5363699494949495, "width": 0.28203104575163396, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2232"}, {"text": "Our work makes three important contributions:", "label": "Author", "bboxes": [{"left": 0.4656960784313725, "top": 0.5363699494949495, "width": 0.28579575163398696, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2233"}, {"text": "Our work examines a different aspect of imperfect AI by focusing on a co-creation scenario that has a lower tolerance for error (e.g. code ought to compile and be free of errors).", "label": "Author", "bboxes": [{"left": 0.5036111111111111, "top": 0.6271755050505051, "width": 0.31638562091503275, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.6444709595959596, "width": 0.6999918300653595, "height": 0.011320707070706981, "page": 2}, {"left": 0.1200016339869281, "top": 0.6617676767676768, "width": 0.042686274509803904, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2234"}, {"text": "Motivated by these studies, we also consider the effect that confidence and translation alternatives (which have explanatory power, discussed in Section 5.1.2) have on the utility and acceptance of a generative AI system.", "label": "Author", "bboxes": [{"left": 0.16632843137254902, "top": 0.6617676767676768, "width": 0.6536715686274509, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.6790631313131313, "width": 0.6417124183006536, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2235"}, {"text": "A significant portion of this work is recounted in Allamanis et al.", "label": "Author", "bboxes": [{"left": 0.8101274509803922, "top": 0.2898977272727273, "width": 0.0103856209150327, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.30719444444444444, "width": 0.3759493464052288, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2236"}, {"text": "In this paper, we focus on TransCoder [79], an unsupervised neural machine translation (NMT) model that transforms source code from one programming language to another.", "label": "Author", "bboxes": [{"left": 0.36813888888888885, "top": 0.35908333333333337, "width": 0.4518594771241831, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.37637878787878787, "width": 0.5894232026143791, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2237"}, {"text": "Given the imperfect output of state-of-the-art NMT models, we posit that such systems will act in concert with human software engineers as a collaborative partner or teammate.", "label": "Author", "bboxes": [{"left": 0.1200016339869281, "top": 0.7396010101010101, "width": 0.7, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.7568964646464647, "width": 0.34180555555555553, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2238"}, {"text": "We find their question of how to determine", "label": "Author", "bboxes": [{"left": 0.5636078431372549, "top": 0.8087853535353535, "width": 0.2563970588235295, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2239"}, {"text": "We discuss three areas relevant to our work: recent advances in the use of AI techniques, and specifically deep generative models, in software engineering; studies of the utility of imperfect AI; and studies of human-AI co-creation with generative AI.", "label": "Author", "bboxes": [{"left": 0.11929738562091505, "top": 0.1428800505050505, "width": 0.7006944444444443, "height": 0.011320707070707065, "page": 2}, {"left": 0.1200016339869281, "top": 0.16017676767676767, "width": 0.7, "height": 0.011320707070707065, "page": 2}, {"left": 0.11945915032679738, "top": 0.17747222222222223, "width": 0.11533333333333334, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2240"}, {"text": "Despite identifying favorable outcomes of human-AI partnerships, we note that these outcomes are all subjective: the quality of the generated output lies in the perceptions of the people using the system.", "label": "Author", "bboxes": [{"left": 0.196281045751634, "top": 0.5790757575757576, "width": 0.6854493464052289, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.5963724747474748, "width": 0.5234330065359476, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2241"}, {"text": "Given that the probabilistic nature of generative models implies the potential for some amount of error in their outputs, we seek to understand the extent to which they may be useful in scenarios that have an objective bar of quality.", "label": "Author", "bboxes": [{"left": 0.845047385620915, "top": 0.6136679292929292, "width": 0.03494771241830075, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.6309646464646465, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.6482613636363637, "width": 0.6471192810457516, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2242"}, {"text": "Some of this work has examined the extent to which deep generative models provide humans with augmented capabilities, addressing the question of the extent to which the human-AI team produces outcomes better than those produced by either party alone (e.g. [29, 37, 57, 70, 92, 100]).", "label": "Author", "bboxes": [{"left": 0.28667483660130716, "top": 0.4752979797979798, "width": 0.5933284313725491, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.49259469696969693, "width": 0.7002696078431372, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.5098901515151516, "width": 0.4178333333333333, "height": 0.011320707070706981, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2243"}, {"text": "We developed an exploratory design scenario in order to engage software engineers in a discussion about the role of generative AI in application modernization.", "label": "Author", "bboxes": [{"left": 0.17929901960784314, "top": 0.7062714646464646, "width": 0.7007042483660132, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.7235669191919193, "width": 0.2531307189542483, "height": 0.011320707070706981, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2244"}, {"text": "We created a series of three progressively-enhancing UX variants within the scenario that illustrated different ways that a software engineer might interact with an NMT model to translate source code from one language to another.", "label": "Author", "bboxes": [{"left": 0.5315718954248366, "top": 0.775455808080808, "width": 0.34891339869281035, "height": 0.011320707070707092, "page": 3}, {"left": 0.17963562091503268, "top": 0.7927525252525253, "width": 0.700357843137255, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.8100492424242424, "width": 0.3260277777777777, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2245"}, {"text": "Our scenario used real output from TransCoder that contained flaws.", "label": "Author", "bboxes": [{"left": 0.1362794117647059, "top": 0.31151893939393943, "width": 0.405109477124183, "height": 0.011320707070707037, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2246"}, {"text": "There was also an issue with our reconstitution of tokenized code resulting in a discrepancy in an exception message string.", "label": "Author", "bboxes": [{"left": 0.7323251633986928, "top": 0.32881439393939393, "width": 0.08766993464052286, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.3461111111111111, "width": 0.6602205882352942, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2247"}, {"text": "These flaws provided a degree of realism to our study; several participants actually discovered these errors during the study, leading to discussions on the utility of imperfect AI.", "label": "Author", "bboxes": [{"left": 0.7838807189542484, "top": 0.3461111111111111, "width": 0.03611274509803919, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.36340656565656565, "width": 0.7016045751633987, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.3807032828282828, "width": 0.310468954248366, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2248"}, {"text": "We conducted an interview study with software engineers in which we used the design scenario to spark discussions around the role of generative AI in application modernization.", "label": "Author", "bboxes": [{"left": 0.11929738562091505, "top": 0.4422209595959596, "width": 0.7006944444444444, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.45951767676767674, "width": 0.3838970588235294, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2249"}, {"text": "First, we discussed the participants background, their level of experience with both Java and Python, and their experiences in modernizing legacy applications.", "label": "Author", "bboxes": [{"left": 0.2741323529411765, "top": 0.49410984848484846, "width": 0.5458676470588235, "height": 0.011320707070707037, "page": 4}, {"left": 0.1200016339869281, "top": 0.5114065656565656, "width": 0.39246078431372555, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2250"}, {"text": "If participants recalled highly-relevant experiences, we delved into more detail about their modernization project, how it was organized, what their role was, and what specific pain points existed in their work.", "label": "Author", "bboxes": [{"left": 0.5161323529411764, "top": 0.5114065656565656, "width": 0.30547385620915024, "height": 0.011320707070707092, "page": 4}, {"left": 0.11945915032679738, "top": 0.5287020202020203, "width": 0.7005441176470588, "height": 0.011320707070706981, "page": 4}, {"left": 0.1200016339869281, "top": 0.5459987373737374, "width": 0.24486437908496733, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2251"}, {"text": "Next, we showed each UX variant and allowed them to explore the interface while thinking aloud to capture their general impressions.", "label": "Author", "bboxes": [{"left": 0.3685359477124183, "top": 0.5459987373737374, "width": 0.4514656862745097, "height": 0.011320707070707092, "page": 4}, {"left": 0.11945915032679738, "top": 0.5632941919191919, "width": 0.34624836601307185, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2252"}, {"text": "After reviewing all three UX variants, we asked participants to brainstorm with us on broader applications of generative AI in application modernization.", "label": "Author", "bboxes": [{"left": 0.576640522875817, "top": 0.5978876262626263, "width": 0.24335294117647055, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.6151830808080808, "width": 0.6802516339869281, "height": 0.011320707070706981, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2253"}, {"text": "As a seed to the brainstorm, we showed three examples of how generative AI might provide unique value: translating complex algorithms, translating large codebases, and converting 3rd party library usage across languages.", "label": "Author", "bboxes": [{"left": 0.8039591503267974, "top": 0.6151830808080808, "width": 0.016034313725490135, "height": 0.011320707070706981, "page": 4}, {"left": 0.1200016339869281, "top": 0.6324797979797979, "width": 0.6999918300653595, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.6497765151515152, "width": 0.6274297385620915, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2254"}, {"text": "These ideas were generated through our pilot testing.", "label": "Author", "bboxes": [{"left": 0.7511192810457517, "top": 0.6497765151515152, "width": 0.06887745098039222, "height": 0.011320707070707092, "page": 4}, {"left": 0.11945915032679738, "top": 0.6670719696969697, "width": 0.24704411764705886, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2255"}, {"text": "We conducted a thematic analysis of our data to identify important ideas and themes from our interviews.", "label": "Author", "bboxes": [{"left": 0.17929901960784314, "top": 0.4566830808080808, "width": 0.6401535947712419, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2256"}, {"text": "We followed the process described by Braun and Clarke [12, 22] in which researchers familiarize themselves with their data, generate and group codes to identify higher-level themes, and iteratively refine codes and themes through collaborative discussion.", "label": "Author", "bboxes": [{"left": 0.6783006535947712, "top": 0.473979797979798, "width": 0.2017009803921569, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.4912752525252525, "width": 0.6999918300653595, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.5085719696969697, "width": 0.6115179738562091, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2257"}, {"text": "We begin by highlighting the difficulties faced by our software engineers in modernizing legacy applications, motivating the need for AI support.", "label": "Author", "bboxes": [{"left": 0.196281045751634, "top": 0.6642386363636363, "width": 0.6853316993464053, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.6815340909090909, "width": 0.20596568627450976, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2258"}, {"text": "We then discuss the four major themes we identified in our data: acceptance through verification, human-AI patterns of interaction, the utility of imperfect AI, and future opportunities for generative AI in application modernization.", "label": "Author", "bboxes": [{"left": 0.38903921568627453, "top": 0.6815340909090909, "width": 0.4909558823529412, "height": 0.011320707070707092, "page": 5}, {"left": 0.17963562091503268, "top": 0.6988308080808081, "width": 0.7003643790849672, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.7161275252525252, "width": 0.16073529411764706, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2259"}, {"text": "Our interviews generated a wealth of material: 11 hours of recorded videos, approximately 63 pages of notes, and a corpus of approximately 400 pages of interview transcripts containing about 89k words.", "label": "Author", "bboxes": [{"left": 0.196281045751634, "top": 0.5258686868686868, "width": 0.6837222222222223, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.5431641414141414, "width": 0.5334330065359476, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2260"}, {"text": "Next, we developed more integrative codes (similar to grounded theory axial codes) using a shared online document and visual collaboration tools.", "label": "Author", "bboxes": [{"left": 0.2336356209150327, "top": 0.5950530303030303, "width": 0.646357843137255, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.6123497474747475, "width": 0.23816993464052286, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2261"}, {"text": "While there are no agreed guidelines about required sample sizes for this kind of analysis [67], we followed commonly-accepted practices to identify saturation (e.g. when we stopped identifying new codes in our interview transcripts) [35, 60].", "label": "Author", "bboxes": [{"left": 0.42180392156862745, "top": 0.6123497474747475, "width": 0.45819117647058827, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.629645202020202, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.6469419191919192, "width": 0.28689542483660124, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2262"}, {"text": "We recruited 11 full-time software engineers within our organization, an international information technology company.", "label": "Author", "bboxes": [{"left": 0.17929901960784314, "top": 0.1428800505050505, "width": 0.7029509803921569, "height": 0.011320707070707065, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2263"}, {"text": "For each UX variant, we asked participants about what they liked and disliked and how they might improve the design.", "label": "Author", "bboxes": [{"left": 0.18000163398692812, "top": 0.2824848484848485, "width": 0.7022565359477124, "height": 0.011320707070707037, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2264"}, {"text": "We also asked specific questions for each variant.", "label": "Author", "bboxes": [{"left": 0.17929901960784314, "top": 0.29978156565656566, "width": 0.2858235294117647, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2265"}, {"text": "For Figure 1A, we asked participants whether it was important to them to understand how the AI model produced the translation, and whether the AI is doing the right thing in modernizing a legacy app by translating code from one language to another, or whether it should do something else.", "label": "Author", "bboxes": [{"left": 0.4682385620915033, "top": 0.29978156565656566, "width": 0.41175816993464043, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.3170782828282828, "width": 0.7000049019607842, "height": 0.011320707070707037, "page": 5}, {"left": 0.18000163398692812, "top": 0.3343737373737374, "width": 0.6161290849673202, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2266"}, {"text": "For Figure 1B, we asked participants if they understood why the highlighted tokens were highlighted, and what other information could help them better understand the AIs confidence.", "label": "Author", "bboxes": [{"left": 0.7995915032679739, "top": 0.3343737373737374, "width": 0.08202614379084949, "height": 0.011320707070707092, "page": 5}, {"left": 0.17945915032679738, "top": 0.3516704545454546, "width": 0.7005457516339869, "height": 0.011320707070707037, "page": 5}, {"left": 0.18000163398692812, "top": 0.3689671717171717, "width": 0.33134477124183004, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2267"}, {"text": "For Figure 1C, we asked participants if they understood why the AI produced the alternatives it produced, and what other information could help them better understand how those alternatives were generated.", "label": "Author", "bboxes": [{"left": 0.5149869281045752, "top": 0.3689671717171717, "width": 0.36539542483660126, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.3862626262626263, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.40355934343434346, "width": 0.16886764705882354, "height": 0.011320707070707037, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2268"}, {"text": "All participants who were involved in an application modernization project expressed some desire for help, and our focus on the code translation use case positively resonated with them.", "label": "Author", "bboxes": [{"left": 0.6127941176470588, "top": 0.13855555555555557, "width": 0.20720588235294113, "height": 0.011320707070707065, "page": 6}, {"left": 0.1200016339869281, "top": 0.15585227272727273, "width": 0.7000000000000001, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.17314898989898989, "width": 0.18908496732026148, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2269"}, {"text": "We recognize that trust is a complex, multi-faceted construct that has been described as being an attitude [78], an intention [62], and a behavior [1].", "label": "Author", "bboxes": [{"left": 0.1362794117647059, "top": 0.35429040404040407, "width": 0.683720588235294, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.37158712121212123, "width": 0.19429738562091503, "height": 0.011320707070706981, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2270"}, {"text": "In this paper, we adopt the AI-focused attitudinal view espoused by Madsen and Gregor [59], that trust is the extent to which a user is confident in, and willing to act on the basis of, the recommendations, actions, and decisions of an artificially intelligent decision aid.", "label": "Author", "bboxes": [{"left": 0.3170130718954248, "top": 0.37158712121212123, "width": 0.5032614379084968, "height": 0.011320707070706981, "page": 6}, {"left": 0.1200016339869281, "top": 0.3888838383838384, "width": 0.7016029411764706, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.40617929292929295, "width": 0.3763137254901961, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2271"}, {"text": "Thus, in our discussions with participants around the topic of trust, our emphasis was on understanding their attitudes toward acceptance of AI-generated code through their expressions of desire to incorporate (or not incorporate) such code in their work  i.e. their willing[ness] to act, rather than their past behavior of having acted, as no participants had previously encountered a code-generating AI system.", "label": "Author", "bboxes": [{"left": 0.5, "top": 0.40617929292929295, "width": 0.32000163398692816, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.42347601010101005, "width": 0.700281045751634, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.44077146464646466, "width": 0.700281045751634, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.4580681818181818, "width": 0.7008594771241831, "height": 0.011320707070707037, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2272"}, {"text": "If Im going to put my production systems functionality behind this, I really want some degree of trust that this thing is robust... Every piece of code that we put into production has been reviewed by at least one other person. And so we try hard to basically build that trust through review and familiarity in humans... wed need basically the same level of trust for a system like this. (P4)", "label": "Author", "bboxes": [{"left": 0.15988398692810457, "top": 0.28397727272727274, "width": 0.6202369281045752, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.3012727272727273, "width": 0.6202287581699346, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.3185694444444444, "width": 0.620235294117647, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.33586616161616156, "width": 0.3387598039215686, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2273"}, {"text": "We observed that discrepancies between our participants mental models and the actual operation of the NMT model was a source of confusion.", "label": "Author", "bboxes": [{"left": 0.3874705882352941, "top": 0.27692550505050506, "width": 0.49253594771241826, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.2942222222222222, "width": 0.3674183006535948, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2274"}, {"text": "Despite the feelings that having an understanding of the NMT models operation wasnt important, we observed that having such understanding does have benefits.", "label": "Author", "bboxes": [{"left": 0.196281045751634, "top": 0.15585227272727273, "width": 0.6837124183006537, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.17314898989898989, "width": 0.28227450980392155, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2275"}, {"text": "We initially thought that the translation would produce only content in the form of the translated code.", "label": "Author", "bboxes": [{"left": 0.5411143790849673, "top": 0.2564772727272727, "width": 0.27888725490196087, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.2737727272727273, "width": 0.34687254901960785, "height": 0.013969696969696965, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2276"}, {"text": "We learned that the low-confidence highlighting (Figure 1B) could also help developers to organize their work.", "label": "Author", "bboxes": [{"left": 0.47146241830065366, "top": 0.2737727272727273, "width": 0.3485392156862745, "height": 0.011320707070707092, "page": 8}, {"left": 0.11966339869281045, "top": 0.29106944444444444, "width": 0.32970915032679743, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2277"}, {"text": "Thus, we questioned whether we would encounter 9", "label": "Author", "bboxes": [{"left": 0.5153872549019607, "top": 0.8125239898989899, "width": 0.3048888888888891, "height": 0.011320707070707092, "page": 8}, {"left": 0.46734967320261434, "top": 0.8307196969696969, "width": 0.005299019607843236, "height": 0.008805555555555622, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2278"}, {"text": "To probe at this idea, we selected a code sample with an incorrect translation in order to spark discussion on the utility of erroneous output.", "label": "Author", "bboxes": [{"left": 0.5986127450980392, "top": 0.12126010101010101, "width": 0.28138235294117653, "height": 0.011320707070707092, "page": 9}, {"left": 0.17945915032679738, "top": 0.13855555555555557, "width": 0.5489722222222222, "height": 0.011320707070707065, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2279"}, {"text": "A lot of us in the software development field, we sometimes have the feeling that for a given problem set, we have to solve the entire problem. But, thats not necessarily true... if you can help someone do part of the task... maybe it will be more effective at smaller, less complex code fragments. Then, [as] you get feedback from users, you build into it more intelligence, then maybe it becomes more effective on larger, more complex pieces of code. (P3)", "label": "Author", "bboxes": [{"left": 0.21988562091503266, "top": 0.6361439393939394, "width": 0.6218349673202614, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.653439393939394, "width": 0.6202271241830066, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.6707361111111111, "width": 0.620235294117647, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.6880328282828283, "width": 0.6202320261437908, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.7053282828282829, "width": 0.1111127450980392, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2280"}, {"text": "Throughout our interviews, participants made a number of suggestions for how generative AI models, as well as the user interfaces that incorporate them, could support these activities.", "label": "Author", "bboxes": [{"left": 0.15410457516339868, "top": 0.1947689393939394, "width": 0.6658872549019607, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.21206439393939394, "width": 0.4301666666666667, "height": 0.011320707070707037, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2281"}, {"text": "Generating documentation from source is an active area of research in the machine learning community (e.g. [44, 48, 65, 91]) and our results highlight the importance of this functionality in user experiences, even when generated documentation may be imperfect.", "label": "Author", "bboxes": [{"left": 0.1362794117647059, "top": 0.6726565656565657, "width": 0.685330065359477, "height": 0.011320707070707092, "page": 10}, {"left": 0.11966339869281045, "top": 0.6899532828282828, "width": 0.7003284313725489, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.7072487373737373, "width": 0.2015539215686275, "height": 0.011320707070707092, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2282"}, {"text": "Indeed, the ML community has explored many methods for generating tests [31, 86, 87] as well as generating data for tests [50, 83], and our findings reinforce the need to incorporate these techniques into mainstream application modernization tools.", "label": "Author", "bboxes": [{"left": 0.7174019607843137, "top": 0.6137676767676767, "width": 0.16298202614379076, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.631064393939394, "width": 0.6999967320261437, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.6483598484848484, "width": 0.6001633986928104, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2283"}, {"text": "We note that our system only examined one kind of human-AI interaction pattern, in which the human provided code and the translator produced a translation.", "label": "Author", "bboxes": [{"left": 0.1362794117647059, "top": 0.17314898989898989, "width": 0.6837254901960784, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.19044444444444444, "width": 0.25008986928104576, "height": 0.011320707070707092, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2284"}, {"text": "Although the system did allow for some subsequent actions by the human (e.g. hovering to see confidence scores or clicking to see alternate translations), our tool falls short of the tightly-coupled human-machine interactions envisioned by some of the informants in Seeber et al.", "label": "Author", "bboxes": [{"left": 0.3737614379084967, "top": 0.19044444444444444, "width": 0.44623366013071897, "height": 0.011320707070707092, "page": 12}, {"left": 0.11956209150326796, "top": 0.20774116161616163, "width": 0.70043954248366, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.2250366161616162, "width": 0.48959150326797396, "height": 0.011320707070707037, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2285"}, {"text": "However, our system was solely driven by human initiative.", "label": "Author", "bboxes": [{"left": 0.19201633986928104, "top": 0.2596300505050505, "width": 0.3544820261437909, "height": 0.011320707070707037, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2286"}, {"text": "For example, we believe that there is a richer design space in which AI translations can be provided at a number of different granularities, from highly-automated, bulk translation of large codebases to highly-controlled, real-time translation of code as it is being written.", "label": "Author", "bboxes": [{"left": 0.4044983660130719, "top": 0.2942222222222222, "width": 0.4155, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.31151893939393943, "width": 0.7000032679738563, "height": 0.011320707070707037, "page": 12}, {"left": 0.1200016339869281, "top": 0.32881439393939393, "width": 0.4891813725490196, "height": 0.011320707070707092, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2287"}, {"text": "Our results motivate the need for explainable generative AI.", "label": "Author", "bboxes": [{"left": 0.1362794117647059, "top": 0.5709621212121212, "width": 0.35481535947712417, "height": 0.011320707070707092, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2288"}, {"text": "We recognize that some work has been done in this space, such as in visualizing how transformer models produce their output (e.g. [42]).", "label": "Author", "bboxes": [{"left": 0.6477712418300653, "top": 0.605554292929293, "width": 0.17261111111111116, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.6228510101010101, "width": 0.6410506535947712, "height": 0.011320707070707092, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2289"}, {"text": "We also discovered how this information possessed explanatory power, helping participants both understand the codes underlying intent and identify potential errors in the translation.", "label": "Author", "bboxes": [{"left": 0.15865359477124183, "top": 0.13855555555555557, "width": 0.6613415032679738, "height": 0.011320707070707065, "page": 12}, {"left": 0.1200016339869281, "top": 0.15585227272727273, "width": 0.4283954248366013, "height": 0.011320707070707092, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2290"}, {"text": "Our study identified a number of areas that could be pursued for the use of generative AI in application modernization.", "label": "Author", "bboxes": [{"left": 0.1200016339869281, "top": 0.7093320707070708, "width": 0.7022598039215686, "height": 0.011320707070706981, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2291"}, {"text": "NMT models such as the one we used are typically trained in an unsupervised manner, in part due to the", "label": "Author", "bboxes": [{"left": 0.197437908496732, "top": 0.8131098484848485, "width": 0.6225539215686273, "height": 0.011320707070707092, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2292"}, {"text": "Although the import of generative capabilities may seem to be about the production of an artifact itself, we found that they instead may also be a means rather than just an end .", "label": "Author", "bboxes": [{"left": 0.734733660130719, "top": 0.20774116161616163, "width": 0.14526960784313725, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.2250366161616162, "width": 0.6999934640522876, "height": 0.011320707070707037, "page": 13}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.19047058823529414, "height": 0.013969696969696993, "page": 13}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2293"}, {"text": "Thus, we challenge the community to consider use cases for generative AI that involve improving the state of human understanding of a domain or artifact, rather than considering the only utility of generative models to be the output itself.", "label": "Author", "bboxes": [{"left": 0.7405228758169935, "top": 0.27692550505050506, "width": 0.1394836601307189, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.2942222222222222, "width": 0.6999918300653596, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.31151893939393943, "width": 0.6130996732026144, "height": 0.011320707070707037, "page": 13}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2294"}, {"text": "Our study is a preliminary examination of how probabilistic generative models can be applied to a use case that requires an objective level of quality.", "label": "Author", "bboxes": [{"left": 0.18000163398692812, "top": 0.3841628787878788, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4014583333333333, "width": 0.16816013071895422, "height": 0.011320707070707092, "page": 13}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2295"}, {"text": "Although we have discovered that our design scenario did demonstrate compelling user experiences, we caution that our results may not generalize to other domains or use cases beyond the code translation task we examined (e.g. using generative models for molecular discovery).", "label": "Author", "bboxes": [{"left": 0.3518071895424837, "top": 0.4014583333333333, "width": 0.5284705882352941, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4187550505050505, "width": 0.7, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4360517676767677, "width": 0.435392156862745, "height": 0.011320707070707092, "page": 13}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2296"}, {"text": "In addition, our finding that acceptance of a models output is established through verification, rather than explanation of the models operation, is not meant to diminish the importance of further research and exploration into how generative models might explain themselves, their operation, and their limitations.", "label": "Author", "bboxes": [{"left": 0.6190571895424837, "top": 0.4360517676767677, "width": 0.26094117647058823, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4533472222222222, "width": 0.7000032679738561, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4706439393939394, "width": 0.7016029411764706, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4879406565656566, "width": 0.22450816993464054, "height": 0.011320707070707092, "page": 13}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2297"}, {"text": "Indeed, having an accurate mental model did help participants understand the models confidence levels, and we hypothesize that generative AI user interfaces that provide salient details of a models operation can help people form accurate mental models.", "label": "Author", "bboxes": [{"left": 0.4081830065359477, "top": 0.4879406565656566, "width": 0.47181045751633993, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.5052361111111111, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.5225328282828283, "width": 0.33629901960784314, "height": 0.011320707070707092, "page": 13}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2298"}, {"text": "We have examined how such a model would be received by software engineers in the context of modernizing legacy applications.", "label": "Author", "bboxes": [{"left": 0.6593709150326797, "top": 0.6470656565656565, "width": 0.22062418300653597, "height": 0.011320707070707092, "page": 13}, {"left": 0.17945915032679738, "top": 0.6643623737373737, "width": 0.5504983660130719, "height": 0.011320707070707092, "page": 13}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2299"}, {"text": "(1) We identify that although software engineers had concerns about the quality of the code produced by an NMT model, their concerns were tempered by the fact that integration of any NMT-produced code would follow existing best practices in software engineering (e.g. code reviews & testing), thus ensuring that any potentially-flawed code would be subject to human review and revision before its inclusion in a product or service.", "label": "Author", "bboxes": [{"left": 0.19782843137254902, "top": 0.5662108585858586, "width": 0.682609477124183, "height": 0.011320707070706981, "page": 1}, {"left": 0.21988562091503266, "top": 0.5835063131313132, "width": 0.6601078431372549, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.6008030303030303, "width": 0.6601127450980393, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.6180997474747475, "width": 0.565908496732026, "height": 0.011320707070706981, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2300"}, {"text": "(2) We highlight how humans and AI systems both have unique and important roles to play in the context application modernization, and discuss new opportunities for generative technologies to be applied across the application modernization lifecycle.", "label": "Author", "bboxes": [{"left": 0.19782843137254902, "top": 0.6699886363636364, "width": 0.6653366013071894, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.6872840909090908, "width": 0.6601176470588235, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.7045808080808081, "width": 0.21366993464052292, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2301"}, {"text": "(3) We motivate the need to conduct further investigations into the design of human-AI partnerships that result better outcomes than what could be accomplished by human or AI effort alone.", "label": "Author", "bboxes": [{"left": 0.19782843137254902, "top": 0.7218762626262626, "width": 0.6666699346405229, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.7391729797979797, "width": 0.47371078431372543, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2302"}, {"text": "Our work makes three important contributions:", "label": "Contribution", "bboxes": [{"left": 0.4656960784313725, "top": 0.5363699494949495, "width": 0.28579575163398696, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2303"}, {"text": "Our research focuses on understanding the extent to which generative models are still useful to human stakeholders despite their potential to produce imperfect output.", "label": "Novelty", "bboxes": [{"left": 0.196281045751634, "top": 0.2942222222222222, "width": 0.6837222222222222, "height": 0.013969696969696965, "page": 1}, {"left": 0.18000163398692812, "top": 0.31151893939393943, "width": 0.3034836601307189, "height": 0.011320707070707037, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2304"}, {"text": "However, code is unique compared to other kinds of natural languages: code is much more brittle, and swapping even a few characters or tokens can completely change its meaning or effect.", "label": "Novelty", "bboxes": [{"left": 0.7353856209150327, "top": 0.13855555555555557, "width": 0.14461764705882363, "height": 0.011320707070707065, "page": 1}, {"left": 0.18000163398692812, "top": 0.15585227272727273, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.17314898989898989, "width": 0.27594117647058813, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2305"}, {"text": "Indeed, while state-of-the-art NMT models  e.g. Tufano et al.", "label": "Novelty", "bboxes": [{"left": 0.37473856209150325, "top": 0.24233333333333332, "width": 0.3630326797385621, "height": 0.011320707070707065, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2306"}, {"text": "[26] discovered that once people observed an automated system make errors, their distrust in the system increased unless an explanation was provided regarding why the system might make an error; however, these explanations also increased trust and reliance on the system even when unwarranted, signaling the importance and difficulty of providing explanations that help people appropriately calibrate their trust.", "label": "Novelty", "bboxes": [{"left": 0.6673627450980392, "top": 0.50610101010101, "width": 0.1526339869281047, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.5233977272727273, "width": 0.7000032679738563, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.5406931818181818, "width": 0.6999983660130719, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.557989898989899, "width": 0.6999918300653596, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.5752866161616161, "width": 0.25388398692810465, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Imperfect AI", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2307"}, {"text": "Our work examines a different aspect of imperfect AI by focusing on a co-creation scenario that has a lower tolerance for error (e.g. code ought to compile and be free of errors).", "label": "Novelty", "bboxes": [{"left": 0.5036111111111111, "top": 0.6271755050505051, "width": 0.31638562091503275, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.6444709595959596, "width": 0.6999918300653595, "height": 0.011320707070706981, "page": 2}, {"left": 0.1200016339869281, "top": 0.6617676767676768, "width": 0.042686274509803904, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Imperfect AI", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2308"}, {"text": "Despite identifying favorable outcomes of human-AI partnerships, we note that these outcomes are all subjective: the quality of the generated output lies in the perceptions of the people using the system.", "label": "Novelty", "bboxes": [{"left": 0.196281045751634, "top": 0.5790757575757576, "width": 0.6854493464052289, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.5963724747474748, "width": 0.5234330065359476, "height": 0.011320707070707092, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2309"}, {"text": "We created a series of three progressively-enhancing UX variants within the scenario that illustrated different ways that a software engineer might interact with an NMT model to translate source code from one language to another.", "label": "Novelty", "bboxes": [{"left": 0.5315718954248366, "top": 0.775455808080808, "width": 0.34891339869281035, "height": 0.011320707070707092, "page": 3}, {"left": 0.17963562091503268, "top": 0.7927525252525253, "width": 0.700357843137255, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.8100492424242424, "width": 0.3260277777777777, "height": 0.011320707070707092, "page": 3}], "section": "3 DESIGN SCENARIO", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2310"}, {"text": "Although the translated code was syntactically correct, there was a logic error in the translation of a loop that resulted in wrong values being returned.", "label": "Novelty", "bboxes": [{"left": 0.5450359477124184, "top": 0.31151893939393943, "width": 0.2753431372549019, "height": 0.011320707070707037, "page": 4}, {"left": 0.1200016339869281, "top": 0.32881439393939393, "width": 0.6087058823529412, "height": 0.011320707070707092, "page": 4}], "section": "3 DESIGN SCENARIO", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2311"}, {"text": "Next, we showed each UX variant and allowed them to explore the interface while thinking aloud to capture their general impressions.", "label": "Novelty", "bboxes": [{"left": 0.3685359477124183, "top": 0.5459987373737374, "width": 0.4514656862745097, "height": 0.011320707070707092, "page": 4}, {"left": 0.11945915032679738, "top": 0.5632941919191919, "width": 0.34624836601307185, "height": 0.011320707070707092, "page": 4}], "section": "4 METHOD", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2312"}, {"text": "While there are no agreed guidelines about required sample sizes for this kind of analysis [67], we followed commonly-accepted practices to identify saturation (e.g. when we stopped identifying new codes in our interview transcripts) [35, 60].", "label": "Novelty", "bboxes": [{"left": 0.42180392156862745, "top": 0.6123497474747475, "width": 0.45819117647058827, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.629645202020202, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.6469419191919192, "width": 0.28689542483660124, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2313"}, {"text": "However, participants ultimately felt that understanding how the NMT model worked was not a prerequisite for being able to put that model to productive use.", "label": "Novelty", "bboxes": [{"left": 0.6855424836601307, "top": 0.6149835858585858, "width": 0.13445098039215686, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.6322790404040404, "width": 0.6999918300653595, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.6495757575757577, "width": 0.14551960784313722, "height": 0.011320707070706981, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2314"}, {"text": "P3 expressed the same sentiment, I guess Im a little curious how it works, but maybe I dont wanna know [how it really works].", "label": "Novelty", "bboxes": [{"left": 0.2719477124183006, "top": 0.6668712121212121, "width": 0.5480571895424837, "height": 0.011320707070707092, "page": 6}, {"left": 0.11945915032679738, "top": 0.6841679292929292, "width": 0.22023039215686274, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2315"}, {"text": "However, the", "label": "Novelty", "bboxes": [{"left": 0.7418039215686274, "top": 0.8127979797979797, "width": 0.07818790849673207, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2316"}, {"text": "P7 described how one of the alternatives more explicitly demonstrated the codes functionality: The first [option] is more Pythonic because its taking advantage of the built-in functionality of an array... however this [second option] is far more explicit... this is whats happening behind the scenes.", "label": "Novelty", "bboxes": [{"left": 0.18000163398692812, "top": 0.582290404040404, "width": 0.6999967320261438, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.5995871212121212, "width": 0.6999967320261437, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.6168825757575758, "width": 0.3927761437908497, "height": 0.011320707070706981, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2317"}, {"text": "P4 also gained insight by comparing two alternate translations: This is clearly... toggling between those two styles of iteration... its interesting to see what the AI is... thinking between these two different programming constructs.", "label": "Novelty", "bboxes": [{"left": 0.5764411764705882, "top": 0.6168825757575758, "width": 0.30356372549019606, "height": 0.011320707070706981, "page": 7}, {"left": 0.18000163398692812, "top": 0.6341792929292929, "width": 0.7022483660130718, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.6514747474747474, "width": 0.3796209150326797, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2318"}, {"text": "P3 expected that the translator would be able to define this def function signature line with the function name correct, but expressed confusion when the translator wasnt as confident as it could be.", "label": "Novelty", "bboxes": [{"left": 0.319202614379085, "top": 0.3461111111111111, "width": 0.5607924836601308, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.36340656565656565, "width": 0.6217630718954248, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2319"}, {"text": "Thus, although participants generally said they dont care (P7) about knowing how the translation was produced, explanations of the models mechanics may be helpful for avoiding confusion caused by confidence discrepancies.", "label": "Novelty", "bboxes": [{"left": 0.17956209150326796, "top": 0.513104797979798, "width": 0.7020539215686273, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.5304015151515151, "width": 0.6808611111111109, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2320"}, {"text": "Despite the feelings that having an understanding of the NMT models operation wasnt important, we observed that having such understanding does have benefits.", "label": "Novelty", "bboxes": [{"left": 0.196281045751634, "top": 0.15585227272727273, "width": 0.6837124183006537, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.17314898989898989, "width": 0.28227450980392155, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2321"}, {"text": "It would be different if the i as an index... lets see... is it used? Oh actually it is used... Now theres a mistake the code.. The statement d [ i ] = i, is wrong [but] the other [alternate translation] is correct. (P1)", "label": "Novelty", "bboxes": [{"left": 0.15988398692810457, "top": 0.5847146464646464, "width": 0.620233660130719, "height": 0.011320707070707092, "page": 8}, {"left": 0.17517973856209149, "top": 0.602010101010101, "width": 0.5580114379084968, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2322"}, {"text": "Thus, although many participants expressed a willingness to accept imperfectly-translated code as a starting point, acceptance may depend on how many errors the translated code contains as well as the nature of those errors (e.g. if they are easy to spot and fix, or if they take attention away from the central problem).", "label": "Novelty", "bboxes": [{"left": 0.196281045751634, "top": 0.725564393939394, "width": 0.6853316993464054, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.7428598484848485, "width": 0.7000049019607842, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.7601565656565656, "width": 0.5177058823529412, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2323"}, {"text": "In some cases, although not an explicit part of the study, participants actually found the errors in the translation.", "label": "Novelty", "bboxes": [{"left": 0.196281045751634, "top": 0.15585227272727273, "width": 0.6859738562091504, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2324"}, {"text": "For example, P4 found a string mismatch: There is an actual functional difference here on line 21 of the Python code where theres an extra space inserted into that error string.", "label": "Novelty", "bboxes": [{"left": 0.18000163398692812, "top": 0.17314898989898989, "width": 0.6999950980392157, "height": 0.011320707070707092, "page": 9}, {"left": 0.17945915032679738, "top": 0.19044444444444444, "width": 0.35301470588235295, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2325"}, {"text": "Despite the positive reception of potentially error-prone code, there did seem to be a threshold regarding how many errors would be acceptable.", "label": "Novelty", "bboxes": [{"left": 0.196281045751634, "top": 0.33999116161616166, "width": 0.6841045751633988, "height": 0.011320707070707037, "page": 9}, {"left": 0.18000163398692812, "top": 0.35728661616161617, "width": 0.16003431372549018, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2326"}, {"text": "As summarized by P6, If its egregiously wrong then no, Id rather do it myself. But as long as its giving me a head start of any sort, Ill take it.", "label": "Novelty", "bboxes": [{"left": 0.3435669934640523, "top": 0.35728661616161617, "width": 0.5364281045751634, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.3745833333333334, "width": 0.3072499999999999, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2327"}, {"text": "A lot of us in the software development field, we sometimes have the feeling that for a given problem set, we have to solve the entire problem. But, thats not necessarily true... if you can help someone do part of the task... maybe it will be more effective at smaller, less complex code fragments. Then, [as] you get feedback from users, you build into it more intelligence, then maybe it becomes more effective on larger, more complex pieces of code. (P3)", "label": "Novelty", "bboxes": [{"left": 0.21988562091503266, "top": 0.6361439393939394, "width": 0.6218349673202614, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.653439393939394, "width": 0.6202271241830066, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.6707361111111111, "width": 0.620235294117647, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.6880328282828283, "width": 0.6202320261437908, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.7053282828282829, "width": 0.1111127450980392, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2328"}, {"text": "The place where I can see something like this being very helpful would be where you really have a truly dead codebase thats just a complete black box that nobody can, or wants, to understand... because for that you need understanding if the goal is to keep the darn thing limping while you transition to something else. (P4)", "label": "Novelty", "bboxes": [{"left": 0.15988398692810457, "top": 0.3502790404040404, "width": 0.6202352941176471, "height": 0.011320707070707092, "page": 10}, {"left": 0.15988398692810457, "top": 0.367574494949495, "width": 0.620235294117647, "height": 0.011320707070706981, "page": 10}, {"left": 0.15988398692810457, "top": 0.3848712121212121, "width": 0.6198333333333333, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2329"}, {"text": "Translation could also help software engineers better understand codebases in languages with which they are less familiar: Ive got a bunch of engineers that know Python, but they dont know Java. So, it might be easier to delve in and spelunk through... auto gen[erated] Python code than it would be to go back and spelunk the original Java. (P4).", "label": "Novelty", "bboxes": [{"left": 0.1362794117647059, "top": 0.40408333333333335, "width": 0.6837156862745097, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.4213800505050505, "width": 0.6999983660130719, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.438675505050505, "width": 0.6986323529411764, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2330"}, {"text": "Generating a whole slew of automated unit tests (P4) as well as more realistic test values (P3) ensures that the right hand side code not only runs and doesnt crash but produces expected output (P4).", "label": "Novelty", "bboxes": [{"left": 0.196281045751634, "top": 0.5964709595959595, "width": 0.6837173202614378, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.6137676767676767, "width": 0.5337369281045752, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2331"}, {"text": "Many participants described how code translation could be used to take advantage of functionality defined in third party libraries, because the same libraries in Java dont exist in Python, or similar libraries exist but theyre not... syntactically the same. (P0).", "label": "Novelty", "bboxes": [{"left": 0.196281045751634, "top": 0.17314898989898989, "width": 0.6837238562091503, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.19044444444444444, "width": 0.7022483660130718, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.20774116161616163, "width": 0.16701960784313727, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2332"}, {"text": "P10 described, If you had a library that you know was in another language and you want to use it, but they dont have it in your language, thats great.", "label": "Novelty", "bboxes": [{"left": 0.3506928104575163, "top": 0.20774116161616163, "width": 0.5293137254901961, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.2250366161616162, "width": 0.3784575163398691, "height": 0.011320707070707037, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2333"}, {"text": "P3 discussed how he had to manage libraries for the same set of APIs across multiple languages: Were up to nine different languages now total and and each one seems to have its own, like, name capitalization conventions and things like that.", "label": "Novelty", "bboxes": [{"left": 0.5622549019607843, "top": 0.2250366161616162, "width": 0.3177385620915034, "height": 0.011320707070707037, "page": 11}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.6999901960784313, "height": 0.011320707070707065, "page": 11}, {"left": 0.18000163398692812, "top": 0.2596300505050505, "width": 0.44238725490196074, "height": 0.011320707070707037, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2334"}, {"text": "P8 described how the code translation process wasnt just about converting from one language to another, but also about identifying opportunities to leverage language-specific features to create more optimal code: The whole point of using the latest version is to tap into the new features... in JDK 8 they have lambdas... it is much faster. If it could do something like this I would use it like hell!", "label": "Novelty", "bboxes": [{"left": 0.4910179738562091, "top": 0.2942222222222222, "width": 0.3889754901960785, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.31151893939393943, "width": 0.6999934640522876, "height": 0.011320707070707037, "page": 11}, {"left": 0.18000163398692812, "top": 0.32881439393939393, "width": 0.7003807189542482, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.3461111111111111, "width": 0.5514640522875816, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2335"}, {"text": "P9 emphasized how the alternate translations provided an educational opportunity while doing code modernization work because having new options makes it much more easy for me to explore newer methods in the newer language.", "label": "Novelty", "bboxes": [{"left": 0.7354101307189543, "top": 0.3461111111111111, "width": 0.1445833333333334, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.36340656565656565, "width": 0.7005441176470587, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.3807032828282828, "width": 0.5465408496732026, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2336"}, {"text": "Although the system did allow for some subsequent actions by the human (e.g. hovering to see confidence scores or clicking to see alternate translations), our tool falls short of the tightly-coupled human-machine interactions envisioned by some of the informants in Seeber et al.", "label": "Novelty", "bboxes": [{"left": 0.3737614379084967, "top": 0.19044444444444444, "width": 0.44623366013071897, "height": 0.011320707070707092, "page": 12}, {"left": 0.11956209150326796, "top": 0.20774116161616163, "width": 0.70043954248366, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.2250366161616162, "width": 0.48959150326797396, "height": 0.011320707070707037, "page": 12}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2337"}, {"text": "However, our system was solely driven by human initiative.", "label": "Novelty", "bboxes": [{"left": 0.19201633986928104, "top": 0.2596300505050505, "width": 0.3544820261437909, "height": 0.011320707070707037, "page": 12}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2338"}, {"text": "Future work is needed to investigate different kinds of interaction patterns, including different mixed-initiative patterns [43, 82], to understand their effect on a software engineers productivity and capability.", "label": "Novelty", "bboxes": [{"left": 0.5501748366013072, "top": 0.2596300505050505, "width": 0.2698169934640522, "height": 0.011320707070707037, "page": 12}, {"left": 0.1200016339869281, "top": 0.27692550505050506, "width": 0.6999901960784314, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.2942222222222222, "width": 0.28084967320261434, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2339"}, {"text": "For example, we believe that there is a richer design space in which AI translations can be provided at a number of different granularities, from highly-automated, bulk translation of large codebases to highly-controlled, real-time translation of code as it is being written.", "label": "Novelty", "bboxes": [{"left": 0.4044983660130719, "top": 0.2942222222222222, "width": 0.4155, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.31151893939393943, "width": 0.7000032679738563, "height": 0.011320707070707037, "page": 12}, {"left": 0.1200016339869281, "top": 0.32881439393939393, "width": 0.4891813725490196, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2340"}, {"text": "While conventional software development tools are generally quite reliable, software engineers themselves are not.", "label": "Novelty", "bboxes": [{"left": 0.11929738562091505, "top": 0.3807032828282828, "width": 0.7029509803921569, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.2 Toward Explainable Generative AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2341"}, {"text": "However, more work is needed to understand how such visualizations and other explanatory methods impact peoples willingness to use generative models and their productivity in using them.", "label": "Novelty", "bboxes": [{"left": 0.7647222222222222, "top": 0.6228510101010101, "width": 0.05688725490196078, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.6401477272727273, "width": 0.6999934640522876, "height": 0.011320707070706981, "page": 12}, {"left": 0.1200016339869281, "top": 0.6574431818181818, "width": 0.3733954248366013, "height": 0.011320707070706981, "page": 12}], "section": "6 DISCUSSION @@ 6.2 Toward Explainable Generative AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2342"}, {"text": "Although the import of generative capabilities may seem to be about the production of an artifact itself, we found that they instead may also be a means rather than just an end .", "label": "Novelty", "bboxes": [{"left": 0.734733660130719, "top": 0.20774116161616163, "width": 0.14526960784313725, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.2250366161616162, "width": 0.6999934640522876, "height": 0.011320707070707037, "page": 13}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.19047058823529414, "height": 0.013969696969696993, "page": 13}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2343"}, {"text": "Although we have discovered that our design scenario did demonstrate compelling user experiences, we caution that our results may not generalize to other domains or use cases beyond the code translation task we examined (e.g. using generative models for molecular discovery).", "label": "Novelty", "bboxes": [{"left": 0.3518071895424837, "top": 0.4014583333333333, "width": 0.5284705882352941, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4187550505050505, "width": 0.7, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4360517676767677, "width": 0.435392156862745, "height": 0.011320707070707092, "page": 13}], "section": "7 LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2344"}, {"text": "Future work should explore how different patterns of interaction and initiative, as well as capabilities beyond code translation, can enable effective human-AI partnerships with generative models that produce better outcomes than what could be accomplished by either party alone.", "label": "Novelty", "bboxes": [{"left": 0.5499705882352941, "top": 0.7681388888888888, "width": 0.33002287581699363, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.7854356060606061, "width": 0.6999934640522876, "height": 0.011320707070706981, "page": 13}, {"left": 0.17945915032679738, "top": 0.8027323232323232, "width": 0.6550016339869281, "height": 0.011320707070707092, "page": 13}], "section": "8 CONCLUSION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2345"}, {"text": "(1) We identify that although software engineers had concerns about the quality of the code produced by an NMT model, their concerns were tempered by the fact that integration of any NMT-produced code would follow existing best practices in software engineering (e.g. code reviews & testing), thus ensuring that any potentially-flawed code would be subject to human review and revision before its inclusion in a product or service.", "label": "Novelty", "bboxes": [{"left": 0.19782843137254902, "top": 0.5662108585858586, "width": 0.682609477124183, "height": 0.011320707070706981, "page": 1}, {"left": 0.21988562091503266, "top": 0.5835063131313132, "width": 0.6601078431372549, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.6008030303030303, "width": 0.6601127450980393, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.6180997474747475, "width": 0.565908496732026, "height": 0.011320707070706981, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2346"}, {"text": "Leveraging the naturalness hypothesis [2]  code is a form of human communication with similar statistical properties as natural languages", "label": "Objective", "bboxes": [{"left": 0.6600539215686274, "top": 0.7613636363636364, "width": 0.15994771241830075, "height": 0.011320707070707092, "page": 0}, {"left": 0.1200016339869281, "top": 0.7786603535353535, "width": 0.7016029411764706, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2347"}, {"text": "We address these questions by considering the use of an NMT model within the context of application modernization [27, 64, 72].", "label": "Objective", "bboxes": [{"left": 0.196281045751634, "top": 0.3807032828282828, "width": 0.6861993464052287, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.398, "width": 0.11356862745098037, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2348"}, {"text": "The question of how to design effective interactions between human and machine teammates is of great interest to the IUI community.", "label": "Objective", "bboxes": [{"left": 0.46508006535947716, "top": 0.7568964646464647, "width": 0.3549150326797385, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.7741931818181818, "width": 0.4423055555555556, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2349"}, {"text": "[81] recently outlined a set of research questions spanning many aspects of human-AI collaboration, such as how tasks should be allocated across both parties and how trust could be established with a machine teammate.", "label": "Objective", "bboxes": [{"left": 0.6422565359477124, "top": 0.7741931818181818, "width": 0.1777401960784315, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.791489898989899, "width": 0.6999950980392158, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.8087853535353535, "width": 0.43995424836601305, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2350"}, {"text": "We find their question of how to determine", "label": "Objective", "bboxes": [{"left": 0.5636078431372549, "top": 0.8087853535353535, "width": 0.2563970588235295, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2351"}, {"text": "Less attention has been given to scenarios with objective outcomes, such as in software engineering where correct code is of prime importance.", "label": "Objective", "bboxes": [{"left": 0.7070245098039216, "top": 0.5963724747474748, "width": 0.1729705882352941, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.6136679292929292, "width": 0.661658496732026, "height": 0.011320707070707092, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2352"}, {"text": "Given that the probabilistic nature of generative models implies the potential for some amount of error in their outputs, we seek to understand the extent to which they may be useful in scenarios that have an objective bar of quality.", "label": "Objective", "bboxes": [{"left": 0.845047385620915, "top": 0.6136679292929292, "width": 0.03494771241830075, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.6309646464646465, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.6482613636363637, "width": 0.6471192810457516, "height": 0.011320707070707092, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2353"}, {"text": "Some of this work has examined the extent to which deep generative models provide humans with augmented capabilities, addressing the question of the extent to which the human-AI team produces outcomes better than those produced by either party alone (e.g. [29, 37, 57, 70, 92, 100]).", "label": "Objective", "bboxes": [{"left": 0.28667483660130716, "top": 0.4752979797979798, "width": 0.5933284313725491, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.49259469696969693, "width": 0.7002696078431372, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.5098901515151516, "width": 0.4178333333333333, "height": 0.011320707070706981, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2354"}, {"text": "One author led each interview, guiding participants through each phase and asking all interview questions.", "label": "Objective", "bboxes": [{"left": 0.5073251633986928, "top": 0.6843686868686868, "width": 0.31267647058823533, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.7016641414141414, "width": 0.33079901960784314, "height": 0.011320707070707092, "page": 4}], "section": "4 METHOD", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2355"}, {"text": "The two other authors took running notes in a shared online document and occasionally stepped in to ask clarifying questions, probe deeper on topics of interest, or provide technical explanations for how the translation algorithm worked.", "label": "Objective", "bboxes": [{"left": 0.4544705882352942, "top": 0.7016641414141414, "width": 0.3655245098039215, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.7189608585858587, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.7362575757575758, "width": 0.33273856209150326, "height": 0.011320707070707092, "page": 4}], "section": "4 METHOD", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2356"}, {"text": "We also asked specific questions for each variant.", "label": "Objective", "bboxes": [{"left": 0.17929901960784314, "top": 0.29978156565656566, "width": 0.2858235294117647, "height": 0.011320707070707092, "page": 5}], "section": "4 METHOD @@ 4.2 Variant-Specific Questions", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2357"}, {"text": "Some participants had questions about the underlying mechanics of the NMT model, as they were curious how it works (P5) and would like to know what its thinking (P10).", "label": "Objective", "bboxes": [{"left": 0.33141666666666664, "top": 0.5976868686868687, "width": 0.49019444444444443, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.6149835858585858, "width": 0.561625816993464, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2358"}, {"text": "The place where I can see something like this being very helpful would be where you really have a truly dead codebase thats just a complete black box that nobody can, or wants, to understand... because for that you need understanding if the goal is to keep the darn thing limping while you transition to something else. (P4)", "label": "Objective", "bboxes": [{"left": 0.15988398692810457, "top": 0.3502790404040404, "width": 0.6202352941176471, "height": 0.011320707070707092, "page": 10}, {"left": 0.15988398692810457, "top": 0.367574494949495, "width": 0.620235294117647, "height": 0.011320707070706981, "page": 10}, {"left": 0.15988398692810457, "top": 0.3848712121212121, "width": 0.6198333333333333, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2359"}, {"text": "Our study is a preliminary examination of how probabilistic generative models can be applied to a use case that requires an objective level of quality.", "label": "Objective", "bboxes": [{"left": 0.18000163398692812, "top": 0.3841628787878788, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4014583333333333, "width": 0.16816013071895422, "height": 0.011320707070707092, "page": 13}], "section": "7 LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2360"}, {"text": "Our research focuses on understanding the extent to which generative models are still useful to human stakeholders despite their potential to produce imperfect output.", "label": "Method", "bboxes": [{"left": 0.196281045751634, "top": 0.2942222222222222, "width": 0.6837222222222222, "height": 0.013969696969696965, "page": 1}, {"left": 0.18000163398692812, "top": 0.31151893939393943, "width": 0.3034836601307189, "height": 0.011320707070707037, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2361"}, {"text": "Given the probabilistic nature of deep learning models, we posit that there will always be some amount of noise in their output; correspondingly, deep generative models that produce code as output may exhibit some amount of deviation from a programmers intentions.", "label": "Method", "bboxes": [{"left": 0.18000163398692812, "top": 0.20774116161616163, "width": 0.7002810457516339, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.2250366161616162, "width": 0.6999934640522876, "height": 0.011320707070707037, "page": 1}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.19113398692810454, "height": 0.011320707070707065, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2362"}, {"text": "We address these questions by considering the use of an NMT model within the context of application modernization [27, 64, 72].", "label": "Method", "bboxes": [{"left": 0.196281045751634, "top": 0.3807032828282828, "width": 0.6861993464052287, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.398, "width": 0.11356862745098037, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2363"}, {"text": "To explore the utility of NMT models, we conducted a series of scenario-based design interviews with 11 professional software engineers who work across a variety of technology areas.", "label": "Method", "bboxes": [{"left": 0.196281045751634, "top": 0.4671843434343434, "width": 0.6837124183006537, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.4844810606060606, "width": 0.3955375816993464, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2364"}, {"text": "Our work examines a different aspect of imperfect AI by focusing on a co-creation scenario that has a lower tolerance for error (e.g. code ought to compile and be free of errors).", "label": "Method", "bboxes": [{"left": 0.5036111111111111, "top": 0.6271755050505051, "width": 0.31638562091503275, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.6444709595959596, "width": 0.6999918300653595, "height": 0.011320707070706981, "page": 2}, {"left": 0.1200016339869281, "top": 0.6617676767676768, "width": 0.042686274509803904, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Imperfect AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2365"}, {"text": "A significant portion of this work is recounted in Allamanis et al.", "label": "Method", "bboxes": [{"left": 0.8101274509803922, "top": 0.2898977272727273, "width": 0.0103856209150327, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.30719444444444444, "width": 0.3759493464052288, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.1 AI Techniques for Software Engineering", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2366"}, {"text": "Given the imperfect output of state-of-the-art NMT models, we posit that such systems will act in concert with human software engineers as a collaborative partner or teammate.", "label": "Method", "bboxes": [{"left": 0.1200016339869281, "top": 0.7396010101010101, "width": 0.7, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.7568964646464647, "width": 0.34180555555555553, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2367"}, {"text": "We discuss three areas relevant to our work: recent advances in the use of AI techniques, and specifically deep generative models, in software engineering; studies of the utility of imperfect AI; and studies of human-AI co-creation with generative AI.", "label": "Method", "bboxes": [{"left": 0.11929738562091505, "top": 0.1428800505050505, "width": 0.7006944444444443, "height": 0.011320707070707065, "page": 2}, {"left": 0.1200016339869281, "top": 0.16017676767676767, "width": 0.7, "height": 0.011320707070707065, "page": 2}, {"left": 0.11945915032679738, "top": 0.17747222222222223, "width": 0.11533333333333334, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2368"}, {"text": "Despite identifying favorable outcomes of human-AI partnerships, we note that these outcomes are all subjective: the quality of the generated output lies in the perceptions of the people using the system.", "label": "Method", "bboxes": [{"left": 0.196281045751634, "top": 0.5790757575757576, "width": 0.6854493464052289, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.5963724747474748, "width": 0.5234330065359476, "height": 0.011320707070707092, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2369"}, {"text": "Some of this work has examined the extent to which deep generative models provide humans with augmented capabilities, addressing the question of the extent to which the human-AI team produces outcomes better than those produced by either party alone (e.g. [29, 37, 57, 70, 92, 100]).", "label": "Method", "bboxes": [{"left": 0.28667483660130716, "top": 0.4752979797979798, "width": 0.5933284313725491, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.49259469696969693, "width": 0.7002696078431372, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.5098901515151516, "width": 0.4178333333333333, "height": 0.011320707070706981, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2370"}, {"text": "We developed an exploratory design scenario in order to engage software engineers in a discussion about the role of generative AI in application modernization.", "label": "Method", "bboxes": [{"left": 0.17929901960784314, "top": 0.7062714646464646, "width": 0.7007042483660132, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.7235669191919193, "width": 0.2531307189542483, "height": 0.011320707070706981, "page": 3}], "section": "3 DESIGN SCENARIO", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2371"}, {"text": "Our scenario used real output from TransCoder that contained flaws.", "label": "Method", "bboxes": [{"left": 0.1362794117647059, "top": 0.31151893939393943, "width": 0.405109477124183, "height": 0.011320707070707037, "page": 4}], "section": "3 DESIGN SCENARIO", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2372"}, {"text": "We conducted an interview study with software engineers in which we used the design scenario to spark discussions around the role of generative AI in application modernization.", "label": "Method", "bboxes": [{"left": 0.11929738562091505, "top": 0.4422209595959596, "width": 0.7006944444444444, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.45951767676767674, "width": 0.3838970588235294, "height": 0.011320707070707092, "page": 4}], "section": "4 METHOD", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2373"}, {"text": "We conducted a thematic analysis of our data to identify important ideas and themes from our interviews.", "label": "Method", "bboxes": [{"left": 0.17929901960784314, "top": 0.4566830808080808, "width": 0.6401535947712419, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2374"}, {"text": "We begin by highlighting the difficulties faced by our software engineers in modernizing legacy applications, motivating the need for AI support.", "label": "Method", "bboxes": [{"left": 0.196281045751634, "top": 0.6642386363636363, "width": 0.6853316993464053, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.6815340909090909, "width": 0.20596568627450976, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2375"}, {"text": "Our interviews generated a wealth of material: 11 hours of recorded videos, approximately 63 pages of notes, and a corpus of approximately 400 pages of interview transcripts containing about 89k words.", "label": "Method", "bboxes": [{"left": 0.196281045751634, "top": 0.5258686868686868, "width": 0.6837222222222223, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.5431641414141414, "width": 0.5334330065359476, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2376"}, {"text": "We recruited 11 full-time software engineers within our organization, an international information technology company.", "label": "Method", "bboxes": [{"left": 0.17929901960784314, "top": 0.1428800505050505, "width": 0.7029509803921569, "height": 0.011320707070707065, "page": 5}], "section": "4 METHOD @@ 4.1 Participants", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2377"}, {"text": "For each UX variant, we asked participants about what they liked and disliked and how they might improve the design.", "label": "Method", "bboxes": [{"left": 0.18000163398692812, "top": 0.2824848484848485, "width": 0.7022565359477124, "height": 0.011320707070707037, "page": 5}], "section": "4 METHOD @@ 4.2 Variant-Specific Questions", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2378"}, {"text": "All participants who were involved in an application modernization project expressed some desire for help, and our focus on the code translation use case positively resonated with them.", "label": "Method", "bboxes": [{"left": 0.6127941176470588, "top": 0.13855555555555557, "width": 0.20720588235294113, "height": 0.011320707070707065, "page": 6}, {"left": 0.1200016339869281, "top": 0.15585227272727273, "width": 0.7000000000000001, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.17314898989898989, "width": 0.18908496732026148, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2379"}, {"text": "We recognize that trust is a complex, multi-faceted construct that has been described as being an attitude [78], an intention [62], and a behavior [1].", "label": "Method", "bboxes": [{"left": 0.1362794117647059, "top": 0.35429040404040407, "width": 0.683720588235294, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.37158712121212123, "width": 0.19429738562091503, "height": 0.011320707070706981, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2380"}, {"text": "If Im going to put my production systems functionality behind this, I really want some degree of trust that this thing is robust... Every piece of code that we put into production has been reviewed by at least one other person. And so we try hard to basically build that trust through review and familiarity in humans... wed need basically the same level of trust for a system like this. (P4)", "label": "Method", "bboxes": [{"left": 0.15988398692810457, "top": 0.28397727272727274, "width": 0.6202369281045752, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.3012727272727273, "width": 0.6202287581699346, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.3185694444444444, "width": 0.620235294117647, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.33586616161616156, "width": 0.3387598039215686, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2381"}, {"text": "We observed that discrepancies between our participants mental models and the actual operation of the NMT model was a source of confusion.", "label": "Method", "bboxes": [{"left": 0.3874705882352941, "top": 0.27692550505050506, "width": 0.49253594771241826, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.2942222222222222, "width": 0.3674183006535948, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2382"}, {"text": "Despite the feelings that having an understanding of the NMT models operation wasnt important, we observed that having such understanding does have benefits.", "label": "Method", "bboxes": [{"left": 0.196281045751634, "top": 0.15585227272727273, "width": 0.6837124183006537, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.17314898989898989, "width": 0.28227450980392155, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2383"}, {"text": "We initially thought that the translation would produce only content in the form of the translated code.", "label": "Method", "bboxes": [{"left": 0.5411143790849673, "top": 0.2564772727272727, "width": 0.27888725490196087, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.2737727272727273, "width": 0.34687254901960785, "height": 0.013969696969696965, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2384"}, {"text": "Thus, we questioned whether we would encounter 9", "label": "Method", "bboxes": [{"left": 0.5153872549019607, "top": 0.8125239898989899, "width": 0.3048888888888891, "height": 0.011320707070707092, "page": 8}, {"left": 0.46734967320261434, "top": 0.8307196969696969, "width": 0.005299019607843236, "height": 0.008805555555555622, "page": 8}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2385"}, {"text": "To probe at this idea, we selected a code sample with an incorrect translation in order to spark discussion on the utility of erroneous output.", "label": "Method", "bboxes": [{"left": 0.5986127450980392, "top": 0.12126010101010101, "width": 0.28138235294117653, "height": 0.011320707070707092, "page": 9}, {"left": 0.17945915032679738, "top": 0.13855555555555557, "width": 0.5489722222222222, "height": 0.011320707070707065, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2386"}, {"text": "A lot of us in the software development field, we sometimes have the feeling that for a given problem set, we have to solve the entire problem. But, thats not necessarily true... if you can help someone do part of the task... maybe it will be more effective at smaller, less complex code fragments. Then, [as] you get feedback from users, you build into it more intelligence, then maybe it becomes more effective on larger, more complex pieces of code. (P3)", "label": "Method", "bboxes": [{"left": 0.21988562091503266, "top": 0.6361439393939394, "width": 0.6218349673202614, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.653439393939394, "width": 0.6202271241830066, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.6707361111111111, "width": 0.620235294117647, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.6880328282828283, "width": 0.6202320261437908, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.7053282828282829, "width": 0.1111127450980392, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2387"}, {"text": "Throughout our interviews, participants made a number of suggestions for how generative AI models, as well as the user interfaces that incorporate them, could support these activities.", "label": "Method", "bboxes": [{"left": 0.15410457516339868, "top": 0.1947689393939394, "width": 0.6658872549019607, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.21206439393939394, "width": 0.4301666666666667, "height": 0.011320707070707037, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2388"}, {"text": "Generating documentation from source is an active area of research in the machine learning community (e.g. [44, 48, 65, 91]) and our results highlight the importance of this functionality in user experiences, even when generated documentation may be imperfect.", "label": "Method", "bboxes": [{"left": 0.1362794117647059, "top": 0.6726565656565657, "width": 0.685330065359477, "height": 0.011320707070707092, "page": 10}, {"left": 0.11966339869281045, "top": 0.6899532828282828, "width": 0.7003284313725489, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.7072487373737373, "width": 0.2015539215686275, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2389"}, {"text": "Indeed, the ML community has explored many methods for generating tests [31, 86, 87] as well as generating data for tests [50, 83], and our findings reinforce the need to incorporate these techniques into mainstream application modernization tools.", "label": "Method", "bboxes": [{"left": 0.7174019607843137, "top": 0.6137676767676767, "width": 0.16298202614379076, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.631064393939394, "width": 0.6999967320261437, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.6483598484848484, "width": 0.6001633986928104, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2390"}, {"text": "We note that our system only examined one kind of human-AI interaction pattern, in which the human provided code and the translator produced a translation.", "label": "Method", "bboxes": [{"left": 0.1362794117647059, "top": 0.17314898989898989, "width": 0.6837254901960784, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.19044444444444444, "width": 0.25008986928104576, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2391"}, {"text": "Our results motivate the need for explainable generative AI.", "label": "Method", "bboxes": [{"left": 0.1362794117647059, "top": 0.5709621212121212, "width": 0.35481535947712417, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.2 Toward Explainable Generative AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2392"}, {"text": "We also discovered how this information possessed explanatory power, helping participants both understand the codes underlying intent and identify potential errors in the translation.", "label": "Method", "bboxes": [{"left": 0.15865359477124183, "top": 0.13855555555555557, "width": 0.6613415032679738, "height": 0.011320707070707065, "page": 12}, {"left": 0.1200016339869281, "top": 0.15585227272727273, "width": 0.4283954248366013, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2393"}, {"text": "Our study identified a number of areas that could be pursued for the use of generative AI in application modernization.", "label": "Method", "bboxes": [{"left": 0.1200016339869281, "top": 0.7093320707070708, "width": 0.7022598039215686, "height": 0.011320707070706981, "page": 12}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2394"}, {"text": "NMT models such as the one we used are typically trained in an unsupervised manner, in part due to the", "label": "Method", "bboxes": [{"left": 0.197437908496732, "top": 0.8131098484848485, "width": 0.6225539215686273, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2395"}, {"text": "Although the import of generative capabilities may seem to be about the production of an artifact itself, we found that they instead may also be a means rather than just an end .", "label": "Method", "bboxes": [{"left": 0.734733660130719, "top": 0.20774116161616163, "width": 0.14526960784313725, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.2250366161616162, "width": 0.6999934640522876, "height": 0.011320707070707037, "page": 13}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.19047058823529414, "height": 0.013969696969696993, "page": 13}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2396"}, {"text": "Our study is a preliminary examination of how probabilistic generative models can be applied to a use case that requires an objective level of quality.", "label": "Method", "bboxes": [{"left": 0.18000163398692812, "top": 0.3841628787878788, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4014583333333333, "width": 0.16816013071895422, "height": 0.011320707070707092, "page": 13}], "section": "7 LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2397"}, {"text": "We have examined how such a model would be received by software engineers in the context of modernizing legacy applications.", "label": "Method", "bboxes": [{"left": 0.6593709150326797, "top": 0.6470656565656565, "width": 0.22062418300653597, "height": 0.011320707070707092, "page": 13}, {"left": 0.17945915032679738, "top": 0.6643623737373737, "width": 0.5504983660130719, "height": 0.011320707070707092, "page": 13}], "section": "8 CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2398"}, {"text": "(1) We identify that although software engineers had concerns about the quality of the code produced by an NMT model, their concerns were tempered by the fact that integration of any NMT-produced code would follow existing best practices in software engineering (e.g. code reviews & testing), thus ensuring that any potentially-flawed code would be subject to human review and revision before its inclusion in a product or service.", "label": "Method", "bboxes": [{"left": 0.19782843137254902, "top": 0.5662108585858586, "width": 0.682609477124183, "height": 0.011320707070706981, "page": 1}, {"left": 0.21988562091503266, "top": 0.5835063131313132, "width": 0.6601078431372549, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.6008030303030303, "width": 0.6601127450980393, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.6180997474747475, "width": 0.565908496732026, "height": 0.011320707070706981, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2399"}, {"text": "[90] show how such scores encourage use of an autonomous system.", "label": "Result", "bboxes": [{"left": 0.795936274509804, "top": 0.6098787878787879, "width": 0.024057189542483703, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.6271755050505051, "width": 0.3799656862745099, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Imperfect AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2400"}, {"text": "We find their question of how to determine", "label": "Result", "bboxes": [{"left": 0.5636078431372549, "top": 0.8087853535353535, "width": 0.2563970588235295, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2401"}, {"text": "Participants were identified using an internal expertise-location tool that allowed us to find people in a software engineering role with experience in both Java and Python.", "label": "Result", "bboxes": [{"left": 0.18000163398692812, "top": 0.16017676767676767, "width": 0.6999918300653596, "height": 0.011320707070707065, "page": 5}, {"left": 0.18000163398692812, "top": 0.17747222222222223, "width": 0.36624836601307187, "height": 0.011320707070707092, "page": 5}], "section": "4 METHOD @@ 4.1 Participants", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2402"}, {"text": "Overall, many participants felt that even the erroneous output was desirable and that it would be something they would find useful to have.", "label": "Result", "bboxes": [{"left": 0.196281045751634, "top": 0.2250366161616162, "width": 0.6840931372549021, "height": 0.011320707070707037, "page": 9}, {"left": 0.17945915032679738, "top": 0.24233333333333332, "width": 0.16245588235294117, "height": 0.011320707070707065, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2403"}, {"text": "One helpful improvement to the code translation user interface would be to show a stronger visual correspondence between input and output source code.", "label": "Result", "bboxes": [{"left": 0.3282320261437908, "top": 0.7344722222222222, "width": 0.49176633986928114, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.7517676767676768, "width": 0.4176830065359477, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2404"}, {"text": "Interface features such as confidence highlighting helped steer human attention toward potentially problematic areas, and alternate translations provided explanatory power for why a segment of translated code was rated as low confidence by the model; in addition, alternate translations provided clues toward the semantic meaning of the code and helped participants find logical errors in it.", "label": "Result", "bboxes": [{"left": 0.28210130718954246, "top": 0.7162499999999999, "width": 0.5982777777777779, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.7335467171717173, "width": 0.6999967320261437, "height": 0.011320707070706981, "page": 13}, {"left": 0.18000163398692812, "top": 0.7508434343434344, "width": 0.6999967320261437, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.7681388888888888, "width": 0.36549999999999994, "height": 0.011320707070707092, "page": 13}], "section": "8 CONCLUSION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2405"}, {"text": "Further, having interactive UX features such as confidence highlighting and alternate translations helped software engineers better understand how the NMT model produced its output, and even find bugs in its translations.", "label": "Result", "bboxes": [{"left": 0.7894558823529412, "top": 0.6180997474747475, "width": 0.09054901960784312, "height": 0.011320707070706981, "page": 1}, {"left": 0.21988562091503266, "top": 0.635395202020202, "width": 0.6601094771241831, "height": 0.011320707070706981, "page": 1}, {"left": 0.21988562091503266, "top": 0.6526919191919193, "width": 0.5861127450980392, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2406"}, {"text": "Given the probabilistic nature of deep learning models, we posit that there will always be some amount of noise in their output; correspondingly, deep generative models that produce code as output may exhibit some amount of deviation from a programmers intentions.", "label": "Conclusion", "bboxes": [{"left": 0.18000163398692812, "top": 0.20774116161616163, "width": 0.7002810457516339, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.2250366161616162, "width": 0.6999934640522876, "height": 0.011320707070707037, "page": 1}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.19113398692810454, "height": 0.011320707070707065, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2407"}, {"text": "[88]s work on learning bug-fixing patches and TransCoder [79]  are often capable of producing high-quality source code, the code that they generate may not always be error-free 1 .", "label": "Conclusion", "bboxes": [{"left": 0.7413741830065359, "top": 0.24233333333333332, "width": 0.13862091503267981, "height": 0.011320707070707065, "page": 1}, {"left": 0.18000163398692812, "top": 0.2596300505050505, "width": 0.7003807189542484, "height": 0.011320707070707037, "page": 1}, {"left": 0.18000163398692812, "top": 0.27495328282828285, "width": 0.2385506535947712, "height": 0.015941919191919174, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2408"}, {"text": "This context is both timely and relevant: timely, because many organizations are currently faced with the problem of how to migrate complex, legacy applications to the cloud when they do not necessarily have the resources or expertise to do so [34, 36]; and relevant, because NMT models can specifically help in the process of translating from legacy languages or application frameworks (e.g. COBOL, J2EE) into modern ones (e.g. Python, Go).", "label": "Conclusion", "bboxes": [{"left": 0.29723529411764704, "top": 0.398, "width": 0.5827647058823529, "height": 0.011320707070707092, "page": 1}, {"left": 0.17945915032679738, "top": 0.4152954545454545, "width": 0.7005441176470588, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.43259217171717174, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.4498888888888889, "width": 0.6977091503267974, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2409"}, {"text": "[28] found a similar result, that giving a good reason for a poor recommendation made people more likely to accept that poor recommendation.", "label": "Conclusion", "bboxes": [{"left": 0.46092483660130723, "top": 0.5752866161616161, "width": 0.35907352941176474, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.5925820707070707, "width": 0.5094346405228758, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Imperfect AI", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2410"}, {"text": "[81] recently outlined a set of research questions spanning many aspects of human-AI collaboration, such as how tasks should be allocated across both parties and how trust could be established with a machine teammate.", "label": "Conclusion", "bboxes": [{"left": 0.6422565359477124, "top": 0.7741931818181818, "width": 0.1777401960784315, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.791489898989899, "width": 0.6999950980392158, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.8087853535353535, "width": 0.43995424836601305, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2411"}, {"text": "Given that the probabilistic nature of generative models implies the potential for some amount of error in their outputs, we seek to understand the extent to which they may be useful in scenarios that have an objective bar of quality.", "label": "Conclusion", "bboxes": [{"left": 0.845047385620915, "top": 0.6136679292929292, "width": 0.03494771241830075, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.6309646464646465, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.6482613636363637, "width": 0.6471192810457516, "height": 0.011320707070707092, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2412"}, {"text": "As discussed by Wolf [97, p. 252], scenario-based design [is] a method that anticipates and leverages scenarios of possible use early on in system development. (italics in original).", "label": "Conclusion", "bboxes": [{"left": 0.8645130718954248, "top": 0.7408636363636364, "width": 0.015485294117646986, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.7581603535353535, "width": 0.6999967320261438, "height": 0.013969696969697076, "page": 3}, {"left": 0.18000163398692812, "top": 0.775455808080808, "width": 0.34738725490196076, "height": 0.013969696969697076, "page": 3}], "section": "3 DESIGN SCENARIO", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2413"}, {"text": "Each variant appeared as a partially-functional prototype, through which participants could explore features of interest.", "label": "Conclusion", "bboxes": [{"left": 0.7441045751633987, "top": 0.13855555555555557, "width": 0.07589379084967318, "height": 0.011320707070707065, "page": 4}, {"left": 0.1200016339869281, "top": 0.15585227272727273, "width": 0.6380718954248366, "height": 0.011320707070707092, "page": 4}], "section": "3 DESIGN SCENARIO", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2414"}, {"text": "Participants were informed that the translation results were produced by an AI system and that the translations may be imperfect.", "label": "Conclusion", "bboxes": [{"left": 0.49800326797385625, "top": 0.580590909090909, "width": 0.3219983660130718, "height": 0.011320707070707092, "page": 4}, {"left": 0.11945915032679738, "top": 0.5978876262626263, "width": 0.4535294117647058, "height": 0.011320707070707092, "page": 4}], "section": "4 METHOD", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2415"}, {"text": "We begin by highlighting the difficulties faced by our software engineers in modernizing legacy applications, motivating the need for AI support.", "label": "Conclusion", "bboxes": [{"left": 0.196281045751634, "top": 0.6642386363636363, "width": 0.6853316993464053, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.6815340909090909, "width": 0.20596568627450976, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2416"}, {"text": "For Figure 1B, we asked participants if they understood why the highlighted tokens were highlighted, and what other information could help them better understand the AIs confidence.", "label": "Conclusion", "bboxes": [{"left": 0.7995915032679739, "top": 0.3343737373737374, "width": 0.08202614379084949, "height": 0.011320707070707092, "page": 5}, {"left": 0.17945915032679738, "top": 0.3516704545454546, "width": 0.7005457516339869, "height": 0.011320707070707037, "page": 5}, {"left": 0.18000163398692812, "top": 0.3689671717171717, "width": 0.33134477124183004, "height": 0.011320707070707092, "page": 5}], "section": "4 METHOD @@ 4.2 Variant-Specific Questions", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2417"}, {"text": "For Figure 1C, we asked participants if they understood why the AI produced the alternatives it produced, and what other information could help them better understand how those alternatives were generated.", "label": "Conclusion", "bboxes": [{"left": 0.5149869281045752, "top": 0.3689671717171717, "width": 0.36539542483660126, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.3862626262626263, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.40355934343434346, "width": 0.16886764705882354, "height": 0.011320707070707037, "page": 5}], "section": "4 METHOD @@ 4.2 Variant-Specific Questions", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2418"}, {"text": "Thus, in our discussions with participants around the topic of trust, our emphasis was on understanding their attitudes toward acceptance of AI-generated code through their expressions of desire to incorporate (or not incorporate) such code in their work  i.e. their willing[ness] to act, rather than their past behavior of having acted, as no participants had previously encountered a code-generating AI system.", "label": "Conclusion", "bboxes": [{"left": 0.5, "top": 0.40617929292929295, "width": 0.32000163398692816, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.42347601010101005, "width": 0.700281045751634, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.44077146464646466, "width": 0.700281045751634, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.4580681818181818, "width": 0.7008594771241831, "height": 0.011320707070707037, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2419"}, {"text": "He described a situation in which individual contributors working on a code conversion task dont have to understand how the background of the application of the AI works, [because] they have to do the conversion [task].", "label": "Conclusion", "bboxes": [{"left": 0.8025277777777777, "top": 0.7782045454545454, "width": 0.01746895424836603, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.7955012626262626, "width": 0.7000032679738561, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.8127979797979797, "width": 0.6181160130718955, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2420"}, {"text": "As a user, you... accept whats going on here as a useful and correct process and you probably do not even care how its done because it will speed up your work... and at the end of the day, youll say, okay, Im done. (P1)", "label": "Conclusion", "bboxes": [{"left": 0.15988398692810457, "top": 0.7424835858585859, "width": 0.620235294117647, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.759780303030303, "width": 0.6197320261437909, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2421"}, {"text": "P7 described how one of the alternatives more explicitly demonstrated the codes functionality: The first [option] is more Pythonic because its taking advantage of the built-in functionality of an array... however this [second option] is far more explicit... this is whats happening behind the scenes.", "label": "Conclusion", "bboxes": [{"left": 0.18000163398692812, "top": 0.582290404040404, "width": 0.6999967320261438, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.5995871212121212, "width": 0.6999967320261437, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.6168825757575758, "width": 0.3927761437908497, "height": 0.011320707070706981, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2422"}, {"text": "Understanding the models operation could also eliminate some of the confusion caused by the confidence highlights in Figure 1B.", "label": "Conclusion", "bboxes": [{"left": 0.196281045751634, "top": 0.2250366161616162, "width": 0.6837124183006537, "height": 0.011320707070707037, "page": 7}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.07773856209150323, "height": 0.011320707070707065, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2423"}, {"text": "P3 expected that the translator would be able to define this def function signature line with the function name correct, but expressed confusion when the translator wasnt as confident as it could be.", "label": "Conclusion", "bboxes": [{"left": 0.319202614379085, "top": 0.3461111111111111, "width": 0.5607924836601308, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.36340656565656565, "width": 0.6217630718954248, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2424"}, {"text": "P2 said, [If its] 42% confident about this translation, maybe... [there could be] a link that you could click on to see why it is not confident.", "label": "Conclusion", "bboxes": [{"left": 0.18000163398692812, "top": 0.4785126262626263, "width": 0.7003807189542484, "height": 0.011320707070707037, "page": 7}, {"left": 0.18000163398692812, "top": 0.4958093434343434, "width": 0.1106241830065359, "height": 0.011320707070707037, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2425"}, {"text": "Thus, although participants generally said they dont care (P7) about knowing how the translation was produced, explanations of the models mechanics may be helpful for avoiding confusion caused by confidence discrepancies.", "label": "Conclusion", "bboxes": [{"left": 0.17956209150326796, "top": 0.513104797979798, "width": 0.7020539215686273, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.5304015151515151, "width": 0.6808611111111109, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2426"}, {"text": "architects overseeing the entire project have to know how this AI is working because they are the ones responsible for making this [code translation] project [happen].", "label": "Conclusion", "bboxes": [{"left": 0.18000163398692812, "top": 0.12126010101010101, "width": 0.6999901960784313, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.13855555555555557, "width": 0.3186437908496731, "height": 0.011320707070707065, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2427"}, {"text": "P4 specifically called out how code reviews are used to give people feedback to improve their skills and how such reviews could be used to improve the NMT model.", "label": "Conclusion", "bboxes": [{"left": 0.6023415032679739, "top": 0.6969078282828283, "width": 0.27765849673202614, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.7142045454545455, "width": 0.6992320261437909, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2428"}, {"text": "With humans, you can teach through review...if somebody is continuously making these kind of mistakes, and were catching them in review, that person is going to get annoyed by a million comments... you could probably have some sort of reinforcement or sort of iterative learning in your model that would teach it. (P4)", "label": "Conclusion", "bboxes": [{"left": 0.21988562091503266, "top": 0.7398131313131313, "width": 0.6218267973856211, "height": 0.011320707070707092, "page": 7}, {"left": 0.21988562091503266, "top": 0.7571098484848484, "width": 0.6202287581699346, "height": 0.011320707070707092, "page": 7}, {"left": 0.21988562091503266, "top": 0.7744065656565657, "width": 0.619748366013072, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2429"}, {"text": "That just confuses me because this line of code is syntactically correct. I would write that line of code with 100% confidence. Im confused about why the translator would be confused about that assignment. (P0)", "label": "Conclusion", "bboxes": [{"left": 0.21988562091503266, "top": 0.40631313131313135, "width": 0.6202369281045752, "height": 0.011320707070707092, "page": 7}, {"left": 0.21988562091503266, "top": 0.42360858585858585, "width": 0.5968660130718955, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2430"}, {"text": "Participants also appreciated the feature of the alternate translation UI in which downstream edits could be made when selecting an alternate translation (e.g. because selecting an alternate may have renamed a variable or changed a loop index).", "label": "Conclusion", "bboxes": [{"left": 0.1362794117647059, "top": 0.6210934343434343, "width": 0.6837173202614379, "height": 0.011320707070707092, "page": 8}, {"left": 0.11945915032679738, "top": 0.6383901515151515, "width": 0.7005359477124182, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.6556868686868687, "width": 0.08280392156862747, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2431"}, {"text": "Their comments provided insight into how these features could be used to drive human-AI workflows and how human actions could provide feedback to improve the NMT model.", "label": "Conclusion", "bboxes": [{"left": 0.345468954248366, "top": 0.16017676767676767, "width": 0.47452777777777777, "height": 0.011320707070707065, "page": 8}, {"left": 0.1200016339869281, "top": 0.17747222222222223, "width": 0.6000310457516341, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2432"}, {"text": "We learned that the low-confidence highlighting (Figure 1B) could also help developers to organize their work.", "label": "Conclusion", "bboxes": [{"left": 0.47146241830065366, "top": 0.2737727272727273, "width": 0.3485392156862745, "height": 0.011320707070707092, "page": 8}, {"left": 0.11966339869281045, "top": 0.29106944444444444, "width": 0.32970915032679743, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2433"}, {"text": "P4 emphasized the utility of this approach: Having something to draw my attention to places where its more likely that theres an error would be super helpful.", "label": "Conclusion", "bboxes": [{"left": 0.5622058823529411, "top": 0.3083661616161616, "width": 0.25951307189542494, "height": 0.011320707070707092, "page": 8}, {"left": 0.11834640522875817, "top": 0.32566161616161615, "width": 0.7033039215686274, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2434"}, {"text": "P5 pointed out that, If you used this [confidence highlighting] on a big code base... you could know where to look, where there might be problems.", "label": "Conclusion", "bboxes": [{"left": 0.41302941176470587, "top": 0.3602550505050505, "width": 0.4069624183006536, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.377550505050505, "width": 0.47501143790849676, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2435"}, {"text": "Thus, the meta-information provided by the existence of a highlight can help steer human activity, complementing Louie et al.", "label": "Conclusion", "bboxes": [{"left": 0.5986601307189542, "top": 0.377550505050505, "width": 0.22133823529411767, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.39484722222222224, "width": 0.5582124183006536, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2436"}, {"text": "s notion of how humans may steer the output of a generative model [57].", "label": "Conclusion", "bboxes": [{"left": 0.6818823529411765, "top": 0.39484722222222224, "width": 0.13811111111111107, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.41214267676767674, "width": 0.3058398692810458, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2437"}, {"text": "Many participants also pointed out how edits to low-confidence translated code could serve as feedback to the NMT model to improve its training.", "label": "Conclusion", "bboxes": [{"left": 0.4296519607843137, "top": 0.41214267676767674, "width": 0.3903529411764706, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.4294393939393939, "width": 0.4909395424836601, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2438"}, {"text": "Thus, we questioned whether we would encounter 9", "label": "Conclusion", "bboxes": [{"left": 0.5153872549019607, "top": 0.8125239898989899, "width": 0.3048888888888891, "height": 0.011320707070707092, "page": 8}, {"left": 0.46734967320261434, "top": 0.8307196969696969, "width": 0.005299019607843236, "height": 0.008805555555555622, "page": 8}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2439"}, {"text": "Once they saw the alternate translations, they began to understand why the NMT model had highlighted certain tokens as low-confidence: precisely because there were alternate ways of translating those tokens.", "label": "Conclusion", "bboxes": [{"left": 0.49856372549019606, "top": 0.4738510101010101, "width": 0.3214297385620915, "height": 0.011320707070707037, "page": 8}, {"left": 0.1200016339869281, "top": 0.49114772727272726, "width": 0.700001633986928, "height": 0.013969696969696965, "page": 8}, {"left": 0.1200016339869281, "top": 0.5084444444444445, "width": 0.25410620915032683, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2440"}, {"text": "Thus, the alternate translations served as explanations for the highlights.", "label": "Conclusion", "bboxes": [{"left": 0.37777124183006533, "top": 0.5084444444444445, "width": 0.4367598039215687, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2441"}, {"text": "Both P1 and P3 were able to identify a logical error in the translated code because they reviewed the alternate translations.", "label": "Conclusion", "bboxes": [{"left": 0.30445424836601304, "top": 0.5430366161616161, "width": 0.5159248366013072, "height": 0.013969696969696965, "page": 8}, {"left": 0.1200016339869281, "top": 0.5603333333333333, "width": 0.21128104575163403, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2442"}, {"text": "Thus, although many participants expressed a willingness to accept imperfectly-translated code as a starting point, acceptance may depend on how many errors the translated code contains as well as the nature of those errors (e.g. if they are easy to spot and fix, or if they take attention away from the central problem).", "label": "Conclusion", "bboxes": [{"left": 0.196281045751634, "top": 0.725564393939394, "width": 0.6853316993464054, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.7428598484848485, "width": 0.7000049019607842, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.7601565656565656, "width": 0.5177058823529412, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2443"}, {"text": "For large codebases, the downside of imperfectly-translated code may be less than the upside of simply having automatically-produced translations.", "label": "Conclusion", "bboxes": [{"left": 0.196281045751634, "top": 0.5760176767676768, "width": 0.6837173202614379, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.5933143939393939, "width": 0.21612581699346403, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2444"}, {"text": "P3 discussed how the NMT model could fit into a workflow for translating a large codebase.", "label": "Conclusion", "bboxes": [{"left": 0.39976633986928106, "top": 0.5933143939393939, "width": 0.48023366013071883, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.6106098484848484, "width": 0.056977124183006544, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2445"}, {"text": "P3 described how there may be individual differences in the value people derive from NMT translations.", "label": "Conclusion", "bboxes": [{"left": 0.49092810457516334, "top": 0.3745833333333334, "width": 0.38907843137254905, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.3918787878787879, "width": 0.2254787581699346, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2446"}, {"text": "I think for someone to start using a tool like this, it would need to prove that its adding some value... It probably varies from person to person. Some people wont be satisfied with anything you give them, and others will be satisfied with if it can be 10% effective in doing translations perhaps. And most people fall somewhere in the middle there. (P3)", "label": "Conclusion", "bboxes": [{"left": 0.21988562091503266, "top": 0.4174128787878788, "width": 0.6202352941176472, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.4347083333333333, "width": 0.6202352941176472, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.4520050505050505, "width": 0.620232026143791, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.4693017676767677, "width": 0.14249509803921567, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2447"}, {"text": "Participants described the lifecycle of modernizing applications as consisting of three general phases: (1) creating an understanding of the legacy application and its architecture by reviewing code and documentation; (2) performing the migration work, which may include translating and/or refactoring the code; and (3) reviewing and testing the migrated code.", "label": "Conclusion", "bboxes": [{"left": 0.1200016339869281, "top": 0.1428800505050505, "width": 0.6999967320261439, "height": 0.011320707070707065, "page": 10}, {"left": 0.1200016339869281, "top": 0.16017676767676767, "width": 0.7, "height": 0.011320707070707065, "page": 10}, {"left": 0.1200016339869281, "top": 0.17747222222222223, "width": 0.7000032679738563, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.1947689393939394, "width": 0.03042647058823529, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2448"}, {"text": "Throughout our interviews, participants made a number of suggestions for how generative AI models, as well as the user interfaces that incorporate them, could support these activities.", "label": "Conclusion", "bboxes": [{"left": 0.15410457516339868, "top": 0.1947689393939394, "width": 0.6658872549019607, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.21206439393939394, "width": 0.4301666666666667, "height": 0.011320707070707037, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2449"}, {"text": "Generating documentation from source is an active area of research in the machine learning community (e.g. [44, 48, 65, 91]) and our results highlight the importance of this functionality in user experiences, even when generated documentation may be imperfect.", "label": "Conclusion", "bboxes": [{"left": 0.1362794117647059, "top": 0.6726565656565657, "width": 0.685330065359477, "height": 0.011320707070707092, "page": 10}, {"left": 0.11966339869281045, "top": 0.6899532828282828, "width": 0.7003284313725489, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.7072487373737373, "width": 0.2015539215686275, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2450"}, {"text": "Participants also described how generative methods could be used to fill in missing documentation to help others understand ones own code, to create an understanding of poorly-documented code, or to re-align documentation with code when drift has occurred [30, 85].", "label": "Conclusion", "bboxes": [{"left": 0.1362794117647059, "top": 0.4559722222222222, "width": 0.6837140522875816, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.4732689393939394, "width": 0.7000000000000001, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.49056439393939394, "width": 0.2220196078431373, "height": 0.011320707070707037, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2451"}, {"text": "P6 described how his team has strict requirements for writing documentation for every function because it is used to generate Swagger API documentation 3 .", "label": "Conclusion", "bboxes": [{"left": 0.3455964052287582, "top": 0.49056439393939394, "width": 0.4746748366013071, "height": 0.011320707070707037, "page": 10}, {"left": 0.1200016339869281, "top": 0.5058888888888888, "width": 0.44208986928104577, "height": 0.013292929292929356, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2452"}, {"text": "The place where I can see something like this being very helpful would be where you really have a truly dead codebase thats just a complete black box that nobody can, or wants, to understand... because for that you need understanding if the goal is to keep the darn thing limping while you transition to something else. (P4)", "label": "Conclusion", "bboxes": [{"left": 0.15988398692810457, "top": 0.3502790404040404, "width": 0.6202352941176471, "height": 0.011320707070707092, "page": 10}, {"left": 0.15988398692810457, "top": 0.367574494949495, "width": 0.620235294117647, "height": 0.011320707070706981, "page": 10}, {"left": 0.15988398692810457, "top": 0.3848712121212121, "width": 0.6198333333333333, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2453"}, {"text": "P6 described how generative methods could help him form a clearer picture of system architecture: If AI could actually somehow... help me keep track of how the current function Im looking at fits into the bigger picture... I think thats really beneficial.", "label": "Conclusion", "bboxes": [{"left": 0.564735294117647, "top": 0.27388005050505054, "width": 0.25525653594771247, "height": 0.011320707070707037, "page": 10}, {"left": 0.1200016339869281, "top": 0.29117676767676764, "width": 0.6999901960784313, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.3084722222222222, "width": 0.5288464052287583, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2454"}, {"text": "P4 described how generative methods could be used to reverse engineer the architecture programmatically and deal with dead codebases.", "label": "Conclusion", "bboxes": [{"left": 0.6523235294117646, "top": 0.3084722222222222, "width": 0.16767156862745103, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.3257689393939394, "width": 0.6683921568627451, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2455"}, {"text": "Translation could also help software engineers better understand codebases in languages with which they are less familiar: Ive got a bunch of engineers that know Python, but they dont know Java. So, it might be easier to delve in and spelunk through... auto gen[erated] Python code than it would be to go back and spelunk the original Java. (P4).", "label": "Conclusion", "bboxes": [{"left": 0.1362794117647059, "top": 0.40408333333333335, "width": 0.6837156862745097, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.4213800505050505, "width": 0.6999983660130719, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.438675505050505, "width": 0.6986323529411764, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2456"}, {"text": "P6: As long as it is giving me a head start of any sort I will take it because for me the overhead to write such comments for every function that I code is very high. As you can imagine in a practical system there is going to be hundreds of such functions... its a lot of human hours.", "label": "Conclusion", "bboxes": [{"left": 0.15988398692810457, "top": 0.6188522727272727, "width": 0.620233660130719, "height": 0.011320707070707092, "page": 10}, {"left": 0.15988398692810457, "top": 0.6361489898989898, "width": 0.6202254901960784, "height": 0.011320707070707203, "page": 10}, {"left": 0.15988398692810457, "top": 0.6534444444444445, "width": 0.3494117647058824, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2457"}, {"text": "a standard formatting (P5) because reading other peoples code is much easier when you all adhere to the same standards. (P5).", "label": "Conclusion", "bboxes": [{"left": 0.18000163398692812, "top": 0.12126010101010101, "width": 0.6999918300653596, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.13855555555555557, "width": 0.09240849673202614, "height": 0.011320707070707065, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2458"}, {"text": "Many participants described how code translation could be used to take advantage of functionality defined in third party libraries, because the same libraries in Java dont exist in Python, or similar libraries exist but theyre not... syntactically the same. (P0).", "label": "Conclusion", "bboxes": [{"left": 0.196281045751634, "top": 0.17314898989898989, "width": 0.6837238562091503, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.19044444444444444, "width": 0.7022483660130718, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.20774116161616163, "width": 0.16701960784313727, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2459"}, {"text": "P8 described how the code translation process wasnt just about converting from one language to another, but also about identifying opportunities to leverage language-specific features to create more optimal code: The whole point of using the latest version is to tap into the new features... in JDK 8 they have lambdas... it is much faster. If it could do something like this I would use it like hell!", "label": "Conclusion", "bboxes": [{"left": 0.4910179738562091, "top": 0.2942222222222222, "width": 0.3889754901960785, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.31151893939393943, "width": 0.6999934640522876, "height": 0.011320707070707037, "page": 11}, {"left": 0.18000163398692812, "top": 0.32881439393939393, "width": 0.7003807189542482, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.3461111111111111, "width": 0.5514640522875816, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2460"}, {"text": "P9 emphasized how the alternate translations provided an educational opportunity while doing code modernization work because having new options makes it much more easy for me to explore newer methods in the newer language.", "label": "Conclusion", "bboxes": [{"left": 0.7354101307189543, "top": 0.3461111111111111, "width": 0.1445833333333334, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.36340656565656565, "width": 0.7005441176470587, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.3807032828282828, "width": 0.5465408496732026, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2461"}, {"text": "Overwhelmingly, they felt that a tool that could do code translation work within the application modernization context would be useful as it would help them get their own work done more quickly and effectively (e.g. as discussed by P3).", "label": "Conclusion", "bboxes": [{"left": 0.7779673202614379, "top": 0.7250416666666666, "width": 0.10364052287581704, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.7423383838383839, "width": 0.7000049019607841, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.7596338383838384, "width": 0.5886764705882352, "height": 0.011320707070707092, "page": 11}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "2462"}, {"text": "Instead of viewing the review and correction of the translated code as additional effort, they felt that the skeletal framework provided by the NMT model would be a useful starting place that they could then adjust and correct as necessary, as they usually do with code produced by their colleagues (e.g. as discussed by P4).", "label": "Conclusion", "bboxes": [{"left": 0.772109477124183, "top": 0.7596338383838384, "width": 0.10788562091503273, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.7769305555555555, "width": 0.7003725490196078, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.7942272727272728, "width": 0.6999934640522875, "height": 0.011320707070707092, "page": 11}, {"left": 0.17945915032679738, "top": 0.8115227272727272, "width": 0.38836764705882354, "height": 0.011320707070707092, "page": 11}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "2463"}, {"text": "Additional support provided by the NMT model by 12", "label": "Conclusion", "bboxes": [{"left": 0.5714967320261437, "top": 0.8115227272727272, "width": 0.3088872549019608, "height": 0.011320707070707092, "page": 11}, {"left": 0.5245702614379084, "top": 0.8307196969696969, "width": 0.010598039215686361, "height": 0.008805555555555622, "page": 11}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "2464"}, {"text": "They recognize the human capacity to err in themselves and their colleagues, and thus have developed processes and ways of working in which these errors are expected: code reviews [9, 63], test-driven development [23, 61], and root-cause analysis [53] are all ways of finding, fixing, and learning from the mistakes that are bound to happen in any software engineering effort.", "label": "Conclusion", "bboxes": [{"left": 0.11956209150326796, "top": 0.398, "width": 0.70043954248366, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.4152954545454545, "width": 0.6999918300653595, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.43259217171717174, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.4498888888888889, "width": 0.2140424836601308, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.2 Toward Explainable Generative AI", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "2465"}, {"text": "Techniques that help people understand the mechanics of generative models, as well as techniques that are able to generate specific rationales for why a given artifact was generated, could help people further adopt generative models in their own work practices.", "label": "Conclusion", "bboxes": [{"left": 0.49474183006535943, "top": 0.5709621212121212, "width": 0.3252581699346405, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.5882588383838384, "width": 0.6999918300653595, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.605554292929293, "width": 0.5245964052287583, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.2 Toward Explainable Generative AI", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "2466"}, {"text": "Our study identified a number of areas that could be pursued for the use of generative AI in application modernization.", "label": "Conclusion", "bboxes": [{"left": 0.1200016339869281, "top": 0.7093320707070708, "width": 0.7022598039215686, "height": 0.011320707070706981, "page": 12}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "2467"}, {"text": "In addition, task-oriented displays of the checklist mentioned by P10 could be used to help organize and track review efforts.", "label": "Conclusion", "bboxes": [{"left": 0.2592777777777778, "top": 0.7612209595959595, "width": 0.5607156862745097, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.7785176767676768, "width": 0.2006748366013072, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "2468"}, {"text": "Although the import of generative capabilities may seem to be about the production of an artifact itself, we found that they instead may also be a means rather than just an end .", "label": "Conclusion", "bboxes": [{"left": 0.734733660130719, "top": 0.20774116161616163, "width": 0.14526960784313725, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.2250366161616162, "width": 0.6999934640522876, "height": 0.011320707070707037, "page": 13}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.19047058823529414, "height": 0.013969696969696993, "page": 13}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "2469"}, {"text": "Thus, we challenge the community to consider use cases for generative AI that involve improving the state of human understanding of a domain or artifact, rather than considering the only utility of generative models to be the output itself.", "label": "Conclusion", "bboxes": [{"left": 0.7405228758169935, "top": 0.27692550505050506, "width": 0.1394836601307189, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.2942222222222222, "width": 0.6999918300653596, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.31151893939393943, "width": 0.6130996732026144, "height": 0.011320707070707037, "page": 13}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "2470"}, {"text": "The process of using these models to perform an initial translation, followed by manual correction, could provide a corpus of correctly-translated code that would enable supervised training, resulting in further quality improvements to the translator.", "label": "Conclusion", "bboxes": [{"left": 0.655609477124183, "top": 0.12126010101010101, "width": 0.22438398692810468, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.13855555555555557, "width": 0.7000032679738561, "height": 0.011320707070707065, "page": 13}, {"left": 0.17945915032679738, "top": 0.15585227272727273, "width": 0.5682941176470588, "height": 0.011320707070707092, "page": 13}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "2471"}, {"text": "Although we have discovered that our design scenario did demonstrate compelling user experiences, we caution that our results may not generalize to other domains or use cases beyond the code translation task we examined (e.g. using generative models for molecular discovery).", "label": "Conclusion", "bboxes": [{"left": 0.3518071895424837, "top": 0.4014583333333333, "width": 0.5284705882352941, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4187550505050505, "width": 0.7, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4360517676767677, "width": 0.435392156862745, "height": 0.011320707070707092, "page": 13}], "section": "7 LIMITATIONS", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2472"}, {"text": "Future work should explore how different patterns of interaction and initiative, as well as capabilities beyond code translation, can enable effective human-AI partnerships with generative models that produce better outcomes than what could be accomplished by either party alone.", "label": "Conclusion", "bboxes": [{"left": 0.5499705882352941, "top": 0.7681388888888888, "width": 0.33002287581699363, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.7854356060606061, "width": 0.6999934640522876, "height": 0.011320707070706981, "page": 13}, {"left": 0.17945915032679738, "top": 0.8027323232323232, "width": 0.6550016339869281, "height": 0.011320707070707092, "page": 13}], "section": "8 CONCLUSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "2473"}, {"text": "(1) We identify that although software engineers had concerns about the quality of the code produced by an NMT model, their concerns were tempered by the fact that integration of any NMT-produced code would follow existing best practices in software engineering (e.g. code reviews & testing), thus ensuring that any potentially-flawed code would be subject to human review and revision before its inclusion in a product or service.", "label": "Conclusion", "bboxes": [{"left": 0.19782843137254902, "top": 0.5662108585858586, "width": 0.682609477124183, "height": 0.011320707070706981, "page": 1}, {"left": 0.21988562091503266, "top": 0.5835063131313132, "width": 0.6601078431372549, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.6008030303030303, "width": 0.6601127450980393, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.6180997474747475, "width": 0.565908496732026, "height": 0.011320707070706981, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2474"}, {"text": "(3) We motivate the need to conduct further investigations into the design of human-AI partnerships that result better outcomes than what could be accomplished by human or AI effort alone.", "label": "Conclusion", "bboxes": [{"left": 0.19782843137254902, "top": 0.7218762626262626, "width": 0.6666699346405229, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.7391729797979797, "width": 0.47371078431372543, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2475"}, {"text": "Given the probabilistic nature of deep learning models, we posit that there will always be some amount of noise in their output; correspondingly, deep generative models that produce code as output may exhibit some amount of deviation from a programmers intentions.", "label": "Future Work", "bboxes": [{"left": 0.18000163398692812, "top": 0.20774116161616163, "width": 0.7002810457516339, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.2250366161616162, "width": 0.6999934640522876, "height": 0.011320707070707037, "page": 1}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.19113398692810454, "height": 0.011320707070707065, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2476"}, {"text": "Through a thematic analysis [12, 22], we identified four themes when considering the design of user experiences that leverage generative models in software engineering: acceptance through verification, human-AI patterns of interaction, the utility of imperfect AI, and future opportunities for generative AI in application modernization.", "label": "Future Work", "bboxes": [{"left": 0.5792075163398693, "top": 0.4844810606060606, "width": 0.30079575163398686, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.5017765151515151, "width": 0.7017254901960783, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.5190732323232323, "width": 0.7000032679738561, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.5363699494949495, "width": 0.28203104575163396, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2477"}, {"text": "Given the imperfect output of state-of-the-art NMT models, we posit that such systems will act in concert with human software engineers as a collaborative partner or teammate.", "label": "Future Work", "bboxes": [{"left": 0.1200016339869281, "top": 0.7396010101010101, "width": 0.7, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.7568964646464647, "width": 0.34180555555555553, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2478"}, {"text": "The use of scenarios to elicit feedback on future UX design has a long history in HCI (e.g. [17, 77]), and recently this method has been applied to the design of AI systems as well (e.g. [8, 55, 97]).", "label": "Future Work", "bboxes": [{"left": 0.43606209150326797, "top": 0.7235669191919193, "width": 0.4443202614379084, "height": 0.011320707070706981, "page": 3}, {"left": 0.18000163398692812, "top": 0.7408636363636364, "width": 0.6808464052287582, "height": 0.011320707070707092, "page": 3}], "section": "3 DESIGN SCENARIO", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2479"}, {"text": "We begin by highlighting the difficulties faced by our software engineers in modernizing legacy applications, motivating the need for AI support.", "label": "Future Work", "bboxes": [{"left": 0.196281045751634, "top": 0.6642386363636363, "width": 0.6853316993464053, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.6815340909090909, "width": 0.20596568627450976, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2480"}, {"text": "We then discuss the four major themes we identified in our data: acceptance through verification, human-AI patterns of interaction, the utility of imperfect AI, and future opportunities for generative AI in application modernization.", "label": "Future Work", "bboxes": [{"left": 0.38903921568627453, "top": 0.6815340909090909, "width": 0.4909558823529412, "height": 0.011320707070707092, "page": 5}, {"left": 0.17963562091503268, "top": 0.6988308080808081, "width": 0.7003643790849672, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.7161275252525252, "width": 0.16073529411764706, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2481"}, {"text": "If Im going to put my production systems functionality behind this, I really want some degree of trust that this thing is robust... Every piece of code that we put into production has been reviewed by at least one other person. And so we try hard to basically build that trust through review and familiarity in humans... wed need basically the same level of trust for a system like this. (P4)", "label": "Future Work", "bboxes": [{"left": 0.15988398692810457, "top": 0.28397727272727274, "width": 0.6202369281045752, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.3012727272727273, "width": 0.6202287581699346, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.3185694444444444, "width": 0.620235294117647, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.33586616161616156, "width": 0.3387598039215686, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2482"}, {"text": "P9 offered a more nuanced view on who would need to have an understanding of how the AI model operated.", "label": "Future Work", "bboxes": [{"left": 0.1362794117647059, "top": 0.7782045454545454, "width": 0.6625980392156862, "height": 0.013969696969697076, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2483"}, {"text": "As a user, you... accept whats going on here as a useful and correct process and you probably do not even care how its done because it will speed up your work... and at the end of the day, youll say, okay, Im done. (P1)", "label": "Future Work", "bboxes": [{"left": 0.15988398692810457, "top": 0.7424835858585859, "width": 0.620235294117647, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.759780303030303, "width": 0.6197320261437909, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2484"}, {"text": "P6 further imagined that Im just going to rewrite this and through my action the AI can learn.", "label": "Future Work", "bboxes": [{"left": 0.196281045751634, "top": 0.7947171717171718, "width": 0.5606781045751634, "height": 0.011320707070706981, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2485"}, {"text": "[81], the allocation of tasks in a human-AI collaboration need not be symmetric.", "label": "Future Work", "bboxes": [{"left": 0.1953660130718954, "top": 0.1947689393939394, "width": 0.4767516339869282, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2486"}, {"text": "A lot of us in the software development field, we sometimes have the feeling that for a given problem set, we have to solve the entire problem. But, thats not necessarily true... if you can help someone do part of the task... maybe it will be more effective at smaller, less complex code fragments. Then, [as] you get feedback from users, you build into it more intelligence, then maybe it becomes more effective on larger, more complex pieces of code. (P3)", "label": "Future Work", "bboxes": [{"left": 0.21988562091503266, "top": 0.6361439393939394, "width": 0.6218349673202614, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.653439393939394, "width": 0.6202271241830066, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.6707361111111111, "width": 0.620235294117647, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.6880328282828283, "width": 0.6202320261437908, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.7053282828282829, "width": 0.1111127450980392, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2487"}, {"text": "I think for someone to start using a tool like this, it would need to prove that its adding some value... It probably varies from person to person. Some people wont be satisfied with anything you give them, and others will be satisfied with if it can be 10% effective in doing translations perhaps. And most people fall somewhere in the middle there. (P3)", "label": "Future Work", "bboxes": [{"left": 0.21988562091503266, "top": 0.4174128787878788, "width": 0.6202352941176472, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.4347083333333333, "width": 0.6202352941176472, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.4520050505050505, "width": 0.620232026143791, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.4693017676767677, "width": 0.14249509803921567, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2488"}, {"text": "The place where I can see something like this being very helpful would be where you really have a truly dead codebase thats just a complete black box that nobody can, or wants, to understand... because for that you need understanding if the goal is to keep the darn thing limping while you transition to something else. (P4)", "label": "Future Work", "bboxes": [{"left": 0.15988398692810457, "top": 0.3502790404040404, "width": 0.6202352941176471, "height": 0.011320707070707092, "page": 10}, {"left": 0.15988398692810457, "top": 0.367574494949495, "width": 0.620235294117647, "height": 0.011320707070706981, "page": 10}, {"left": 0.15988398692810457, "top": 0.3848712121212121, "width": 0.6198333333333333, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2489"}, {"text": "P6: As long as it is giving me a head start of any sort I will take it because for me the overhead to write such comments for every function that I code is very high. As you can imagine in a practical system there is going to be hundreds of such functions... its a lot of human hours.", "label": "Future Work", "bboxes": [{"left": 0.15988398692810457, "top": 0.6188522727272727, "width": 0.620233660130719, "height": 0.011320707070707092, "page": 10}, {"left": 0.15988398692810457, "top": 0.6361489898989898, "width": 0.6202254901960784, "height": 0.011320707070707203, "page": 10}, {"left": 0.15988398692810457, "top": 0.6534444444444445, "width": 0.3494117647058824, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2490"}, {"text": "Indeed, the ML community has explored many methods for generating tests [31, 86, 87] as well as generating data for tests [50, 83], and our findings reinforce the need to incorporate these techniques into mainstream application modernization tools.", "label": "Future Work", "bboxes": [{"left": 0.7174019607843137, "top": 0.6137676767676767, "width": 0.16298202614379076, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.631064393939394, "width": 0.6999967320261437, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.6483598484848484, "width": 0.6001633986928104, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2491"}, {"text": "As discussed in Section 5.1.1, testing seems to be one of the primary ways that software engineers will accept and use AI-translated code: Im very much a supporter of test-driven development, which typically means you write tests that validate behavior. (P3).", "label": "Future Work", "bboxes": [{"left": 0.3488790849673203, "top": 0.46170959595959593, "width": 0.5311241830065359, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.47900505050505054, "width": 0.7003807189542482, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.4963017676767677, "width": 0.29514215686274514, "height": 0.011320707070706981, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2492"}, {"text": "Future work is needed to investigate different kinds of interaction patterns, including different mixed-initiative patterns [43, 82], to understand their effect on a software engineers productivity and capability.", "label": "Future Work", "bboxes": [{"left": 0.5501748366013072, "top": 0.2596300505050505, "width": 0.2698169934640522, "height": 0.011320707070707037, "page": 12}, {"left": 0.1200016339869281, "top": 0.27692550505050506, "width": 0.6999901960784314, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.2942222222222222, "width": 0.28084967320261434, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2493"}, {"text": "Our results motivate the need for explainable generative AI.", "label": "Future Work", "bboxes": [{"left": 0.1362794117647059, "top": 0.5709621212121212, "width": 0.35481535947712417, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.2 Toward Explainable Generative AI", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2494"}, {"text": "Techniques that help people understand the mechanics of generative models, as well as techniques that are able to generate specific rationales for why a given artifact was generated, could help people further adopt generative models in their own work practices.", "label": "Future Work", "bboxes": [{"left": 0.49474183006535943, "top": 0.5709621212121212, "width": 0.3252581699346405, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.5882588383838384, "width": 0.6999918300653595, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.605554292929293, "width": 0.5245964052287583, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.2 Toward Explainable Generative AI", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2495"}, {"text": "One such area is in further developing intelligent user interfaces tailored specifically for code translation.", "label": "Future Work", "bboxes": [{"left": 0.1200016339869281, "top": 0.7266287878787879, "width": 0.6222647058823529, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2496"}, {"text": "The process of using these models to perform an initial translation, followed by manual correction, could provide a corpus of correctly-translated code that would enable supervised training, resulting in further quality improvements to the translator.", "label": "Future Work", "bboxes": [{"left": 0.655609477124183, "top": 0.12126010101010101, "width": 0.22438398692810468, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.13855555555555557, "width": 0.7000032679738561, "height": 0.011320707070707065, "page": 13}, {"left": 0.17945915032679738, "top": 0.15585227272727273, "width": 0.5682941176470588, "height": 0.011320707070707092, "page": 13}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2497"}, {"text": "In addition, our finding that acceptance of a models output is established through verification, rather than explanation of the models operation, is not meant to diminish the importance of further research and exploration into how generative models might explain themselves, their operation, and their limitations.", "label": "Future Work", "bboxes": [{"left": 0.6190571895424837, "top": 0.4360517676767677, "width": 0.26094117647058823, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4533472222222222, "width": 0.7000032679738561, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4706439393939394, "width": 0.7016029411764706, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4879406565656566, "width": 0.22450816993464054, "height": 0.011320707070707092, "page": 13}], "section": "7 LIMITATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2498"}, {"text": "Future work should explore how different patterns of interaction and initiative, as well as capabilities beyond code translation, can enable effective human-AI partnerships with generative models that produce better outcomes than what could be accomplished by either party alone.", "label": "Future Work", "bboxes": [{"left": 0.5499705882352941, "top": 0.7681388888888888, "width": 0.33002287581699363, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.7854356060606061, "width": 0.6999934640522876, "height": 0.011320707070706981, "page": 13}, {"left": 0.17945915032679738, "top": 0.8027323232323232, "width": 0.6550016339869281, "height": 0.011320707070707092, "page": 13}], "section": "8 CONCLUSION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2499"}, {"text": "Further, having interactive UX features such as confidence highlighting and alternate translations helped software engineers better understand how the NMT model produced its output, and even find bugs in its translations.", "label": "Future Work", "bboxes": [{"left": 0.7894558823529412, "top": 0.6180997474747475, "width": 0.09054901960784312, "height": 0.011320707070706981, "page": 1}, {"left": 0.21988562091503266, "top": 0.635395202020202, "width": 0.6601094771241831, "height": 0.011320707070706981, "page": 1}, {"left": 0.21988562091503266, "top": 0.6526919191919193, "width": 0.5861127450980392, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2500"}, {"text": "(3) We motivate the need to conduct further investigations into the design of human-AI partnerships that result better outcomes than what could be accomplished by human or AI effort alone.", "label": "Future Work", "bboxes": [{"left": 0.19782843137254902, "top": 0.7218762626262626, "width": 0.6666699346405229, "height": 0.011320707070707092, "page": 1}, {"left": 0.21988562091503266, "top": 0.7391729797979797, "width": 0.47371078431372543, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2501"}, {"text": "Given that the probabilistic nature of generative models implies the potential for some amount of error in their outputs, we seek to understand the extent to which they may be useful in scenarios that have an objective bar of quality.", "label": "Objective", "bboxes": [{"left": 0.845047385620915, "top": 0.6136679292929292, "width": 0.03494771241830075, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.6309646464646465, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.6482613636363637, "width": 0.6471192810457516, "height": 0.011320707070707092, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": 0.7735995054244995, "is_author_statement": true, "is_in_expected_section": false, "id": "2502"}, {"text": "Future work is needed to investigate different kinds of interaction patterns, including different mixed-initiative patterns [43, 82], to understand their effect on a software engineers productivity and capability.", "label": "Objective", "bboxes": [{"left": 0.5501748366013072, "top": 0.2596300505050505, "width": 0.2698169934640522, "height": 0.011320707070707037, "page": 12}, {"left": 0.1200016339869281, "top": 0.27692550505050506, "width": 0.6999901960784314, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.2942222222222222, "width": 0.28084967320261434, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "prob": 0.483548104763031, "is_author_statement": false, "is_in_expected_section": false, "id": "2503"}, {"text": "The use of scenarios to elicit feedback on future UX design has a long history in HCI (e.g. [17, 77]), and recently this method has been applied to the design of AI systems as well (e.g. [8, 55, 97]).", "label": "Method", "bboxes": [{"left": 0.43606209150326797, "top": 0.7235669191919193, "width": 0.4443202614379084, "height": 0.011320707070706981, "page": 3}, {"left": 0.18000163398692812, "top": 0.7408636363636364, "width": 0.6808464052287582, "height": 0.011320707070707092, "page": 3}], "section": "3 DESIGN SCENARIO", "prob": 0.5702952742576599, "is_author_statement": false, "is_in_expected_section": true, "id": "2504"}, {"text": "We followed the process described by Braun and Clarke [12, 22] in which researchers familiarize themselves with their data, generate and group codes to identify higher-level themes, and iteratively refine codes and themes through collaborative discussion.", "label": "Method", "bboxes": [{"left": 0.6783006535947712, "top": 0.473979797979798, "width": 0.2017009803921569, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.4912752525252525, "width": 0.6999918300653595, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.5085719696969697, "width": 0.6115179738562091, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "prob": 0.5495303273200989, "is_author_statement": true, "is_in_expected_section": false, "id": "2505"}, {"text": "Participants described the lifecycle of modernizing applications as consisting of three general phases: (1) creating an understanding of the legacy application and its architecture by reviewing code and documentation; (2) performing the migration work, which may include translating and/or refactoring the code; and (3) reviewing and testing the migrated code.", "label": "Method", "bboxes": [{"left": 0.1200016339869281, "top": 0.1428800505050505, "width": 0.6999967320261439, "height": 0.011320707070707065, "page": 10}, {"left": 0.1200016339869281, "top": 0.16017676767676767, "width": 0.7, "height": 0.011320707070707065, "page": 10}, {"left": 0.1200016339869281, "top": 0.17747222222222223, "width": 0.7000032679738563, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.1947689393939394, "width": 0.03042647058823529, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.536973237991333, "is_author_statement": false, "is_in_expected_section": false, "id": "2506"}, {"text": "P4 described how generative methods could be used to reverse engineer the architecture programmatically and deal with dead codebases.", "label": "Method", "bboxes": [{"left": 0.6523235294117646, "top": 0.3084722222222222, "width": 0.16767156862745103, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.3257689393939394, "width": 0.6683921568627451, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.4569770097732544, "is_author_statement": false, "is_in_expected_section": false, "id": "2507"}, {"text": "We created a series of three progressively-enhancing UX variants within the scenario that illustrated different ways that a software engineer might interact with an NMT model to translate source code from one language to another.", "label": "Method", "bboxes": [{"left": 0.5315718954248366, "top": 0.775455808080808, "width": 0.34891339869281035, "height": 0.011320707070707092, "page": 3}, {"left": 0.17963562091503268, "top": 0.7927525252525253, "width": 0.700357843137255, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.8100492424242424, "width": 0.3260277777777777, "height": 0.011320707070707092, "page": 3}], "section": "3 DESIGN SCENARIO", "prob": 0.4496774673461914, "is_author_statement": true, "is_in_expected_section": true, "id": "2508"}, {"text": "The UX variants were designed to resemble a simple programming environment with two code panes: a source pane on the left for input code, and a target pane on the right for the code output by the NMT model.", "label": "Method", "bboxes": [{"left": 0.1362794117647059, "top": 0.12126010101010101, "width": 0.6853725490196079, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.13855555555555557, "width": 0.6204558823529411, "height": 0.011320707070707065, "page": 4}], "section": "3 DESIGN SCENARIO", "prob": 0.38904812932014465, "is_author_statement": false, "is_in_expected_section": true, "id": "2509"}, {"text": "In this way, the NMT model can work collaboratively with the human: the model proposes multiple solutions, the human chooses one, and if necessary, the model follows-through to make a series of changes that are consistent with one another.", "label": "Method", "bboxes": [{"left": 0.210890522875817, "top": 0.6729823232323232, "width": 0.6091029411764706, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.6902790404040404, "width": 0.6999918300653596, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.707574494949495, "width": 0.17150326797385623, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.3750298023223877, "is_author_statement": false, "is_in_expected_section": false, "id": "2510"}, {"text": "Next, we developed more integrative codes (similar to grounded theory axial codes) using a shared online document and visual collaboration tools.", "label": "Method", "bboxes": [{"left": 0.2336356209150327, "top": 0.5950530303030303, "width": 0.646357843137255, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.6123497474747475, "width": 0.23816993464052286, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "prob": 0.3688286542892456, "is_author_statement": true, "is_in_expected_section": false, "id": "2511"}, {"text": "Generating documentation from source is an active area of research in the machine learning community (e.g. [44, 48, 65, 91]) and our results highlight the importance of this functionality in user experiences, even when generated documentation may be imperfect.", "label": "Result", "bboxes": [{"left": 0.1362794117647059, "top": 0.6726565656565657, "width": 0.685330065359477, "height": 0.011320707070707092, "page": 10}, {"left": 0.11966339869281045, "top": 0.6899532828282828, "width": 0.7003284313725489, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.7072487373737373, "width": 0.2015539215686275, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.964076817035675, "is_author_statement": true, "is_in_expected_section": true, "id": "2512"}, {"text": "Indeed, the ML community has explored many methods for generating tests [31, 86, 87] as well as generating data for tests [50, 83], and our findings reinforce the need to incorporate these techniques into mainstream application modernization tools.", "label": "Result", "bboxes": [{"left": 0.7174019607843137, "top": 0.6137676767676767, "width": 0.16298202614379076, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.631064393939394, "width": 0.6999967320261437, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.6483598484848484, "width": 0.6001633986928104, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.8828402161598206, "is_author_statement": true, "is_in_expected_section": true, "id": "2513"}, {"text": "Our results motivate the need for explainable generative AI.", "label": "Result", "bboxes": [{"left": 0.1362794117647059, "top": 0.5709621212121212, "width": 0.35481535947712417, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.2 Toward Explainable Generative AI", "prob": 0.8513228297233582, "is_author_statement": true, "is_in_expected_section": true, "id": "2514"}, {"text": "As a result, participants felt that having an understanding of the mechanics of the NMT model was less important for accepting its output than just reviewing and testing that output.", "label": "Result", "bboxes": [{"left": 0.3382892156862745, "top": 0.4498888888888889, "width": 0.48170424836601305, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.4671843434343434, "width": 0.6372418300653595, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.2 Toward Explainable Generative AI", "prob": 0.8313383460044861, "is_author_statement": false, "is_in_expected_section": true, "id": "2515"}, {"text": "Participants responded positively to the confidence highlights (Figure 1B) and the alternate translations (Figure 1C) and felt that both views were helpful.", "label": "Result", "bboxes": [{"left": 0.1200016339869281, "top": 0.1428800505050505, "width": 0.7008709150326797, "height": 0.011320707070707065, "page": 8}, {"left": 0.1200016339869281, "top": 0.16017676767676767, "width": 0.22180882352941178, "height": 0.011320707070707065, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.8210458159446716, "is_author_statement": false, "is_in_expected_section": true, "id": "2516"}, {"text": "Despite identifying favorable outcomes of human-AI partnerships, we note that these outcomes are all subjective: the quality of the generated output lies in the perceptions of the people using the system.", "label": "Result", "bboxes": [{"left": 0.196281045751634, "top": 0.5790757575757576, "width": 0.6854493464052289, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.5963724747474748, "width": 0.5234330065359476, "height": 0.011320707070707092, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": 0.772724449634552, "is_author_statement": true, "is_in_expected_section": false, "id": "2517"}, {"text": "All participants who were involved in an application modernization project expressed some desire for help, and our focus on the code translation use case positively resonated with them.", "label": "Result", "bboxes": [{"left": 0.6127941176470588, "top": 0.13855555555555557, "width": 0.20720588235294113, "height": 0.011320707070707065, "page": 6}, {"left": 0.1200016339869281, "top": 0.15585227272727273, "width": 0.7000000000000001, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.17314898989898989, "width": 0.18908496732026148, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS", "prob": 0.7582169771194458, "is_author_statement": true, "is_in_expected_section": true, "id": "2518"}, {"text": "[92] found that artwork that was restored with the aid of a generative model was rated as preferable to artwork restored by a human alone.", "label": "Result", "bboxes": [{"left": 0.3698022875816993, "top": 0.5444835858585858, "width": 0.510202614379085, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.5617790404040404, "width": 0.3098823529411765, "height": 0.011320707070707092, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "prob": 0.7444602847099304, "is_author_statement": false, "is_in_expected_section": false, "id": "2519"}, {"text": "Indeed, having an accurate mental model did help participants understand the models confidence levels, and we hypothesize that generative AI user interfaces that provide salient details of a models operation can help people form accurate mental models.", "label": "Result", "bboxes": [{"left": 0.4081830065359477, "top": 0.4879406565656566, "width": 0.47181045751633993, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.5052361111111111, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.5225328282828283, "width": 0.33629901960784314, "height": 0.011320707070707092, "page": 13}], "section": "7 LIMITATIONS", "prob": 0.7354360818862915, "is_author_statement": true, "is_in_expected_section": false, "id": "2520"}, {"text": "Participants expressed similar desires to establish trust with the code translation tool.", "label": "Result", "bboxes": [{"left": 0.572968954248366, "top": 0.2429570707070707, "width": 0.24702450980392165, "height": 0.011320707070707065, "page": 6}, {"left": 0.1200016339869281, "top": 0.2602537878787879, "width": 0.2681421568627452, "height": 0.011320707070707037, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.717616617679596, "is_author_statement": false, "is_in_expected_section": true, "id": "2521"}, {"text": "However, our system was solely driven by human initiative.", "label": "Result", "bboxes": [{"left": 0.19201633986928104, "top": 0.2596300505050505, "width": 0.3544820261437909, "height": 0.011320707070707037, "page": 12}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": 0.6973761320114136, "is_author_statement": true, "is_in_expected_section": true, "id": "2522"}, {"text": "Thus, the alternate translations served as explanations for the highlights.", "label": "Result", "bboxes": [{"left": 0.37777124183006533, "top": 0.5084444444444445, "width": 0.4367598039215687, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.6946296691894531, "is_author_statement": false, "is_in_expected_section": true, "id": "2523"}, {"text": "Despite the feelings that having an understanding of the NMT models operation wasnt important, we observed that having such understanding does have benefits.", "label": "Result", "bboxes": [{"left": 0.196281045751634, "top": 0.15585227272727273, "width": 0.6837124183006537, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.17314898989898989, "width": 0.28227450980392155, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.694159746170044, "is_author_statement": true, "is_in_expected_section": true, "id": "2524"}, {"text": "For example, P4 found a string mismatch: There is an actual functional difference here on line 21 of the Python code where theres an extra space inserted into that error string.", "label": "Result", "bboxes": [{"left": 0.18000163398692812, "top": 0.17314898989898989, "width": 0.6999950980392157, "height": 0.011320707070707092, "page": 9}, {"left": 0.17945915032679738, "top": 0.19044444444444444, "width": 0.35301470588235295, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.6866694688796997, "is_author_statement": false, "is_in_expected_section": true, "id": "2525"}, {"text": "Three participants presented as women; the other eight presented as men.", "label": "Result", "bboxes": [{"left": 0.44777777777777783, "top": 0.2293611111111111, "width": 0.43447222222222226, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "prob": 0.6858747601509094, "is_author_statement": false, "is_in_expected_section": true, "id": "2526"}, {"text": "The presence of the alternate translations seemed to provide an implicit explanation that helped participants better understand the model.", "label": "Result", "bboxes": [{"left": 0.7615032679738563, "top": 0.5476982323232323, "width": 0.11849999999999994, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.5649936868686869, "width": 0.7022581699346404, "height": 0.011320707070706981, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.6627179980278015, "is_author_statement": false, "is_in_expected_section": true, "id": "2527"}, {"text": "[28] found a similar result, that giving a good reason for a poor recommendation made people more likely to accept that poor recommendation.", "label": "Result", "bboxes": [{"left": 0.46092483660130723, "top": 0.5752866161616161, "width": 0.35907352941176474, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.5925820707070707, "width": 0.5094346405228758, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Imperfect AI", "prob": 0.6252177357673645, "is_author_statement": false, "is_in_expected_section": false, "id": "2528"}, {"text": "Although the import of generative capabilities may seem to be about the production of an artifact itself, we found that they instead may also be a means rather than just an end .", "label": "Result", "bboxes": [{"left": 0.734733660130719, "top": 0.20774116161616163, "width": 0.14526960784313725, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.2250366161616162, "width": 0.6999934640522876, "height": 0.011320707070707037, "page": 13}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.19047058823529414, "height": 0.013969696969696993, "page": 13}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": 0.6119279265403748, "is_author_statement": true, "is_in_expected_section": true, "id": "2529"}, {"text": "For Figure 1C, we asked participants if they understood why the AI produced the alternatives it produced, and what other information could help them better understand how those alternatives were generated.", "label": "Result", "bboxes": [{"left": 0.5149869281045752, "top": 0.3689671717171717, "width": 0.36539542483660126, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.3862626262626263, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.40355934343434346, "width": 0.16886764705882354, "height": 0.011320707070707037, "page": 5}], "section": "4 METHOD @@ 4.2 Variant-Specific Questions", "prob": 0.5925738215446472, "is_author_statement": true, "is_in_expected_section": false, "id": "2530"}, {"text": "We observed that discrepancies between our participants mental models and the actual operation of the NMT model was a source of confusion.", "label": "Result", "bboxes": [{"left": 0.3874705882352941, "top": 0.27692550505050506, "width": 0.49253594771241826, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.2942222222222222, "width": 0.3674183006535948, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5561975836753845, "is_author_statement": true, "is_in_expected_section": true, "id": "2531"}, {"text": "P3 discussed how the NMT model could fit into a workflow for translating a large codebase.", "label": "Result", "bboxes": [{"left": 0.39976633986928106, "top": 0.5933143939393939, "width": 0.48023366013071883, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.6106098484848484, "width": 0.056977124183006544, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.5556954145431519, "is_author_statement": false, "is_in_expected_section": true, "id": "2532"}, {"text": "These ideas were generated through our pilot testing.", "label": "Result", "bboxes": [{"left": 0.7511192810457517, "top": 0.6497765151515152, "width": 0.06887745098039222, "height": 0.011320707070707092, "page": 4}, {"left": 0.11945915032679738, "top": 0.6670719696969697, "width": 0.24704411764705886, "height": 0.011320707070707092, "page": 4}], "section": "4 METHOD", "prob": 0.547524631023407, "is_author_statement": true, "is_in_expected_section": false, "id": "2533"}, {"text": "However, the", "label": "Result", "bboxes": [{"left": 0.7418039215686274, "top": 0.8127979797979797, "width": 0.07818790849673207, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5340478420257568, "is_author_statement": false, "is_in_expected_section": true, "id": "2534"}, {"text": "Interface features such as confidence highlighting helped steer human attention toward potentially problematic areas, and alternate translations provided explanatory power for why a segment of translated code was rated as low confidence by the model; in addition, alternate translations provided clues toward the semantic meaning of the code and helped participants find logical errors in it.", "label": "Result", "bboxes": [{"left": 0.28210130718954246, "top": 0.7162499999999999, "width": 0.5982777777777779, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.7335467171717173, "width": 0.6999967320261437, "height": 0.011320707070706981, "page": 13}, {"left": 0.18000163398692812, "top": 0.7508434343434344, "width": 0.6999967320261437, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.7681388888888888, "width": 0.36549999999999994, "height": 0.011320707070707092, "page": 13}], "section": "8 CONCLUSION", "prob": 0.5334740281105042, "is_author_statement": false, "is_in_expected_section": true, "id": "2535"}, {"text": "Each variant appeared as a partially-functional prototype, through which participants could explore features of interest.", "label": "Result", "bboxes": [{"left": 0.7441045751633987, "top": 0.13855555555555557, "width": 0.07589379084967318, "height": 0.011320707070707065, "page": 4}, {"left": 0.1200016339869281, "top": 0.15585227272727273, "width": 0.6380718954248366, "height": 0.011320707070707092, "page": 4}], "section": "3 DESIGN SCENARIO", "prob": 0.5267216563224792, "is_author_statement": false, "is_in_expected_section": false, "id": "2536"}, {"text": "If Im going to put my production systems functionality behind this, I really want some degree of trust that this thing is robust... Every piece of code that we put into production has been reviewed by at least one other person. And so we try hard to basically build that trust through review and familiarity in humans... wed need basically the same level of trust for a system like this. (P4)", "label": "Result", "bboxes": [{"left": 0.15988398692810457, "top": 0.28397727272727274, "width": 0.6202369281045752, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.3012727272727273, "width": 0.6202287581699346, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.3185694444444444, "width": 0.620235294117647, "height": 0.011320707070707092, "page": 6}, {"left": 0.15988398692810457, "top": 0.33586616161616156, "width": 0.3387598039215686, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.5041751265525818, "is_author_statement": true, "is_in_expected_section": true, "id": "2537"}, {"text": "P1 expressed the same sentiment, citing efficiency gains as a reason for accepting the models output without understanding how it was produced.", "label": "Result", "bboxes": [{"left": 0.620390522875817, "top": 0.7014646464646463, "width": 0.2012205882352942, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.718760101010101, "width": 0.6715718954248366, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.4892727732658386, "is_author_statement": false, "is_in_expected_section": true, "id": "2538"}, {"text": "Multiple participants pointed out how this interaction pattern would enable a feedback loop to improve the NMT model (P4, P6, P7).", "label": "Result", "bboxes": [{"left": 0.29516013071895425, "top": 0.707574494949495, "width": 0.5252222222222223, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.7248712121212121, "width": 0.26698529411764704, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.484051913022995, "is_author_statement": false, "is_in_expected_section": true, "id": "2539"}, {"text": "P2 described how the translations, even with small mistakes, were preferable to manually rewriting code.", "label": "Result", "bboxes": [{"left": 0.7465686274509804, "top": 0.24233333333333332, "width": 0.1334346405228758, "height": 0.011320707070707065, "page": 9}, {"left": 0.18000163398692812, "top": 0.2596300505050505, "width": 0.4977091503267974, "height": 0.011320707070707037, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.47466665506362915, "is_author_statement": false, "is_in_expected_section": true, "id": "2540"}, {"text": "resistance to a tool that was not guaranteed to produce correct results.", "label": "Result", "bboxes": [{"left": 0.18000163398692812, "top": 0.12126010101010101, "width": 0.41493464052287576, "height": 0.013969696969696965, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.4725630283355713, "is_author_statement": false, "is_in_expected_section": true, "id": "2541"}, {"text": "Each interview was attended by at least three of the authors.", "label": "Result", "bboxes": [{"left": 0.1362794117647059, "top": 0.6843686868686868, "width": 0.367361111111111, "height": 0.011320707070707092, "page": 4}], "section": "4 METHOD", "prob": 0.46502354741096497, "is_author_statement": false, "is_in_expected_section": false, "id": "2542"}, {"text": "P8 similarly suggested, An explanation of why the [model had] low confidence... would be helpful.", "label": "Result", "bboxes": [{"left": 0.29427941176470584, "top": 0.4958093434343434, "width": 0.587374183006536, "height": 0.011320707070707037, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.4613124132156372, "is_author_statement": false, "is_in_expected_section": true, "id": "2543"}, {"text": "A lot of us in the software development field, we sometimes have the feeling that for a given problem set, we have to solve the entire problem. But, thats not necessarily true... if you can help someone do part of the task... maybe it will be more effective at smaller, less complex code fragments. Then, [as] you get feedback from users, you build into it more intelligence, then maybe it becomes more effective on larger, more complex pieces of code. (P3)", "label": "Result", "bboxes": [{"left": 0.21988562091503266, "top": 0.6361439393939394, "width": 0.6218349673202614, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.653439393939394, "width": 0.6202271241830066, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.6707361111111111, "width": 0.620235294117647, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.6880328282828283, "width": 0.6202320261437908, "height": 0.011320707070707092, "page": 9}, {"left": 0.21988562091503266, "top": 0.7053282828282829, "width": 0.1111127450980392, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.4559834599494934, "is_author_statement": true, "is_in_expected_section": true, "id": "2544"}, {"text": "Finally, P7 discussed how an AI partner might monitor his work and make proactive suggestions: It would be cool if the AI knew what I was trying to build, like a controller or view, and it would finish it for me. Like give me a shell of what Im trying to do so I can fill in the details.", "label": "Result", "bboxes": [{"left": 0.196281045751634, "top": 0.398, "width": 0.6837124183006538, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.4152954545454545, "width": 0.7000000000000001, "height": 0.011320707070707092, "page": 11}, {"left": 0.17945915032679738, "top": 0.43259217171717174, "width": 0.2852254901960784, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.45342475175857544, "is_author_statement": false, "is_in_expected_section": true, "id": "2545"}, {"text": "Many participants also pointed out how edits to low-confidence translated code could serve as feedback to the NMT model to improve its training.", "label": "Result", "bboxes": [{"left": 0.4296519607843137, "top": 0.41214267676767674, "width": 0.3903529411764706, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.4294393939393939, "width": 0.4909395424836601, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "prob": 0.4486599564552307, "is_author_statement": false, "is_in_expected_section": true, "id": "2546"}, {"text": "Our interviews generated a wealth of material: 11 hours of recorded videos, approximately 63 pages of notes, and a corpus of approximately 400 pages of interview transcripts containing about 89k words.", "label": "Result", "bboxes": [{"left": 0.196281045751634, "top": 0.5258686868686868, "width": 0.6837222222222223, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.5431641414141414, "width": 0.5334330065359476, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "prob": 0.44779255986213684, "is_author_statement": true, "is_in_expected_section": true, "id": "2547"}, {"text": "P3 described how there may be individual differences in the value people derive from NMT translations.", "label": "Result", "bboxes": [{"left": 0.49092810457516334, "top": 0.3745833333333334, "width": 0.38907843137254905, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.3918787878787879, "width": 0.2254787581699346, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "prob": 0.4388822913169861, "is_author_statement": false, "is_in_expected_section": true, "id": "2548"}, {"text": "We also discovered how this information possessed explanatory power, helping participants both understand the codes underlying intent and identify potential errors in the translation.", "label": "Result", "bboxes": [{"left": 0.15865359477124183, "top": 0.13855555555555557, "width": 0.6613415032679738, "height": 0.011320707070707065, "page": 12}, {"left": 0.1200016339869281, "top": 0.15585227272727273, "width": 0.4283954248366013, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "prob": 0.41934138536453247, "is_author_statement": true, "is_in_expected_section": true, "id": "2549"}, {"text": "Through a thematic analysis [12, 22], we identified four themes when considering the design of user experiences that leverage generative models in software engineering: acceptance through verification, human-AI patterns of interaction, the utility of imperfect AI, and future opportunities for generative AI in application modernization.", "label": "Result", "bboxes": [{"left": 0.5792075163398693, "top": 0.4844810606060606, "width": 0.30079575163398686, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.5017765151515151, "width": 0.7017254901960783, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.5190732323232323, "width": 0.7000032679738561, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.5363699494949495, "width": 0.28203104575163396, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.4119412302970886, "is_author_statement": true, "is_in_expected_section": true, "id": "2550"}, {"text": "P9 emphasized how the alternate translations provided an educational opportunity while doing code modernization work because having new options makes it much more easy for me to explore newer methods in the newer language.", "label": "Result", "bboxes": [{"left": 0.7354101307189543, "top": 0.3461111111111111, "width": 0.1445833333333334, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.36340656565656565, "width": 0.7005441176470587, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.3807032828282828, "width": 0.5465408496732026, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.3951842784881592, "is_author_statement": false, "is_in_expected_section": true, "id": "2551"}, {"text": "The process of using these models to perform an initial translation, followed by manual correction, could provide a corpus of correctly-translated code that would enable supervised training, resulting in further quality improvements to the translator.", "label": "Result", "bboxes": [{"left": 0.655609477124183, "top": 0.12126010101010101, "width": 0.22438398692810468, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.13855555555555557, "width": 0.7000032679738561, "height": 0.011320707070707065, "page": 13}, {"left": 0.17945915032679738, "top": 0.15585227272727273, "width": 0.5682941176470588, "height": 0.011320707070707092, "page": 13}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "prob": 0.3821297287940979, "is_author_statement": false, "is_in_expected_section": true, "id": "2552"}, {"text": "Participants identified the importance of being able to give feedback (P4), such as by unflag[ing] (P7) low-confidence tokens identified by the NMT model.", "label": "Result", "bboxes": [{"left": 0.35497712418300653, "top": 0.679611111111111, "width": 0.5254019607843138, "height": 0.011320707070707092, "page": 7}, {"left": 0.17834640522875816, "top": 0.6969078282828283, "width": 0.4203398692810458, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.3814699053764343, "is_author_statement": false, "is_in_expected_section": true, "id": "2553"}, {"text": "In recent years, several efforts have focused on the use of AI and machine learning techniques for various tasks related to software engineering, including code completion [15, 41, 75, 84], code classification [49, 68], API recommendation [16, 33], variable and method naming [3, 5], type inference [39, 93], bug detection and repair [25, 40, 71, 74, 89, 95], comment description and generation [4, 44, 48, 65, 80, 91], code change summarization [66], and code clone detection [96].", "label": "Result", "bboxes": [{"left": 0.1200016339869281, "top": 0.23800883838383838, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.25530555555555556, "width": 0.7016078431372549, "height": 0.011320707070707092, "page": 2}, {"left": 0.11963562091503267, "top": 0.2726022727272727, "width": 0.7003627450980393, "height": 0.011320707070707037, "page": 2}, {"left": 0.1200016339869281, "top": 0.2898977272727273, "width": 0.6862254901960784, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.1 AI Techniques for Software Engineering", "prob": 0.3728429675102234, "is_author_statement": false, "is_in_expected_section": false, "id": "2554"}, {"text": "5.4.2 Performing Migration Work.", "label": "Result", "bboxes": [{"left": 0.1200016339869281, "top": 0.7371553030303031, "width": 0.202531045751634, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.33727848529815674, "is_author_statement": false, "is_in_expected_section": true, "id": "2555"}, {"text": "Thus, in our discussions with participants around the topic of trust, our emphasis was on understanding their attitudes toward acceptance of AI-generated code through their expressions of desire to incorporate (or not incorporate) such code in their work  i.e. their willing[ness] to act, rather than their past behavior of having acted, as no participants had previously encountered a code-generating AI system.", "label": "Result", "bboxes": [{"left": 0.5, "top": 0.40617929292929295, "width": 0.32000163398692816, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.42347601010101005, "width": 0.700281045751634, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.44077146464646466, "width": 0.700281045751634, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.4580681818181818, "width": 0.7008594771241831, "height": 0.011320707070707037, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "prob": 0.3336925804615021, "is_author_statement": true, "is_in_expected_section": true, "id": "2556"}, {"text": "P6 described how generative methods could help him form a clearer picture of system architecture: If AI could actually somehow... help me keep track of how the current function Im looking at fits into the bigger picture... I think thats really beneficial.", "label": "Result", "bboxes": [{"left": 0.564735294117647, "top": 0.27388005050505054, "width": 0.25525653594771247, "height": 0.011320707070707037, "page": 10}, {"left": 0.1200016339869281, "top": 0.29117676767676764, "width": 0.6999901960784313, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.3084722222222222, "width": 0.5288464052287583, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.3161233365535736, "is_author_statement": false, "is_in_expected_section": true, "id": "2557"}, {"text": "P6 described how his team has strict requirements for writing documentation for every function because it is used to generate Swagger API documentation 3 .", "label": "Result", "bboxes": [{"left": 0.3455964052287582, "top": 0.49056439393939394, "width": 0.4746748366013071, "height": 0.011320707070707037, "page": 10}, {"left": 0.1200016339869281, "top": 0.5058888888888888, "width": 0.44208986928104577, "height": 0.013292929292929356, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "prob": 0.3137372136116028, "is_author_statement": false, "is_in_expected_section": true, "id": "2558"}], "uist-0": [{"text": "Our fnal design uses object-based saliency and edge detection to highlight contrast along subject and image borders, outlining potential distractors in these regions.", "label": "Author", "bboxes": [{"left": 0.7099232026143791, "top": 0.5848876262626262, "width": 0.20215359477124184, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5987247474747475, "width": 0.39254411764705877, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.6125618686868687, "width": 0.3697091503267974, "height": 0.011320707070707092, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2559"}, {"text": "We describe the implementation and evaluation of a capture-time tool that interactively displays these overlays and fnd that the tool is helpful for making users more confdent in their ability to take decluttered photos that clearly convey their intended story.", "label": "Author", "bboxes": [{"left": 0.8929346405228759, "top": 0.6125618686868687, "width": 0.019140522875817134, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.6263989898989899, "width": 0.3925359477124183, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.6402361111111111, "width": 0.3925408496732027, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.6540732323232322, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.667909090909091, "width": 0.35453758169934635, "height": 0.011320707070707092, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2560"}, {"text": "We are interested in designing camera interfaces that can encourage users to incorporate these exploratory stages of the design process into their photographic process.", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.1649078282828283, "width": 0.3787549019607842, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.19257954545454545, "width": 0.2330882352941177, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2561"}, {"text": "In particular, we wondered how we might be able to bring some of these benefts of sketching to photography to promote this behavior of intentional exploration, and if that might help users notice unexpected mistakes, such as unwanted clutter in their photos.", "label": "Author", "bboxes": [{"left": 0.755766339869281, "top": 0.19257954545454545, "width": 0.15633006535947713, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.20641919191919192, "width": 0.39256209150326793, "height": 0.011320707070707037, "page": 1}, {"left": 0.5195343137254902, "top": 0.22025631313131314, "width": 0.39416993464052286, "height": 0.011320707070707037, "page": 1}, {"left": 0.5195343137254902, "top": 0.23409343434343433, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.24792803030303032, "width": 0.20314869281045744, "height": 0.011320707070707065, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2562"}, {"text": "We aimed to design feedback through such annotations directly in the camera.", "label": "Author", "bboxes": [{"left": 0.8381944444444445, "top": 0.26176767676767676, "width": 0.07390196078431366, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.275604797979798, "width": 0.3948137254901961, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2563"}, {"text": "We describe our process towards designing an abstracted annotation of a photo and study how that infuences how users address capturing decluttered photos.", "label": "Author", "bboxes": [{"left": 0.5188316993464052, "top": 0.2894419191919192, "width": 0.3957352941176472, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.30327777777777776, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.317114898989899, "width": 0.17600163398692814, "height": 0.011320707070707037, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2564"}, {"text": "Our fnal annotation design involves color-coded edge highlighting (very similar to the focus peaking feature that can be found on a number of commercial cameras [2, 32]) focused on regions around the subject(s) and image borders where clutter can be most distracting.", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.3309520202020202, "width": 0.3787647058823528, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.3447891414141414, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.3586237373737373, "width": 0.39255555555555544, "height": 0.011323232323232402, "page": 1}, {"left": 0.5195343137254902, "top": 0.37246338383838384, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.38630050505050506, "width": 0.06763725490196082, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2565"}, {"text": "In our in-camera app implementation, users also have the option to toggle on all edges (see Figure 7).", "label": "Author", "bboxes": [{"left": 0.6521601307189543, "top": 0.4139747474747475, "width": 0.25993300653594775, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.42781186868686866, "width": 0.3351830065359478, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2566"}, {"text": "We show edges within the subject(s) also in yellow and background edges not within the subject(s) and image borders in white (see Figure 1).", "label": "Author", "bboxes": [{"left": 0.8583725490196078, "top": 0.42781186868686866, "width": 0.054259803921568595, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.4416489898989899, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.45548484848484855, "width": 0.3948218954248366, "height": 0.011320707070707037, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2567"}, {"text": "We see similarities in this photographic process and the design processphotographers are essentially iterating on their design (or image) in the camera as they consider these diferent aspects of photography and storytelling.", "label": "Author", "bboxes": [{"left": 0.32279411764705884, "top": 0.500455808080808, "width": 0.15767320261437906, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5142929292929292, "width": 0.39256535947712423, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5281300505050505, "width": 0.3929362745098039, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5419671717171717, "width": 0.3856290849673203, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2568"}, {"text": "In order to realize the proposed interaction, we additionally present a proposal of an algorithm for visually annotating potential clutter by highlighting relevant edges around salient objects and around the image border.", "label": "Author", "bboxes": [{"left": 0.5195343137254902, "top": 0.6303383838383838, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6441527777777778, "width": 0.39283660130718945, "height": 0.011343434343434322, "page": 1}, {"left": 0.5195343137254902, "top": 0.6580126262626262, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6718497474747475, "width": 0.10806045751633997, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2569"}, {"text": "In this work, our main focus is on the design process and the prototype appour goal is not to propose an exact abstraction interface, but to understand how this style of in-camera interaction might infuence users photographic process, specifcally with regards to decluttering.", "label": "Author", "bboxes": [{"left": 0.6321372549019608, "top": 0.6718497474747475, "width": 0.2799591503267973, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.685685606060606, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6995227272727274, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7133573232323233, "width": 0.39293627450980395, "height": 0.011320707070707092, "page": 1}, {"left": 0.5189918300653594, "top": 0.7271969696969698, "width": 0.1704934640522876, "height": 0.011320707070706981, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2570"}, {"text": "We contextualize our work of designing an abstraction-based camera overlay within the most relevant work in image manipulation and camera guidance.", "label": "Author", "bboxes": [{"left": 0.5188316993464052, "top": 0.7739381313131313, "width": 0.3957385620915034, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.78777398989899, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.801611111111111, "width": 0.12963235294117648, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2571"}, {"text": "Specifcally in this paper, we contribute:", "label": "Author", "bboxes": [{"left": 0.5195343137254902, "top": 0.48190151515151514, "width": 0.23960294117647052, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2572"}, {"text": "Most similar to our work is the focus peaking feature in commercial cameras and the work of E et al.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.6764747474747476, "width": 0.3787549019607843, "height": 0.011320707070707092, "page": 2}, {"left": 0.08791013071895425, "top": 0.6903093434343435, "width": 0.25865032679738564, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2573"}, {"text": "Focus peaking is relevant to our work due to its methods of visualizationit similarly highlights subsets of edges in the images, but instead for the purpose of highlighting regions that are in focus [2, 32].", "label": "Author", "bboxes": [{"left": 0.16690359477124184, "top": 0.70414898989899, "width": 0.31603267973856214, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.7179861111111111, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.7318232323232324, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.7456578282828283, "width": 0.09034313725490194, "height": 0.01132323232323229, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2574"}, {"text": "We are inspired by the iterative loop of testing and evaluating that these forms of feedback encourage.", "label": "Author", "bboxes": [{"left": 0.23955718954248367, "top": 0.8286818181818182, "width": 0.24091013071895423, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.8425189393939394, "width": 0.3592222222222222, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2575"}, {"text": "However, rather than focusing on accuracy and refnement of camera settings, we are interested in understanding how similar overlays", "label": "Author", "bboxes": [{"left": 0.4497549019607843, "top": 0.8425189393939394, "width": 0.03318137254901965, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.8563560606060606, "width": 0.3925555555555555, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.8701931818181818, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2576"}, {"text": "We havent seen work that focuses on helping users declutter their compositions in the camera.", "label": "Author", "bboxes": [{"left": 0.38518790849673207, "top": 0.6349633838383839, "width": 0.09527941176470583, "height": 0.011320707070707092, "page": 2}, {"left": 0.08736437908496732, "top": 0.6488005050505051, "width": 0.39309640522875816, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6626376262626262, "width": 0.06918137254901961, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2577"}, {"text": "Both are useful for our goal of designing visual overlays that are friendly for users to process interactively.", "label": "Author", "bboxes": [{"left": 0.22155228758169934, "top": 0.22025631313131314, "width": 0.2589183006535948, "height": 0.011320707070707037, "page": 2}, {"left": 0.08753921568627451, "top": 0.23409343434343433, "width": 0.3951862745098039, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2578"}, {"text": "In this section, we describe the steps we took to design our abstraction interface for decluttering images.", "label": "Author", "bboxes": [{"left": 0.5195343137254902, "top": 0.2787487373737374, "width": 0.39502777777777776, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.2925858585858586, "width": 0.25750816993464054, "height": 0.011320707070707037, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2579"}, {"text": "We hypothesized that abstracting an image could help evenly spread the photographers attention across the image, efectively drawing the photographers attention away from the details of the main subject to other areas of the image.", "label": "Author", "bboxes": [{"left": 0.7807009803921569, "top": 0.2925858585858586, "width": 0.13139869281045757, "height": 0.011320707070707037, "page": 2}, {"left": 0.5195343137254902, "top": 0.30642297979797983, "width": 0.39256209150326793, "height": 0.011320707070707037, "page": 2}, {"left": 0.5195343137254902, "top": 0.320260101010101, "width": 0.39255882352941185, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.33409722222222227, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.3479343434343435, "width": 0.0775032679738562, "height": 0.011320707070706981, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2580"}, {"text": "To test out the concept of abstraction guidance, we started with a low-fdelity Wizard-of-Oz (WoZ) prototype [14] where experimenters manually drew abstraction overlays.", "label": "Author", "bboxes": [{"left": 0.51909477124183, "top": 0.47561237373737375, "width": 0.39299673202614394, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195277777777778, "top": 0.4894469696969697, "width": 0.395047385620915, "height": 0.011323232323232346, "page": 2}, {"left": 0.5195310457516339, "top": 0.5032840909090909, "width": 0.27888398692810457, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2581"}, {"text": "We look to extend this idea by designing visual overlays that help the user identify clutter.", "label": "Author", "bboxes": [{"left": 0.8461960784313725, "top": 0.1510719696969697, "width": 0.06589379084967328, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.39283169934640527, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.08914869281045756, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2582"}, {"text": "We fnd decluttering particularly of interest because it starts to draw on the storytelling aspects of photography.", "label": "Author", "bboxes": [{"left": 0.6115114379084967, "top": 0.17874494949494948, "width": 0.3005849673202614, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.3500392156862745, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2583"}, {"text": "We informally tested our low-fdelity prototype with 19 participants (9 male, 10 female), 18 to 41 years old (  = 24).", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.5586338383838384, "width": 0.3787581699346404, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.5698333333333333, "width": 0.3122205882352942, "height": 0.013969696969697076, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2584"}, {"text": "We ran the study using the iPhones default Camera and Photos apps.", "label": "Author", "bboxes": [{"left": 0.6059803921568627, "top": 0.613979797979798, "width": 0.308593137254902, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.6278194444444445, "width": 0.12365359477124171, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2585"}, {"text": "We then handed them the phone to frame and take a photo of the scene.", "label": "Author", "bboxes": [{"left": 0.7563415032679738, "top": 0.6693295454545455, "width": 0.15574509803921577, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.6831666666666667, "width": 0.2629950980392156, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2586"}, {"text": "We would then apply a new transparency and draw the corresponding abstraction overlay for the new photo for the participant to review.", "label": "Author", "bboxes": [{"left": 0.7447679738562092, "top": 0.7661893939393939, "width": 0.16786601307189553, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.7800265151515151, "width": 0.39283496732026146, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.7938623737373738, "width": 0.2584950980392158, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2587"}, {"text": "While we didnt ask if they wanted to take another photo after this review step, occasionally (6) participants would ask if they could take a third photo and we would repeat the process above, presenting them with a third overlay.", "label": "Author", "bboxes": [{"left": 0.781687908496732, "top": 0.7938623737373738, "width": 0.13040522875816996, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8076994949494949, "width": 0.39504084967320263, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8215366161616162, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 2}, {"left": 0.5189918300653594, "top": 0.8353737373737373, "width": 0.3931062091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8492108585858587, "width": 0.04691176470588243, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2588"}, {"text": "From our WoZ prototype studies, we saw promising signs that the participants in fact noticed high level", "label": "Author", "bboxes": [{"left": 0.6788071895424836, "top": 0.8705959595959595, "width": 0.23328921568627448, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2589"}, {"text": "Given the observations from our low-fdelity prototype, we were motivated to continue with this concept and move onto the step of answering the second question: What is an abstraction of a photo?", "label": "Author", "bboxes": [{"left": 0.5195343137254902, "top": 0.4595542929292929, "width": 0.3925408496732027, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.4733914141414141, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.4872260101010101, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5010656565656566, "width": 0.04084313725490207, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2590"}, {"text": "What should such a visualization look like, and how might we implement them?", "label": "Author", "bboxes": [{"left": 0.564031045751634, "top": 0.5010656565656566, "width": 0.348047385620915, "height": 0.011320707070707092, "page": 3}, {"left": 0.5189918300653594, "top": 0.5149027777777778, "width": 0.13085784313725501, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2591"}, {"text": "In particular, we saw that the abstraction overlay seemed most helpful for the purposes of noticing unwanted clutter in an image, so we decided that we would target designing an abstraction overlay that provides decluttering guidance.", "label": "Author", "bboxes": [{"left": 0.654781045751634, "top": 0.5149027777777778, "width": 0.2572941176470589, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.528739898989899, "width": 0.39254411764705877, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5425770202020203, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5564141414141415, "width": 0.3522385620915033, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2592"}, {"text": "Note that while these participants behaviors matched what we hoped for, there were limitations to our study design such that they cannot be directly mapped to how a user might respond to seeing this style of overlay interactively in the camera.", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.31581944444444443, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.3296565656565657, "width": 0.3929183006535949, "height": 0.011320707070707037, "page": 3}, {"left": 0.5195343137254902, "top": 0.3434936868686869, "width": 0.3925392156862745, "height": 0.011320707070707037, "page": 3}, {"left": 0.5195343137254902, "top": 0.357330808080808, "width": 0.2899150326797386, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2593"}, {"text": "To test the interactive experience, we needed to fnd a way to automatically generate these overlays.", "label": "Author", "bboxes": [{"left": 0.5191013071895425, "top": 0.3988396464646464, "width": 0.39297875816993466, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.4126780303030303, "width": 0.2290866013071896, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2594"}, {"text": "Our eyes are drawn to regions of high contrast.", "label": "Author", "bboxes": [{"left": 0.6131372549019608, "top": 0.6492032828282828, "width": 0.3011977124183006, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2595"}, {"text": "In this paper, we will refer to this as subject-background separation (SBS) In art, this is more commonly referred to as the fgure-ground relationship, a Gestalt Psychology principle [20].", "label": "Author", "bboxes": [{"left": 0.8358839869281045, "top": 0.718388888888889, "width": 0.07781209150326807, "height": 0.011320707070706981, "page": 3}, {"left": 0.5189918300653594, "top": 0.7322032828282828, "width": 0.39398039215686287, "height": 0.011343434343434322, "page": 3}, {"left": 0.5195294117647059, "top": 0.7460606060606061, "width": 0.3925457516339871, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.75989898989899, "width": 0.29585784313725483, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2596"}, {"text": "We use subjectbackground separation in our work to make it easier to relate the concept to more familiar terms.", "label": "Author", "bboxes": [{"left": 0.8190490196078432, "top": 0.75989898989899, "width": 0.09551470588235278, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.7737361111111111, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.7875732323232324, "width": 0.18805228758169934, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2597"}, {"text": "Again, this is better known as edge ficker, but we chose to specify image border to diferentiate it from the term edge, as edge is", "label": "Author", "bboxes": [{"left": 0.5190212418300654, "top": 0.8705959595959595, "width": 0.3934395424836602, "height": 0.011320707070707092, "page": 3}, {"left": 0.5178790849673203, "top": 0.8844318181818183, "width": 0.3941993464052288, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2598"}, {"text": "To answer our questions on how to design an abstraction overlay for decluttering, we looked to existing literature to better understand how photographers think about directing the viewers attention for efective storytelling [9, 20, 30].", "label": "Author", "bboxes": [{"left": 0.7007532679738562, "top": 0.5800189393939394, "width": 0.21133823529411766, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.593854797979798, "width": 0.3925441176470589, "height": 0.011320707070706981, "page": 3}, {"left": 0.5195343137254902, "top": 0.6076919191919191, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.6215290404040403, "width": 0.3946633986928104, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2599"}, {"text": "Given these principles, we wondered what annotation methods photographers currently used for highlighting clutter.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.41673611111111114, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.43057323232323236, "width": 0.3299183006535948, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2600"}, {"text": "We additionally were inspired by a line of research in nonphotorealistic rendering to generate stylized image abstractions [15, 22, 36].", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.8152474747474748, "width": 0.37873856209150325, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.394171568627451, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.04186274509803921, "height": 0.011320707070706981, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2601"}, {"text": "Specifcally, since we knew our goal would be to display the overlay interactively in the camera, we focused on Winnemller et al.", "label": "Author", "bboxes": [{"left": 0.13295588235294117, "top": 0.8429217171717173, "width": 0.34749019607843135, "height": 0.011320707070706981, "page": 4}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.39255065359477126, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.02210130718954248, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2602"}, {"text": "Inspired by this idea of using outlines to highlight contrast and the lack of contrast, we hoped to recreate this outlining as an overlay directly in the camera (see Figure 5), while also extending it to contrast along the image borders.", "label": "Author", "bboxes": [{"left": 0.33908660130718954, "top": 0.6492032828282828, "width": 0.1413643790849673, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6630378787878788, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 4}, {"left": 0.0874656862745098, "top": 0.6907146464646465, "width": 0.39298529411764704, "height": 0.011320707070706981, "page": 4}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.0474656862745098, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2603"}, {"text": "We looked at diferent approaches to executing this concept as one potential direction to pursue for our abstraction overlay.", "label": "Author", "bboxes": [{"left": 0.13794117647058823, "top": 0.7045517676767676, "width": 0.34250816993464056, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.718388888888889, "width": 0.3948022875816993, "height": 0.011320707070706981, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2604"}, {"text": "Combining concepts from these two abstraction ideas, we have three components in total to consider: line drawing, location context, and color fattening.", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.6698838383838384, "width": 0.37625980392156844, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6837209595959596, "width": 0.3950245098039217, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6975580808080808, "width": 0.14877450980392148, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2605"}, {"text": "We additionally give ourselves line color (for the purposes of color-coding lines based on their location context), and image darkening (to provide more contrast against the line drawings) as parameters to consider in designing abstraction overlays.", "label": "Author", "bboxes": [{"left": 0.6719754901960785, "top": 0.6975580808080808, "width": 0.2403758169934641, "height": 0.011320707070707092, "page": 4}, {"left": 0.5190996732026144, "top": 0.7113926767676768, "width": 0.39545424836601306, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7252323232323232, "width": 0.39255228758169936, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7390694444444444, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7529065656565657, "width": 0.05348856209150332, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2606"}, {"text": "We note that while this overlay is helpful for identifying potential issues along the subject-background boundary, it does not help to draw any attention towards potential clutter to address along image borders.", "label": "Author", "bboxes": [{"left": 0.4615571895424837, "top": 0.5689431818181818, "width": 0.01890849673202616, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5827790404040404, "width": 0.39502450980392156, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5966161616161616, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6104532828282828, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.624290404040404, "width": 0.04843464052287581, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2607"}, {"text": "Adjusting these parameters, we designed and implemented a range of potential overlay proposals.", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.7667424242424242, "width": 0.37625816993464045, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7805795454545456, "width": 0.21088071895424831, "height": 0.011320707070706981, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2608"}, {"text": "Since we did not yet need these to work directly on a camera, these proposals were prototyped in Python.", "label": "Author", "bboxes": [{"left": 0.7330571895424837, "top": 0.7805795454545456, "width": 0.1790212418300654, "height": 0.011320707070706981, "page": 4}, {"left": 0.5195343137254902, "top": 0.7944166666666667, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8082537878787878, "width": 0.04727450980392156, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2609"}, {"text": "Aiming to capture visual and conceptual diversity in the overlays, we narrowed down the set of overlay options to the 6 in Figure 6 to study further:", "label": "Author", "bboxes": [{"left": 0.5704558823529412, "top": 0.8082537878787878, "width": 0.3416209150326798, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8220909090909091, "width": 0.3925424836601308, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8359280303030302, "width": 0.15035620915032677, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2610"}, {"text": "We chose to also include a few videos to show the process of making adjustments while framing a photo, to mimic how the overlay might appear if being used in the camera.", "label": "Author", "bboxes": [{"left": 0.7293709150326797, "top": 0.8313459595959597, "width": 0.18324673202614394, "height": 0.011320707070707092, "page": 5}, {"left": 0.5191683006535948, "top": 0.8451830808080808, "width": 0.3929117647058823, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.8590202020202021, "width": 0.3925408496732027, "height": 0.011320707070706981, "page": 5}, {"left": 0.5195343137254902, "top": 0.8728573232323232, "width": 0.04645588235294129, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2611"}, {"text": "We ran an informal design survey through Qualtrics with 29 participants (demographics information not collected) to try to understand if these overlay visualizations were interpretable by novice photographers, and if there were strong preferences between the overlay options.", "label": "Author", "bboxes": [{"left": 0.2217156862745098, "top": 0.8152474747474748, "width": 0.25874183006535956, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.39501307189542484, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.3925457516339869, "height": 0.011320707070706981, "page": 5}, {"left": 0.08790522875816993, "top": 0.8567563131313132, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.24487581699346406, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2612"}, {"text": "Given the elements supported by the qualitative feedback, (f) appeared to be the best candidate for our overlayit included the color-coded line drawings to help defne the subject and possible distractors along the borders, while being less noisy than showing all outlines.", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.6076919191919191, "width": 0.37715522875816987, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6215290404040403, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6353661616161616, "width": 0.39255882352941185, "height": 0.011320707070707203, "page": 6}, {"left": 0.5195343137254902, "top": 0.6492032828282828, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.0717908496732026, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2613"}, {"text": "However, since (d) was the most popular based on overall counts, we decided to go with a hybrid approach for our fnal abstraction overlay, enabling participants to switch back and forth between showing all lines, including those within the subject and background for additional context, and hiding these extra lines for minimal distraction.", "label": "Author", "bboxes": [{"left": 0.5963316993464052, "top": 0.6630404040404041, "width": 0.31576470588235295, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.676875, "width": 0.3928316993464054, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6907146464646465, "width": 0.392563725490196, "height": 0.011320707070706981, "page": 6}, {"left": 0.5195343137254902, "top": 0.7045517676767676, "width": 0.39255718954248375, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.718388888888889, "width": 0.39256209150326793, "height": 0.011320707070706981, "page": 6}, {"left": 0.5195343137254902, "top": 0.7322260101010101, "width": 0.14158660130718959, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2614"}, {"text": "Both color fattening and image darkening made the image less clear, so we wanted our fnal overlay to only have one of these components.", "label": "Author", "bboxes": [{"left": 0.6679379084967321, "top": 0.5246704545454546, "width": 0.24415849673202605, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.5385075757575758, "width": 0.39294281045751644, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.552344696969697, "width": 0.17990522875816994, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2615"}, {"text": "Since many participants specifcally noted that the darkening was crucial for benefting from the line drawing, we chose the image darkening component over color fattening for our abstraction overlay implementation.", "label": "Author", "bboxes": [{"left": 0.702828431372549, "top": 0.552344696969697, "width": 0.20964215686274512, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.5661818181818182, "width": 0.3925686274509803, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.5800189393939394, "width": 0.3928316993464054, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.593854797979798, "width": 0.32315686274509803, "height": 0.011320707070706981, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2616"}, {"text": "Therefore we decided line drawing should be part of our fnal overlay design.", "label": "Author", "bboxes": [{"left": 0.545983660130719, "top": 0.24793055555555554, "width": 0.36610947712418296, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.26176767676767676, "width": 0.09069934640522881, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2617"}, {"text": "Results of both steps of our design prototyping process made us hopeful of the potential of an abstraction-based overlay.", "label": "Author", "bboxes": [{"left": 0.5195343137254902, "top": 0.7875732323232324, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8014103535353535, "width": 0.3338431372549021, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2618"}, {"text": "However, we had yet to try these overlays in an interactive manner.", "label": "Author", "bboxes": [{"left": 0.8570473856209151, "top": 0.8014103535353535, "width": 0.05666013071895415, "height": 0.011320707070707092, "page": 6}, {"left": 0.5189918300653594, "top": 0.8152474747474748, "width": 0.35448529411764707, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2619"}, {"text": "To do so, we needed an implementation that would run interactively on a phone.", "label": "Author", "bboxes": [{"left": 0.8774673202614379, "top": 0.8152474747474748, "width": 0.03462581699346401, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195294117647059, "top": 0.8290820707070707, "width": 0.39293627450980384, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195294117647059, "top": 0.8429166666666668, "width": 0.07397222222222222, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2620"}, {"text": "In this section, I describe our fnal interface along with the implementation and algorithm to enable it.", "label": "Author", "bboxes": [{"left": 0.5986437908496732, "top": 0.8429166666666668, "width": 0.3134493464052287, "height": 0.011320707070707092, "page": 6}, {"left": 0.5189918300653594, "top": 0.8567588383838384, "width": 0.30795098039215696, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2621"}, {"text": "They are also informed of our overall goalto determine which overlay is best for evaluating images based on the two decluttering guidelines.", "label": "Author", "bboxes": [{"left": 0.37722058823529414, "top": 0.35587247474747474, "width": 0.10571241830065359, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.36970959595959596, "width": 0.39284150326797385, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.3835467171717172, "width": 0.36827941176470586, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2622"}, {"text": "Finally at the end of the survey, we asked participants to summarize their choices: Please provide a brief explanation for your choices why did you fnd these overlays most helpful? Are there specifc characteristics of the overlays that you like (e.g., line drawing, color fattening, image darkening, or color)?", "label": "Author", "bboxes": [{"left": 0.08790522875816993, "top": 0.5576111111111111, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5714457070707071, "width": 0.39418464052287583, "height": 0.011320707070707092, "page": 6}, {"left": 0.08736437908496732, "top": 0.5852853535353535, "width": 0.3931013071895425, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5991224747474747, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.6129595959595959, "width": 0.24230065359477126, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2623"}, {"text": "We hoped to gain some understanding of whether or not they had a general feeling of which overlays were helpful (maybe for diferent scenarios and considerations), and why.", "label": "Author", "bboxes": [{"left": 0.3346454248366013, "top": 0.6129595959595959, "width": 0.1458218954248366, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.626794191919192, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 6}, {"left": 0.08736764705882354, "top": 0.6406287878787879, "width": 0.39309967320261435, "height": 0.011320707070707203, "page": 6}, {"left": 0.08790522875816993, "top": 0.6544709595959596, "width": 0.15414542483660132, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2624"}, {"text": "Table 1 presents the results from our design survey.", "label": "Author", "bboxes": [{"left": 0.2654705882352941, "top": 0.6907146464646465, "width": 0.21526960784313726, "height": 0.011320707070706981, "page": 6}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.0867434640522876, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2625"}, {"text": "We choose the most abstracted form of the overlay (solid black, minimal edges) as the default as we imagine users starting in a more", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.8705959595959595, "width": 0.37788071895424835, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8844318181818183, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2626"}, {"text": "Figure 8 walks through our algorithm for generating our abstraction overlay.", "label": "Author", "bboxes": [{"left": 0.5195343137254902, "top": 0.37707575757575756, "width": 0.3950196078431373, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.3909128787878788, "width": 0.07646568627450978, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2627"}, {"text": "Given an image, our tool detects edges throughout the image for the line drawing [3].", "label": "Author", "bboxes": [{"left": 0.6001977124183007, "top": 0.3909128787878788, "width": 0.3118823529411764, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.40475, "width": 0.2031225490196079, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2628"}, {"text": "However, we want to be able to determine the relevant context in order to focus on edges related to SBS and IBF.", "label": "Author", "bboxes": [{"left": 0.7263137254901961, "top": 0.40475, "width": 0.18577777777777782, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.4185871212121212, "width": 0.3925392156862745, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.43242424242424243, "width": 0.0913643790849673, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2629"}, {"text": "In particular, we need to identify a border around the subject.", "label": "Author", "bboxes": [{"left": 0.6145588235294118, "top": 0.43242424242424243, "width": 0.29752124183006523, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.44626136363636365, "width": 0.06928267973856206, "height": 0.011320707070707037, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2630"}, {"text": "We realized we could estimate the image subjects using object-based saliency maps [4].", "label": "Author", "bboxes": [{"left": 0.5924673202614379, "top": 0.44626136363636365, "width": 0.322078431372549, "height": 0.011320707070707037, "page": 7}, {"left": 0.5195343137254902, "top": 0.4600972222222222, "width": 0.21462091503267966, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2631"}, {"text": "We used both algorithms of the shelf.", "label": "Author", "bboxes": [{"left": 0.738218954248366, "top": 0.4600972222222222, "width": 0.17386601307189542, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.47393434343434343, "width": 0.0542565359477124, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2632"}, {"text": "For determining appropriate parameters for the edge detection [3], we iteratively tested parameters to achieve a balance between having enough defnition in the edges and having too much noise.", "label": "Author", "bboxes": [{"left": 0.5774640522875817, "top": 0.47393434343434343, "width": 0.33709640522875817, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.48777146464646465, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195326797385621, "top": 0.5016060606060606, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.515445707070707, "width": 0.07180065359477128, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2633"}, {"text": "To implement this, our fnal camera tool has 3 layers: the camera view, a black layer of varying degrees of opacity, and a color-coded outlines layer.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.5108333333333334, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 7}, {"left": 0.08754411764705881, "top": 0.5246679292929293, "width": 0.3929035947712418, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.5385075757575758, "width": 0.08537745098039216, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2634"}, {"text": "As shown in Figure 7, by default the black layer is opaque (opacity is 1.0) and only the edges most relevant to our two decluttering principles (Section 3.2.1), subject-background separation and image border ficker, are visible.", "label": "Author", "bboxes": [{"left": 0.1769248366013072, "top": 0.5385075757575758, "width": 0.30353267973856213, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.552344696969697, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.5661818181818182, "width": 0.39501307189542484, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.5800189393939394, "width": 0.24379575163398692, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2635"}, {"text": "Given this saliency map, we segment the image into regions describing the subject, subject border, image border, and remaining background.", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.5292828282828282, "width": 0.37625816993464045, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195408496732026, "top": 0.5431174242424243, "width": 0.3950114379084968, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.5569570707070707, "width": 0.09839542483660124, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2636"}, {"text": "We then used simple morphological operations to obtain a subject border mask specifcally, we subtracted a slightly eroded version of this mask from a dilated version to capture the boundary between the subject(s) and the background immediately surrounding.", "label": "Author", "bboxes": [{"left": 0.8321584967320261, "top": 0.5984671717171717, "width": 0.07992647058823543, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6123042929292929, "width": 0.39416830065359487, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6261414141414141, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6399760101010101, "width": 0.3950114379084967, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6538093434343435, "width": 0.3244640522875817, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2637"}, {"text": "We merge these to form two edge-based overlays: one showing all edges color-coded, and the other only showing the relevant edges around the subject and image borders.", "label": "Author", "bboxes": [{"left": 0.8025751633986928, "top": 0.7368371212121213, "width": 0.10950326797385612, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7506742424242424, "width": 0.3941617647058824, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7645113636363637, "width": 0.3925424836601308, "height": 0.011320707070706981, "page": 7}, {"left": 0.5195343137254902, "top": 0.7783484848484848, "width": 0.11392156862745106, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2638"}, {"text": "We wanted to study how users would react to our abstraction guidance tool.", "label": "Author", "bboxes": [{"left": 0.5188316993464052, "top": 0.8290845959595959, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8429217171717173, "width": 0.08177941176470593, "height": 0.011320707070706981, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2639"}, {"text": "We conducted a small formative pilot study to inform our summative user study design.", "label": "Author", "bboxes": [{"left": 0.6041846405228759, "top": 0.8429217171717173, "width": 0.30789379084967305, "height": 0.011320707070706981, "page": 7}, {"left": 0.5195343137254902, "top": 0.8567588383838384, "width": 0.2107450980392157, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2640"}, {"text": "In particular, in running our pilot, we also wanted to understand if a no guidance interface was a reasonable baseline to compare our tool against.", "label": "Author", "bboxes": [{"left": 0.7353006535947713, "top": 0.8567588383838384, "width": 0.17704901960784303, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8705959595959595, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.2970669934640522, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2641"}, {"text": "Our overlay tool is build on top of a basic iOS camera app.", "label": "Author", "bboxes": [{"left": 0.5195343137254902, "top": 0.18796969696969698, "width": 0.3393496732026142, "height": 0.011320707070707037, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2642"}, {"text": "On top of this, we implemented our camera overlays and added the necessary UI elements to adjust settings on our overlay.", "label": "Author", "bboxes": [{"left": 0.7016617647058823, "top": 0.2294810606060606, "width": 0.21068627450980404, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.24331818181818182, "width": 0.3950130718954248, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.25715530303030304, "width": 0.12212908496732022, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2643"}, {"text": "Based on our learnings from the design survey (Section 3.2.3), we chose to go with a design inspired by a combination of overlay options (d) and (f).", "label": "Author", "bboxes": [{"left": 0.08790522875816993, "top": 0.45548484848484855, "width": 0.39253758169934644, "height": 0.011320707070707037, "page": 7}, {"left": 0.08790522875816993, "top": 0.46932196969696965, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.48315909090909087, "width": 0.11031209150326797, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2644"}, {"text": "Thus, our fnal abstraction overlay is a contextaware line drawing.", "label": "Author", "bboxes": [{"left": 0.2018888888888889, "top": 0.48315909090909087, "width": 0.28103431372549015, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4969962121212121, "width": 0.11882843137254902, "height": 0.011320707070707037, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2645"}, {"text": "We ended the study with open-ended interviews asking about what they liked/disliked about the tool and how the interaction infuenced their thought process as they took photos.", "label": "Author", "bboxes": [{"left": 0.5718594771241831, "top": 0.8272853535353536, "width": 0.3402140522875816, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.8411338383838384, "width": 0.39502614379084977, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.8549709595959595, "width": 0.35224346405228757, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2646"}, {"text": "We ran these user studies over Zoom, asking the participant to adjust the webcam when possible to keep their photographing within view.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.75989898989899, "width": 0.37625816993464056, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.7737361111111111, "width": 0.392547385620915, "height": 0.011320707070707092, "page": 8}, {"left": 0.08736437908496732, "top": 0.7875732323232324, "width": 0.07515359477124182, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2647"}, {"text": "Additionally to prep for the study, we confrmed beforehand that the participant had a phone running iOS 13 or higher (or found a way to drop of a device in a socially distant manner outdoors).", "label": "Author", "bboxes": [{"left": 0.16619444444444445, "top": 0.7875732323232324, "width": 0.316733660130719, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.8014103535353535, "width": 0.3928169934640523, "height": 0.011320707070707092, "page": 8}, {"left": 0.0874656862745098, "top": 0.8152474747474748, "width": 0.39326307189542486, "height": 0.011320707070707092, "page": 8}, {"left": 0.08789869281045751, "top": 0.8290820707070707, "width": 0.06204411764705883, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2648"}, {"text": "We distributed the app using TestFlight.", "label": "Author", "bboxes": [{"left": 0.15370915032679738, "top": 0.8290820707070707, "width": 0.24400980392156862, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2649"}, {"text": "After getting the participants consent, we started by walking through having them install TestFlight and subsequently our guidance tool.", "label": "Author", "bboxes": [{"left": 0.40148366013071896, "top": 0.8290820707070707, "width": 0.0789607843137255, "height": 0.011320707070707092, "page": 8}, {"left": 0.08789869281045751, "top": 0.8429154040404041, "width": 0.392545751633987, "height": 0.011320707070706981, "page": 8}, {"left": 0.08789869281045751, "top": 0.8567487373737374, "width": 0.3713709150326797, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2650"}, {"text": "As additional preparation, we also stepped through using the iPhone screen recording functionality with the microphone to ensure sound", "label": "Author", "bboxes": [{"left": 0.46441013071895426, "top": 0.8567487373737374, "width": 0.01603431372549019, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.3925424836601307, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.8844318181818183, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2651"}, {"text": "Due to the COVID-19 pandemic, we had to run our studies remotely.", "label": "Author", "bboxes": [{"left": 0.08790522875816993, "top": 0.5385075757575758, "width": 0.3947990196078432, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2652"}, {"text": "Overall, we approximately followed the fnal study design of E et al. for their capture-time composition guidance tool [17] and modify the design for work in remote settings.", "label": "Author", "bboxes": [{"left": 0.08790522875816993, "top": 0.5523421717171717, "width": 0.3947990196078432, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.5661818181818182, "width": 0.3929313725490196, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.5800189393939394, "width": 0.22812745098039217, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2653"}, {"text": "We liked how the diference in scale resulted in a range of complexity while composing.", "label": "Author", "bboxes": [{"left": 0.3148235294117647, "top": 0.593854797979798, "width": 0.16562254901960788, "height": 0.011320707070706981, "page": 8}, {"left": 0.08790522875816993, "top": 0.6076919191919191, "width": 0.3721029411764706, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2654"}, {"text": "To modify this task design to work for remote studies, we decided to keep the structure of having 3 tasks per condition at diferent scales (small, medium, and large), but instead of specifying the subjects, we asked that participants choose their own subjects to photograph.", "label": "Author", "bboxes": [{"left": 0.4649248366013072, "top": 0.6076919191919191, "width": 0.015526143790849711, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.6215265151515151, "width": 0.392545751633987, "height": 0.011320707070706981, "page": 8}, {"left": 0.08790522875816993, "top": 0.6353598484848485, "width": 0.392545751633987, "height": 0.011320707070707203, "page": 8}, {"left": 0.08790522875816993, "top": 0.6491931818181818, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.39253758169934644, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.07291503267973855, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2655"}, {"text": "Since the locations were all diferent, here we just counterbalanced condition to avoid biases from learning efects.", "label": "Author", "bboxes": [{"left": 0.18713071895424838, "top": 0.7322260101010101, "width": 0.2933251633986928, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.7460618686868686, "width": 0.3819346405228758, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2656"}, {"text": "was also captured as we asked participants to think aloud as they captured photos for each task.", "label": "Author", "bboxes": [{"left": 0.5189918300653594, "top": 0.4675353535353535, "width": 0.39346405228758174, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.48137247474747474, "width": 0.18015522875816992, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2657"}, {"text": "After reading the document, we asked that the participants briefy describe the principles in their own words to confrm understanding.", "label": "Author", "bboxes": [{"left": 0.6354803921568627, "top": 0.605905303030303, "width": 0.2765947712418302, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.6197424242424242, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.6335795454545454, "width": 0.13826307189542486, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2658"}, {"text": "For each photo task, we asked participants to focus on the overall clarity of the story.", "label": "Author", "bboxes": [{"left": 0.6614297385620915, "top": 0.6335795454545454, "width": 0.25064869281045743, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.6474166666666666, "width": 0.248187908496732, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2659"}, {"text": "We encouraged them to explore the process of framing the image, but to limit each task to around 1-2 minutes.", "label": "Author", "bboxes": [{"left": 0.7713954248366013, "top": 0.6474166666666666, "width": 0.1406797385620916, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.6612537878787879, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.6750909090909091, "width": 0.12286437908496739, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2660"}, {"text": "We quickly walked all participants through the basic (no guidance) camera app.", "label": "Author", "bboxes": [{"left": 0.6464624183006535, "top": 0.6750909090909091, "width": 0.2656176470588235, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.6889255050505051, "width": 0.2221764705882353, "height": 0.011320707070707092, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2661"}, {"text": "When evaluating their favorited photos per task, participants did believe that the photos captured using our tool had better subjectbackground separation (Mdn = 6, IQR = 4-5), than those captured using the no guidance baseline (Mdn = 4, IQR = 2-5) [ V = 0, p = . 003].", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.5338383838383839, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5476755050505051, "width": 0.39501470588235305, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5615126262626263, "width": 0.39254084967320263, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5727121212121212, "width": 0.3925571895424837, "height": 0.013969696969696965, "page": 9}, {"left": 0.0881421568627451, "top": 0.5888888888888889, "width": 0.03333496732026142, "height": 0.011618686868686834, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2662"}, {"text": "Therefore we see that while participants are more confdent in their ability to take clear photos, and found the tool helpful for achieving their favorite resulting photos, they didnt necessarily fnd that their photos were better with regard to their personal preferences or the decluttering principles.", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.275604797979798, "width": 0.37625980392156855, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.2894419191919192, "width": 0.3928153594771241, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.30327651515151516, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3171098484848485, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.2501111111111112, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2663"}, {"text": "Thus, even though we did not fnd signifcant changes in CSI, we decided that we should compare against a baseline that provided a little more assistance.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.6583712121212121, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6722083333333333, "width": 0.39253758169934644, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6860454545454545, "width": 0.1452941176470588, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2664"}, {"text": "As we described earlier, photographers will sometimes use a grayscale display in their current practice, to help emphasize contrast in order to consider overall clarity and decluttering.", "label": "Author", "bboxes": [{"left": 0.23811601307189542, "top": 0.6860454545454545, "width": 0.2423349673202614, "height": 0.011320707070707092, "page": 9}, {"left": 0.08736437908496732, "top": 0.6998825757575757, "width": 0.39307843137254905, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.713719696969697, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.7275568181818182, "width": 0.07524346405228759, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2665"}, {"text": "We therefore use grayscale as our baseline condition.", "label": "Author", "bboxes": [{"left": 0.16683333333333333, "top": 0.7275568181818182, "width": 0.3158741830065359, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2666"}, {"text": "In addition to encouraging more confdence, we found that the tool further encouraged creativity through exploring the space in new ways: It was really helpful with how I take photos because normally its more just snap and done. This one was more like, can I move things out of the background, can I move the subject to frame it to not have a distracting background? Another thing that I dont normally do is pivot the camera and usually just move within a fat plane (P12).", "label": "Author", "bboxes": [{"left": 0.6019640522875817, "top": 0.6492032828282828, "width": 0.3103202614379085, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6768775252525252, "width": 0.39254575163398686, "height": 0.011343434343434433, "page": 9}, {"left": 0.5195343137254902, "top": 0.6907335858585859, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.704574494949495, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7184116161616161, "width": 0.3925408496732027, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7322487373737374, "width": 0.39255065359477126, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7460618686868686, "width": 0.10836928104575161, "height": 0.011343434343434433, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2667"}, {"text": "Again we saw that the tool made the participants more confdent in their ability to address the decluttering principles of subject-background separation and image border ficker (Mdn = 6, IQR = 5-7), versus no guidance (Mdn = 5, IQR = 4-6) [ V = 8, p = . 03].", "label": "Author", "bboxes": [{"left": 0.7150800653594771, "top": 0.10956060606060607, "width": 0.19701143790849684, "height": 0.011320707070707065, "page": 9}, {"left": 0.5195343137254902, "top": 0.12339772727272727, "width": 0.3950114379084967, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1372348484848485, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.3941535947712419, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.162270202020202, "width": 0.1635751633986927, "height": 0.013969696969696965, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2668"}, {"text": "For each favorited photo per task, we asked participants to self-assess them based on subject-background separation and image border ficker.", "label": "Author", "bboxes": [{"left": 0.6867712418300653, "top": 0.1649078282828283, "width": 0.22531372549019613, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.21975653594771238, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2669"}, {"text": "Though we actually did not see a signifcant improvement in overall self-assessed quality in terms of these principles, participants did believe that the tool was helpful for the task of capturing clear and decluttered images (Mdn = 6, IQR = 5-6), versus no guidance (Mdn = 4, IQR = 4-6) [ V = 134, p = . 003].", "label": "Author", "bboxes": [{"left": 0.7431601307189543, "top": 0.1925820707070707, "width": 0.1689199346405228, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.20641666666666666, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.22025631313131314, "width": 0.39255065359477126, "height": 0.011320707070707037, "page": 9}, {"left": 0.5195343137254902, "top": 0.23409343434343433, "width": 0.39254411764705877, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.24529292929292928, "width": 0.39417156862745095, "height": 0.013969696969696965, "page": 9}, {"left": 0.5188022875816993, "top": 0.2591300505050505, "width": 0.05838235294117655, "height": 0.013969696969696965, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2670"}, {"text": "We found that overall, this study design worked reasonably well in the Zoom environment.", "label": "Author", "bboxes": [{"left": 0.24475000000000002, "top": 0.4231426767676768, "width": 0.23571078431372544, "height": 0.011320707070707092, "page": 9}, {"left": 0.08736437908496732, "top": 0.436979797979798, "width": 0.301578431372549, "height": 0.011320707070706981, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2671"}, {"text": "However, even with just 5 participants we were seeing signifcant results suggesting that our tool was preferable to no guidance for these photo tasks.", "label": "Author", "bboxes": [{"left": 0.3926045751633987, "top": 0.436979797979798, "width": 0.08783823529411766, "height": 0.011320707070706981, "page": 9}, {"left": 0.08736437908496732, "top": 0.45081691919191924, "width": 0.3955653594771242, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.46465277777777775, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.4784873737373737, "width": 0.034145424836601296, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2672"}, {"text": "We also did not fnd signifcant diferences in CSI, but did fnd support for the increased confdence and descriptions of how the tool encouraged participants to explore more in the qualitative feedback.", "label": "Author", "bboxes": [{"left": 0.7312189542483659, "top": 0.3590290404040404, "width": 0.18085947712418315, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.37286363636363634, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3866969696969697, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.40054040404040403, "width": 0.19635457516339871, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2673"}, {"text": "We ran remote studies over Zoom with 18 participants (6 male, 10 female), 24 to 32 years old (  = 29), to understand if the tool would help users declutter photos, and if users felt creative while using the tool.", "label": "Author", "bboxes": [{"left": 0.08720261437908497, "top": 0.8152474747474748, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8264469696969697, "width": 0.39254901960784316, "height": 0.013969696969696965, "page": 9}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.392545751633987, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.04986111111111112, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2674"}, {"text": "Users experienced two diferent conditions of the tool: a baseline grayscale overlay, and our tool highlighting edges along the subject and image borders.", "label": "Author", "bboxes": [{"left": 0.14141339869281047, "top": 0.8567588383838384, "width": 0.3390294117647059, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.39253921568627453, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8844318181818183, "width": 0.18255718954248368, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2675"}, {"text": "We ran a pilot study (n = 5) to test this study design in a remote setting.", "label": "Author", "bboxes": [{"left": 0.08720261437908497, "top": 0.3587222222222222, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.3725568181818182, "width": 0.04485947712418299, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2676"}, {"text": "For these pilots, we compared our tool to a no guidance baseline condition.", "label": "Author", "bboxes": [{"left": 0.14241176470588235, "top": 0.38639646464646465, "width": 0.3380473856209151, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.40023358585858587, "width": 0.11229411764705884, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2677"}, {"text": "Additionally, we found in our low-fdelity prototype that the abstraction also encouraged participants to be more aware of composition.", "label": "Author", "bboxes": [{"left": 0.5190212418300654, "top": 0.7460618686868686, "width": 0.395531045751634, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195375816993464, "top": 0.7598964646464647, "width": 0.394799019607843, "height": 0.011320707070707092, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2678"}, {"text": "What might our abstraction overlay look like if our focus were instead exploring composition overall rather than decluttering?", "label": "Author", "bboxes": [{"left": 0.5188316993464052, "top": 0.7737361111111111, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.7875707070707071, "width": 0.39317320261437916, "height": 0.011320707070707092, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2679"}, {"text": "We would be interested in running a similar design survey asking participants to select overlays based on which best assisted in understanding the overall composition and seeing if reactions signifcantly difered from our survey focused on decluttering.", "label": "Author", "bboxes": [{"left": 0.5188316993464052, "top": 0.8014040404040403, "width": 0.3957140522875817, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.8152373737373737, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.8290719696969697, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.8429217171717173, "width": 0.3722058823529413, "height": 0.011320707070706981, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2680"}, {"text": "Our design survey (Section 3.2.3) provided us with a lot of interesting insight on how participants might imagine an abstraction overlay to assist with decluttering.", "label": "Author", "bboxes": [{"left": 0.5195343137254902, "top": 0.5150921717171717, "width": 0.3950114379084968, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.5289267676767677, "width": 0.39254575163398686, "height": 0.011320707070706981, "page": 10}, {"left": 0.5195343137254902, "top": 0.542760101010101, "width": 0.21325653594771243, "height": 0.011320707070707092, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2681"}, {"text": "For example, we saw the potential benefts of color fattening especially for SBS.", "label": "Author", "bboxes": [{"left": 0.7371241830065359, "top": 0.542760101010101, "width": 0.17742156862745095, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.5566022727272727, "width": 0.31233496732026134, "height": 0.011320707070707092, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2682"}, {"text": "In fact, we would be interested in comparing how such an interface might compare to other grid-based composition guidance [17], as well as understanding how these diferent camera interfaces", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.8567588383838384, "width": 0.37874509803921563, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.8705959595959595, "width": 0.39417156862745106, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2683"}, {"text": "An interaction that we didnt expect that emerged from the studies was that participants used the tools ability to identify subjects to assess how someone might view their photos.", "label": "Author", "bboxes": [{"left": 0.23522058823529413, "top": 0.5426098484848485, "width": 0.2452369281045752, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.5564457070707071, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.5702828282828283, "width": 0.3928169934640523, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.5841199494949495, "width": 0.04381862745098038, "height": 0.011320707070707092, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2684"}, {"text": "It additionally could be interesting to experiment more thoroughly with diferent types of edge detection to see which would best match what humans actually perceive as noisee.g., in our user studies, we often found that textures like carpet ended up appearing as a lot of noise.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.43472979797979794, "width": 0.37875490196078443, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.44856691919191916, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4624040404040404, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4762411616161616, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4900782828282828, "width": 0.15979901960784315, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2685"}, {"text": "If we remove the requirement of our camera guidance running interactively, there are further approaches that can be considered both for identifying objects in the scene for location context and for generating line drawings.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.503915404040404, "width": 0.3762761437908496, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.5177525252525252, "width": 0.39256699346405227, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.5315883838383838, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.545425505050505, "width": 0.17374673202614382, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2686"}, {"text": "We can imagine instead of the interactive guidance, having an overlay that is shown upon reviewing previously captured photos (similar to our low-fdelity prototype interaction).", "label": "Author", "bboxes": [{"left": 0.2653137254901961, "top": 0.545425505050505, "width": 0.21762581699346406, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.5592626262626262, "width": 0.3925588235294118, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.5730997474747475, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.5869368686868687, "width": 0.07448039215686274, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2687"}, {"text": "We would like to thank Grifn Dietz for her assistance in transitioning to virtual user studies, Mitchell Gordon for his feedback and insightful discussions, and Matthew Cong and Charlene Seto Kung for their help with fgure photos.", "label": "Author", "bboxes": [{"left": 0.5188316993464052, "top": 0.30579419191919194, "width": 0.3957303921568628, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.31962878787878785, "width": 0.39293627450980395, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.3334684343434344, "width": 0.39255228758169936, "height": 0.011320707070707037, "page": 11}, {"left": 0.5195343137254902, "top": 0.34730555555555553, "width": 0.2267565359477124, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2688"}, {"text": "We would also like to thank all of our participants for their time and feedback.", "label": "Author", "bboxes": [{"left": 0.7496928104575163, "top": 0.34730555555555553, "width": 0.1627777777777778, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.36114267676767675, "width": 0.3086519607843138, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2689"}, {"text": "Our research is supported by the Brown Institute for Media Innovation, Adobe Research, the Microsoft Research Dissertation Grant, and the Hasso Plattner Institute Design Thinking Research Program.", "label": "Author", "bboxes": [{"left": 0.8324754901960785, "top": 0.36114267676767675, "width": 0.07962091503267965, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.37497979797979797, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.3888156565656566, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.4026527777777778, "width": 0.3219264705882354, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2690"}, {"text": "Inspired by the use of sketching in design to capture higher level structure, our goal was to bring some of the benefts of the abstraction in a sketch-like representation to photography.", "label": "Author", "bboxes": [{"left": 0.8624264705882352, "top": 0.515864898989899, "width": 0.049663398692810445, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5297020202020202, "width": 0.39416830065359487, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5435391414141414, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5573762626262626, "width": 0.2483790849673203, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2691"}, {"text": "In this paper, we walked through the design process behind our abstraction overlay and guidance tool.", "label": "Author", "bboxes": [{"left": 0.7710147058823529, "top": 0.5573762626262626, "width": 0.14108169934640524, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5712133838383838, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.585050505050505, "width": 0.08394607843137247, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2692"}, {"text": "We demonstrated this capture-time tool and how it encourages users to explore creative options to address the concept of decluttering.", "label": "Author", "bboxes": [{"left": 0.6071323529411765, "top": 0.585050505050505, "width": 0.3049656862745096, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5988876262626263, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.6127234848484848, "width": 0.09129084967320267, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2693"}, {"text": "In this paper, we have described a type of annotation-based guidance.", "label": "Author", "bboxes": [{"left": 0.08790522875816993, "top": 0.7322260101010101, "width": 0.3950359477124183, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.7460606060606061, "width": 0.03092973856209151, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2694"}, {"text": "While there isnt necessarily a clear defnition of failure, the main failure case with our current algorithm is when the main subject is not properly captured in the saliency map.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.28252272727272726, "width": 0.3762745098039215, "height": 0.011320707070707037, "page": 11}, {"left": 0.08791013071895425, "top": 0.2963573232323232, "width": 0.3925637254901961, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.3101969696969697, "width": 0.30394444444444446, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2695"}, {"text": "We considered this in our design and tried to design the visualization to be robust to this case by providing the option of showing all lines.", "label": "Author", "bboxes": [{"left": 0.39503758169934644, "top": 0.3101969696969697, "width": 0.08542973856209146, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.3240340909090909, "width": 0.39255718954248364, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.33787121212121213, "width": 0.33403594771241835, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2696"}, {"text": "Due to the modular nature of our implementation, better object detection or object-centric saliency methods methods can be used in the future.", "label": "Author", "bboxes": [{"left": 0.08790522875816993, "top": 0.39321590909090914, "width": 0.392562091503268, "height": 0.011320707070707037, "page": 11}, {"left": 0.08790522875816993, "top": 0.4070555555555555, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4208926767676767, "width": 0.07798692810457518, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2697"}, {"text": "Therefore we wonder if it is ok to have a slightly more opinionated interface (at least for some training) as is suggested by one of our design survey participants: Making it very clear about the distinction between whether the overlay is used to indicate if the two properties are good, bad, or both would be very helpful (P18).", "label": "Author", "bboxes": [{"left": 0.7584624183006535, "top": 0.12339772727272727, "width": 0.1536307189542485, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1372348484848485, "width": 0.3928316993464054, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.3950424836601307, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.39255555555555555, "height": 0.01134343434343435, "page": 11}, {"left": 0.5195343137254902, "top": 0.17876767676767677, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.20352124183006526, "height": 0.011343434343434322, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2698"}, {"text": "In this case, we could imagine having the interface give specifc guidance such as rotating the phone, removing an object from the scene, or moving the camera closer to the subject.", "label": "Author", "bboxes": [{"left": 0.7272107843137254, "top": 0.1925820707070707, "width": 0.1848856209150327, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.20641666666666666, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.22025631313131314, "width": 0.39255228758169936, "height": 0.011320707070707037, "page": 11}, {"left": 0.5195343137254902, "top": 0.23409343434343433, "width": 0.1253349673202615, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2699"}, {"text": "In designing our overlays, we were somewhat limited in the methods that we used in order to produce something that could be computed interactively.", "label": "Author", "bboxes": [{"left": 0.08790522875816993, "top": 0.1856641414141414, "width": 0.395031045751634, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.1995012626262626, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.21333838383838383, "width": 0.11609640522875818, "height": 0.011320707070707092, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2700"}, {"text": "Our proposed technical approach was more a means of enabling the interaction of interest and not a core contribution.", "label": "Author", "bboxes": [{"left": 0.20764542483660128, "top": 0.21333838383838383, "width": 0.2728235294117648, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.22717550505050504, "width": 0.3950277777777778, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.24101136363636363, "width": 0.055751633986928104, "height": 0.011320707070707065, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2701"}, {"text": "We additionally found a simpler algorithm was easier to explain to users when describing the tool for studies, and that users could interpret the visualizations in diferent contexts more easily.", "label": "Author", "bboxes": [{"left": 0.1473202614379085, "top": 0.24101136363636363, "width": 0.33314379084967316, "height": 0.011320707070707065, "page": 11}, {"left": 0.08790522875816993, "top": 0.2548484848484849, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.26868560606060604, "width": 0.3946486928104575, "height": 0.011320707070707037, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2702"}, {"text": "our design process for determining what an abstraction overlay for decluttering might look like,  an interactive in-camera app that shows this edge highlighting overlay to users as decluttering guidance, and  a user evaluation comparing this overlay to a grayscale overlay, a baseline method that many photographers currently employ for decluttering photos.", "label": "Author", "bboxes": [{"left": 0.5456274509803921, "top": 0.49985606060606064, "width": 0.366467320261438, "height": 0.012236111111111059, "page": 1}, {"left": 0.5594183006535948, "top": 0.5146085858585858, "width": 0.23930882352941185, "height": 0.011320707070707092, "page": 1}, {"left": 0.5456176470588235, "top": 0.5275252525252525, "width": 0.368954248366013, "height": 0.012241161616161622, "page": 1}, {"left": 0.5594183006535948, "top": 0.5422828282828283, "width": 0.32428431372549027, "height": 0.011320707070707092, "page": 1}, {"left": 0.5456176470588235, "top": 0.5551994949494949, "width": 0.3664787581699346, "height": 0.012241161616161622, "page": 1}, {"left": 0.5594183006535948, "top": 0.5699545454545455, "width": 0.35515849673202615, "height": 0.011320707070707092, "page": 1}, {"left": 0.5594183006535948, "top": 0.5837941919191919, "width": 0.22229084967320267, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2703"}, {"text": "Through our design process, we aimed to answer two questions: (1) Will an abstracted visualization be efective in encouraging the user to see parts of the image outside of the main subject?", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.3617714646464647, "width": 0.37801797385620906, "height": 0.011320707070706981, "page": 2}, {"left": 0.51909477124183, "top": 0.3756085858585859, "width": 0.3930032679738562, "height": 0.011320707070706981, "page": 2}, {"left": 0.5195343137254902, "top": 0.389445707070707, "width": 0.3600441176470588, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2704"}, {"text": "Specifcally in this paper, we contribute:", "label": "Contribution", "bboxes": [{"left": 0.5195343137254902, "top": 0.48190151515151514, "width": 0.23960294117647052, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2705"}, {"text": "Our proposed technical approach was more a means of enabling the interaction of interest and not a core contribution.", "label": "Contribution", "bboxes": [{"left": 0.20764542483660128, "top": 0.21333838383838383, "width": 0.2728235294117648, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.22717550505050504, "width": 0.3950277777777778, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.24101136363636363, "width": 0.055751633986928104, "height": 0.011320707070707065, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2706"}, {"text": "Our fnal design uses object-based saliency and edge detection to highlight contrast along subject and image borders, outlining potential distractors in these regions.", "label": "Novelty", "bboxes": [{"left": 0.7099232026143791, "top": 0.5848876262626262, "width": 0.20215359477124184, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5987247474747475, "width": 0.39254411764705877, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.6125618686868687, "width": 0.3697091503267974, "height": 0.011320707070707092, "page": 0}], "section": "", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2707"}, {"text": "However, changes that can be made at the editing stages are limited, and often many mistakes cannot be fxed without returning to the photo location.", "label": "Novelty", "bboxes": [{"left": 0.42646405228758166, "top": 0.3067348484848485, "width": 0.055609477124183015, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.32057449494949497, "width": 0.392562091503268, "height": 0.011320707070707037, "page": 1}, {"left": 0.08790522875816993, "top": 0.3344090909090909, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.34824873737373735, "width": 0.09090196078431372, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2708"}, {"text": "While still at the scene, the photographer has many more options to reframe the image to clarify the subject and remove distractors, helping to improve these images and better direct viewers eyes.", "label": "Novelty", "bboxes": [{"left": 0.20579084967320263, "top": 0.389760101010101, "width": 0.274671568627451, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.4035972222222222, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.41743308080808084, "width": 0.39283169934640527, "height": 0.011320707070706981, "page": 1}, {"left": 0.08790522875816993, "top": 0.431270202020202, "width": 0.12111928104575163, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2709"}, {"text": "Similarly while taking photos, it is cheap to iterate and generate more prototypes or photos in the moment.", "label": "Novelty", "bboxes": [{"left": 0.30590522875816994, "top": 0.6388257575757575, "width": 0.17617320261437908, "height": 0.011320707070707203, "page": 1}, {"left": 0.08790522875816993, "top": 0.6526628787878788, "width": 0.3925588235294118, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.6665000000000001, "width": 0.0764624183006536, "height": 0.011320707070706981, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2710"}, {"text": "On the other hand, experienced photographers tend to capture many photos of a given scene: they know how to consider diferent options (e.g., composition, lighting, or pose), and recognize the challenges of not having the option to physically move the camera or elements in the image at edit-time.", "label": "Novelty", "bboxes": [{"left": 0.10418464052287582, "top": 0.4451073232323232, "width": 0.3762761437908496, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.45894444444444443, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.47278156565656565, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.48661868686868687, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.500455808080808, "width": 0.2307794117647059, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2711"}, {"text": "Depth of feld can enable some blurring of the background, but each pixel of the image is pigmented, every object in the frame is captured.", "label": "Novelty", "bboxes": [{"left": 0.25223202614379087, "top": 0.7771931818181819, "width": 0.22823529411764704, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.7910328282828284, "width": 0.3929362745098039, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.8048699494949495, "width": 0.19265686274509808, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2712"}, {"text": "A sketch on the other hand is a selective representation, an abstracted view of the idea or concept being explored.", "label": "Novelty", "bboxes": [{"left": 0.28498529411764706, "top": 0.8048699494949495, "width": 0.19548202614379084, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.8187070707070708, "width": 0.392562091503268, "height": 0.011320707070706981, "page": 1}, {"left": 0.08790522875816993, "top": 0.8325441919191919, "width": 0.08947875816993466, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2713"}, {"text": "In this work, our main focus is on the design process and the prototype appour goal is not to propose an exact abstraction interface, but to understand how this style of in-camera interaction might infuence users photographic process, specifcally with regards to decluttering.", "label": "Novelty", "bboxes": [{"left": 0.6321372549019608, "top": 0.6718497474747475, "width": 0.2799591503267973, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.685685606060606, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6995227272727274, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7133573232323233, "width": 0.39293627450980395, "height": 0.011320707070707092, "page": 1}, {"left": 0.5189918300653594, "top": 0.7271969696969698, "width": 0.1704934640522876, "height": 0.011320707070706981, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2714"}, {"text": "Focus peaking is relevant to our work due to its methods of visualizationit similarly highlights subsets of edges in the images, but instead for the purpose of highlighting regions that are in focus [2, 32].", "label": "Novelty", "bboxes": [{"left": 0.16690359477124184, "top": 0.70414898989899, "width": 0.31603267973856214, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.7179861111111111, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.7318232323232324, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.7456578282828283, "width": 0.09034313725490194, "height": 0.01132323232323229, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2715"}, {"text": "However, similar to a lot of other existing feedback in commercial cameras, the focus of this feature is more to assist users to more quickly arrive at the appropriate camera settings.", "label": "Novelty", "bboxes": [{"left": 0.18176633986928104, "top": 0.7456603535353535, "width": 0.299078431372549, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.7594974747474748, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.773334595959596, "width": 0.3734133986928104, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2716"}, {"text": "However, rather than focusing on accuracy and refnement of camera settings, we are interested in understanding how similar overlays", "label": "Novelty", "bboxes": [{"left": 0.4497549019607843, "top": 0.8425189393939394, "width": 0.03318137254901965, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.8563560606060606, "width": 0.3925555555555555, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.8701931818181818, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2717"}, {"text": "While these aesthetics evaluation algorithms may consider clutter, they provide limited information to the user for interpreting these quality measures.", "label": "Novelty", "bboxes": [{"left": 0.2787745098039216, "top": 0.607290404040404, "width": 0.2016911764705882, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6211275252525252, "width": 0.3925637254901961, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6349633838383839, "width": 0.2941813725490196, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2718"}, {"text": "Additionally, while these methods focus on generating output from existing media, Winnemller et al.", "label": "Novelty", "bboxes": [{"left": 0.0873921568627451, "top": 0.24793055555555554, "width": 0.39307843137254905, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790849673202614, "top": 0.2617651515151515, "width": 0.20777450980392162, "height": 0.011320707070707037, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2719"}, {"text": "Nonetheless, these arent intended to train the user to be more aware of background clutter while they are taking photos.", "label": "Novelty", "bboxes": [{"left": 0.331390522875817, "top": 0.42760984848484845, "width": 0.1490816993464052, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.44144696969696967, "width": 0.39283660130718956, "height": 0.011320707070707092, "page": 2}, {"left": 0.08736437908496732, "top": 0.4552840909090909, "width": 0.1834820261437909, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "2720"}, {"text": "We look to extend this idea by designing visual overlays that help the user identify clutter.", "label": "Novelty", "bboxes": [{"left": 0.8461960784313725, "top": 0.1510719696969697, "width": 0.06589379084967328, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.39283169934640527, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.08914869281045756, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2721"}, {"text": "While we didnt ask if they wanted to take another photo after this review step, occasionally (6) participants would ask if they could take a third photo and we would repeat the process above, presenting them with a third overlay.", "label": "Novelty", "bboxes": [{"left": 0.781687908496732, "top": 0.7938623737373738, "width": 0.13040522875816996, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8076994949494949, "width": 0.39504084967320263, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8215366161616162, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 2}, {"left": 0.5189918300653594, "top": 0.8353737373737373, "width": 0.3931062091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8492108585858587, "width": 0.04691176470588243, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2722"}, {"text": "In all but one case, participants noticed clutter in the background upon seeing their photo with the abstracted overlay.", "label": "Novelty", "bboxes": [{"left": 0.3226356209150327, "top": 0.46932196969696965, "width": 0.16028921568627452, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.48315909090909087, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.08736437908496732, "top": 0.4969962121212121, "width": 0.16638725490196077, "height": 0.011320707070707037, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2723"}, {"text": "In the one case where the participant did not make adjustments to address clutter, a whiteboard mostly flled up the background of the image, and was involved in the action in the image and therefore did not clutter the image, but instead helped tell the intended story.", "label": "Novelty", "bboxes": [{"left": 0.2574150326797386, "top": 0.4969962121212121, "width": 0.22303267973856206, "height": 0.011320707070707037, "page": 3}, {"left": 0.08790522875816993, "top": 0.5108333333333334, "width": 0.39292156862745103, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.5246704545454546, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.5385050505050505, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.552344696969697, "width": 0.22507352941176473, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2724"}, {"text": "Note that while these participants behaviors matched what we hoped for, there were limitations to our study design such that they cannot be directly mapped to how a user might respond to seeing this style of overlay interactively in the camera.", "label": "Novelty", "bboxes": [{"left": 0.5358137254901961, "top": 0.31581944444444443, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.3296565656565657, "width": 0.3929183006535949, "height": 0.011320707070707037, "page": 3}, {"left": 0.5195343137254902, "top": 0.3434936868686869, "width": 0.3925392156862745, "height": 0.011320707070707037, "page": 3}, {"left": 0.5195343137254902, "top": 0.357330808080808, "width": 0.2899150326797386, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2725"}, {"text": "Photography books describe that contrast is key to directing attention [20].", "label": "Novelty", "bboxes": [{"left": 0.5358137254901961, "top": 0.6353661616161616, "width": 0.37625816993464045, "height": 0.011320707070707203, "page": 3}, {"left": 0.5195408496732026, "top": 0.6492007575757576, "width": 0.08829248366013087, "height": 0.01132323232323229, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2726"}, {"text": "Our eyes are drawn to regions of high contrast.", "label": "Novelty", "bboxes": [{"left": 0.6131372549019608, "top": 0.6492032828282828, "width": 0.3011977124183006, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2727"}, {"text": "To achieve the most contrast, the photographer should place light objects on a dark background, or dark objects on a light background.", "label": "Novelty", "bboxes": [{"left": 0.51909477124183, "top": 0.6630404040404041, "width": 0.39298039215686287, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.6768775252525252, "width": 0.39479901960784314, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2728"}, {"text": "It is good practice to have this contrast around the subject as it will help make the subject distinct from the background.", "label": "Novelty", "bboxes": [{"left": 0.5195343137254902, "top": 0.6907121212121212, "width": 0.39254411764705877, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.7045517676767676, "width": 0.31300816993464053, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2729"}, {"text": "The contrast will clarify the story and declutter the overall image.", "label": "Novelty", "bboxes": [{"left": 0.8361781045751634, "top": 0.7045517676767676, "width": 0.07590032679738568, "height": 0.011320707070707092, "page": 3}, {"left": 0.5189918300653594, "top": 0.718388888888889, "width": 0.31323366013071896, "height": 0.011320707070706981, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2730"}, {"text": "On the other hand, contrast in other regions especially the border of the image, will distract, causing the eye to be attracted away from the focal subject.", "label": "Novelty", "bboxes": [{"left": 0.5358137254901961, "top": 0.8014103535353535, "width": 0.37655065359477113, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.8152474747474748, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.8290820707070707, "width": 0.1380457516339869, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2731"}, {"text": "In particular, contrast near the border of the image can draw the viewers attention outwards rather than within the imagewe call this image border ficker (IBF) [20].", "label": "Novelty", "bboxes": [{"left": 0.6620490196078431, "top": 0.8290820707070707, "width": 0.25003104575163393, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.8429154040404041, "width": 0.39254575163398686, "height": 0.011320707070706981, "page": 3}, {"left": 0.5189967320261437, "top": 0.856736111111111, "width": 0.3953545751633988, "height": 0.011343434343434433, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2732"}, {"text": "Again, this is better known as edge ficker, but we chose to specify image border to diferentiate it from the term edge, as edge is", "label": "Novelty", "bboxes": [{"left": 0.5190212418300654, "top": 0.8705959595959595, "width": 0.3934395424836602, "height": 0.011320707070707092, "page": 3}, {"left": 0.5178790849673203, "top": 0.8844318181818183, "width": 0.3941993464052288, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2733"}, {"text": "Photographers recommend a number of methods to be able to more easily see the contrast in an image (see Figure 4).", "label": "Novelty", "bboxes": [{"left": 0.42193300653594773, "top": 0.43057323232323236, "width": 0.06098366013071893, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.44440909090909086, "width": 0.39292483660130717, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.4582462121212121, "width": 0.24496568627450982, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2734"}, {"text": "These range from squinting at the image to better focus on the contrast with low-level details blurred, to viewing the image in grayscale to better focus on contrast in the absence of color, to explicitly outlining boundaries along which there is clear contrast between the subject and background [9, 20, 30].", "label": "Novelty", "bboxes": [{"left": 0.33594281045751634, "top": 0.4582462121212121, "width": 0.14697549019607842, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.4720833333333333, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.48591792929292926, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.49975757575757573, "width": 0.395014705882353, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.513594696969697, "width": 0.3925408496732027, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5274318181818182, "width": 0.13601307189542483, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2735"}, {"text": "Inspired by this idea of using outlines to highlight contrast and the lack of contrast, we hoped to recreate this outlining as an overlay directly in the camera (see Figure 5), while also extending it to contrast along the image borders.", "label": "Novelty", "bboxes": [{"left": 0.33908660130718954, "top": 0.6492032828282828, "width": 0.1413643790849673, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6630378787878788, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 4}, {"left": 0.0874656862745098, "top": 0.6907146464646465, "width": 0.39298529411764704, "height": 0.011320707070706981, "page": 4}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.0474656862745098, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2736"}, {"text": "We additionally give ourselves line color (for the purposes of color-coding lines based on their location context), and image darkening (to provide more contrast against the line drawings) as parameters to consider in designing abstraction overlays.", "label": "Novelty", "bboxes": [{"left": 0.6719754901960785, "top": 0.6975580808080808, "width": 0.2403758169934641, "height": 0.011320707070707092, "page": 4}, {"left": 0.5190996732026144, "top": 0.7113926767676768, "width": 0.39545424836601306, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7252323232323232, "width": 0.39255228758169936, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7390694444444444, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7529065656565657, "width": 0.05348856209150332, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2737"}, {"text": "Figure 5 shows two examples of images that do not satisfy the decluttering principles, along with approximations of Glovers suggested outlining to emphasize contrast around the subject [20].", "label": "Novelty", "bboxes": [{"left": 0.10418464052287582, "top": 0.5412689393939394, "width": 0.37625816993464056, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5551060606060606, "width": 0.3950163398692811, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5689431818181818, "width": 0.3700343137254902, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2738"}, {"text": "We note that while this overlay is helpful for identifying potential issues along the subject-background boundary, it does not help to draw any attention towards potential clutter to address along image borders.", "label": "Novelty", "bboxes": [{"left": 0.4615571895424837, "top": 0.5689431818181818, "width": 0.01890849673202616, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5827790404040404, "width": 0.39502450980392156, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5966161616161616, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6104532828282828, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.624290404040404, "width": 0.04843464052287581, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2739"}, {"text": "The video would automatically loop, but participants also had controls to pause and play.", "label": "Novelty", "bboxes": [{"left": 0.8148153594771241, "top": 0.7621603535353535, "width": 0.09725653594771244, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.7759974747474748, "width": 0.3925408496732027, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.7898345959595959, "width": 0.02864705882352947, "height": 0.011320707070707092, "page": 5}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2740"}, {"text": "We chose to also include a few videos to show the process of making adjustments while framing a photo, to mimic how the overlay might appear if being used in the camera.", "label": "Novelty", "bboxes": [{"left": 0.7293709150326797, "top": 0.8313459595959597, "width": 0.18324673202614394, "height": 0.011320707070707092, "page": 5}, {"left": 0.5191683006535948, "top": 0.8451830808080808, "width": 0.3929117647058823, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.8590202020202021, "width": 0.3925408496732027, "height": 0.011320707070706981, "page": 5}, {"left": 0.5195343137254902, "top": 0.8728573232323232, "width": 0.04645588235294129, "height": 0.011320707070707092, "page": 5}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2741"}, {"text": "Given the elements supported by the qualitative feedback, (f) appeared to be the best candidate for our overlayit included the color-coded line drawings to help defne the subject and possible distractors along the borders, while being less noisy than showing all outlines.", "label": "Novelty", "bboxes": [{"left": 0.5358137254901961, "top": 0.6076919191919191, "width": 0.37715522875816987, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6215290404040403, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6353661616161616, "width": 0.39255882352941185, "height": 0.011320707070707203, "page": 6}, {"left": 0.5195343137254902, "top": 0.6492032828282828, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.0717908496732026, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2742"}, {"text": "However, since (d) was the most popular based on overall counts, we decided to go with a hybrid approach for our fnal abstraction overlay, enabling participants to switch back and forth between showing all lines, including those within the subject and background for additional context, and hiding these extra lines for minimal distraction.", "label": "Novelty", "bboxes": [{"left": 0.5963316993464052, "top": 0.6630404040404041, "width": 0.31576470588235295, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.676875, "width": 0.3928316993464054, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6907146464646465, "width": 0.392563725490196, "height": 0.011320707070706981, "page": 6}, {"left": 0.5195343137254902, "top": 0.7045517676767676, "width": 0.39255718954248375, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.718388888888889, "width": 0.39256209150326793, "height": 0.011320707070706981, "page": 6}, {"left": 0.5195343137254902, "top": 0.7322260101010101, "width": 0.14158660130718959, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2743"}, {"text": "However, a few (3) noted that this fattening caused blurring that made the subject unclear.", "label": "Novelty", "bboxes": [{"left": 0.5195343137254902, "top": 0.5108333333333334, "width": 0.39256535947712423, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.5246704545454546, "width": 0.14570424836601303, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2744"}, {"text": "Many (10) participants also mentioned that the darkening of the image was particularly helpful for seeing the lines due to the contrast, but with the caveat that it made the original image harder to see.", "label": "Novelty", "bboxes": [{"left": 0.5358137254901961, "top": 0.317114898989899, "width": 0.3762745098039215, "height": 0.011320707070707037, "page": 6}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.39255555555555544, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.3447891414141414, "width": 0.39283496732026146, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.3586262626262626, "width": 0.03891339869281052, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2745"}, {"text": "However, they also noted that outlining everything can be noisy (9), and especially did not want too many lines within the subject (6).", "label": "Novelty", "bboxes": [{"left": 0.7612761437908497, "top": 0.37246338383838384, "width": 0.1508169934640522, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.38630050505050506, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 6}, {"left": 0.5189918300653594, "top": 0.4001376262626263, "width": 0.2565686274509804, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2746"}, {"text": "One participant very clearly summarized the potential reasoning for this: if the most important thing for subject background separation is the contrast, then the overlays should try to mute lines as much as possible.", "label": "Novelty", "bboxes": [{"left": 0.3555800653594771, "top": 0.8429217171717173, "width": 0.12527287581699342, "height": 0.011320707070706981, "page": 6}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.392562091503268, "height": 0.011343434343434433, "page": 6}, {"left": 0.08790522875816993, "top": 0.8706161616161616, "width": 0.3941617647058824, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8844545454545455, "width": 0.3585980392156863, "height": 0.011320707070706981, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2747"}, {"text": "However, we had yet to try these overlays in an interactive manner.", "label": "Novelty", "bboxes": [{"left": 0.8570473856209151, "top": 0.8014103535353535, "width": 0.05666013071895415, "height": 0.011320707070707092, "page": 6}, {"left": 0.5189918300653594, "top": 0.8152474747474748, "width": 0.35448529411764707, "height": 0.011320707070707092, "page": 6}], "section": "4 IMPLEMENTATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2748"}, {"text": "subject is still well separated, then you know you have the contrast.", "label": "Novelty", "bboxes": [{"left": 0.5195343137254902, "top": 0.10958333333333334, "width": 0.39416666666666655, "height": 0.011320707070707065, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2749"}, {"text": "If theres no contrast, then your eye doesnt notice objects on the border.", "label": "Novelty", "bboxes": [{"left": 0.7962418300653595, "top": 0.12342045454545454, "width": 0.11744771241830065, "height": 0.011320707070707078, "page": 6}, {"left": 0.5195343137254902, "top": 0.13725757575757574, "width": 0.29499346405228755, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2750"}, {"text": "However, overall", "label": "Novelty", "bboxes": [{"left": 0.13287418300653595, "top": 0.718388888888889, "width": 0.09934313725490196, "height": 0.011320707070706981, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2751"}, {"text": "However, we want to be able to determine the relevant context in order to focus on edges related to SBS and IBF.", "label": "Novelty", "bboxes": [{"left": 0.7263137254901961, "top": 0.40475, "width": 0.18577777777777782, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.4185871212121212, "width": 0.3925392156862745, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.43242424242424243, "width": 0.0913643790849673, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2752"}, {"text": "We liked how the diference in scale resulted in a range of complexity while composing.", "label": "Novelty", "bboxes": [{"left": 0.3148235294117647, "top": 0.593854797979798, "width": 0.16562254901960788, "height": 0.011320707070706981, "page": 8}, {"left": 0.08790522875816993, "top": 0.6076919191919191, "width": 0.3721029411764706, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2753"}, {"text": "To modify this task design to work for remote studies, we decided to keep the structure of having 3 tasks per condition at diferent scales (small, medium, and large), but instead of specifying the subjects, we asked that participants choose their own subjects to photograph.", "label": "Novelty", "bboxes": [{"left": 0.4649248366013072, "top": 0.6076919191919191, "width": 0.015526143790849711, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.6215265151515151, "width": 0.392545751633987, "height": 0.011320707070706981, "page": 8}, {"left": 0.08790522875816993, "top": 0.6353598484848485, "width": 0.392545751633987, "height": 0.011320707070707203, "page": 8}, {"left": 0.08790522875816993, "top": 0.6491931818181818, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.39253758169934644, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.07291503267973855, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2754"}, {"text": "They were told that following these guidelines would be helpful in telling a clear story in their photos, but also that these guidelines are just to provide some possible perspectives to consider and that participants are by no means required to follow them (e.g., if communicating their story involved intentionally having the subject blend into the background they should do so).", "label": "Novelty", "bboxes": [{"left": 0.7942794117647058, "top": 0.5228838383838385, "width": 0.1178022875816993, "height": 0.011320707070706981, "page": 8}, {"left": 0.5195343137254902, "top": 0.536719696969697, "width": 0.3929183006535948, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.5505568181818182, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.5643939393939394, "width": 0.39292647058823527, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.5782310606060607, "width": 0.3929183006535948, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.5920656565656566, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.605905303030303, "width": 0.11229411764705888, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2755"}, {"text": "We encouraged them to explore the process of framing the image, but to limit each task to around 1-2 minutes.", "label": "Novelty", "bboxes": [{"left": 0.7713954248366013, "top": 0.6474166666666666, "width": 0.1406797385620916, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.6612537878787879, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.6750909090909091, "width": 0.12286437908496739, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2756"}, {"text": "Therefore we see that while participants are more confdent in their ability to take clear photos, and found the tool helpful for achieving their favorite resulting photos, they didnt necessarily fnd that their photos were better with regard to their personal preferences or the decluttering principles.", "label": "Novelty", "bboxes": [{"left": 0.5358137254901961, "top": 0.275604797979798, "width": 0.37625980392156855, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.2894419191919192, "width": 0.3928153594771241, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.30327651515151516, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3171098484848485, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.2501111111111112, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2757"}, {"text": "Thus, even though we did not fnd signifcant changes in CSI, we decided that we should compare against a baseline that provided a little more assistance.", "label": "Novelty", "bboxes": [{"left": 0.10418464052287582, "top": 0.6583712121212121, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6722083333333333, "width": 0.39253758169934644, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6860454545454545, "width": 0.1452941176470588, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2758"}, {"text": "As we described earlier, photographers will sometimes use a grayscale display in their current practice, to help emphasize contrast in order to consider overall clarity and decluttering.", "label": "Novelty", "bboxes": [{"left": 0.23811601307189542, "top": 0.6860454545454545, "width": 0.2423349673202614, "height": 0.011320707070707092, "page": 9}, {"left": 0.08736437908496732, "top": 0.6998825757575757, "width": 0.39307843137254905, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.713719696969697, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.7275568181818182, "width": 0.07524346405228759, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2759"}, {"text": "Though we actually did not see a signifcant improvement in overall self-assessed quality in terms of these principles, participants did believe that the tool was helpful for the task of capturing clear and decluttered images (Mdn = 6, IQR = 5-6), versus no guidance (Mdn = 4, IQR = 4-6) [ V = 134, p = . 003].", "label": "Novelty", "bboxes": [{"left": 0.7431601307189543, "top": 0.1925820707070707, "width": 0.1689199346405228, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.20641666666666666, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.22025631313131314, "width": 0.39255065359477126, "height": 0.011320707070707037, "page": 9}, {"left": 0.5195343137254902, "top": 0.23409343434343433, "width": 0.39254411764705877, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.24529292929292928, "width": 0.39417156862745095, "height": 0.013969696969696965, "page": 9}, {"left": 0.5188022875816993, "top": 0.2591300505050505, "width": 0.05838235294117655, "height": 0.013969696969696965, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2760"}, {"text": "However, even with just 5 participants we were seeing signifcant results suggesting that our tool was preferable to no guidance for these photo tasks.", "label": "Novelty", "bboxes": [{"left": 0.3926045751633987, "top": 0.436979797979798, "width": 0.08783823529411766, "height": 0.011320707070706981, "page": 9}, {"left": 0.08736437908496732, "top": 0.45081691919191924, "width": 0.3955653594771242, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.46465277777777775, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.4784873737373737, "width": 0.034145424836601296, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2761"}, {"text": "Another participant described that the external representation provided by the interface assisted in the process of exploring the scene and quickly evaluating diferent options: It caused me to experiment more... didnt see it as a rule that I needed to minimize lines, but the tool made it easy to move around and check by that metric, how good it was (P9).", "label": "Novelty", "bboxes": [{"left": 0.5358137254901961, "top": 0.75989898989899, "width": 0.37626960784313734, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7737361111111111, "width": 0.3925375816993464, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7875732323232324, "width": 0.39256045751633983, "height": 0.011343434343434211, "page": 9}, {"left": 0.5195343137254902, "top": 0.8014330808080808, "width": 0.3925408496732028, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.815270202020202, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.8290845959595959, "width": 0.1808725490196078, "height": 0.011343434343434433, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2762"}, {"text": "We also did not fnd signifcant diferences in CSI, but did fnd support for the increased confdence and descriptions of how the tool encouraged participants to explore more in the qualitative feedback.", "label": "Novelty", "bboxes": [{"left": 0.7312189542483659, "top": 0.3590290404040404, "width": 0.18085947712418315, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.37286363636363634, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3866969696969697, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.40054040404040403, "width": 0.19635457516339871, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2763"}, {"text": "We ran remote studies over Zoom with 18 participants (6 male, 10 female), 24 to 32 years old (  = 29), to understand if the tool would help users declutter photos, and if users felt creative while using the tool.", "label": "Novelty", "bboxes": [{"left": 0.08720261437908497, "top": 0.8152474747474748, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8264469696969697, "width": 0.39254901960784316, "height": 0.013969696969696965, "page": 9}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.392545751633987, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.04986111111111112, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2764"}, {"text": "Participants described feeling like they could take fewer photos, because they could be more confdent in each photo they took: Usually when I take photos, I take a ton at once, but didnt do that here. I didnt need to because I was being so precise. I noticed myself reconsidering the composition more: e.g. should I have these things in the edges? (P15).", "label": "Novelty", "bboxes": [{"left": 0.5995686274509804, "top": 0.4556868686868687, "width": 0.3125098039215686, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.4695239898989899, "width": 0.3925359477124184, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.48336111111111113, "width": 0.39255882352941185, "height": 0.011343434343434322, "page": 9}, {"left": 0.5195343137254902, "top": 0.497219696969697, "width": 0.3925441176470589, "height": 0.011320707070707037, "page": 9}, {"left": 0.5195343137254902, "top": 0.5110568181818181, "width": 0.39255228758169936, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.5248712121212121, "width": 0.15625980392156857, "height": 0.011343434343434322, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2765"}, {"text": "On the other hand, another participant (P18) had a slightly different interpretation of the lack of a consistent subject or in this case, no identifed subject.", "label": "Novelty", "bboxes": [{"left": 0.10418464052287582, "top": 0.6671426767676767, "width": 0.37873529411764717, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.680979797979798, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.6948169191919191, "width": 0.16211928104575163, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2766"}, {"text": "In the frst case, contrast is goodso it should be subdued to guarantee the contrast is still clear without details.", "label": "Novelty", "bboxes": [{"left": 0.8779656862745099, "top": 0.5981136363636363, "width": 0.03411274509803919, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.6119507575757576, "width": 0.39254575163398686, "height": 0.011320707070706981, "page": 10}, {"left": 0.5195343137254902, "top": 0.6257878787878788, "width": 0.23100980392156867, "height": 0.011320707070706981, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.1 Other Abstraction Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2767"}, {"text": "In the second case, contrast is badso it should be emphasized to make sure the photographer realizes its presence and has a chance to remove it from the frame.", "label": "Novelty", "bboxes": [{"left": 0.753501633986928, "top": 0.6257878787878788, "width": 0.15857679738562103, "height": 0.011320707070706981, "page": 10}, {"left": 0.5195343137254902, "top": 0.639625, "width": 0.3928218954248365, "height": 0.011320707070706981, "page": 10}, {"left": 0.5195343137254902, "top": 0.6534621212121212, "width": 0.39479738562091515, "height": 0.011320707070707092, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.1 Other Abstraction Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2768"}, {"text": "to remove the stove, but notices the dark panel in the background, which she doesnt want.", "label": "Novelty", "bboxes": [{"left": 0.08790522875816993, "top": 0.4044406565656566, "width": 0.3941617647058823, "height": 0.011320707070707092, "page": 10}, {"left": 0.08736437908496732, "top": 0.4182777777777778, "width": 0.1494330065359477, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2769"}, {"text": "She turns the camera back towards the white wall, but notices the high contrast object at the top left entering her shot.", "label": "Novelty", "bboxes": [{"left": 0.2409967320261438, "top": 0.4182777777777778, "width": 0.23945098039215684, "height": 0.011320707070707092, "page": 10}, {"left": 0.08736437908496732, "top": 0.432114898989899, "width": 0.39555882352941174, "height": 0.011320707070707092, "page": 10}, {"left": 0.08791013071895425, "top": 0.445949494949495, "width": 0.07363888888888888, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2770"}, {"text": "However, notice how through this process, this choice became much more intentional as she was aware of the alternative options she had in this space and decided that this best suited her preferences.", "label": "Novelty", "bboxes": [{"left": 0.23069444444444445, "top": 0.4736161616161616, "width": 0.25136764705882353, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.4874621212121212, "width": 0.39253921568627453, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.5012992424242424, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.5151363636363636, "width": 0.13551633986928108, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2771"}, {"text": "However, it can be easy for photographers to miss this clutter in the moment while paying attention to details in the subject.", "label": "Novelty", "bboxes": [{"left": 0.5195343137254902, "top": 0.5020277777777777, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.515864898989899, "width": 0.3392222222222221, "height": 0.011320707070707092, "page": 11}], "section": "7 CONCLUSION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2772"}, {"text": "However through the design prototyping and user studies, weve noticed that decluttering seems difcult for people to address, perhaps because it is a new and unfamiliar concept.", "label": "Novelty", "bboxes": [{"left": 0.3117565359477124, "top": 0.7737361111111111, "width": 0.168704248366013, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.7875732323232324, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.8014103535353535, "width": 0.39503431372549025, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.09830882352941177, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2773"}, {"text": "As a result, people still made mistakes even while being aware of specifc clutter to consider and carefully refning their photos.", "label": "Novelty", "bboxes": [{"left": 0.18980392156862744, "top": 0.8152474747474748, "width": 0.29066339869281044, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.07358496732026142, "height": 0.011320707070706981, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2774"}, {"text": "While there isnt necessarily a clear defnition of failure, the main failure case with our current algorithm is when the main subject is not properly captured in the saliency map.", "label": "Novelty", "bboxes": [{"left": 0.10418464052287582, "top": 0.28252272727272726, "width": 0.3762745098039215, "height": 0.011320707070707037, "page": 11}, {"left": 0.08791013071895425, "top": 0.2963573232323232, "width": 0.3925637254901961, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.3101969696969697, "width": 0.30394444444444446, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2775"}, {"text": "However, even this incorrect information can be informative to the user.", "label": "Novelty", "bboxes": [{"left": 0.4255996732026144, "top": 0.33787121212121213, "width": 0.05647385620915035, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.35170833333333335, "width": 0.38000816993464054, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2776"}, {"text": "(If you dont have a preference between a few similar overlays, feel free to pick up to 3)  (Optional) You will be asked to explain your preferences at the end of the survey, but feel free to explain any specifc thoughts you have based on this image/video here.", "label": "Novelty", "bboxes": [{"left": 0.30324509803921573, "top": 0.48441666666666666, "width": 0.17722058823529407, "height": 0.011320707070707092, "page": 6}, {"left": 0.1277892156862745, "top": 0.4982525252525252, "width": 0.33478921568627457, "height": 0.011320707070707037, "page": 6}, {"left": 0.11398856209150326, "top": 0.511169191919192, "width": 0.36647385620915035, "height": 0.012241161616161622, "page": 6}, {"left": 0.1277892156862745, "top": 0.5259267676767677, "width": 0.3526781045751634, "height": 0.011320707070707092, "page": 6}, {"left": 0.1277892156862745, "top": 0.5397638888888889, "width": 0.30390522875817, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2777"}, {"text": "In this work, our main focus is on the design process and the prototype appour goal is not to propose an exact abstraction interface, but to understand how this style of in-camera interaction might infuence users photographic process, specifcally with regards to decluttering.", "label": "Objective", "bboxes": [{"left": 0.6321372549019608, "top": 0.6718497474747475, "width": 0.2799591503267973, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.685685606060606, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6995227272727274, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7133573232323233, "width": 0.39293627450980395, "height": 0.011320707070707092, "page": 1}, {"left": 0.5189918300653594, "top": 0.7271969696969698, "width": 0.1704934640522876, "height": 0.011320707070706981, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2778"}, {"text": "Focus peaking is relevant to our work due to its methods of visualizationit similarly highlights subsets of edges in the images, but instead for the purpose of highlighting regions that are in focus [2, 32].", "label": "Objective", "bboxes": [{"left": 0.16690359477124184, "top": 0.70414898989899, "width": 0.31603267973856214, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.7179861111111111, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.7318232323232324, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.7456578282828283, "width": 0.09034313725490194, "height": 0.01132323232323229, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2779"}, {"text": "The goal of this line of work tends to be to serve one of two purposes, generating an artistic result, or reducing data for visual communication [22].", "label": "Objective", "bboxes": [{"left": 0.10418464052287582, "top": 0.1925820707070707, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.20641919191919192, "width": 0.3925555555555555, "height": 0.011320707070707037, "page": 2}, {"left": 0.08790522875816993, "top": 0.22025631313131314, "width": 0.12822222222222224, "height": 0.011320707070707037, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2780"}, {"text": "Both are useful for our goal of designing visual overlays that are friendly for users to process interactively.", "label": "Objective", "bboxes": [{"left": 0.22155228758169934, "top": 0.22025631313131314, "width": 0.2589183006535948, "height": 0.011320707070707037, "page": 2}, {"left": 0.08753921568627451, "top": 0.23409343434343433, "width": 0.3951862745098039, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2781"}, {"text": "Given the observations from our low-fdelity prototype, we were motivated to continue with this concept and move onto the step of answering the second question: What is an abstraction of a photo?", "label": "Objective", "bboxes": [{"left": 0.5195343137254902, "top": 0.4595542929292929, "width": 0.3925408496732027, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.4733914141414141, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.4872260101010101, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5010656565656566, "width": 0.04084313725490207, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2782"}, {"text": "In particular, we saw that the abstraction overlay seemed most helpful for the purposes of noticing unwanted clutter in an image, so we decided that we would target designing an abstraction overlay that provides decluttering guidance.", "label": "Objective", "bboxes": [{"left": 0.654781045751634, "top": 0.5149027777777778, "width": 0.2572941176470589, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.528739898989899, "width": 0.39254411764705877, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5425770202020203, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5564141414141415, "width": 0.3522385620915033, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2783"}, {"text": "To answer our questions on how to design an abstraction overlay for decluttering, we looked to existing literature to better understand how photographers think about directing the viewers attention for efective storytelling [9, 20, 30].", "label": "Objective", "bboxes": [{"left": 0.7007532679738562, "top": 0.5800189393939394, "width": 0.21133823529411766, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.593854797979798, "width": 0.3925441176470589, "height": 0.011320707070706981, "page": 3}, {"left": 0.5195343137254902, "top": 0.6076919191919191, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.6215290404040403, "width": 0.3946633986928104, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2784"}, {"text": "Specifcally, since we knew our goal would be to display the overlay interactively in the camera, we focused on Winnemller et al.", "label": "Objective", "bboxes": [{"left": 0.13295588235294117, "top": 0.8429217171717173, "width": 0.34749019607843135, "height": 0.011320707070706981, "page": 4}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.39255065359477126, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.02210130718954248, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2785"}, {"text": "We additionally give ourselves line color (for the purposes of color-coding lines based on their location context), and image darkening (to provide more contrast against the line drawings) as parameters to consider in designing abstraction overlays.", "label": "Objective", "bboxes": [{"left": 0.6719754901960785, "top": 0.6975580808080808, "width": 0.2403758169934641, "height": 0.011320707070707092, "page": 4}, {"left": 0.5190996732026144, "top": 0.7113926767676768, "width": 0.39545424836601306, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7252323232323232, "width": 0.39255228758169936, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7390694444444444, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7529065656565657, "width": 0.05348856209150332, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2786"}, {"text": "After each condition, participants were asked to complete surveys with a number of Likert questions (on a 7-point scale) about their experience using the tool along with the Creativity Support Index (CSI) questions (0 to 100) [13].", "label": "Objective", "bboxes": [{"left": 0.5358137254901961, "top": 0.7304381313131313, "width": 0.37874183006535944, "height": 0.011320707070707092, "page": 8}, {"left": 0.5191683006535948, "top": 0.7442752525252525, "width": 0.3929101307189542, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.7581123737373737, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.7719494949494949, "width": 0.22514542483660138, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2787"}, {"text": "Thus upon capturing the fnal photo, she has intentionally refned some details that were brought to her attention by the overlay, and is therefore more confdent that this photo achieves her goals.", "label": "Objective", "bboxes": [{"left": 0.5736503267973856, "top": 0.5802171717171717, "width": 0.3388055555555556, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.5940568181818182, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6078914141414141, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6217310606060606, "width": 0.057751633986928064, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2788"}, {"text": "study [17], with questions adapted to highlight decluttering principles rather than composition.", "label": "Objective", "bboxes": [{"left": 0.08790522875816993, "top": 0.2848686868686869, "width": 0.395014705882353, "height": 0.011320707070707037, "page": 9}, {"left": 0.08790522875816993, "top": 0.29870580808080804, "width": 0.17384150326797387, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2789"}, {"text": "He decided that since the large scale task shouldnt have a single focal subject, the lack of yellow aligned with the expectations of this goal.", "label": "Objective", "bboxes": [{"left": 0.25447875816993465, "top": 0.6948169191919191, "width": 0.22597222222222219, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.7086527777777778, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 10}, {"left": 0.08736437908496732, "top": 0.722489898989899, "width": 0.2026290849673203, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2790"}, {"text": "In fact, a few participants observed that the goals difered between fnding clear separation between the subject and background and fnding distracting constrast in the background/along the image border.", "label": "Objective", "bboxes": [{"left": 0.8355245098039216, "top": 0.5566022727272727, "width": 0.07709803921568625, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.5704393939393939, "width": 0.3928267973856209, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.5842765151515151, "width": 0.3950114379084967, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.5981136363636363, "width": 0.3550424836601307, "height": 0.011320707070707092, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.1 Other Abstraction Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2791"}, {"text": "Inspired by the use of sketching in design to capture higher level structure, our goal was to bring some of the benefts of the abstraction in a sketch-like representation to photography.", "label": "Objective", "bboxes": [{"left": 0.8624264705882352, "top": 0.515864898989899, "width": 0.049663398692810445, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5297020202020202, "width": 0.39416830065359487, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5435391414141414, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5573762626262626, "width": 0.2483790849673203, "height": 0.011320707070707092, "page": 11}], "section": "7 CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2792"}, {"text": "The goal in pursuing this design is to give the user creative fexibility and avoiding having the system express its opinion on how the fnal image should look [17].", "label": "Objective", "bboxes": [{"left": 0.12251633986928105, "top": 0.7460606060606061, "width": 0.35794607843137255, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.75989898989899, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.7737361111111111, "width": 0.22018300653594775, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2793"}, {"text": "Through our design process, we aimed to answer two questions: (1) Will an abstracted visualization be efective in encouraging the user to see parts of the image outside of the main subject?", "label": "Objective", "bboxes": [{"left": 0.5358137254901961, "top": 0.3617714646464647, "width": 0.37801797385620906, "height": 0.011320707070706981, "page": 2}, {"left": 0.51909477124183, "top": 0.3756085858585859, "width": 0.3930032679738562, "height": 0.011320707070706981, "page": 2}, {"left": 0.5195343137254902, "top": 0.389445707070707, "width": 0.3600441176470588, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2794"}, {"text": "Our fnal design uses object-based saliency and edge detection to highlight contrast along subject and image borders, outlining potential distractors in these regions.", "label": "Method", "bboxes": [{"left": 0.7099232026143791, "top": 0.5848876262626262, "width": 0.20215359477124184, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5987247474747475, "width": 0.39254411764705877, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.6125618686868687, "width": 0.3697091503267974, "height": 0.011320707070707092, "page": 0}], "section": "", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2795"}, {"text": "We are interested in designing camera interfaces that can encourage users to incorporate these exploratory stages of the design process into their photographic process.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.1649078282828283, "width": 0.3787549019607842, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.19257954545454545, "width": 0.2330882352941177, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2796"}, {"text": "Our fnal annotation design involves color-coded edge highlighting (very similar to the focus peaking feature that can be found on a number of commercial cameras [2, 32]) focused on regions around the subject(s) and image borders where clutter can be most distracting.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.3309520202020202, "width": 0.3787647058823528, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.3447891414141414, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.3586237373737373, "width": 0.39255555555555544, "height": 0.011323232323232402, "page": 1}, {"left": 0.5195343137254902, "top": 0.37246338383838384, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.38630050505050506, "width": 0.06763725490196082, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2797"}, {"text": "We see similarities in this photographic process and the design processphotographers are essentially iterating on their design (or image) in the camera as they consider these diferent aspects of photography and storytelling.", "label": "Method", "bboxes": [{"left": 0.32279411764705884, "top": 0.500455808080808, "width": 0.15767320261437906, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5142929292929292, "width": 0.39256535947712423, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5281300505050505, "width": 0.3929362745098039, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5419671717171717, "width": 0.3856290849673203, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2798"}, {"text": "In order to realize the proposed interaction, we additionally present a proposal of an algorithm for visually annotating potential clutter by highlighting relevant edges around salient objects and around the image border.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.6303383838383838, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6441527777777778, "width": 0.39283660130718945, "height": 0.011343434343434322, "page": 1}, {"left": 0.5195343137254902, "top": 0.6580126262626262, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6718497474747475, "width": 0.10806045751633997, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2799"}, {"text": "We contextualize our work of designing an abstraction-based camera overlay within the most relevant work in image manipulation and camera guidance.", "label": "Method", "bboxes": [{"left": 0.5188316993464052, "top": 0.7739381313131313, "width": 0.3957385620915034, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.78777398989899, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.801611111111111, "width": 0.12963235294117648, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2800"}, {"text": "Specifcally in this paper, we contribute:", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.48190151515151514, "width": 0.23960294117647052, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2801"}, {"text": "Most similar to our work is the focus peaking feature in commercial cameras and the work of E et al.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.6764747474747476, "width": 0.3787549019607843, "height": 0.011320707070707092, "page": 2}, {"left": 0.08791013071895425, "top": 0.6903093434343435, "width": 0.25865032679738564, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2802"}, {"text": "We havent seen work that focuses on helping users declutter their compositions in the camera.", "label": "Method", "bboxes": [{"left": 0.38518790849673207, "top": 0.6349633838383839, "width": 0.09527941176470583, "height": 0.011320707070707092, "page": 2}, {"left": 0.08736437908496732, "top": 0.6488005050505051, "width": 0.39309640522875816, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6626376262626262, "width": 0.06918137254901961, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2803"}, {"text": "Both are useful for our goal of designing visual overlays that are friendly for users to process interactively.", "label": "Method", "bboxes": [{"left": 0.22155228758169934, "top": 0.22025631313131314, "width": 0.2589183006535948, "height": 0.011320707070707037, "page": 2}, {"left": 0.08753921568627451, "top": 0.23409343434343433, "width": 0.3951862745098039, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2804"}, {"text": "In this section, we describe the steps we took to design our abstraction interface for decluttering images.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.2787487373737374, "width": 0.39502777777777776, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.2925858585858586, "width": 0.25750816993464054, "height": 0.011320707070707037, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2805"}, {"text": "To test out the concept of abstraction guidance, we started with a low-fdelity Wizard-of-Oz (WoZ) prototype [14] where experimenters manually drew abstraction overlays.", "label": "Method", "bboxes": [{"left": 0.51909477124183, "top": 0.47561237373737375, "width": 0.39299673202614394, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195277777777778, "top": 0.4894469696969697, "width": 0.395047385620915, "height": 0.011323232323232346, "page": 2}, {"left": 0.5195310457516339, "top": 0.5032840909090909, "width": 0.27888398692810457, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2806"}, {"text": "We look to extend this idea by designing visual overlays that help the user identify clutter.", "label": "Method", "bboxes": [{"left": 0.8461960784313725, "top": 0.1510719696969697, "width": 0.06589379084967328, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.39283169934640527, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.08914869281045756, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2807"}, {"text": "We informally tested our low-fdelity prototype with 19 participants (9 male, 10 female), 18 to 41 years old (  = 24).", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.5586338383838384, "width": 0.3787581699346404, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.5698333333333333, "width": 0.3122205882352942, "height": 0.013969696969697076, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2808"}, {"text": "From our WoZ prototype studies, we saw promising signs that the participants in fact noticed high level", "label": "Method", "bboxes": [{"left": 0.6788071895424836, "top": 0.8705959595959595, "width": 0.23328921568627448, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2809"}, {"text": "Given the observations from our low-fdelity prototype, we were motivated to continue with this concept and move onto the step of answering the second question: What is an abstraction of a photo?", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.4595542929292929, "width": 0.3925408496732027, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.4733914141414141, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.4872260101010101, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5010656565656566, "width": 0.04084313725490207, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2810"}, {"text": "Note that while these participants behaviors matched what we hoped for, there were limitations to our study design such that they cannot be directly mapped to how a user might respond to seeing this style of overlay interactively in the camera.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.31581944444444443, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.3296565656565657, "width": 0.3929183006535949, "height": 0.011320707070707037, "page": 3}, {"left": 0.5195343137254902, "top": 0.3434936868686869, "width": 0.3925392156862745, "height": 0.011320707070707037, "page": 3}, {"left": 0.5195343137254902, "top": 0.357330808080808, "width": 0.2899150326797386, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2811"}, {"text": "Our eyes are drawn to regions of high contrast.", "label": "Method", "bboxes": [{"left": 0.6131372549019608, "top": 0.6492032828282828, "width": 0.3011977124183006, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2812"}, {"text": "Again, this is better known as edge ficker, but we chose to specify image border to diferentiate it from the term edge, as edge is", "label": "Method", "bboxes": [{"left": 0.5190212418300654, "top": 0.8705959595959595, "width": 0.3934395424836602, "height": 0.011320707070707092, "page": 3}, {"left": 0.5178790849673203, "top": 0.8844318181818183, "width": 0.3941993464052288, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2813"}, {"text": "To answer our questions on how to design an abstraction overlay for decluttering, we looked to existing literature to better understand how photographers think about directing the viewers attention for efective storytelling [9, 20, 30].", "label": "Method", "bboxes": [{"left": 0.7007532679738562, "top": 0.5800189393939394, "width": 0.21133823529411766, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.593854797979798, "width": 0.3925441176470589, "height": 0.011320707070706981, "page": 3}, {"left": 0.5195343137254902, "top": 0.6076919191919191, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.6215290404040403, "width": 0.3946633986928104, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2814"}, {"text": "Given these principles, we wondered what annotation methods photographers currently used for highlighting clutter.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.41673611111111114, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.43057323232323236, "width": 0.3299183006535948, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2815"}, {"text": "We additionally were inspired by a line of research in nonphotorealistic rendering to generate stylized image abstractions [15, 22, 36].", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.8152474747474748, "width": 0.37873856209150325, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.394171568627451, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.04186274509803921, "height": 0.011320707070706981, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2816"}, {"text": "Inspired by this idea of using outlines to highlight contrast and the lack of contrast, we hoped to recreate this outlining as an overlay directly in the camera (see Figure 5), while also extending it to contrast along the image borders.", "label": "Method", "bboxes": [{"left": 0.33908660130718954, "top": 0.6492032828282828, "width": 0.1413643790849673, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6630378787878788, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 4}, {"left": 0.0874656862745098, "top": 0.6907146464646465, "width": 0.39298529411764704, "height": 0.011320707070706981, "page": 4}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.0474656862745098, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2817"}, {"text": "Combining concepts from these two abstraction ideas, we have three components in total to consider: line drawing, location context, and color fattening.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.6698838383838384, "width": 0.37625980392156844, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6837209595959596, "width": 0.3950245098039217, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6975580808080808, "width": 0.14877450980392148, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2818"}, {"text": "We note that while this overlay is helpful for identifying potential issues along the subject-background boundary, it does not help to draw any attention towards potential clutter to address along image borders.", "label": "Method", "bboxes": [{"left": 0.4615571895424837, "top": 0.5689431818181818, "width": 0.01890849673202616, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5827790404040404, "width": 0.39502450980392156, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5966161616161616, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6104532828282828, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.624290404040404, "width": 0.04843464052287581, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2819"}, {"text": "Adjusting these parameters, we designed and implemented a range of potential overlay proposals.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.7667424242424242, "width": 0.37625816993464045, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7805795454545456, "width": 0.21088071895424831, "height": 0.011320707070706981, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2820"}, {"text": "We chose to also include a few videos to show the process of making adjustments while framing a photo, to mimic how the overlay might appear if being used in the camera.", "label": "Method", "bboxes": [{"left": 0.7293709150326797, "top": 0.8313459595959597, "width": 0.18324673202614394, "height": 0.011320707070707092, "page": 5}, {"left": 0.5191683006535948, "top": 0.8451830808080808, "width": 0.3929117647058823, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.8590202020202021, "width": 0.3925408496732027, "height": 0.011320707070706981, "page": 5}, {"left": 0.5195343137254902, "top": 0.8728573232323232, "width": 0.04645588235294129, "height": 0.011320707070707092, "page": 5}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2821"}, {"text": "We ran an informal design survey through Qualtrics with 29 participants (demographics information not collected) to try to understand if these overlay visualizations were interpretable by novice photographers, and if there were strong preferences between the overlay options.", "label": "Method", "bboxes": [{"left": 0.2217156862745098, "top": 0.8152474747474748, "width": 0.25874183006535956, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.39501307189542484, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.3925457516339869, "height": 0.011320707070706981, "page": 5}, {"left": 0.08790522875816993, "top": 0.8567563131313132, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.24487581699346406, "height": 0.011320707070707092, "page": 5}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2822"}, {"text": "Given the elements supported by the qualitative feedback, (f) appeared to be the best candidate for our overlayit included the color-coded line drawings to help defne the subject and possible distractors along the borders, while being less noisy than showing all outlines.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.6076919191919191, "width": 0.37715522875816987, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6215290404040403, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6353661616161616, "width": 0.39255882352941185, "height": 0.011320707070707203, "page": 6}, {"left": 0.5195343137254902, "top": 0.6492032828282828, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.0717908496732026, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2823"}, {"text": "Both color fattening and image darkening made the image less clear, so we wanted our fnal overlay to only have one of these components.", "label": "Method", "bboxes": [{"left": 0.6679379084967321, "top": 0.5246704545454546, "width": 0.24415849673202605, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.5385075757575758, "width": 0.39294281045751644, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.552344696969697, "width": 0.17990522875816994, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2824"}, {"text": "Therefore we decided line drawing should be part of our fnal overlay design.", "label": "Method", "bboxes": [{"left": 0.545983660130719, "top": 0.24793055555555554, "width": 0.36610947712418296, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.26176767676767676, "width": 0.09069934640522881, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2825"}, {"text": "Results of both steps of our design prototyping process made us hopeful of the potential of an abstraction-based overlay.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.7875732323232324, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8014103535353535, "width": 0.3338431372549021, "height": 0.011320707070707092, "page": 6}], "section": "4 IMPLEMENTATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2826"}, {"text": "They are also informed of our overall goalto determine which overlay is best for evaluating images based on the two decluttering guidelines.", "label": "Method", "bboxes": [{"left": 0.37722058823529414, "top": 0.35587247474747474, "width": 0.10571241830065359, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.36970959595959596, "width": 0.39284150326797385, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.3835467171717172, "width": 0.36827941176470586, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2827"}, {"text": "Finally at the end of the survey, we asked participants to summarize their choices: Please provide a brief explanation for your choices why did you fnd these overlays most helpful? Are there specifc characteristics of the overlays that you like (e.g., line drawing, color fattening, image darkening, or color)?", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.5576111111111111, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5714457070707071, "width": 0.39418464052287583, "height": 0.011320707070707092, "page": 6}, {"left": 0.08736437908496732, "top": 0.5852853535353535, "width": 0.3931013071895425, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5991224747474747, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.6129595959595959, "width": 0.24230065359477126, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2828"}, {"text": "Table 1 presents the results from our design survey.", "label": "Method", "bboxes": [{"left": 0.2654705882352941, "top": 0.6907146464646465, "width": 0.21526960784313726, "height": 0.011320707070706981, "page": 6}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.0867434640522876, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2829"}, {"text": "We choose the most abstracted form of the overlay (solid black, minimal edges) as the default as we imagine users starting in a more", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.8705959595959595, "width": 0.37788071895424835, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8844318181818183, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2830"}, {"text": "Figure 8 walks through our algorithm for generating our abstraction overlay.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.37707575757575756, "width": 0.3950196078431373, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.3909128787878788, "width": 0.07646568627450978, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2831"}, {"text": "To implement this, our fnal camera tool has 3 layers: the camera view, a black layer of varying degrees of opacity, and a color-coded outlines layer.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.5108333333333334, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 7}, {"left": 0.08754411764705881, "top": 0.5246679292929293, "width": 0.3929035947712418, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.5385075757575758, "width": 0.08537745098039216, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2832"}, {"text": "Given this saliency map, we segment the image into regions describing the subject, subject border, image border, and remaining background.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.5292828282828282, "width": 0.37625816993464045, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195408496732026, "top": 0.5431174242424243, "width": 0.3950114379084968, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.5569570707070707, "width": 0.09839542483660124, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2833"}, {"text": "We wanted to study how users would react to our abstraction guidance tool.", "label": "Method", "bboxes": [{"left": 0.5188316993464052, "top": 0.8290845959595959, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8429217171717173, "width": 0.08177941176470593, "height": 0.011320707070706981, "page": 7}], "section": "5 USER EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2834"}, {"text": "Our overlay tool is build on top of a basic iOS camera app.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.18796969696969698, "width": 0.3393496732026142, "height": 0.011320707070707037, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.2 Mobile Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2835"}, {"text": "Based on our learnings from the design survey (Section 3.2.3), we chose to go with a design inspired by a combination of overlay options (d) and (f).", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.45548484848484855, "width": 0.39253758169934644, "height": 0.011320707070707037, "page": 7}, {"left": 0.08790522875816993, "top": 0.46932196969696965, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.48315909090909087, "width": 0.11031209150326797, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2836"}, {"text": "We ended the study with open-ended interviews asking about what they liked/disliked about the tool and how the interaction infuenced their thought process as they took photos.", "label": "Method", "bboxes": [{"left": 0.5718594771241831, "top": 0.8272853535353536, "width": 0.3402140522875816, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.8411338383838384, "width": 0.39502614379084977, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.8549709595959595, "width": 0.35224346405228757, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2837"}, {"text": "We ran these user studies over Zoom, asking the participant to adjust the webcam when possible to keep their photographing within view.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.75989898989899, "width": 0.37625816993464056, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.7737361111111111, "width": 0.392547385620915, "height": 0.011320707070707092, "page": 8}, {"left": 0.08736437908496732, "top": 0.7875732323232324, "width": 0.07515359477124182, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2838"}, {"text": "Due to the COVID-19 pandemic, we had to run our studies remotely.", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.5385075757575758, "width": 0.3947990196078432, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2839"}, {"text": "was also captured as we asked participants to think aloud as they captured photos for each task.", "label": "Method", "bboxes": [{"left": 0.5189918300653594, "top": 0.4675353535353535, "width": 0.39346405228758174, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.48137247474747474, "width": 0.18015522875816992, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2840"}, {"text": "After reading the document, we asked that the participants briefy describe the principles in their own words to confrm understanding.", "label": "Method", "bboxes": [{"left": 0.6354803921568627, "top": 0.605905303030303, "width": 0.2765947712418302, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.6197424242424242, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.6335795454545454, "width": 0.13826307189542486, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2841"}, {"text": "When evaluating their favorited photos per task, participants did believe that the photos captured using our tool had better subjectbackground separation (Mdn = 6, IQR = 4-5), than those captured using the no guidance baseline (Mdn = 4, IQR = 2-5) [ V = 0, p = . 003].", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.5338383838383839, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5476755050505051, "width": 0.39501470588235305, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5615126262626263, "width": 0.39254084967320263, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5727121212121212, "width": 0.3925571895424837, "height": 0.013969696969696965, "page": 9}, {"left": 0.0881421568627451, "top": 0.5888888888888889, "width": 0.03333496732026142, "height": 0.011618686868686834, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2842"}, {"text": "Therefore we see that while participants are more confdent in their ability to take clear photos, and found the tool helpful for achieving their favorite resulting photos, they didnt necessarily fnd that their photos were better with regard to their personal preferences or the decluttering principles.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.275604797979798, "width": 0.37625980392156855, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.2894419191919192, "width": 0.3928153594771241, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.30327651515151516, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3171098484848485, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.2501111111111112, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2843"}, {"text": "Thus, even though we did not fnd signifcant changes in CSI, we decided that we should compare against a baseline that provided a little more assistance.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.6583712121212121, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6722083333333333, "width": 0.39253758169934644, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6860454545454545, "width": 0.1452941176470588, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2844"}, {"text": "In addition to encouraging more confdence, we found that the tool further encouraged creativity through exploring the space in new ways: It was really helpful with how I take photos because normally its more just snap and done. This one was more like, can I move things out of the background, can I move the subject to frame it to not have a distracting background? Another thing that I dont normally do is pivot the camera and usually just move within a fat plane (P12).", "label": "Method", "bboxes": [{"left": 0.6019640522875817, "top": 0.6492032828282828, "width": 0.3103202614379085, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6768775252525252, "width": 0.39254575163398686, "height": 0.011343434343434433, "page": 9}, {"left": 0.5195343137254902, "top": 0.6907335858585859, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.704574494949495, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7184116161616161, "width": 0.3925408496732027, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7322487373737374, "width": 0.39255065359477126, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7460618686868686, "width": 0.10836928104575161, "height": 0.011343434343434433, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2845"}, {"text": "Again we saw that the tool made the participants more confdent in their ability to address the decluttering principles of subject-background separation and image border ficker (Mdn = 6, IQR = 5-7), versus no guidance (Mdn = 5, IQR = 4-6) [ V = 8, p = . 03].", "label": "Method", "bboxes": [{"left": 0.7150800653594771, "top": 0.10956060606060607, "width": 0.19701143790849684, "height": 0.011320707070707065, "page": 9}, {"left": 0.5195343137254902, "top": 0.12339772727272727, "width": 0.3950114379084967, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1372348484848485, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.3941535947712419, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.162270202020202, "width": 0.1635751633986927, "height": 0.013969696969696965, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2846"}, {"text": "We found that overall, this study design worked reasonably well in the Zoom environment.", "label": "Method", "bboxes": [{"left": 0.24475000000000002, "top": 0.4231426767676768, "width": 0.23571078431372544, "height": 0.011320707070707092, "page": 9}, {"left": 0.08736437908496732, "top": 0.436979797979798, "width": 0.301578431372549, "height": 0.011320707070706981, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2847"}, {"text": "We also did not fnd signifcant diferences in CSI, but did fnd support for the increased confdence and descriptions of how the tool encouraged participants to explore more in the qualitative feedback.", "label": "Method", "bboxes": [{"left": 0.7312189542483659, "top": 0.3590290404040404, "width": 0.18085947712418315, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.37286363636363634, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3866969696969697, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.40054040404040403, "width": 0.19635457516339871, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2848"}, {"text": "We ran remote studies over Zoom with 18 participants (6 male, 10 female), 24 to 32 years old (  = 29), to understand if the tool would help users declutter photos, and if users felt creative while using the tool.", "label": "Method", "bboxes": [{"left": 0.08720261437908497, "top": 0.8152474747474748, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8264469696969697, "width": 0.39254901960784316, "height": 0.013969696969696965, "page": 9}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.392545751633987, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.04986111111111112, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2849"}, {"text": "We ran a pilot study (n = 5) to test this study design in a remote setting.", "label": "Method", "bboxes": [{"left": 0.08720261437908497, "top": 0.3587222222222222, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.3725568181818182, "width": 0.04485947712418299, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2850"}, {"text": "Additionally, we found in our low-fdelity prototype that the abstraction also encouraged participants to be more aware of composition.", "label": "Method", "bboxes": [{"left": 0.5190212418300654, "top": 0.7460618686868686, "width": 0.395531045751634, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195375816993464, "top": 0.7598964646464647, "width": 0.394799019607843, "height": 0.011320707070707092, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.2 Abstraction for Composition?", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2851"}, {"text": "Our design survey (Section 3.2.3) provided us with a lot of interesting insight on how participants might imagine an abstraction overlay to assist with decluttering.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.5150921717171717, "width": 0.3950114379084968, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.5289267676767677, "width": 0.39254575163398686, "height": 0.011320707070706981, "page": 10}, {"left": 0.5195343137254902, "top": 0.542760101010101, "width": 0.21325653594771243, "height": 0.011320707070707092, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.1 Other Abstraction Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2852"}, {"text": "In fact, we would be interested in comparing how such an interface might compare to other grid-based composition guidance [17], as well as understanding how these diferent camera interfaces", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.8567588383838384, "width": 0.37874509803921563, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.8705959595959595, "width": 0.39417156862745106, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.2 Abstraction for Composition?", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2853"}, {"text": "An interaction that we didnt expect that emerged from the studies was that participants used the tools ability to identify subjects to assess how someone might view their photos.", "label": "Method", "bboxes": [{"left": 0.23522058823529413, "top": 0.5426098484848485, "width": 0.2452369281045752, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.5564457070707071, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.5702828282828283, "width": 0.3928169934640523, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.5841199494949495, "width": 0.04381862745098038, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2854"}, {"text": "It additionally could be interesting to experiment more thoroughly with diferent types of edge detection to see which would best match what humans actually perceive as noisee.g., in our user studies, we often found that textures like carpet ended up appearing as a lot of noise.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.43472979797979794, "width": 0.37875490196078443, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.44856691919191916, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4624040404040404, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4762411616161616, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4900782828282828, "width": 0.15979901960784315, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2855"}, {"text": "If we remove the requirement of our camera guidance running interactively, there are further approaches that can be considered both for identifying objects in the scene for location context and for generating line drawings.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.503915404040404, "width": 0.3762761437908496, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.5177525252525252, "width": 0.39256699346405227, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.5315883838383838, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.545425505050505, "width": 0.17374673202614382, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2856"}, {"text": "We would like to thank Grifn Dietz for her assistance in transitioning to virtual user studies, Mitchell Gordon for his feedback and insightful discussions, and Matthew Cong and Charlene Seto Kung for their help with fgure photos.", "label": "Method", "bboxes": [{"left": 0.5188316993464052, "top": 0.30579419191919194, "width": 0.3957303921568628, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.31962878787878785, "width": 0.39293627450980395, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.3334684343434344, "width": 0.39255228758169936, "height": 0.011320707070707037, "page": 11}, {"left": 0.5195343137254902, "top": 0.34730555555555553, "width": 0.2267565359477124, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ ACKNOWLEDGMENTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2857"}, {"text": "Inspired by the use of sketching in design to capture higher level structure, our goal was to bring some of the benefts of the abstraction in a sketch-like representation to photography.", "label": "Method", "bboxes": [{"left": 0.8624264705882352, "top": 0.515864898989899, "width": 0.049663398692810445, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5297020202020202, "width": 0.39416830065359487, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5435391414141414, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5573762626262626, "width": 0.2483790849673203, "height": 0.011320707070707092, "page": 11}], "section": "7 CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2858"}, {"text": "In this paper, we have described a type of annotation-based guidance.", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.7322260101010101, "width": 0.3950359477124183, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.7460606060606061, "width": 0.03092973856209151, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2859"}, {"text": "While there isnt necessarily a clear defnition of failure, the main failure case with our current algorithm is when the main subject is not properly captured in the saliency map.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.28252272727272726, "width": 0.3762745098039215, "height": 0.011320707070707037, "page": 11}, {"left": 0.08791013071895425, "top": 0.2963573232323232, "width": 0.3925637254901961, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.3101969696969697, "width": 0.30394444444444446, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2860"}, {"text": "Therefore we wonder if it is ok to have a slightly more opinionated interface (at least for some training) as is suggested by one of our design survey participants: Making it very clear about the distinction between whether the overlay is used to indicate if the two properties are good, bad, or both would be very helpful (P18).", "label": "Method", "bboxes": [{"left": 0.7584624183006535, "top": 0.12339772727272727, "width": 0.1536307189542485, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1372348484848485, "width": 0.3928316993464054, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.3950424836601307, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.39255555555555555, "height": 0.01134343434343435, "page": 11}, {"left": 0.5195343137254902, "top": 0.17876767676767677, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.20352124183006526, "height": 0.011343434343434322, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2861"}, {"text": "In designing our overlays, we were somewhat limited in the methods that we used in order to produce something that could be computed interactively.", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.1856641414141414, "width": 0.395031045751634, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.1995012626262626, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.21333838383838383, "width": 0.11609640522875818, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2862"}, {"text": "our design process for determining what an abstraction overlay for decluttering might look like,  an interactive in-camera app that shows this edge highlighting overlay to users as decluttering guidance, and  a user evaluation comparing this overlay to a grayscale overlay, a baseline method that many photographers currently employ for decluttering photos.", "label": "Method", "bboxes": [{"left": 0.5456274509803921, "top": 0.49985606060606064, "width": 0.366467320261438, "height": 0.012236111111111059, "page": 1}, {"left": 0.5594183006535948, "top": 0.5146085858585858, "width": 0.23930882352941185, "height": 0.011320707070707092, "page": 1}, {"left": 0.5456176470588235, "top": 0.5275252525252525, "width": 0.368954248366013, "height": 0.012241161616161622, "page": 1}, {"left": 0.5594183006535948, "top": 0.5422828282828283, "width": 0.32428431372549027, "height": 0.011320707070707092, "page": 1}, {"left": 0.5456176470588235, "top": 0.5551994949494949, "width": 0.3664787581699346, "height": 0.012241161616161622, "page": 1}, {"left": 0.5594183006535948, "top": 0.5699545454545455, "width": 0.35515849673202615, "height": 0.011320707070707092, "page": 1}, {"left": 0.5594183006535948, "top": 0.5837941919191919, "width": 0.22229084967320267, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2863"}, {"text": "Through our design process, we aimed to answer two questions: (1) Will an abstracted visualization be efective in encouraging the user to see parts of the image outside of the main subject?", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.3617714646464647, "width": 0.37801797385620906, "height": 0.011320707070706981, "page": 2}, {"left": 0.51909477124183, "top": 0.3756085858585859, "width": 0.3930032679738562, "height": 0.011320707070706981, "page": 2}, {"left": 0.5195343137254902, "top": 0.389445707070707, "width": 0.3600441176470588, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2864"}, {"text": "We show edges within the subject(s) also in yellow and background edges not within the subject(s) and image borders in white (see Figure 1).", "label": "Result", "bboxes": [{"left": 0.8583725490196078, "top": 0.42781186868686866, "width": 0.054259803921568595, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.4416489898989899, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.45548484848484855, "width": 0.3948218954248366, "height": 0.011320707070707037, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2865"}, {"text": "We see similarities in this photographic process and the design processphotographers are essentially iterating on their design (or image) in the camera as they consider these diferent aspects of photography and storytelling.", "label": "Result", "bboxes": [{"left": 0.32279411764705884, "top": 0.500455808080808, "width": 0.15767320261437906, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5142929292929292, "width": 0.39256535947712423, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5281300505050505, "width": 0.3929362745098039, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5419671717171717, "width": 0.3856290849673203, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2866"}, {"text": "We chose to also include a few videos to show the process of making adjustments while framing a photo, to mimic how the overlay might appear if being used in the camera.", "label": "Result", "bboxes": [{"left": 0.7293709150326797, "top": 0.8313459595959597, "width": 0.18324673202614394, "height": 0.011320707070707092, "page": 5}, {"left": 0.5191683006535948, "top": 0.8451830808080808, "width": 0.3929117647058823, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.8590202020202021, "width": 0.3925408496732027, "height": 0.011320707070706981, "page": 5}, {"left": 0.5195343137254902, "top": 0.8728573232323232, "width": 0.04645588235294129, "height": 0.011320707070707092, "page": 5}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2867"}, {"text": "The results do not show a clear-cut best overlay design.", "label": "Result", "bboxes": [{"left": 0.17833333333333334, "top": 0.7045517676767676, "width": 0.30251470588235296, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.718388888888889, "width": 0.0412941176470588, "height": 0.011320707070706981, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2868"}, {"text": "Counts show that the participants found", "label": "Result", "bboxes": [{"left": 0.3705555555555556, "top": 0.7598964646464647, "width": 0.10991176470588232, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.7737361111111111, "width": 0.1306421568627451, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2869"}, {"text": "Therefore we see that while participants are more confdent in their ability to take clear photos, and found the tool helpful for achieving their favorite resulting photos, they didnt necessarily fnd that their photos were better with regard to their personal preferences or the decluttering principles.", "label": "Result", "bboxes": [{"left": 0.5358137254901961, "top": 0.275604797979798, "width": 0.37625980392156855, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.2894419191919192, "width": 0.3928153594771241, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.30327651515151516, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3171098484848485, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.2501111111111112, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2870"}, {"text": "For instance, how they might consider the context from the users process to determine when each might be most relevant to show.", "label": "Result", "bboxes": [{"left": 0.27555228758169936, "top": 0.10956060606060607, "width": 0.2073888888888889, "height": 0.011320707070707065, "page": 11}, {"left": 0.08790522875816993, "top": 0.12339772727272727, "width": 0.3925555555555555, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.1372348484848485, "width": 0.1918545751633987, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.2 Abstraction for Composition?", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2871"}, {"text": "While these aesthetics evaluation algorithms may consider clutter, they provide limited information to the user for interpreting these quality measures.", "label": "Conclusion", "bboxes": [{"left": 0.2787745098039216, "top": 0.607290404040404, "width": 0.2016911764705882, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6211275252525252, "width": 0.3925637254901961, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6349633838383839, "width": 0.2941813725490196, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2872"}, {"text": "We hypothesized that abstracting an image could help evenly spread the photographers attention across the image, efectively drawing the photographers attention away from the details of the main subject to other areas of the image.", "label": "Conclusion", "bboxes": [{"left": 0.7807009803921569, "top": 0.2925858585858586, "width": 0.13139869281045757, "height": 0.011320707070707037, "page": 2}, {"left": 0.5195343137254902, "top": 0.30642297979797983, "width": 0.39256209150326793, "height": 0.011320707070707037, "page": 2}, {"left": 0.5195343137254902, "top": 0.320260101010101, "width": 0.39255882352941185, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.33409722222222227, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.3479343434343435, "width": 0.0775032679738562, "height": 0.011320707070706981, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2873"}, {"text": "We fnd decluttering particularly of interest because it starts to draw on the storytelling aspects of photography.", "label": "Conclusion", "bboxes": [{"left": 0.6115114379084967, "top": 0.17874494949494948, "width": 0.3005849673202614, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.3500392156862745, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2874"}, {"text": "Two experimenters were involved in the prototype testing, and when possible, both were present.", "label": "Conclusion", "bboxes": [{"left": 0.7991601307189543, "top": 0.5863080808080808, "width": 0.11293954248366023, "height": 0.011320707070707092, "page": 2}, {"left": 0.5189918300653594, "top": 0.600145202020202, "width": 0.3931013071895426, "height": 0.011320707070707092, "page": 2}, {"left": 0.5189918300653594, "top": 0.613979797979798, "width": 0.08266993464052297, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2875"}, {"text": "While we didnt ask if they wanted to take another photo after this review step, occasionally (6) participants would ask if they could take a third photo and we would repeat the process above, presenting them with a third overlay.", "label": "Conclusion", "bboxes": [{"left": 0.781687908496732, "top": 0.7938623737373738, "width": 0.13040522875816996, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8076994949494949, "width": 0.39504084967320263, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8215366161616162, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 2}, {"left": 0.5189918300653594, "top": 0.8353737373737373, "width": 0.3931062091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8492108585858587, "width": 0.04691176470588243, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2876"}, {"text": "In the one case where the participant did not make adjustments to address clutter, a whiteboard mostly flled up the background of the image, and was involved in the action in the image and therefore did not clutter the image, but instead helped tell the intended story.", "label": "Conclusion", "bboxes": [{"left": 0.2574150326797386, "top": 0.4969962121212121, "width": 0.22303267973856206, "height": 0.011320707070707037, "page": 3}, {"left": 0.08790522875816993, "top": 0.5108333333333334, "width": 0.39292156862745103, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.5246704545454546, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.5385050505050505, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.552344696969697, "width": 0.22507352941176473, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2877"}, {"text": "Moving the camera instead of the objects in the scene was often more practical because the objects could not be moved.", "label": "Conclusion", "bboxes": [{"left": 0.10418464052287582, "top": 0.6492032828282828, "width": 0.37625980392156855, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.31932189542483663, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2878"}, {"text": "One participant noted that the frst overlay made her realize that she hadnt achieved the composition she intended to because she was focused on other aspects of staging the scene.", "label": "Conclusion", "bboxes": [{"left": 0.4552565359477124, "top": 0.7875732323232324, "width": 0.025194444444444408, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.8014103535353535, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.3925392156862745, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.28168137254901965, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2879"}, {"text": "Given the elements supported by the qualitative feedback, (f) appeared to be the best candidate for our overlayit included the color-coded line drawings to help defne the subject and possible distractors along the borders, while being less noisy than showing all outlines.", "label": "Conclusion", "bboxes": [{"left": 0.5358137254901961, "top": 0.6076919191919191, "width": 0.37715522875816987, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6215290404040403, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6353661616161616, "width": 0.39255882352941185, "height": 0.011320707070707203, "page": 6}, {"left": 0.5195343137254902, "top": 0.6492032828282828, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.0717908496732026, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2880"}, {"text": "They mentioned the smoothing of colors being helpful because it essentially simplifes the image and makes it easier to analyze the major color contrasts (P27) and noting that it helped in being able to dissociate the images from what I expect to see into what the colors actually are (P24).", "label": "Conclusion", "bboxes": [{"left": 0.7911535947712418, "top": 0.4416489898989899, "width": 0.12093954248366012, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.45548484848484855, "width": 0.39256209150326793, "height": 0.011343434343434322, "page": 6}, {"left": 0.5195343137254902, "top": 0.469344696969697, "width": 0.39365849673202624, "height": 0.011320707070707092, "page": 6}, {"left": 0.51909477124183, "top": 0.48315909090909087, "width": 0.39299346405228763, "height": 0.011343434343434433, "page": 6}, {"left": 0.5195343137254902, "top": 0.4969962121212121, "width": 0.3948186274509804, "height": 0.011343434343434378, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2881"}, {"text": "Therefore we decided line drawing should be part of our fnal overlay design.", "label": "Conclusion", "bboxes": [{"left": 0.545983660130719, "top": 0.24793055555555554, "width": 0.36610947712418296, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.26176767676767676, "width": 0.09069934640522881, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2882"}, {"text": "One participant very clearly summarized the potential reasoning for this: if the most important thing for subject background separation is the contrast, then the overlays should try to mute lines as much as possible.", "label": "Conclusion", "bboxes": [{"left": 0.3555800653594771, "top": 0.8429217171717173, "width": 0.12527287581699342, "height": 0.011320707070706981, "page": 6}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.392562091503268, "height": 0.011343434343434433, "page": 6}, {"left": 0.08790522875816993, "top": 0.8706161616161616, "width": 0.3941617647058824, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8844545454545455, "width": 0.3585980392156863, "height": 0.011320707070706981, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2883"}, {"text": "Finally at the end of the survey, we asked participants to summarize their choices: Please provide a brief explanation for your choices why did you fnd these overlays most helpful? Are there specifc characteristics of the overlays that you like (e.g., line drawing, color fattening, image darkening, or color)?", "label": "Conclusion", "bboxes": [{"left": 0.08790522875816993, "top": 0.5576111111111111, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5714457070707071, "width": 0.39418464052287583, "height": 0.011320707070707092, "page": 6}, {"left": 0.08736437908496732, "top": 0.5852853535353535, "width": 0.3931013071895425, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5991224747474747, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.6129595959595959, "width": 0.24230065359477126, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 1, "is_author_statement": true, "is_in_expected_section": false, "id": "2884"}, {"text": "A solid and defned yellow outline of the subject would mean that there is likely good SBS.", "label": "Conclusion", "bboxes": [{"left": 0.12704738562091503, "top": 0.6492007575757576, "width": 0.353406862745098, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.17365849673202616, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2885"}, {"text": "Gaps along this edge might signal a lack of contrastthis could mean that the subject is blending into the background, or there are objects directly around the subject that are interfering with the clarity of the subject border.", "label": "Conclusion", "bboxes": [{"left": 0.26522549019607844, "top": 0.6630404040404041, "width": 0.21521732026143792, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3925392156862745, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6907146464646465, "width": 0.3925441176470589, "height": 0.011320707070706981, "page": 7}, {"left": 0.08790522875816993, "top": 0.7045492424242424, "width": 0.29864705882352943, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2886"}, {"text": "The presence of many cyan edges suggests there might be noise and clutter near the image border that could also take attention away from the subject.", "label": "Conclusion", "bboxes": [{"left": 0.38923692810457516, "top": 0.7045492424242424, "width": 0.09121241830065363, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7183825757575757, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7322260101010101, "width": 0.39479575163398695, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2887"}, {"text": "We realized we could estimate the image subjects using object-based saliency maps [4].", "label": "Conclusion", "bboxes": [{"left": 0.5924673202614379, "top": 0.44626136363636365, "width": 0.322078431372549, "height": 0.011320707070707037, "page": 7}, {"left": 0.5195343137254902, "top": 0.4600972222222222, "width": 0.21462091503267966, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2888"}, {"text": "Thus, these maps directly served as a mask of the regions corresponding to salient subjects in the image.", "label": "Conclusion", "bboxes": [{"left": 0.5829395424836602, "top": 0.5846313131313131, "width": 0.3316062091503268, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.5984671717171717, "width": 0.3089656862745098, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2889"}, {"text": "Thus, our fnal abstraction overlay is a contextaware line drawing.", "label": "Conclusion", "bboxes": [{"left": 0.2018888888888889, "top": 0.48315909090909087, "width": 0.28103431372549015, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4969962121212121, "width": 0.11882843137254902, "height": 0.011320707070707037, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2890"}, {"text": "After each condition, participants were asked to complete surveys with a number of Likert questions (on a 7-point scale) about their experience using the tool along with the Creativity Support Index (CSI) questions (0 to 100) [13].", "label": "Conclusion", "bboxes": [{"left": 0.5358137254901961, "top": 0.7304381313131313, "width": 0.37874183006535944, "height": 0.011320707070707092, "page": 8}, {"left": 0.5191683006535948, "top": 0.7442752525252525, "width": 0.3929101307189542, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.7581123737373737, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.7719494949494949, "width": 0.22514542483660138, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2891"}, {"text": "We ran these user studies over Zoom, asking the participant to adjust the webcam when possible to keep their photographing within view.", "label": "Conclusion", "bboxes": [{"left": 0.10418464052287582, "top": 0.75989898989899, "width": 0.37625816993464056, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.7737361111111111, "width": 0.392547385620915, "height": 0.011320707070707092, "page": 8}, {"left": 0.08736437908496732, "top": 0.7875732323232324, "width": 0.07515359477124182, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2892"}, {"text": "Thus, participants completed 6 photo tasks: 3 at each of 2 locations of their choosing, using the baseline condition at one location and the abstraction guidance at the second.", "label": "Conclusion", "bboxes": [{"left": 0.1644607843137255, "top": 0.6768775252525252, "width": 0.31599183006535947, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.6907146464646465, "width": 0.3925441176470589, "height": 0.011320707070706981, "page": 8}, {"left": 0.08790522875816993, "top": 0.7045492424242424, "width": 0.30200653594771243, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2893"}, {"text": "Figure 9 shows a few example images to demonstrate the scale of images chosen by the participants.", "label": "Conclusion", "bboxes": [{"left": 0.393171568627451, "top": 0.7045492424242424, "width": 0.08727777777777779, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.7183825757575757, "width": 0.3929183006535948, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.7322260101010101, "width": 0.09553758169934638, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2894"}, {"text": "They were told that following these guidelines would be helpful in telling a clear story in their photos, but also that these guidelines are just to provide some possible perspectives to consider and that participants are by no means required to follow them (e.g., if communicating their story involved intentionally having the subject blend into the background they should do so).", "label": "Conclusion", "bboxes": [{"left": 0.7942794117647058, "top": 0.5228838383838385, "width": 0.1178022875816993, "height": 0.011320707070706981, "page": 8}, {"left": 0.5195343137254902, "top": 0.536719696969697, "width": 0.3929183006535948, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.5505568181818182, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.5643939393939394, "width": 0.39292647058823527, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.5782310606060607, "width": 0.3929183006535948, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.5920656565656566, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.605905303030303, "width": 0.11229411764705888, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2895"}, {"text": "Therefore we see that while participants are more confdent in their ability to take clear photos, and found the tool helpful for achieving their favorite resulting photos, they didnt necessarily fnd that their photos were better with regard to their personal preferences or the decluttering principles.", "label": "Conclusion", "bboxes": [{"left": 0.5358137254901961, "top": 0.275604797979798, "width": 0.37625980392156855, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.2894419191919192, "width": 0.3928153594771241, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.30327651515151516, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3171098484848485, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.2501111111111112, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2896"}, {"text": "Thus, even though we did not fnd signifcant changes in CSI, we decided that we should compare against a baseline that provided a little more assistance.", "label": "Conclusion", "bboxes": [{"left": 0.10418464052287582, "top": 0.6583712121212121, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6722083333333333, "width": 0.39253758169934644, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6860454545454545, "width": 0.1452941176470588, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2897"}, {"text": "We therefore use grayscale as our baseline condition.", "label": "Conclusion", "bboxes": [{"left": 0.16683333333333333, "top": 0.7275568181818182, "width": 0.3158741830065359, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2898"}, {"text": "It is an active method employed by photographers and thus in some ways does encourage novices to see the image in the ways that expert photographers do.", "label": "Conclusion", "bboxes": [{"left": 0.08790522875816993, "top": 0.7413939393939394, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.08736437908496732, "top": 0.7552297979797981, "width": 0.39308333333333334, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.7690669191919192, "width": 0.15070751633986929, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2899"}, {"text": "In addition to encouraging more confdence, we found that the tool further encouraged creativity through exploring the space in new ways: It was really helpful with how I take photos because normally its more just snap and done. This one was more like, can I move things out of the background, can I move the subject to frame it to not have a distracting background? Another thing that I dont normally do is pivot the camera and usually just move within a fat plane (P12).", "label": "Conclusion", "bboxes": [{"left": 0.6019640522875817, "top": 0.6492032828282828, "width": 0.3103202614379085, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6768775252525252, "width": 0.39254575163398686, "height": 0.011343434343434433, "page": 9}, {"left": 0.5195343137254902, "top": 0.6907335858585859, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.704574494949495, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7184116161616161, "width": 0.3925408496732027, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7322487373737374, "width": 0.39255065359477126, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7460618686868686, "width": 0.10836928104575161, "height": 0.011343434343434433, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2900"}, {"text": "Thus upon capturing the fnal photo, she has intentionally refned some details that were brought to her attention by the overlay, and is therefore more confdent that this photo achieves her goals.", "label": "Conclusion", "bboxes": [{"left": 0.5736503267973856, "top": 0.5802171717171717, "width": 0.3388055555555556, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.5940568181818182, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6078914141414141, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6217310606060606, "width": 0.057751633986928064, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2901"}, {"text": "We also did not fnd signifcant diferences in CSI, but did fnd support for the increased confdence and descriptions of how the tool encouraged participants to explore more in the qualitative feedback.", "label": "Conclusion", "bboxes": [{"left": 0.7312189542483659, "top": 0.3590290404040404, "width": 0.18085947712418315, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.37286363636363634, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3866969696969697, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.40054040404040403, "width": 0.19635457516339871, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2902"}, {"text": "Participants described feeling like they could take fewer photos, because they could be more confdent in each photo they took: Usually when I take photos, I take a ton at once, but didnt do that here. I didnt need to because I was being so precise. I noticed myself reconsidering the composition more: e.g. should I have these things in the edges? (P15).", "label": "Conclusion", "bboxes": [{"left": 0.5995686274509804, "top": 0.4556868686868687, "width": 0.3125098039215686, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.4695239898989899, "width": 0.3925359477124184, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.48336111111111113, "width": 0.39255882352941185, "height": 0.011343434343434322, "page": 9}, {"left": 0.5195343137254902, "top": 0.497219696969697, "width": 0.3925441176470589, "height": 0.011320707070707037, "page": 9}, {"left": 0.5195343137254902, "top": 0.5110568181818181, "width": 0.39255228758169936, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.5248712121212121, "width": 0.15625980392156857, "height": 0.011343434343434322, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2903"}, {"text": "Therefore she tries to fnd a diferent way to capture this room with a more clear subject.", "label": "Conclusion", "bboxes": [{"left": 0.2384264705882353, "top": 0.6394583333333334, "width": 0.24239542483660126, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.6533055555555556, "width": 0.2794117647058824, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2904"}, {"text": "Thus, participants actually suggested using a mixed visualization where regions around the subject used color fattening and distractors near the edge are highlighted through line drawings.", "label": "Conclusion", "bboxes": [{"left": 0.51909477124183, "top": 0.6672979797979798, "width": 0.39297712418300657, "height": 0.011320707070707092, "page": 10}, {"left": 0.5189918300653594, "top": 0.681135101010101, "width": 0.39555555555555555, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.6949722222222222, "width": 0.3427009803921568, "height": 0.011320707070707092, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.1 Other Abstraction Designs", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2905"}, {"text": "It additionally could be interesting to experiment more thoroughly with diferent types of edge detection to see which would best match what humans actually perceive as noisee.g., in our user studies, we often found that textures like carpet ended up appearing as a lot of noise.", "label": "Conclusion", "bboxes": [{"left": 0.10418464052287582, "top": 0.43472979797979794, "width": 0.37875490196078443, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.44856691919191916, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4624040404040404, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4762411616161616, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4900782828282828, "width": 0.15979901960784315, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2906"}, {"text": "There also are a range of segmentation algorithms that could provide additional location context.", "label": "Conclusion", "bboxes": [{"left": 0.24904575163398693, "top": 0.6699583333333333, "width": 0.23142156862745097, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.6837954545454545, "width": 0.3443856209150327, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2907"}, {"text": "However through the design prototyping and user studies, weve noticed that decluttering seems difcult for people to address, perhaps because it is a new and unfamiliar concept.", "label": "Conclusion", "bboxes": [{"left": 0.3117565359477124, "top": 0.7737361111111111, "width": 0.168704248366013, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.7875732323232324, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.8014103535353535, "width": 0.39503431372549025, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.09830882352941177, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2908"}, {"text": "Therefore we wonder if it is ok to have a slightly more opinionated interface (at least for some training) as is suggested by one of our design survey participants: Making it very clear about the distinction between whether the overlay is used to indicate if the two properties are good, bad, or both would be very helpful (P18).", "label": "Conclusion", "bboxes": [{"left": 0.7584624183006535, "top": 0.12339772727272727, "width": 0.1536307189542485, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1372348484848485, "width": 0.3928316993464054, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.3950424836601307, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.39255555555555555, "height": 0.01134343434343435, "page": 11}, {"left": 0.5195343137254902, "top": 0.17876767676767677, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.20352124183006526, "height": 0.011343434343434322, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2909"}, {"text": "In this case, we could imagine having the interface give specifc guidance such as rotating the phone, removing an object from the scene, or moving the camera closer to the subject.", "label": "Conclusion", "bboxes": [{"left": 0.7272107843137254, "top": 0.1925820707070707, "width": 0.1848856209150327, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.20641666666666666, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.22025631313131314, "width": 0.39255228758169936, "height": 0.011320707070707037, "page": 11}, {"left": 0.5195343137254902, "top": 0.23409343434343433, "width": 0.1253349673202615, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2910"}, {"text": "Giving the phone even more authority, one could imagine integrating Fried et al.", "label": "Conclusion", "bboxes": [{"left": 0.6486356209150327, "top": 0.23409343434343433, "width": 0.26346078431372544, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.24792803030303032, "width": 0.23402287581699355, "height": 0.011320707070707065, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "2911"}, {"text": "In designing our overlays, we were somewhat limited in the methods that we used in order to produce something that could be computed interactively.", "label": "Conclusion", "bboxes": [{"left": 0.08790522875816993, "top": 0.1856641414141414, "width": 0.395031045751634, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.1995012626262626, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.21333838383838383, "width": 0.11609640522875818, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2912"}, {"text": "We additionally found a simpler algorithm was easier to explain to users when describing the tool for studies, and that users could interpret the visualizations in diferent contexts more easily.", "label": "Conclusion", "bboxes": [{"left": 0.1473202614379085, "top": 0.24101136363636363, "width": 0.33314379084967316, "height": 0.011320707070707065, "page": 11}, {"left": 0.08790522875816993, "top": 0.2548484848484849, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.26868560606060604, "width": 0.3946486928104575, "height": 0.011320707070707037, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "2913"}, {"text": "again reconsidered this intended composition and rotated the camera to landscape to further emphasize the of-centeredness of the composition.", "label": "Future Work", "bboxes": [{"left": 0.5195343137254902, "top": 0.27430808080808083, "width": 0.39502124183006526, "height": 0.011320707070707037, "page": 3}, {"left": 0.5195343137254902, "top": 0.28814520202020205, "width": 0.39253921568627437, "height": 0.011320707070707037, "page": 3}, {"left": 0.5195343137254902, "top": 0.3019823232323232, "width": 0.07714869281045755, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2914"}, {"text": "It is good practice to have this contrast around the subject as it will help make the subject distinct from the background.", "label": "Future Work", "bboxes": [{"left": 0.5195343137254902, "top": 0.6907121212121212, "width": 0.39254411764705877, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.7045517676767676, "width": 0.31300816993464053, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2915"}, {"text": "The contrast will clarify the story and declutter the overall image.", "label": "Future Work", "bboxes": [{"left": 0.8361781045751634, "top": 0.7045517676767676, "width": 0.07590032679738568, "height": 0.011320707070707092, "page": 3}, {"left": 0.5189918300653594, "top": 0.718388888888889, "width": 0.31323366013071896, "height": 0.011320707070706981, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2916"}, {"text": "In this paper, we will refer to this as subject-background separation (SBS) In art, this is more commonly referred to as the fgure-ground relationship, a Gestalt Psychology principle [20].", "label": "Future Work", "bboxes": [{"left": 0.8358839869281045, "top": 0.718388888888889, "width": 0.07781209150326807, "height": 0.011320707070706981, "page": 3}, {"left": 0.5189918300653594, "top": 0.7322032828282828, "width": 0.39398039215686287, "height": 0.011343434343434322, "page": 3}, {"left": 0.5195294117647059, "top": 0.7460606060606061, "width": 0.3925457516339871, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.75989898989899, "width": 0.29585784313725483, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2917"}, {"text": "On the other hand, contrast in other regions especially the border of the image, will distract, causing the eye to be attracted away from the focal subject.", "label": "Future Work", "bboxes": [{"left": 0.5358137254901961, "top": 0.8014103535353535, "width": 0.37655065359477113, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.8152474747474748, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.8290820707070707, "width": 0.1380457516339869, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2918"}, {"text": "Since we did not yet need these to work directly on a camera, these proposals were prototyped in Python.", "label": "Future Work", "bboxes": [{"left": 0.7330571895424837, "top": 0.7805795454545456, "width": 0.1790212418300654, "height": 0.011320707070706981, "page": 4}, {"left": 0.5195343137254902, "top": 0.7944166666666667, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8082537878787878, "width": 0.04727450980392156, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2919"}, {"text": "Aiming to capture visual and conceptual diversity in the overlays, we narrowed down the set of overlay options to the 6 in Figure 6 to study further:", "label": "Future Work", "bboxes": [{"left": 0.5704558823529412, "top": 0.8082537878787878, "width": 0.3416209150326798, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8220909090909091, "width": 0.3925424836601308, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8359280303030302, "width": 0.15035620915032677, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2920"}, {"text": "Further discussing the latter observation that (a)(c) were more helpful for SBS, several (8) participants specifcally noted that the color fattening was helpful for noticing SBS.", "label": "Future Work", "bboxes": [{"left": 0.5358137254901961, "top": 0.4139747474747475, "width": 0.3762777777777778, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.42781186868686866, "width": 0.39256209150326804, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.4416489898989899, "width": 0.2679607843137255, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2921"}, {"text": "In particular, we need to identify a border around the subject.", "label": "Future Work", "bboxes": [{"left": 0.6145588235294118, "top": 0.43242424242424243, "width": 0.29752124183006523, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.44626136363636365, "width": 0.06928267973856206, "height": 0.011320707070707037, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2922"}, {"text": "As we described earlier, photographers will sometimes use a grayscale display in their current practice, to help emphasize contrast in order to consider overall clarity and decluttering.", "label": "Future Work", "bboxes": [{"left": 0.23811601307189542, "top": 0.6860454545454545, "width": 0.2423349673202614, "height": 0.011320707070707092, "page": 9}, {"left": 0.08736437908496732, "top": 0.6998825757575757, "width": 0.39307843137254905, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.713719696969697, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.7275568181818182, "width": 0.07524346405228759, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2923"}, {"text": "In addition to encouraging more confdence, we found that the tool further encouraged creativity through exploring the space in new ways: It was really helpful with how I take photos because normally its more just snap and done. This one was more like, can I move things out of the background, can I move the subject to frame it to not have a distracting background? Another thing that I dont normally do is pivot the camera and usually just move within a fat plane (P12).", "label": "Future Work", "bboxes": [{"left": 0.6019640522875817, "top": 0.6492032828282828, "width": 0.3103202614379085, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.6768775252525252, "width": 0.39254575163398686, "height": 0.011343434343434433, "page": 9}, {"left": 0.5195343137254902, "top": 0.6907335858585859, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.704574494949495, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7184116161616161, "width": 0.3925408496732027, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7322487373737374, "width": 0.39255065359477126, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7460618686868686, "width": 0.10836928104575161, "height": 0.011343434343434433, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2924"}, {"text": "Participants described feeling like they could take fewer photos, because they could be more confdent in each photo they took: Usually when I take photos, I take a ton at once, but didnt do that here. I didnt need to because I was being so precise. I noticed myself reconsidering the composition more: e.g. should I have these things in the edges? (P15).", "label": "Future Work", "bboxes": [{"left": 0.5995686274509804, "top": 0.4556868686868687, "width": 0.3125098039215686, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.4695239898989899, "width": 0.3925359477124184, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.48336111111111113, "width": 0.39255882352941185, "height": 0.011343434343434322, "page": 9}, {"left": 0.5195343137254902, "top": 0.497219696969697, "width": 0.3925441176470589, "height": 0.011320707070707037, "page": 9}, {"left": 0.5195343137254902, "top": 0.5110568181818181, "width": 0.39255228758169936, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.5248712121212121, "width": 0.15625980392156857, "height": 0.011343434343434322, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2925"}, {"text": "If we remove the requirement of our camera guidance running interactively, there are further approaches that can be considered both for identifying objects in the scene for location context and for generating line drawings.", "label": "Future Work", "bboxes": [{"left": 0.10418464052287582, "top": 0.503915404040404, "width": 0.3762761437908496, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.5177525252525252, "width": 0.39256699346405227, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.5315883838383838, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.545425505050505, "width": 0.17374673202614382, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2926"}, {"text": "Due to the modular nature of our implementation, better object detection or object-centric saliency methods methods can be used in the future.", "label": "Future Work", "bboxes": [{"left": 0.08790522875816993, "top": 0.39321590909090914, "width": 0.392562091503268, "height": 0.011320707070707037, "page": 11}, {"left": 0.08790522875816993, "top": 0.4070555555555555, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4208926767676767, "width": 0.07798692810457518, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "2927"}, {"text": "Through our design process, we aimed to answer two questions: (1) Will an abstracted visualization be efective in encouraging the user to see parts of the image outside of the main subject?", "label": "Future Work", "bboxes": [{"left": 0.5358137254901961, "top": 0.3617714646464647, "width": 0.37801797385620906, "height": 0.011320707070706981, "page": 2}, {"left": 0.51909477124183, "top": 0.3756085858585859, "width": 0.3930032679738562, "height": 0.011320707070706981, "page": 2}, {"left": 0.5195343137254902, "top": 0.389445707070707, "width": 0.3600441176470588, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "2928"}, {"text": "(If you dont have a preference between a few similar overlays, feel free to pick up to 3)  (Optional) You will be asked to explain your preferences at the end of the survey, but feel free to explain any specifc thoughts you have based on this image/video here.", "label": "Future Work", "bboxes": [{"left": 0.30324509803921573, "top": 0.48441666666666666, "width": 0.17722058823529407, "height": 0.011320707070707092, "page": 6}, {"left": 0.1277892156862745, "top": 0.4982525252525252, "width": 0.33478921568627457, "height": 0.011320707070707037, "page": 6}, {"left": 0.11398856209150326, "top": 0.511169191919192, "width": 0.36647385620915035, "height": 0.012241161616161622, "page": 6}, {"left": 0.1277892156862745, "top": 0.5259267676767677, "width": 0.3526781045751634, "height": 0.011320707070707092, "page": 6}, {"left": 0.1277892156862745, "top": 0.5397638888888889, "width": 0.30390522875817, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "2929"}, {"text": "Inspired by the use of sketching in design to capture higher level structure, our goal was to bring some of the benefts of the abstraction in a sketch-like representation to photography.", "label": "Objective", "bboxes": [{"left": 0.8624264705882352, "top": 0.515864898989899, "width": 0.049663398692810445, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5297020202020202, "width": 0.39416830065359487, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5435391414141414, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.5573762626262626, "width": 0.2483790849673203, "height": 0.011320707070707092, "page": 11}], "section": "7 CONCLUSION", "prob": 0.871626079082489, "is_author_statement": true, "is_in_expected_section": true, "id": "2930"}, {"text": "Both are useful for our goal of designing visual overlays that are friendly for users to process interactively.", "label": "Objective", "bboxes": [{"left": 0.22155228758169934, "top": 0.22025631313131314, "width": 0.2589183006535948, "height": 0.011320707070707037, "page": 2}, {"left": 0.08753921568627451, "top": 0.23409343434343433, "width": 0.3951862745098039, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": 0.7700191140174866, "is_author_statement": true, "is_in_expected_section": false, "id": "2931"}, {"text": "our design process for determining what an abstraction overlay for decluttering might look like,  an interactive in-camera app that shows this edge highlighting overlay to users as decluttering guidance, and  a user evaluation comparing this overlay to a grayscale overlay, a baseline method that many photographers currently employ for decluttering photos.", "label": "Method", "bboxes": [{"left": 0.5456274509803921, "top": 0.49985606060606064, "width": 0.366467320261438, "height": 0.012236111111111059, "page": 1}, {"left": 0.5594183006535948, "top": 0.5146085858585858, "width": 0.23930882352941185, "height": 0.011320707070707092, "page": 1}, {"left": 0.5456176470588235, "top": 0.5275252525252525, "width": 0.368954248366013, "height": 0.012241161616161622, "page": 1}, {"left": 0.5594183006535948, "top": 0.5422828282828283, "width": 0.32428431372549027, "height": 0.011320707070707092, "page": 1}, {"left": 0.5456176470588235, "top": 0.5551994949494949, "width": 0.3664787581699346, "height": 0.012241161616161622, "page": 1}, {"left": 0.5594183006535948, "top": 0.5699545454545455, "width": 0.35515849673202615, "height": 0.011320707070707092, "page": 1}, {"left": 0.5594183006535948, "top": 0.5837941919191919, "width": 0.22229084967320267, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.8822916150093079, "is_author_statement": true, "is_in_expected_section": true, "id": "2932"}, {"text": "into two components as well: a method of color fattening to smooth out detailed regions, and a method of line drawing.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.6422095959595959, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6560467171717173, "width": 0.3052107843137255, "height": 0.011320707070706981, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.8477948904037476, "is_author_statement": false, "is_in_expected_section": true, "id": "2933"}, {"text": "This concept leads us to two components: a method of line drawing to determine the potential outline, and a method for considering location context relative to the subject/image frame to determine which lines in the image are relevant to the decluttering principles (e.g., if they are along the subject-background boundary, along the image border, or neither).", "label": "Method", "bboxes": [{"left": 0.0874656862745098, "top": 0.7322260101010101, "width": 0.3929901960784314, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.7460618686868686, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.75989898989899, "width": 0.39253758169934644, "height": 0.011320707070707092, "page": 4}, {"left": 0.08736437908496732, "top": 0.7737361111111111, "width": 0.3930882352941176, "height": 0.011320707070707092, "page": 4}, {"left": 0.0874656862745098, "top": 0.7875732323232324, "width": 0.39297712418300657, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8014103535353535, "width": 0.15209967320261442, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.7361224889755249, "is_author_statement": false, "is_in_expected_section": true, "id": "2934"}, {"text": "s method for automatically remove clutter from the scene [19] directly into the phone.", "label": "Method", "bboxes": [{"left": 0.7573970588235294, "top": 0.24792803030303032, "width": 0.15507352941176478, "height": 0.011320707070707065, "page": 11}, {"left": 0.5195343137254902, "top": 0.26176767676767676, "width": 0.35067156862745097, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": 0.6106480360031128, "is_author_statement": false, "is_in_expected_section": false, "id": "2935"}, {"text": "We then used simple morphological operations to obtain a subject border mask specifcally, we subtracted a slightly eroded version of this mask from a dilated version to capture the boundary between the subject(s) and the background immediately surrounding.", "label": "Method", "bboxes": [{"left": 0.8321584967320261, "top": 0.5984671717171717, "width": 0.07992647058823543, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6123042929292929, "width": 0.39416830065359487, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6261414141414141, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6399760101010101, "width": 0.3950114379084967, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6538093434343435, "width": 0.3244640522875817, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "prob": 0.5604557991027832, "is_author_statement": true, "is_in_expected_section": true, "id": "2936"}, {"text": "On top of this, we implemented our camera overlays and added the necessary UI elements to adjust settings on our overlay.", "label": "Method", "bboxes": [{"left": 0.7016617647058823, "top": 0.2294810606060606, "width": 0.21068627450980404, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.24331818181818182, "width": 0.3950130718954248, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.25715530303030304, "width": 0.12212908496732022, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.2 Mobile Implementation", "prob": 0.5130977630615234, "is_author_statement": true, "is_in_expected_section": true, "id": "2937"}, {"text": "To test out the concept of abstraction guidance, we started with a low-fdelity Wizard-of-Oz (WoZ) prototype [14] where experimenters manually drew abstraction overlays.", "label": "Method", "bboxes": [{"left": 0.51909477124183, "top": 0.47561237373737375, "width": 0.39299673202614394, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195277777777778, "top": 0.4894469696969697, "width": 0.395047385620915, "height": 0.011323232323232346, "page": 2}, {"left": 0.5195310457516339, "top": 0.5032840909090909, "width": 0.27888398692810457, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.5082190632820129, "is_author_statement": true, "is_in_expected_section": true, "id": "2938"}, {"text": "Image processing methods also enable removing clutter in post using a variety of techniques.", "label": "Method", "bboxes": [{"left": 0.21683823529411766, "top": 0.3169141414141414, "width": 0.2660964052287581, "height": 0.011320707070707037, "page": 2}, {"left": 0.08790522875816993, "top": 0.3307512626262626, "width": 0.29164542483660133, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": 0.49977704882621765, "is_author_statement": false, "is_in_expected_section": false, "id": "2939"}, {"text": "We then handed them the phone to frame and take a photo of the scene.", "label": "Method", "bboxes": [{"left": 0.7563415032679738, "top": 0.6693295454545455, "width": 0.15574509803921577, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.6831666666666667, "width": 0.2629950980392156, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.49140921235084534, "is_author_statement": true, "is_in_expected_section": true, "id": "2940"}, {"text": "We additionally give ourselves line color (for the purposes of color-coding lines based on their location context), and image darkening (to provide more contrast against the line drawings) as parameters to consider in designing abstraction overlays.", "label": "Method", "bboxes": [{"left": 0.6719754901960785, "top": 0.6975580808080808, "width": 0.2403758169934641, "height": 0.011320707070707092, "page": 4}, {"left": 0.5190996732026144, "top": 0.7113926767676768, "width": 0.39545424836601306, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7252323232323232, "width": 0.39255228758169936, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7390694444444444, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7529065656565657, "width": 0.05348856209150332, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.48222896456718445, "is_author_statement": true, "is_in_expected_section": true, "id": "2941"}, {"text": "To modify this task design to work for remote studies, we decided to keep the structure of having 3 tasks per condition at diferent scales (small, medium, and large), but instead of specifying the subjects, we asked that participants choose their own subjects to photograph.", "label": "Method", "bboxes": [{"left": 0.4649248366013072, "top": 0.6076919191919191, "width": 0.015526143790849711, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.6215265151515151, "width": 0.392545751633987, "height": 0.011320707070706981, "page": 8}, {"left": 0.08790522875816993, "top": 0.6353598484848485, "width": 0.392545751633987, "height": 0.011320707070707203, "page": 8}, {"left": 0.08790522875816993, "top": 0.6491931818181818, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.39253758169934644, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.07291503267973855, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.4681261479854584, "is_author_statement": true, "is_in_expected_section": false, "id": "2942"}, {"text": "For determining appropriate parameters for the edge detection [3], we iteratively tested parameters to achieve a balance between having enough defnition in the edges and having too much noise.", "label": "Method", "bboxes": [{"left": 0.5774640522875817, "top": 0.47393434343434343, "width": 0.33709640522875817, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.48777146464646465, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195326797385621, "top": 0.5016060606060606, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.515445707070707, "width": 0.07180065359477128, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "prob": 0.45234909653663635, "is_author_statement": true, "is_in_expected_section": true, "id": "2943"}, {"text": "We would then apply a new transparency and draw the corresponding abstraction overlay for the new photo for the participant to review.", "label": "Method", "bboxes": [{"left": 0.7447679738562092, "top": 0.7661893939393939, "width": 0.16786601307189553, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.7800265151515151, "width": 0.39283496732026146, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.7938623737373738, "width": 0.2584950980392158, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.43385955691337585, "is_author_statement": true, "is_in_expected_section": true, "id": "2944"}, {"text": "Based on our learnings from the design survey (Section 3.2.3), we chose to go with a design inspired by a combination of overlay options (d) and (f).", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.45548484848484855, "width": 0.39253758169934644, "height": 0.011320707070707037, "page": 7}, {"left": 0.08790522875816993, "top": 0.46932196969696965, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.48315909090909087, "width": 0.11031209150326797, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "prob": 0.388090580701828, "is_author_statement": true, "is_in_expected_section": true, "id": "2945"}, {"text": "After getting the participants consent, we started by walking through having them install TestFlight and subsequently our guidance tool.", "label": "Method", "bboxes": [{"left": 0.40148366013071896, "top": 0.8290820707070707, "width": 0.0789607843137255, "height": 0.011320707070707092, "page": 8}, {"left": 0.08789869281045751, "top": 0.8429154040404041, "width": 0.392545751633987, "height": 0.011320707070706981, "page": 8}, {"left": 0.08789869281045751, "top": 0.8567487373737374, "width": 0.3713709150326797, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.3796199858188629, "is_author_statement": true, "is_in_expected_section": false, "id": "2946"}, {"text": "In order to realize the proposed interaction, we additionally present a proposal of an algorithm for visually annotating potential clutter by highlighting relevant edges around salient objects and around the image border.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.6303383838383838, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6441527777777778, "width": 0.39283660130718945, "height": 0.011343434343434322, "page": 1}, {"left": 0.5195343137254902, "top": 0.6580126262626262, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6718497474747475, "width": 0.10806045751633997, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.3719342350959778, "is_author_statement": true, "is_in_expected_section": true, "id": "2947"}, {"text": "Our overlay tool is build on top of a basic iOS camera app.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.18796969696969698, "width": 0.3393496732026142, "height": 0.011320707070707037, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.2 Mobile Implementation", "prob": 0.35956212878227234, "is_author_statement": true, "is_in_expected_section": true, "id": "2948"}, {"text": "These segmentations are used to classify and color code the edges: yellow designates edges within and around the subject, cyan for edges around the image border, and white for the remaining background edges.", "label": "Method", "bboxes": [{"left": 0.737967320261438, "top": 0.6953270202020202, "width": 0.174111111111111, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7091616161616161, "width": 0.39254411764705877, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7230012626262626, "width": 0.3941601307189543, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7368371212121213, "width": 0.27936437908496725, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "prob": 0.3505761921405792, "is_author_statement": false, "is_in_expected_section": true, "id": "2949"}, {"text": "We ran these user studies over Zoom, asking the participant to adjust the webcam when possible to keep their photographing within view.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.75989898989899, "width": 0.37625816993464056, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.7737361111111111, "width": 0.392547385620915, "height": 0.011320707070707092, "page": 8}, {"left": 0.08736437908496732, "top": 0.7875732323232324, "width": 0.07515359477124182, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.34723901748657227, "is_author_statement": true, "is_in_expected_section": false, "id": "2950"}, {"text": "The tool can also perform calculations in the background as the user is focused on taking a series of photos.", "label": "Method", "bboxes": [{"left": 0.43020261437908497, "top": 0.6007739898989899, "width": 0.050259803921568647, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.6146111111111111, "width": 0.39504084967320263, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.6284482323232323, "width": 0.2122483660130719, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": 0.33383896946907043, "is_author_statement": false, "is_in_expected_section": false, "id": "2951"}, {"text": "We considered this in our design and tried to design the visualization to be robust to this case by providing the option of showing all lines.", "label": "Method", "bboxes": [{"left": 0.39503758169934644, "top": 0.3101969696969697, "width": 0.08542973856209146, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.3240340909090909, "width": 0.39255718954248364, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.33787121212121213, "width": 0.33403594771241835, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": 0.30141520500183105, "is_author_statement": true, "is_in_expected_section": false, "id": "2952"}, {"text": "Participants noted that the edges helped to defne objects (18) and that they were helpful for noticing edges around the image border (5), supporting the observation shown by the overall counts on (d)(f) being more useful for addressing IBF.", "label": "Result", "bboxes": [{"left": 0.6522238562091504, "top": 0.1925820707070707, "width": 0.2598692810457516, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.20641919191919192, "width": 0.39256209150326793, "height": 0.011320707070707037, "page": 6}, {"left": 0.5195343137254902, "top": 0.22025378787878788, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.23409343434343433, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.24793055555555554, "width": 0.022782679738562184, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.9319215416908264, "is_author_statement": false, "is_in_expected_section": false, "id": "2953"}, {"text": "The results do not show a clear-cut best overlay design.", "label": "Result", "bboxes": [{"left": 0.17833333333333334, "top": 0.7045517676767676, "width": 0.30251470588235296, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.718388888888889, "width": 0.0412941176470588, "height": 0.011320707070706981, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.8568102717399597, "is_author_statement": false, "is_in_expected_section": false, "id": "2954"}, {"text": "We also did not fnd signifcant diferences in CSI, but did fnd support for the increased confdence and descriptions of how the tool encouraged participants to explore more in the qualitative feedback.", "label": "Result", "bboxes": [{"left": 0.7312189542483659, "top": 0.3590290404040404, "width": 0.18085947712418315, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.37286363636363634, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3866969696969697, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.40054040404040403, "width": 0.19635457516339871, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.8543130159378052, "is_author_statement": true, "is_in_expected_section": true, "id": "2955"}, {"text": "We additionally found a simpler algorithm was easier to explain to users when describing the tool for studies, and that users could interpret the visualizations in diferent contexts more easily.", "label": "Result", "bboxes": [{"left": 0.1473202614379085, "top": 0.24101136363636363, "width": 0.33314379084967316, "height": 0.011320707070707065, "page": 11}, {"left": 0.08790522875816993, "top": 0.2548484848484849, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.26868560606060604, "width": 0.3946486928104575, "height": 0.011320707070707037, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": 0.8469750881195068, "is_author_statement": true, "is_in_expected_section": false, "id": "2956"}, {"text": "5.3.1 Summative Study Results.", "label": "Result", "bboxes": [{"left": 0.5195343137254902, "top": 0.10960606060606061, "width": 0.1898382352941177, "height": 0.011320707070707065, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.8350251317024231, "is_author_statement": false, "is_in_expected_section": true, "id": "2957"}, {"text": "5.2.1 Pilot Study Results.", "label": "Result", "bboxes": [{"left": 0.08790522875816993, "top": 0.4231881313131313, "width": 0.1511339869281046, "height": 0.011320707070707037, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": 0.808192253112793, "is_author_statement": false, "is_in_expected_section": true, "id": "2958"}, {"text": "It additionally could be interesting to experiment more thoroughly with diferent types of edge detection to see which would best match what humans actually perceive as noisee.g., in our user studies, we often found that textures like carpet ended up appearing as a lot of noise.", "label": "Result", "bboxes": [{"left": 0.10418464052287582, "top": 0.43472979797979794, "width": 0.37875490196078443, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.44856691919191916, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4624040404040404, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4762411616161616, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4900782828282828, "width": 0.15979901960784315, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": 0.8052585124969482, "is_author_statement": true, "is_in_expected_section": false, "id": "2959"}, {"text": "An interaction that we didnt expect that emerged from the studies was that participants used the tools ability to identify subjects to assess how someone might view their photos.", "label": "Result", "bboxes": [{"left": 0.23522058823529413, "top": 0.5426098484848485, "width": 0.2452369281045752, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.5564457070707071, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.5702828282828283, "width": 0.3928169934640523, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.5841199494949495, "width": 0.04381862745098038, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.7834793329238892, "is_author_statement": true, "is_in_expected_section": true, "id": "2960"}, {"text": "For example, we saw the potential benefts of color fattening especially for SBS.", "label": "Result", "bboxes": [{"left": 0.7371241830065359, "top": 0.542760101010101, "width": 0.17742156862745095, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.5566022727272727, "width": 0.31233496732026134, "height": 0.011320707070707092, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.1 Other Abstraction Designs", "prob": 0.7092025279998779, "is_author_statement": true, "is_in_expected_section": false, "id": "2961"}, {"text": "Both of these conclusions were also supported by the qualitative feedback from participants general impressions.", "label": "Result", "bboxes": [{"left": 0.10418464052287582, "top": 0.8014103535353535, "width": 0.376281045751634, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.28612418300653597, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.7017031311988831, "is_author_statement": false, "is_in_expected_section": false, "id": "2962"}, {"text": "For these pilots, we compared our tool to a no guidance baseline condition.", "label": "Result", "bboxes": [{"left": 0.14241176470588235, "top": 0.38639646464646465, "width": 0.3380473856209151, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.40023358585858587, "width": 0.11229411764705884, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "prob": 0.7001140713691711, "is_author_statement": true, "is_in_expected_section": true, "id": "2963"}, {"text": "Thus, our fnal abstraction overlay is a contextaware line drawing.", "label": "Result", "bboxes": [{"left": 0.2018888888888889, "top": 0.48315909090909087, "width": 0.28103431372549015, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4969962121212121, "width": 0.11882843137254902, "height": 0.011320707070707037, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "prob": 0.6980423927307129, "is_author_statement": true, "is_in_expected_section": false, "id": "2964"}, {"text": "Results of both steps of our design prototyping process made us hopeful of the potential of an abstraction-based overlay.", "label": "Result", "bboxes": [{"left": 0.5195343137254902, "top": 0.7875732323232324, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8014103535353535, "width": 0.3338431372549021, "height": 0.011320707070707092, "page": 6}], "section": "4 IMPLEMENTATION", "prob": 0.679302990436554, "is_author_statement": true, "is_in_expected_section": false, "id": "2965"}, {"text": "Given the elements supported by the qualitative feedback, (f) appeared to be the best candidate for our overlayit included the color-coded line drawings to help defne the subject and possible distractors along the borders, while being less noisy than showing all outlines.", "label": "Result", "bboxes": [{"left": 0.5358137254901961, "top": 0.6076919191919191, "width": 0.37715522875816987, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6215290404040403, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6353661616161616, "width": 0.39255882352941185, "height": 0.011320707070707203, "page": 6}, {"left": 0.5195343137254902, "top": 0.6492032828282828, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.0717908496732026, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.6700519323348999, "is_author_statement": true, "is_in_expected_section": false, "id": "2966"}, {"text": "As shown in Figure 7, by default the black layer is opaque (opacity is 1.0) and only the edges most relevant to our two decluttering principles (Section 3.2.1), subject-background separation and image border ficker, are visible.", "label": "Result", "bboxes": [{"left": 0.1769248366013072, "top": 0.5385075757575758, "width": 0.30353267973856213, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.552344696969697, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.5661818181818182, "width": 0.39501307189542484, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.5800189393939394, "width": 0.24379575163398692, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "prob": 0.6605743765830994, "is_author_statement": true, "is_in_expected_section": false, "id": "2967"}, {"text": "From our WoZ prototype studies, we saw promising signs that the participants in fact noticed high level", "label": "Result", "bboxes": [{"left": 0.6788071895424836, "top": 0.8705959595959595, "width": 0.23328921568627448, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.6594366431236267, "is_author_statement": true, "is_in_expected_section": false, "id": "2968"}, {"text": "Figure 2 shows two such examples.", "label": "Result", "bboxes": [{"left": 0.32743954248366014, "top": 0.593854797979798, "width": 0.15300490196078426, "height": 0.011320707070706981, "page": 3}, {"left": 0.08790522875816993, "top": 0.6076919191919191, "width": 0.05802614379084968, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.6588996052742004, "is_author_statement": false, "is_in_expected_section": false, "id": "2969"}, {"text": "Overall, we approximately followed the fnal study design of E et al. for their capture-time composition guidance tool [17] and modify the design for work in remote settings.", "label": "Result", "bboxes": [{"left": 0.08790522875816993, "top": 0.5523421717171717, "width": 0.3947990196078432, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.5661818181818182, "width": 0.3929313725490196, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.5800189393939394, "width": 0.22812745098039217, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.6510527729988098, "is_author_statement": true, "is_in_expected_section": true, "id": "2970"}, {"text": "We ran an informal design survey through Qualtrics with 29 participants (demographics information not collected) to try to understand if these overlay visualizations were interpretable by novice photographers, and if there were strong preferences between the overlay options.", "label": "Result", "bboxes": [{"left": 0.2217156862745098, "top": 0.8152474747474748, "width": 0.25874183006535956, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.39501307189542484, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.3925457516339869, "height": 0.011320707070706981, "page": 5}, {"left": 0.08790522875816993, "top": 0.8567563131313132, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.24487581699346406, "height": 0.011320707070707092, "page": 5}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.6362000107765198, "is_author_statement": true, "is_in_expected_section": false, "id": "2971"}, {"text": "We hypothesized that abstracting an image could help evenly spread the photographers attention across the image, efectively drawing the photographers attention away from the details of the main subject to other areas of the image.", "label": "Result", "bboxes": [{"left": 0.7807009803921569, "top": 0.2925858585858586, "width": 0.13139869281045757, "height": 0.011320707070707037, "page": 2}, {"left": 0.5195343137254902, "top": 0.30642297979797983, "width": 0.39256209150326793, "height": 0.011320707070707037, "page": 2}, {"left": 0.5195343137254902, "top": 0.320260101010101, "width": 0.39255882352941185, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.33409722222222227, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.3479343434343435, "width": 0.0775032679738562, "height": 0.011320707070706981, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS", "prob": 0.6273934245109558, "is_author_statement": true, "is_in_expected_section": false, "id": "2972"}, {"text": "We therefore use grayscale as our baseline condition.", "label": "Result", "bboxes": [{"left": 0.16683333333333333, "top": 0.7275568181818182, "width": 0.3158741830065359, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.6140095591545105, "is_author_statement": true, "is_in_expected_section": true, "id": "2973"}, {"text": "However through the design prototyping and user studies, weve noticed that decluttering seems difcult for people to address, perhaps because it is a new and unfamiliar concept.", "label": "Result", "bboxes": [{"left": 0.3117565359477124, "top": 0.7737361111111111, "width": 0.168704248366013, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.7875732323232324, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.8014103535353535, "width": 0.39503431372549025, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.09830882352941177, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": 0.6128290891647339, "is_author_statement": false, "is_in_expected_section": false, "id": "2974"}, {"text": "One participant noted that the frst overlay made her realize that she hadnt achieved the composition she intended to because she was focused on other aspects of staging the scene.", "label": "Result", "bboxes": [{"left": 0.4552565359477124, "top": 0.7875732323232324, "width": 0.025194444444444408, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.8014103535353535, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.3925392156862745, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.28168137254901965, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.611934244632721, "is_author_statement": false, "is_in_expected_section": false, "id": "2975"}, {"text": "Figure 8 walks through our algorithm for generating our abstraction overlay.", "label": "Result", "bboxes": [{"left": 0.5195343137254902, "top": 0.37707575757575756, "width": 0.3950196078431373, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.3909128787878788, "width": 0.07646568627450978, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "prob": 0.6020647287368774, "is_author_statement": true, "is_in_expected_section": false, "id": "2976"}, {"text": "These abstraction overlays took shape as rough outlines of the objects in the scene, mostly approximated by basic 2D geometric primitives (see Figure 2 for a few examples).", "label": "Result", "bboxes": [{"left": 0.8032124183006536, "top": 0.5032840909090909, "width": 0.10888235294117643, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.5171224747474747, "width": 0.39416503267973857, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.530959595959596, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.5447967171717172, "width": 0.12021078431372545, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.5815954804420471, "is_author_statement": false, "is_in_expected_section": false, "id": "2977"}, {"text": "The evaluation shows that the tool was helpful for making users more confdent in their ability to take a clear and uncluttered photo.", "label": "Result", "bboxes": [{"left": 0.7848529411764705, "top": 0.5837941919191919, "width": 0.1272418300653596, "height": 0.011320707070707092, "page": 1}, {"left": 0.5594183006535948, "top": 0.5976287878787879, "width": 0.35267647058823526, "height": 0.011320707070707092, "page": 1}, {"left": 0.5594183006535948, "top": 0.6114684343434343, "width": 0.2974591503267974, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.5761016011238098, "is_author_statement": false, "is_in_expected_section": true, "id": "2978"}, {"text": "In particular, it indeed showed promise as assistance for decluttering.", "label": "Result", "bboxes": [{"left": 0.3058218954248366, "top": 0.45548484848484855, "width": 0.17462745098039217, "height": 0.011320707070707037, "page": 3}, {"left": 0.08790522875816993, "top": 0.46932196969696965, "width": 0.23108986928104575, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.558472216129303, "is_author_statement": false, "is_in_expected_section": false, "id": "2979"}, {"text": "For a single photo, participants would see a row of images, frst the original photo and then the 6 overlays applied.", "label": "Result", "bboxes": [{"left": 0.5195408496732026, "top": 0.7206439393939394, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195408496732026, "top": 0.7344772727272727, "width": 0.31623039215686277, "height": 0.011320707070707092, "page": 5}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.5546093583106995, "is_author_statement": false, "is_in_expected_section": false, "id": "2980"}, {"text": "Additionally, we found in our low-fdelity prototype that the abstraction also encouraged participants to be more aware of composition.", "label": "Result", "bboxes": [{"left": 0.5190212418300654, "top": 0.7460618686868686, "width": 0.395531045751634, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195375816993464, "top": 0.7598964646464647, "width": 0.394799019607843, "height": 0.011320707070707092, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.2 Abstraction for Composition?", "prob": 0.5478787422180176, "is_author_statement": true, "is_in_expected_section": false, "id": "2981"}, {"text": "Our eyes are drawn to regions of high contrast.", "label": "Result", "bboxes": [{"left": 0.6131372549019608, "top": 0.6492032828282828, "width": 0.3011977124183006, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.5393251776695251, "is_author_statement": true, "is_in_expected_section": false, "id": "2982"}, {"text": "While we didnt ask if they wanted to take another photo after this review step, occasionally (6) participants would ask if they could take a third photo and we would repeat the process above, presenting them with a third overlay.", "label": "Result", "bboxes": [{"left": 0.781687908496732, "top": 0.7938623737373738, "width": 0.13040522875816996, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8076994949494949, "width": 0.39504084967320263, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8215366161616162, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 2}, {"left": 0.5189918300653594, "top": 0.8353737373737373, "width": 0.3931062091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8492108585858587, "width": 0.04691176470588243, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "prob": 0.5383312702178955, "is_author_statement": true, "is_in_expected_section": false, "id": "2983"}, {"text": "We show edges within the subject(s) also in yellow and background edges not within the subject(s) and image borders in white (see Figure 1).", "label": "Result", "bboxes": [{"left": 0.8583725490196078, "top": 0.42781186868686866, "width": 0.054259803921568595, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.4416489898989899, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.45548484848484855, "width": 0.3948218954248366, "height": 0.011320707070707037, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.5336337685585022, "is_author_statement": true, "is_in_expected_section": true, "id": "2984"}, {"text": "In particular, in running our pilot, we also wanted to understand if a no guidance interface was a reasonable baseline to compare our tool against.", "label": "Result", "bboxes": [{"left": 0.7353006535947713, "top": 0.8567588383838384, "width": 0.17704901960784303, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8705959595959595, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.2970669934640522, "height": 0.011320707070707092, "page": 7}], "section": "5 USER EVALUATION", "prob": 0.5224911570549011, "is_author_statement": true, "is_in_expected_section": true, "id": "2985"}, {"text": "In particular, we saw that the abstraction overlay seemed most helpful for the purposes of noticing unwanted clutter in an image, so we decided that we would target designing an abstraction overlay that provides decluttering guidance.", "label": "Result", "bboxes": [{"left": 0.654781045751634, "top": 0.5149027777777778, "width": 0.2572941176470589, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.528739898989899, "width": 0.39254411764705877, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5425770202020203, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5564141414141415, "width": 0.3522385620915033, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.5214235782623291, "is_author_statement": true, "is_in_expected_section": false, "id": "2986"}, {"text": "We distributed the app using TestFlight.", "label": "Result", "bboxes": [{"left": 0.15370915032679738, "top": 0.8290820707070707, "width": 0.24400980392156862, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.5172408223152161, "is_author_statement": true, "is_in_expected_section": true, "id": "2987"}, {"text": "While there isnt necessarily a clear defnition of failure, the main failure case with our current algorithm is when the main subject is not properly captured in the saliency map.", "label": "Result", "bboxes": [{"left": 0.10418464052287582, "top": 0.28252272727272726, "width": 0.3762745098039215, "height": 0.011320707070707037, "page": 11}, {"left": 0.08791013071895425, "top": 0.2963573232323232, "width": 0.3925637254901961, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.3101969696969697, "width": 0.30394444444444446, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "prob": 0.5123031139373779, "is_author_statement": true, "is_in_expected_section": false, "id": "2988"}, {"text": "Many (10) participants also mentioned that the darkening of the image was particularly helpful for seeing the lines due to the contrast, but with the caveat that it made the original image harder to see.", "label": "Result", "bboxes": [{"left": 0.5358137254901961, "top": 0.317114898989899, "width": 0.3762745098039215, "height": 0.011320707070707037, "page": 6}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.39255555555555544, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.3447891414141414, "width": 0.39283496732026146, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.3586262626262626, "width": 0.03891339869281052, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.49882960319519043, "is_author_statement": false, "is_in_expected_section": false, "id": "2989"}, {"text": "One participant (P1) observed that the tool jumps back and forth in its highlighting of a subject, She interprets this to mean that there is no clear subject in her photo, and confrms that this is consistent with her own perception.", "label": "Result", "bboxes": [{"left": 0.10418464052287582, "top": 0.5979570707070707, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790359477124182, "top": 0.6117916666666666, "width": 0.3925441176470588, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790359477124182, "top": 0.625625, "width": 0.3925441176470588, "height": 0.011320707070707092, "page": 10}, {"left": 0.0873578431372549, "top": 0.6394583333333334, "width": 0.1477516339869281, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.49708548188209534, "is_author_statement": false, "is_in_expected_section": true, "id": "2990"}, {"text": "We note that while this overlay is helpful for identifying potential issues along the subject-background boundary, it does not help to draw any attention towards potential clutter to address along image borders.", "label": "Result", "bboxes": [{"left": 0.4615571895424837, "top": 0.5689431818181818, "width": 0.01890849673202616, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5827790404040404, "width": 0.39502450980392156, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5966161616161616, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6104532828282828, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.624290404040404, "width": 0.04843464052287581, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.49694013595581055, "is_author_statement": true, "is_in_expected_section": false, "id": "2991"}, {"text": "Figure 11 shows how an unexpected edge highlight encourages the participant to explore diferent backgrounds and compositions.", "label": "Result", "bboxes": [{"left": 0.5358137254901961, "top": 0.8429217171717173, "width": 0.37626307189542485, "height": 0.011320707070706981, "page": 9}, {"left": 0.5195343137254902, "top": 0.8567588383838384, "width": 0.3948022875816992, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.4896065294742584, "is_author_statement": false, "is_in_expected_section": true, "id": "2992"}, {"text": "Video demonstration of the tool can be found in the accompanying video fgure.", "label": "Result", "bboxes": [{"left": 0.6828627450980391, "top": 0.31250252525252525, "width": 0.22921568627450994, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.32633964646464647, "width": 0.2425212418300653, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.2 Mobile Implementation", "prob": 0.4892809987068176, "is_author_statement": false, "is_in_expected_section": false, "id": "2993"}, {"text": "Materials for the study can be found in supplemental materials.", "label": "Result", "bboxes": [{"left": 0.2653986928104575, "top": 0.29870580808080804, "width": 0.21504901960784312, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.31254292929292926, "width": 0.15797385620915033, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.4885910451412201, "is_author_statement": false, "is_in_expected_section": true, "id": "2994"}, {"text": "We quickly walked all participants through the basic (no guidance) camera app.", "label": "Result", "bboxes": [{"left": 0.6464624183006535, "top": 0.6750909090909091, "width": 0.2656176470588235, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.6889255050505051, "width": 0.2221764705882353, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "prob": 0.48481979966163635, "is_author_statement": true, "is_in_expected_section": true, "id": "2995"}, {"text": "As a result, the participant has more clearly considered the concept of telling the story of their subject in this space.", "label": "Result", "bboxes": [{"left": 0.2589019607843137, "top": 0.7639987373737374, "width": 0.22154901960784312, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.7778383838383839, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.791675505050505, "width": 0.07733986928104576, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.45847785472869873, "is_author_statement": false, "is_in_expected_section": true, "id": "2996"}, {"text": "Further discussing the latter observation that (a)(c) were more helpful for SBS, several (8) participants specifcally noted that the color fattening was helpful for noticing SBS.", "label": "Result", "bboxes": [{"left": 0.5358137254901961, "top": 0.4139747474747475, "width": 0.3762777777777778, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.42781186868686866, "width": 0.39256209150326804, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.4416489898989899, "width": 0.2679607843137255, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.45319080352783203, "is_author_statement": false, "is_in_expected_section": false, "id": "2997"}, {"text": "Another participant described that the external representation provided by the interface assisted in the process of exploring the scene and quickly evaluating diferent options: It caused me to experiment more... didnt see it as a rule that I needed to minimize lines, but the tool made it easy to move around and check by that metric, how good it was (P9).", "label": "Result", "bboxes": [{"left": 0.5358137254901961, "top": 0.75989898989899, "width": 0.37626960784313734, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7737361111111111, "width": 0.3925375816993464, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7875732323232324, "width": 0.39256045751633983, "height": 0.011343434343434211, "page": 9}, {"left": 0.5195343137254902, "top": 0.8014330808080808, "width": 0.3925408496732028, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.815270202020202, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.8290845959595959, "width": 0.1808725490196078, "height": 0.011343434343434433, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.4447346031665802, "is_author_statement": false, "is_in_expected_section": true, "id": "2998"}, {"text": "On the other hand, another participant (P18) had a slightly different interpretation of the lack of a consistent subject or in this case, no identifed subject.", "label": "Result", "bboxes": [{"left": 0.10418464052287582, "top": 0.6671426767676767, "width": 0.37873529411764717, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.680979797979798, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.6948169191919191, "width": 0.16211928104575163, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.44207727909088135, "is_author_statement": false, "is_in_expected_section": true, "id": "2999"}, {"text": "The image border was a fxed pixel width border along the outer borders of the image, and the remaining background was anything between the subject border and this image border.", "label": "Result", "bboxes": [{"left": 0.8477941176470588, "top": 0.6538093434343435, "width": 0.06428594771241836, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6676527777777778, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.681489898989899, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6953270202020202, "width": 0.21534640522875814, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "prob": 0.4373873770236969, "is_author_statement": false, "is_in_expected_section": false, "id": "3000"}, {"text": "For example in Figure 10, this participant (P7) refnes the camera angle until the edges in the overlay look the way she wants.", "label": "Result", "bboxes": [{"left": 0.5358137254901961, "top": 0.5387083333333333, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.5525454545454546, "width": 0.3544297385620915, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.4273318946361542, "is_author_statement": false, "is_in_expected_section": true, "id": "3001"}, {"text": "s method performs at interactive speeds and produces temporal results, making it appropriate for interactive use on a live camera feed [36].", "label": "Result", "bboxes": [{"left": 0.299, "top": 0.2617651515151515, "width": 0.1814705882352941, "height": 0.011320707070707037, "page": 2}, {"left": 0.08790522875816993, "top": 0.275604797979798, "width": 0.39283169934640527, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.2894419191919192, "width": 0.24884640522875823, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": 0.4264228045940399, "is_author_statement": false, "is_in_expected_section": false, "id": "3002"}, {"text": "For each of the 12 photos/videos, participants were asked:", "label": "Result", "bboxes": [{"left": 0.46039869281045753, "top": 0.3835467171717172, "width": 0.020338235294117657, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.3973838383838384, "width": 0.32248202614379085, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.42392849922180176, "is_author_statement": false, "is_in_expected_section": false, "id": "3003"}, {"text": "Given these principles, we wondered what annotation methods photographers currently used for highlighting clutter.", "label": "Result", "bboxes": [{"left": 0.10418464052287582, "top": 0.41673611111111114, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.43057323232323236, "width": 0.3299183006535948, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.4126272201538086, "is_author_statement": true, "is_in_expected_section": false, "id": "3004"}, {"text": "We ran remote studies over Zoom with 18 participants (6 male, 10 female), 24 to 32 years old (  = 29), to understand if the tool would help users declutter photos, and if users felt creative while using the tool.", "label": "Result", "bboxes": [{"left": 0.08720261437908497, "top": 0.8152474747474748, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8264469696969697, "width": 0.39254901960784316, "height": 0.013969696969696965, "page": 9}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.392545751633987, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.04986111111111112, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "prob": 0.40477198362350464, "is_author_statement": true, "is_in_expected_section": true, "id": "3005"}, {"text": "For edge ficker, it almost seems like the opposite.", "label": "Result", "bboxes": [{"left": 0.5195343137254902, "top": 0.12342045454545454, "width": 0.2735196078431372, "height": 0.011320707070707078, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.4018133878707886, "is_author_statement": false, "is_in_expected_section": false, "id": "3006"}, {"text": "As a reminder, the lines are color-coded such that lines within and immediately around the subject are yellow, lines along the image border are cyan, and remaining lines in the background are white.", "label": "Result", "bboxes": [{"left": 0.10418464052287582, "top": 0.593854797979798, "width": 0.37625816993464056, "height": 0.011320707070706981, "page": 7}, {"left": 0.08790522875816993, "top": 0.6076919191919191, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6215290404040403, "width": 0.392547385620915, "height": 0.011320707070707092, "page": 7}, {"left": 0.08736437908496732, "top": 0.6353661616161616, "width": 0.036310457516339864, "height": 0.011320707070707203, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "prob": 0.3572602868080139, "is_author_statement": false, "is_in_expected_section": false, "id": "3007"}, {"text": "We are inspired by the iterative loop of testing and evaluating that these forms of feedback encourage.", "label": "Result", "bboxes": [{"left": 0.23955718954248367, "top": 0.8286818181818182, "width": 0.24091013071895423, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.8425189393939394, "width": 0.3592222222222222, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "prob": 0.3568287193775177, "is_author_statement": true, "is_in_expected_section": false, "id": "3008"}, {"text": "Therefore we wonder if it is ok to have a slightly more opinionated interface (at least for some training) as is suggested by one of our design survey participants: Making it very clear about the distinction between whether the overlay is used to indicate if the two properties are good, bad, or both would be very helpful (P18).", "label": "Result", "bboxes": [{"left": 0.7584624183006535, "top": 0.12339772727272727, "width": 0.1536307189542485, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1372348484848485, "width": 0.3928316993464054, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.3950424836601307, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.39255555555555555, "height": 0.01134343434343435, "page": 11}, {"left": 0.5195343137254902, "top": 0.17876767676767677, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.20352124183006526, "height": 0.011343434343434322, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "prob": 0.35169631242752075, "is_author_statement": true, "is_in_expected_section": false, "id": "3009"}, {"text": "Inspired by this idea of using outlines to highlight contrast and the lack of contrast, we hoped to recreate this outlining as an overlay directly in the camera (see Figure 5), while also extending it to contrast along the image borders.", "label": "Result", "bboxes": [{"left": 0.33908660130718954, "top": 0.6492032828282828, "width": 0.1413643790849673, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6630378787878788, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 4}, {"left": 0.0874656862745098, "top": 0.6907146464646465, "width": 0.39298529411764704, "height": 0.011320707070706981, "page": 4}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.0474656862745098, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "prob": 0.3214755356311798, "is_author_statement": true, "is_in_expected_section": false, "id": "3010"}], "2102.09039": [{"text": "To address challenge 1), We designed the HTML annotation as a simple extension of the existing HTML and CSS grammar, where instead of specifying a single value for an attribute, a designer can provide multiple candidate values for it, which are to be explored by Spacewalker.", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.7255656565656566, "width": 0.3762794117647059, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7394027777777777, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7532398989898991, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7670770202020202, "width": 0.39255392156862734, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7809141414141415, "width": 0.09825653594771244, "height": 0.011320707070707092, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3011"}, {"text": "To address challenge 2), we enhanced a genetic algorithm by adding feedback mask-based stochastic sampling, to accommodate crowd worker responses from pairwise comparison of UI designsthat tends to yield more reliable feedback than rating each design separately.", "label": "Author", "bboxes": [{"left": 0.6219313725490196, "top": 0.7809141414141415, "width": 0.29016666666666646, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7947512626262626, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.8085883838383839, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.822425505050505, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.8362626262626264, "width": 0.13726470588235296, "height": 0.011320707070706981, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3012"}, {"text": "To address challenge 3), we created a webbased tool that streamlined the entire task of design exploration including task creation, monitoring and evaluation.", "label": "Author", "bboxes": [{"left": 0.6604624183006536, "top": 0.8362626262626264, "width": 0.2541062091503268, "height": 0.011320707070706981, "page": 0}, {"left": 0.5195343137254902, "top": 0.8500997474747475, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.8639368686868688, "width": 0.30686437908496733, "height": 0.011320707070706981, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3013"}, {"text": "In this paper, we present Spacewalker, a tool that allows designers to rapidly search a design space of a web UI for an optimal design within that space (see Figure 1).", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.545685606060606, "width": 0.3787565359477124, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5595227272727272, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5733598484848484, "width": 0.24064379084967324, "height": 0.011320707070707092, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3014"}, {"text": "In a typical HTML page or a CSS specification, designers first annotate each attribute they want to explore using a simple markup extension we designed.", "label": "Author", "bboxes": [{"left": 0.7644967320261438, "top": 0.5733598484848484, "width": 0.14760130718954245, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5871957070707071, "width": 0.3929411764705881, "height": 0.011320707070706981, "page": 0}, {"left": 0.5189918300653594, "top": 0.6010328282828282, "width": 0.36693137254901953, "height": 0.011320707070707092, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3015"}, {"text": "Our tool then parses the annotated HTML or CSS specification, and intelligently generates and distributes various configurations of the web UI to crowd workers for evaluation.", "label": "Author", "bboxes": [{"left": 0.8893267973856209, "top": 0.6010328282828282, "width": 0.023044117647058715, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.6148699494949494, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.6287070707070707, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 0}, {"left": 0.5189918300653594, "top": 0.6425441919191919, "width": 0.24420261437908508, "height": 0.011320707070707092, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3016"}, {"text": "Our research challenges are three-fold:", "label": "Author", "bboxes": [{"left": 0.7668790849673203, "top": 0.6425441919191919, "width": 0.14520751633986928, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.6563813131313132, "width": 0.08375653594771237, "height": 0.011320707070707092, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3017"}, {"text": "Copyrights for third-party components of this work must be honored.", "label": "Author", "bboxes": [{"left": 0.166109477124183, "top": 0.8340328282828282, "width": 0.31610947712418297, "height": 0.008805555555555622, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3018"}, {"text": "In Spacewalker, we aim to address these issues by providing an integrated support for designers to explore a large design space and improve their design.", "label": "Author", "bboxes": [{"left": 0.6606797385620915, "top": 0.7258472222222222, "width": 0.2514183006535947, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7396843434343435, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7535214646464646, "width": 0.2761127450980392, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3019"}, {"text": "We evaluated Spacewalker by asking interaction designers to use it for exploring a set of UI design tasks, and Spacewalker received positive feedback.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.46294444444444444, "width": 0.3762794117647058, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.47678156565656565, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.49061742424242427, "width": 0.10406699346405228, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3020"}, {"text": "To systematically examine how well Spacewalker algorithms can evolve a design by quickly searching a design space, we tested it on six design tasks that range in search space sizes and design types.", "label": "Author", "bboxes": [{"left": 0.19482843137254902, "top": 0.49061742424242427, "width": 0.2859101307189542, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5044545454545455, "width": 0.3941683006535948, "height": 0.011320707070707092, "page": 1}, {"left": 0.08736437908496732, "top": 0.5182916666666667, "width": 0.39310620915032685, "height": 0.011320707070706981, "page": 1}, {"left": 0.08790522875816993, "top": 0.5321287878787879, "width": 0.0798300653594771, "height": 0.011320707070706981, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3021"}, {"text": "Our paper makes the following contributions:", "label": "Author", "bboxes": [{"left": 0.35312581699346407, "top": 0.5598030303030302, "width": 0.12733823529411759, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5736401515151515, "width": 0.1445326797385621, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3022"}, {"text": "Our work is related to three areas of the literature, including UI evaluation methods, crowdsourcing-based design support, and interactive UI design optimization.", "label": "Author", "bboxes": [{"left": 0.08790522875816993, "top": 0.761003787878788, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.7748409090909091, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.7886780303030303, "width": 0.19503431372549018, "height": 0.011320707070707092, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3023"}, {"text": "Similar to previous work, we also embed the crowd in the loop of the design and evaluation process.", "label": "Author", "bboxes": [{"left": 0.4210065359477124, "top": 0.26176767676767676, "width": 0.05946241830065363, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.275604797979798, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.2894419191919192, "width": 0.16637254901960785, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3024"}, {"text": "However, we focus on the design task where an interaction designer has a basic HTML design and wants to obtain an optimal configuration for the design by exploring a large range of options such as colors, fonts and layouts.", "label": "Author", "bboxes": [{"left": 0.25794771241830067, "top": 0.2894419191919192, "width": 0.22290032679738558, "height": 0.011320707070707092, "page": 2}, {"left": 0.08736437908496732, "top": 0.30327777777777776, "width": 0.3931062091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.317114898989899, "width": 0.39256372549019614, "height": 0.011320707070707037, "page": 2}, {"left": 0.08790522875816993, "top": 0.3309520202020202, "width": 0.34070751633986923, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3025"}, {"text": "We also aim to minimize the effort and cost of the designer to perform the task.", "label": "Author", "bboxes": [{"left": 0.4326192810457516, "top": 0.3309520202020202, "width": 0.04784967320261446, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.3447891414141414, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.3586262626262626, "width": 0.027763071895424832, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3026"}, {"text": "Although we employ GA-based algorithms and crowd in our work, Spacewalker is designed to address a general web UI design scenario.", "label": "Author", "bboxes": [{"left": 0.40123856209150327, "top": 0.6492032828282828, "width": 0.07922385620915029, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3373758169934641, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3027"}, {"text": "We also enhanced genetic programming for addressing worker responses from pairwise comparison of designs, which makes genetic programming more applicable for UI optimization.", "label": "Author", "bboxes": [{"left": 0.23991503267973857, "top": 0.7045517676767676, "width": 0.2405457516339869, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.718388888888889, "width": 0.39503594771241834, "height": 0.011320707070706981, "page": 2}, {"left": 0.08790522875816993, "top": 0.7322260101010101, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.7460618686868686, "width": 0.079187908496732, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3028"}, {"text": "We here describe how UI designers or developers would use Spacewalker to explore the design space of their user interfaces.", "label": "Author", "bboxes": [{"left": 0.08720261437908497, "top": 0.8014103535353535, "width": 0.3957352941176471, "height": 0.011320707070707092, "page": 2}, {"left": 0.08736437908496732, "top": 0.8152474747474748, "width": 0.341718954248366, "height": 0.011320707070707092, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3029"}, {"text": "In this section, we discuss the system design and algorithmic details that underline the Spacewalker.", "label": "Author", "bboxes": [{"left": 0.08790522875816993, "top": 0.8100050505050506, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.8238421717171717, "width": 0.18967810457516343, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3030"}, {"text": "As shown in the above example, Spacewalker supports a rich set of methods for exploring a design space through simple HTML extensions, which are intuitive to designers as shown in our experiments.", "label": "Author", "bboxes": [{"left": 0.5190212418300654, "top": 0.469530303030303, "width": 0.39307843137254905, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.48336742424242424, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.49720454545454545, "width": 0.3948153594771242, "height": 0.011320707070707037, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3031"}, {"text": "We here discuss its syntax and parsing details.", "label": "Author", "bboxes": [{"left": 0.5188316993464052, "top": 0.5110416666666667, "width": 0.2766683006535948, "height": 0.011320707070707092, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3032"}, {"text": "In our early exploration, we found conventional GA sensitive to the random initialization of design options.", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.75989898989899, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7737361111111111, "width": 0.27940849673202606, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3033"}, {"text": "To address the issue, we enhance traditional genetic algorithms, for each iteration, by directing rater feedback to genes that participate in a comparison while allowing the rest genes in a sequence to remain stochastic in the downstream evolution.", "label": "Author", "bboxes": [{"left": 0.51909477124183, "top": 0.8429217171717173, "width": 0.39327941176470604, "height": 0.011320707070706981, "page": 4}, {"left": 0.5195343137254902, "top": 0.8567588383838384, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8705959595959595, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.2844787581699346, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3034"}, {"text": "Thus, our fitness function outputs 1 for the preferred design and 0 for the less preferred one.", "label": "Author", "bboxes": [{"left": 0.765313725490196, "top": 0.26176767676767676, "width": 0.1467777777777779, "height": 0.011332070707070707, "page": 4}, {"left": 0.5195343137254902, "top": 0.2749368686868687, "width": 0.394812091503268, "height": 0.011988636363636396, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3035"}, {"text": "Although presenting more than two examples in a gallery design can be another appealing alternative [1, 13], the viewing area for each example would be too small in our case of web design, and may prevent raters from noticing design details that matter.", "label": "Author", "bboxes": [{"left": 0.5190212418300654, "top": 0.2894419191919192, "width": 0.39307352941176465, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.30327777777777776, "width": 0.39283823529411754, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.317114898989899, "width": 0.3925637254901959, "height": 0.011320707070707037, "page": 4}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.35701470588235296, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3036"}, {"text": "On the other hand, with a limited number of worker feedback, which is often the case in reality, a search very likely ends up with a sub-optimal design, as shown in our experiments later.", "label": "Author", "bboxes": [{"left": 0.4618790849673203, "top": 0.36862500000000004, "width": 0.01858986928104578, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.38246212121212125, "width": 0.3925653594771242, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.39629924242424247, "width": 0.39256372549019614, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.4101363636363636, "width": 0.32421895424836605, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3037"}, {"text": "We here focus on Genetic Algorithms, a popular choice that has been used in the literature, with several important enhancements.", "label": "Author", "bboxes": [{"left": 0.39460947712418304, "top": 0.42397222222222225, "width": 0.08585294117647058, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.43780934343434347, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.45164646464646463, "width": 0.2919346405228758, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3038"}, {"text": "For simplicity, we did not use these features in this paper.", "label": "Author", "bboxes": [{"left": 0.46042156862745104, "top": 0.12339772727272727, "width": 0.020320261437908438, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.1372348484848485, "width": 0.32019771241830064, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3039"}, {"text": "We refer an instance of a UI design, which is acquired by selecting a specific option for each attribute to be explored, as a configuration of the design.", "label": "Author", "bboxes": [{"left": 0.4615653594771242, "top": 0.7427878787878788, "width": 0.018895424836601227, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.756625, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.7704621212121212, "width": 0.39256045751633994, "height": 0.011332070707070763, "page": 4}, {"left": 0.08790522875816993, "top": 0.7842992424242424, "width": 0.06349019607843136, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3040"}, {"text": "To adapt the genetic algorithm for searching an optimal UI configuration, we first encode each configuration as a genetic sequence, which is an ordered list of valued attributes whose value is denoted by the index to an", "label": "Author", "bboxes": [{"left": 0.35863071895424836, "top": 0.8119734848484849, "width": 0.12183496732026144, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8258093434343435, "width": 0.3925604575163399, "height": 0.011320707070706981, "page": 4}, {"left": 0.08790522875816993, "top": 0.8396464646464646, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8534835858585857, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3041"}, {"text": "The goal of our initialization process is to maximally cover possible options for each attribute, but meanwhile limit the total number of designs to be compared by human raters.", "label": "Author", "bboxes": [{"left": 0.19848039215686275, "top": 0.3586262626262626, "width": 0.2844591503267974, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.37246338383838384, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.38630050505050506, "width": 0.35276143790849673, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3042"}, {"text": "Therefore, we treat each attribute independently during initialization, and generate design variations by sampling the options of one attribute at a time while affixing the rest exploratory attributes at a random value.", "label": "Author", "bboxes": [{"left": 0.44373856209150325, "top": 0.38630050505050506, "width": 0.03919771241830072, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.4001376262626263, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.4139747474747475, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.42781186868686866, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 5}, {"left": 0.08753921568627451, "top": 0.4416489898989899, "width": 0.034862745098039216, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3043"}, {"text": "To do so, we introduce a bit mask, named feedback mask , for each genetic sequencethat corresponds a design instance, which has the same length as a genetic sequence.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.10956060606060607, "width": 0.3762859477124183, "height": 0.011332070707070707, "page": 5}, {"left": 0.08790522875816993, "top": 0.12339772727272727, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.1372348484848485, "width": 0.22974673202614382, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3044"}, {"text": "For each pair of designs shown to a crowd worker, we compute a diff mask to capture the differences between their genetic sequences where the corresponding bits for the differences are set to 1 .", "label": "Author", "bboxes": [{"left": 0.2854297385620915, "top": 0.1649078282828283, "width": 0.19503758169934643, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.17874494949494948, "width": 0.392562091503268, "height": 0.011332070707070735, "page": 5}, {"left": 0.08790522875816993, "top": 0.1925820707070707, "width": 0.3928398692810458, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.20575126262626264, "width": 0.15151470588235294, "height": 0.011988636363636312, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3045"}, {"text": "The diff mask represents where we would likely gain knowledge by comparing these two designs: in the part where they differ.", "label": "Author", "bboxes": [{"left": 0.241890522875817, "top": 0.20641919191919192, "width": 0.23857189542483662, "height": 0.011320707070707037, "page": 5}, {"left": 0.08790522875816993, "top": 0.22025631313131314, "width": 0.3925571895424837, "height": 0.011320707070707037, "page": 5}, {"left": 0.08736437908496732, "top": 0.23409343434343433, "width": 0.10527941176470589, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3046"}, {"text": "We elaborate on our algorithm below and particularly focus on how the masks operate in the initialization, crossover, and mutation phase.", "label": "Author", "bboxes": [{"left": 0.23217810457516339, "top": 0.317114898989899, "width": 0.24828921568627452, "height": 0.011320707070707037, "page": 5}, {"left": 0.08790522875816993, "top": 0.3309520202020202, "width": 0.39416503267973857, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.3447891414141414, "width": 0.18438235294117644, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3047"}, {"text": "We enhanced the single-point crossover operation of genetic algorithms, by assigning random option values to positions in the sequence (corresponding attributes in the design) where the feedback mask is 0 in the descendants.", "label": "Author", "bboxes": [{"left": 0.17576143790849674, "top": 0.5108333333333334, "width": 0.30470424836601306, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.5246704545454546, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.5385075757575758, "width": 0.39256699346405227, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.5516767676767677, "width": 0.2360114379084967, "height": 0.011988636363636451, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3048"}, {"text": "As a result, we introduce variations to asequencewhere we haveyet to acquire raterfeedback.", "label": "Author", "bboxes": [{"left": 0.32783169934640527, "top": 0.552344696969697, "width": 0.15263888888888882, "height": 0.011320707070707092, "page": 5}, {"left": 0.08753921568627451, "top": 0.5661818181818182, "width": 0.3951879084967319, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3049"}, {"text": "We invited five participants for this remote user study.", "label": "Author", "bboxes": [{"left": 0.7086290849673202, "top": 0.5293876262626263, "width": 0.20346732026143788, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.5432247474747475, "width": 0.10915196078431366, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3050"}, {"text": "These participants resemble HCI researchers and web developers who want to improve their design by quickly examining detailed design options with users at scale.We", "label": "Author", "bboxes": [{"left": 0.7489771241830065, "top": 0.5985719696969697, "width": 0.16312091503267967, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.6124090909090909, "width": 0.39283333333333337, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.6262462121212121, "width": 0.39256209150326793, "height": 0.011320707070706981, "page": 5}, {"left": 0.5195343137254902, "top": 0.6400833333333333, "width": 0.03528594771241833, "height": 0.0251578282828282, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3051"}, {"text": "We evaluate Spacewalker in multiple dimensions.", "label": "Author", "bboxes": [{"left": 0.5188316993464052, "top": 0.3218320707070707, "width": 0.3049624183006536, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3052"}, {"text": "We conduct a user study to investigate whether Spacewalker markup extension to HTML is easy to understand and use by designers and developers, and how they react to the overall support of Spacewalker for design exploration.", "label": "Author", "bboxes": [{"left": 0.828156862745098, "top": 0.3218320707070707, "width": 0.08394117647058819, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.3356691919191919, "width": 0.39256045751633983, "height": 0.011320707070707037, "page": 5}, {"left": 0.5195343137254902, "top": 0.34950631313131314, "width": 0.39416993464052286, "height": 0.011320707070707037, "page": 5}, {"left": 0.5195343137254902, "top": 0.3633434343434343, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.3771805555555555, "width": 0.07033986928104574, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3053"}, {"text": "We also systematically examine how well Spacewalker explores a design space for designers and improve designs over iterations.", "label": "Author", "bboxes": [{"left": 0.5932630718954248, "top": 0.3771805555555555, "width": 0.31910457516339874, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.39101767676767674, "width": 0.39283333333333337, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.40485479797979795, "width": 0.06015686274509813, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3054"}, {"text": "Our front end includes a task authoring interface for a designer to create and launch a task (see Figure 2), a monitor interface for the designer to monitor the task progress and export results (see Figure 4), and a worker interface for the worker to compare a pair of designs (see Figure 3).", "label": "Author", "bboxes": [{"left": 0.15269117647058825, "top": 0.8048611111111111, "width": 0.32805065359477126, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8186982323232324, "width": 0.39283333333333337, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8325353535353536, "width": 0.39255555555555555, "height": 0.011320707070706981, "page": 5}, {"left": 0.08790522875816993, "top": 0.8463724747474748, "width": 0.395031045751634, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8602095959595959, "width": 0.21625000000000005, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3055"}, {"text": "Our backend parses a design", "label": "Author", "bboxes": [{"left": 0.30781045751633984, "top": 0.8602095959595959, "width": 0.17265686274509806, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3056"}, {"text": "Mutation is handled in a similar way to the traditional approach, where we alter one attribute in a genetic sequence based on a mutation rate.", "label": "Author", "bboxes": [{"left": 0.17194281045751633, "top": 0.6215290404040403, "width": 0.30852450980392154, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.6353661616161616, "width": 0.39256045751633994, "height": 0.011320707070707203, "page": 5}, {"left": 0.08790522875816993, "top": 0.6492032828282828, "width": 0.11604084967320262, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3057"}, {"text": "We chose a .03 mutation rate in Spacewalker, which is in line with other genetic algorithms applied on a similar population size.", "label": "Author", "bboxes": [{"left": 0.20760620915032682, "top": 0.6492032828282828, "width": 0.2744771241830065, "height": 0.011320707070707092, "page": 5}, {"left": 0.08736437908496732, "top": 0.6630404040404041, "width": 0.39338071895424837, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.0941470588235294, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3058"}, {"text": "In this study, we evaluate the usability of our proposed HTML extensions by gather informal feedback from web designers.", "label": "Author", "bboxes": [{"left": 0.5195343137254902, "top": 0.45108333333333334, "width": 0.39293790849673205, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.46491919191919195, "width": 0.36536274509803923, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3059"}, {"text": "The goal is to test whether web designers are able to learn and use our markups to specify search criteria and launch a design exploration task, and to gather feedback of the Spacewalker system.", "label": "Author", "bboxes": [{"left": 0.8885424836601308, "top": 0.46491919191919195, "width": 0.02354901960784317, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.47875631313131317, "width": 0.39283660130718956, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.4925934343434344, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.5064305555555556, "width": 0.33367647058823524, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3060"}, {"text": "Finally, we consider nested designs (where one option value depend on another parent value).", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.6907146464646465, "width": 0.37627450980392163, "height": 0.011320707070706981, "page": 5}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.20579411764705885, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3061"}, {"text": "In this case, we link the child choices to the parent choices in the genes.", "label": "Author", "bboxes": [{"left": 0.2982418300653595, "top": 0.7045517676767676, "width": 0.18222712418300657, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.718388888888889, "width": 0.2408839869281046, "height": 0.011320707070706981, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3062"}, {"text": "When a parent is selected, only the relevant child genes would be active, and we only perform the crossover and mutation operations on these genes.", "label": "Author", "bboxes": [{"left": 0.33153104575163395, "top": 0.718388888888889, "width": 0.15054248366013073, "height": 0.011320707070706981, "page": 5}, {"left": 0.08790522875816993, "top": 0.7322260101010101, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.7460618686868686, "width": 0.3255882352941177, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3063"}, {"text": "We asked each participant to add markups to one template HTML web page (the Cover example in Section 5.2.1), specifying exploration options for attributes or style sheet entries that they would like to change.", "label": "Author", "bboxes": [{"left": 0.8247287581699346, "top": 0.6677575757575758, "width": 0.08736274509803932, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.681594696969697, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.6954318181818182, "width": 0.39283496732026146, "height": 0.011332070707070652, "page": 5}, {"left": 0.5195343137254902, "top": 0.7092689393939394, "width": 0.3948202614379086, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3064"}, {"text": "We verbally walked them through the code snippets and demoed the usage, which took 1015 minutes.", "label": "Author", "bboxes": [{"left": 0.5188316993464052, "top": 0.7231060606060606, "width": 0.3932549019607844, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.7369431818181819, "width": 0.2242696078431372, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3065"}, {"text": "Participants then edited the provided the HTML documents in their preferred code or text editors, and we recorded the time used for them to experiment with the markups and complete each task.", "label": "Author", "bboxes": [{"left": 0.747452614379085, "top": 0.7369431818181819, "width": 0.16464542483660127, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.7507790404040403, "width": 0.39503267973856215, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.7646161616161616, "width": 0.39255718954248375, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.7784532828282827, "width": 0.22053431372549026, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3066"}, {"text": "We reviewed their completed HTML specifications to check if they were correct.", "label": "Author", "bboxes": [{"left": 0.7664983660130719, "top": 0.8061275252525252, "width": 0.14806535947712418, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.8199646464646465, "width": 0.3432287581699346, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3067"}, {"text": "All participants were able to learn the Spacewalker markup syntax using the description we provided and were able to create syntactically correct specifications.", "label": "Author", "bboxes": [{"left": 0.6833088235294118, "top": 0.8429217171717173, "width": 0.22878104575163383, "height": 0.011320707070706981, "page": 5}, {"left": 0.5195343137254902, "top": 0.8567588383838384, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 5}, {"left": 0.5189918300653594, "top": 0.8705959595959595, "width": 0.33919934640522875, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3068"}, {"text": "We analyzed the specifications submitted by the participants, and Table 1", "label": "Author", "bboxes": [{"left": 0.8631225490196078, "top": 0.8705959595959595, "width": 0.051452614379085104, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.3929003267973856, "height": 0.011320707070707092, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3069"}, {"text": "We conducted two experiments to evaluate whether Spacewalker was able to efficiently search a design space and generate better designs by utilizing the responses from the crowd workers.", "label": "Author", "bboxes": [{"left": 0.08720261437908497, "top": 0.4878926767676768, "width": 0.39354901960784316, "height": 0.011320707070707092, "page": 6}, {"left": 0.08736437908496732, "top": 0.501729797979798, "width": 0.3933856209150327, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5155669191919192, "width": 0.36811601307189545, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3070"}, {"text": "We compare Spacewalker genetic algorithm against a baseline method that uniformly samples the design space for crowd evaluation.", "label": "Author", "bboxes": [{"left": 0.4607434640522876, "top": 0.5155669191919192, "width": 0.019725490196078443, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5294040404040404, "width": 0.39256699346405227, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5432411616161616, "width": 0.376467320261438, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3071"}, {"text": "In the first experiment, we examine the effect of different search space sizes on the techniques.", "label": "Author", "bboxes": [{"left": 0.46800980392156866, "top": 0.5432411616161616, "width": 0.012450980392156752, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5570782828282829, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5709154040404041, "width": 0.14541993464052283, "height": 0.011320707070706981, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3072"}, {"text": "In the second experiment, we test these search methods on different types of web pages.", "label": "Author", "bboxes": [{"left": 0.23743464052287583, "top": 0.5709154040404041, "width": 0.24303431372549028, "height": 0.011320707070706981, "page": 6}, {"left": 0.08790522875816993, "top": 0.5847525252525253, "width": 0.2873496732026144, "height": 0.011320707070706981, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3073"}, {"text": "After a task was finished, we selected the five designs that received most votes from raters for each method.", "label": "Author", "bboxes": [{"left": 0.3887303921568627, "top": 0.8567588383838384, "width": 0.09173366013071899, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8844318181818183, "width": 0.13575490196078432, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3074"}, {"text": "We received largely positive feedback from the participants.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.34482323232323236, "width": 0.3582385620915032, "height": 0.011320707070707037, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3075"}, {"text": "We conducted both experiments following the same procedure.", "label": "Author", "bboxes": [{"left": 0.3279264705882353, "top": 0.6076919191919191, "width": 0.1550081699346405, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.6215290404040403, "width": 0.22123692810457518, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3076"}, {"text": "For Spacewalker, we used 10 iterations with 50 design samples in each iteration, which requires 25 comparisons by the raters.", "label": "Author", "bboxes": [{"left": 0.31280718954248365, "top": 0.6215290404040403, "width": 0.167656862745098, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.6353661616161616, "width": 0.39256699346405227, "height": 0.011320707070707203, "page": 6}, {"left": 0.08790522875816993, "top": 0.6492032828282828, "width": 0.16876307189542483, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3077"}, {"text": "To reduce the potential influence from a single worker, we used workers who had above 90% approval rate in MTurk, and limited each worker to performing 5 comparisons (10 samples).", "label": "Author", "bboxes": [{"left": 0.25943954248366013, "top": 0.6492032828282828, "width": 0.22102614379084973, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 6}, {"left": 0.0874656862745098, "top": 0.6907146464646465, "width": 0.07770098039215685, "height": 0.011320707070706981, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3078"}, {"text": "To account for raters that may not be responsive after accepting the tasks, we distributed the tasks to 70 raters.", "label": "Author", "bboxes": [{"left": 0.0874656862745098, "top": 0.7045517676767676, "width": 0.3930065359477124, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.718388888888889, "width": 0.25572549019607843, "height": 0.011320707070706981, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3079"}, {"text": "For uniform sampling, to ensure that it receives the same number of rater responses as the genetic method, we randomly deployed 500 samples, which amounts to 250 pairs thus 250 rater responses.", "label": "Author", "bboxes": [{"left": 0.3472794117647059, "top": 0.718388888888889, "width": 0.13480392156862753, "height": 0.011320707070706981, "page": 6}, {"left": 0.08790522875816993, "top": 0.7322260101010101, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.7460618686868686, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.75989898989899, "width": 0.2881356209150327, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3080"}, {"text": "We also used 70 raters here to ensure enough responses.", "label": "Author", "bboxes": [{"left": 0.380687908496732, "top": 0.75989898989899, "width": 0.09978104575163405, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.7737361111111111, "width": 0.23248366013071903, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3081"}, {"text": "We compensated each rater that finished the 5 comparisons 0.5 US dollars.", "label": "Author", "bboxes": [{"left": 0.19680228758169935, "top": 0.8152474747474748, "width": 0.28366666666666673, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.16746241830065361, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3082"}, {"text": "For each search specification, we calculate the percentage of votes received by Spacewalker genetic method (the rest of the votes are received by uniform sampling).", "label": "Author", "bboxes": [{"left": 0.689547385620915, "top": 0.7958598484848485, "width": 0.22502287581699354, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8096969696969697, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8235340909090909, "width": 0.3948202614379086, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3083"}, {"text": "We also conduct one-sample z-tests on the differences between a random draw and the voters preferences for Spacewalker in each search specification.", "label": "Author", "bboxes": [{"left": 0.5188316993464052, "top": 0.8373712121212121, "width": 0.3932565359477125, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8512083333333333, "width": 0.39255882352941185, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8650454545454546, "width": 0.12073856209150324, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3084"}, {"text": "We then deployed another task to a separate group of crowd workers for evaluating the quality of these selected designs.", "label": "Author", "bboxes": [{"left": 0.5799232026143791, "top": 0.1372348484848485, "width": 0.332174836601307, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.3948186274509803, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3085"}, {"text": "We refer to this round of crowd tasks as the cross-method evaluation .", "label": "Author", "bboxes": [{"left": 0.5686683006535947, "top": 0.1925820707070707, "width": 0.3434248366013073, "height": 0.011332070707070735, "page": 6}, {"left": 0.5195343137254902, "top": 0.20641919191919192, "width": 0.06468300653594772, "height": 0.011332070707070735, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3086"}, {"text": "We also randomly shuffled the order of designs for both methods.", "label": "Author", "bboxes": [{"left": 0.8923725490196077, "top": 0.22025631313131314, "width": 0.019725490196078388, "height": 0.011320707070707037, "page": 6}, {"left": 0.5195343137254902, "top": 0.23409343434343433, "width": 0.3695326797385622, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3087"}, {"text": "We ran 100 comparison tasks for each search specification, and the rater must make a choice between one of the two designs.", "label": "Author", "bboxes": [{"left": 0.8927369281045752, "top": 0.23409343434343433, "width": 0.019362745098039258, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.24793055555555554, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.26176767676767676, "width": 0.36411111111111116, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3088"}, {"text": "We added exploration attributes and options to five additional web page templates, which are also based on Bootstrap examples 8 .", "label": "Author", "bboxes": [{"left": 0.816795751633987, "top": 0.4001376262626263, "width": 0.09777777777777785, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.4139747474747475, "width": 0.39416830065359487, "height": 0.011320707070707092, "page": 6}, {"left": 0.5189918300653594, "top": 0.4253421717171717, "width": 0.26498856209150334, "height": 0.013790404040404047, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3089"}, {"text": "We used the following templates in this experiment (search space sizes in parentheses): Album (972), Blog (1080), Cover (972), Dashboard (1215), Pricing (1056), Product (1008).", "label": "Author", "bboxes": [{"left": 0.8923725490196077, "top": 0.45548484848484855, "width": 0.019725490196078388, "height": 0.011320707070707037, "page": 6}, {"left": 0.5195343137254902, "top": 0.46932196969696965, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.48315909090909087, "width": 0.39256045751633983, "height": 0.011332070707070763, "page": 6}, {"left": 0.51909477124183, "top": 0.4969962121212121, "width": 0.22196241830065355, "height": 0.011320707070707037, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3090"}, {"text": "In this experiment, we varied the search space size by using a different number of attributes and options in the design search specification.", "label": "Author", "bboxes": [{"left": 0.8265310457516339, "top": 0.2894419191919192, "width": 0.08803431372549031, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.30327777777777776, "width": 0.39283986928104575, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.317114898989899, "width": 0.3679967320261438, "height": 0.011320707070707037, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3091"}, {"text": "We based our study on the Cover example provided by Bootstrap 8 , and we added Spacewalker markups to create the specifications used in the study.", "label": "Author", "bboxes": [{"left": 0.8923725490196077, "top": 0.317114898989899, "width": 0.019725490196078388, "height": 0.011320707070707037, "page": 6}, {"left": 0.5195343137254902, "top": 0.32848358585858584, "width": 0.39256045751633983, "height": 0.013800505050505063, "page": 6}, {"left": 0.5189918300653594, "top": 0.3447891414141414, "width": 0.3931062091503268, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.3586262626262626, "width": 0.05772875816993461, "height": 0.011320707070707092, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3092"}, {"text": "Although our participants did not encounter much difficulty, this can be challenging as the design space becomes convoluted.", "label": "Author", "bboxes": [{"left": 0.6168954248366013, "top": 0.42046464646464643, "width": 0.2951993464052288, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.43430176767676765, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.44813888888888886, "width": 0.07110294117647065, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3093"}, {"text": "We believe the above extensions can provide a good starting point for designers to understand the search space.", "label": "Author", "bboxes": [{"left": 0.5944493464052287, "top": 0.44813888888888886, "width": 0.3176486928104574, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.4619760101010101, "width": 0.3746160130718954, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3094"}, {"text": "In order to support more advanced dependency, we believe automatic tooling for identifying option dependencies and detecting potential inconsistency would be necessary, which provides opportunity for future work.", "label": "Author", "bboxes": [{"left": 0.6577320261437908, "top": 0.5865088383838384, "width": 0.25683169934640526, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6003459595959596, "width": 0.39503431372549025, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6141830808080808, "width": 0.39417156862745095, "height": 0.011320707070707092, "page": 7}, {"left": 0.5189918300653594, "top": 0.628020202020202, "width": 0.27204084967320274, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3095"}, {"text": "Therefore, even our enhanced genetic algorithm may not converge to an optimal solution with a small number of comparisons.", "label": "Author", "bboxes": [{"left": 0.8170081699346405, "top": 0.6556944444444444, "width": 0.09508333333333341, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.669530303030303, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6833674242424242, "width": 0.279437908496732, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3096"}, {"text": "However, we note that Spacewalker performed well on rather large search spaces in our experiment, and that other approaches, such as uniform sampling, would suffer more in these cases as the probability of encountering one \"good\" example would be minuscule.", "label": "Author", "bboxes": [{"left": 0.6266960784313725, "top": 0.7110416666666667, "width": 0.2853937908496732, "height": 0.011320707070707092, "page": 7}, {"left": 0.5189918300653594, "top": 0.7248787878787879, "width": 0.3933790849673203, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7387159090909091, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7525530303030303, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7663901515151516, "width": 0.08202124183006543, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3097"}, {"text": "In addition, we observe a trend of increased preference for Spacewalker genetic method as the size of the search space increases (see Figure 6).", "label": "Author", "bboxes": [{"left": 0.3930261437908497, "top": 0.36511742424242427, "width": 0.0874395424836601, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.3789532828282828, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.392790404040404, "width": 0.363328431372549, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3098"}, {"text": "Our quantitative experiments for examining the performance of Spacewalker algorithms for searching a design space indicate that it improves designs over time by producing better design candidates, particularly when the search space is large.", "label": "Author", "bboxes": [{"left": 0.5358137254901961, "top": 0.8355744949494949, "width": 0.3762794117647058, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8494116161616162, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8632487373737373, "width": 0.39417320261437905, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8770858585858586, "width": 0.25522549019607843, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3099"}, {"text": "However, we observed", "label": "Author", "bboxes": [{"left": 0.778423202614379, "top": 0.8770858585858586, "width": 0.13367810457516338, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3100"}, {"text": "For the experiment where Spacewalker is used to search for different web page types, we find that crowd raters, from the crossmethod evaluation, preferred the results produced by Spacewalker genetic method in all cases we tested when they are compared with those from the uniform sampling method (see Table 3).", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.43430176767676765, "width": 0.3765588235294117, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.44813888888888886, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4619760101010101, "width": 0.3928333333333333, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4758131313131313, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4896502525252525, "width": 0.3386454248366013, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3101"}, {"text": "Our participants gave us several useful suggestions for improvements.", "label": "Author", "bboxes": [{"left": 0.3268300653594771, "top": 0.8152474747474748, "width": 0.15363888888888894, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.2787058823529412, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3102"}, {"text": "In this section, we discuss the strengths and limitation of our work, and our plan for future work.", "label": "Author", "bboxes": [{"left": 0.08790522875816993, "top": 0.6492032828282828, "width": 0.3941683006535948, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.17506535947712415, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3103"}, {"text": "Our experiments show that the concept of Spacewalker is well received by the designers and developers.", "label": "Author", "bboxes": [{"left": 0.26665196078431375, "top": 0.6630404040404041, "width": 0.21628594771241827, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3948153594771242, "height": 0.011320707070707092, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3104"}, {"text": "We plan to infer these hyperparamters (the number of workers and iterations) needed automatically.", "label": "Author", "bboxes": [{"left": 0.2549166666666666, "top": 0.4416489898989899, "width": 0.22554901960784324, "height": 0.011320707070707092, "page": 9}, {"left": 0.0874656862745098, "top": 0.45548484848484855, "width": 0.3695114379084968, "height": 0.011320707070707037, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3105"}, {"text": "We can train a model to predict a reasonable worker and iteration size by incorporating design complexities.", "label": "Author", "bboxes": [{"left": 0.21833823529411764, "top": 0.4969962121212121, "width": 0.26212091503267976, "height": 0.011320707070707037, "page": 9}, {"left": 0.08736437908496732, "top": 0.5108333333333334, "width": 0.3953578431372549, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3106"}, {"text": "These are beyond the scope of this paper.", "label": "Author", "bboxes": [{"left": 0.0874656862745098, "top": 0.5246704545454546, "width": 0.2468104575163399, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3107"}, {"text": "For each of the tasks, our GA-based algorithm only visited a small portion of the design space.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.17874494949494948, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.1925820707070707, "width": 0.19990686274509806, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3108"}, {"text": "The search space size of our designs ranges from 50 to 10000.", "label": "Author", "bboxes": [{"left": 0.16607516339869283, "top": 0.20641919191919192, "width": 0.31439705882352936, "height": 0.011320707070707037, "page": 9}, {"left": 0.08790522875816993, "top": 0.22025631313131314, "width": 0.05481535947712417, "height": 0.011320707070707037, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3109"}, {"text": "Our search algorithm searches efficiently given a budget that dictates the number of comparisons and raters can be used.Inaddition,", "label": "Author", "bboxes": [{"left": 0.19439869281045752, "top": 0.26176767676767676, "width": 0.2860669934640523, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.275604797979798, "width": 0.3925653594771242, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.2894419191919192, "width": 0.08397222222222221, "height": 0.025156565656565655, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3110"}, {"text": "Finally, we want to emphasize that Spacewalker is able to identify an optimal design in a defined design space, instead of finding a globally optimal design.", "label": "Author", "bboxes": [{"left": 0.10418464052287582, "top": 0.5385075757575758, "width": 0.3787516339869281, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.552344696969697, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5661818181818182, "width": 0.15059640522875817, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3111"}, {"text": "It allows designers to easily instrument a design exploration using our simple markup extension, seamlessly distribute design critique tasks to crowd workers, and quickly receive exploration results with genetic algorithms.", "label": "Author", "bboxes": [{"left": 0.29425816993464055, "top": 0.6630404040404041, "width": 0.18867810457516343, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3941732026143791, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6907146464646465, "width": 0.392562091503268, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.3647826797385621, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3112"}, {"text": "To further our understanding, we intend to compare the automatic process enabled by Spacewalker with the existing manual process for design search in realistic design projects in the future.", "label": "Author", "bboxes": [{"left": 0.28804084967320265, "top": 0.7322260101010101, "width": 0.19242483660130716, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.7460618686868686, "width": 0.39283496732026146, "height": 0.011320707070707092, "page": 9}, {"left": 0.08736437908496732, "top": 0.75989898989899, "width": 0.3955816993464052, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.7737361111111111, "width": 0.15783496732026142, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3113"}, {"text": "effectively utilize crowd worker feedback, our system can quickly explore the search space of a web design, which provides realtime feedback to the designer about the progress of the search.", "label": "Author", "bboxes": [{"left": 0.5195343137254902, "top": 0.10956060606060607, "width": 0.39294281045751633, "height": 0.011320707070707065, "page": 9}, {"left": 0.5195343137254902, "top": 0.12339772727272727, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1372348484848485, "width": 0.3948202614379086, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3114"}, {"text": "Our experiments indicate that Spacewalker is well received by designers, and Spacewalkers genetic search algorithm significantly outperformed a uniform sampling baseline under different search space sizes and web design types.", "label": "Author", "bboxes": [{"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.39293790849673205, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.39293464052287597, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.20023529411764707, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3115"}, {"text": "Our HTML markup for creating exploration specification provides a lightweight and familiar language for designers to specify complex designs and search requirements.", "label": "Author", "bboxes": [{"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.3929183006535948, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8844318181818183, "width": 0.19304248366013071, "height": 0.011320707070707092, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3116"}, {"text": "We would like to thank anonymous reviewers for their insightful feedback for improving the paper.", "label": "Author", "bboxes": [{"left": 0.5188316993464052, "top": 0.24478535353535355, "width": 0.39325816993464047, "height": 0.011320707070707065, "page": 9}, {"left": 0.5195343137254902, "top": 0.25862247474747474, "width": 0.2007859477124183, "height": 0.011320707070707037, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3117"}, {"text": "We would also like to thank the participants in our user studies.", "label": "Author", "bboxes": [{"left": 0.7239754901960784, "top": 0.25862247474747474, "width": 0.1881225490196078, "height": 0.011320707070707037, "page": 9}, {"left": 0.5195343137254902, "top": 0.27245959595959596, "width": 0.1888366013071896, "height": 0.011320707070707037, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3118"}, {"text": "Our paper makes the following contributions:", "label": "Contribution", "bboxes": [{"left": 0.35312581699346407, "top": 0.5598030303030302, "width": 0.12733823529411759, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5736401515151515, "width": 0.1445326797385621, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3119"}, {"text": "Based on the result of each pairwise comparison, Spacewalker learns which design was preferred by a human rater to which the option contributes, and sets the corresponding mask value as 1 , and the rest as 0 .", "label": "Contribution", "bboxes": [{"left": 0.35172712418300656, "top": 0.45548484848484855, "width": 0.1287418300653595, "height": 0.011320707070707037, "page": 5}, {"left": 0.08790522875816993, "top": 0.46932196969696965, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.48315909090909087, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.4963282828282828, "width": 0.34125, "height": 0.01198863636363634, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3120"}, {"text": "However, these existing tools often require a designer to learn a new language that is tailored for working with the underlying algorithm to define a search space.", "label": "Novelty", "bboxes": [{"left": 0.6127401960784314, "top": 0.4488257575757576, "width": 0.3018235294117645, "height": 0.011320707070706981, "page": 0}, {"left": 0.5195343137254902, "top": 0.46266287878787876, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.4765, "width": 0.29832679738562096, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3121"}, {"text": "To address challenge 1), We designed the HTML annotation as a simple extension of the existing HTML and CSS grammar, where instead of specifying a single value for an attribute, a designer can provide multiple candidate values for it, which are to be explored by Spacewalker.", "label": "Novelty", "bboxes": [{"left": 0.5358137254901961, "top": 0.7255656565656566, "width": 0.3762794117647059, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7394027777777777, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7532398989898991, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7670770202020202, "width": 0.39255392156862734, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7809141414141415, "width": 0.09825653594771244, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3122"}, {"text": "In a typical HTML page or a CSS specification, designers first annotate each attribute they want to explore using a simple markup extension we designed.", "label": "Novelty", "bboxes": [{"left": 0.7644967320261438, "top": 0.5733598484848484, "width": 0.14760130718954245, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5871957070707071, "width": 0.3929411764705881, "height": 0.011320707070706981, "page": 0}, {"left": 0.5189918300653594, "top": 0.6010328282828282, "width": 0.36693137254901953, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3123"}, {"text": "Although these classical approaches are widely used, they require substantial engineering", "label": "Novelty", "bboxes": [{"left": 0.3324281045751634, "top": 0.7399747474747475, "width": 0.14803431372549014, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.7538118686868687, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3124"}, {"text": "Although existing methods are widely adopted, they often require substantial engineering effort to build and instrument a test.", "label": "Novelty", "bboxes": [{"left": 0.5358137254901961, "top": 0.5736401515151515, "width": 0.3787549019607843, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.5874772727272727, "width": 0.3948153594771242, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3125"}, {"text": "To study how users react to design alternatives at scale, A/B testing is widely used where variants of a design are tested with different user populations and user behaviors are logged and statistically analyzed by user experience researchers [6, 7].", "label": "Novelty", "bboxes": [{"left": 0.7624820261437909, "top": 0.47678156565656565, "width": 0.14961274509803912, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.49061742424242427, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 1}, {"left": 0.5191683006535948, "top": 0.5044545454545455, "width": 0.39292647058823527, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.5182916666666667, "width": 0.3950424836601307, "height": 0.011320707070706981, "page": 1}, {"left": 0.5195343137254902, "top": 0.5321287878787879, "width": 0.13736928104575163, "height": 0.011320707070706981, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3126"}, {"text": "This instructs Spacewalker to explore three different alternatives for the page background: image \" bg1.jpg \", image \" bg2.jpg \", and a solid background with a dark gray color \" #333 \".", "label": "Novelty", "bboxes": [{"left": 0.51909477124183, "top": 0.23409343434343433, "width": 0.3930049019607845, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.24726262626262627, "width": 0.39255392156862745, "height": 0.011988636363636368, "page": 2}, {"left": 0.5195343137254902, "top": 0.26109974747474746, "width": 0.28547549019607843, "height": 0.011988636363636396, "page": 2}], "section": "3 USING SPACEWALKER", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3127"}, {"text": "Behind the scene, Spacewalker creates three page designs with each using a different background choice.", "label": "Novelty", "bboxes": [{"left": 0.8086650326797385, "top": 0.26176767676767676, "width": 0.10503758169934652, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.275604797979798, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.2894419191919192, "width": 0.11716339869281045, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3128"}, {"text": "However, we focus on the design task where an interaction designer has a basic HTML design and wants to obtain an optimal configuration for the design by exploring a large range of options such as colors, fonts and layouts.", "label": "Novelty", "bboxes": [{"left": 0.25794771241830067, "top": 0.2894419191919192, "width": 0.22290032679738558, "height": 0.011320707070707092, "page": 2}, {"left": 0.08736437908496732, "top": 0.30327777777777776, "width": 0.3931062091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.317114898989899, "width": 0.39256372549019614, "height": 0.011320707070707037, "page": 2}, {"left": 0.08790522875816993, "top": 0.3309520202020202, "width": 0.34070751633986923, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Crowd-Powered UI Design & Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3129"}, {"text": "Despite the adoption of the crowd, these interactive GA solutions rely on implicit information, such as click location that is difficult to generalize to other design tasks.", "label": "Novelty", "bboxes": [{"left": 0.37754738562091505, "top": 0.5800189393939394, "width": 0.10539052287581696, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.593854797979798, "width": 0.39255555555555555, "height": 0.011320707070706981, "page": 2}, {"left": 0.08790522875816993, "top": 0.6076919191919191, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6215290404040403, "width": 0.1281732026143791, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Interactive UI Design Optimization", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3130"}, {"text": "Although we employ GA-based algorithms and crowd in our work, Spacewalker is designed to address a general web UI design scenario.", "label": "Novelty", "bboxes": [{"left": 0.40123856209150327, "top": 0.6492032828282828, "width": 0.07922385620915029, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3373758169934641, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Interactive UI Design Optimization", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3131"}, {"text": "It allows designers to specify the design space for exploration using a simple extension of HTML tags.", "label": "Novelty", "bboxes": [{"left": 0.4289395424836602, "top": 0.6768775252525252, "width": 0.051532679738562015, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6907146464646465, "width": 0.3925604575163399, "height": 0.011320707070706981, "page": 2}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.14835784313725492, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Interactive UI Design Optimization", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3132"}, {"text": "Although she has written an HTML prototype of the page, she is uncertain about a few of design aspects of the page, such as color schemes, font sizes, and background choices.", "label": "Novelty", "bboxes": [{"left": 0.15116339869281045, "top": 0.8429217171717173, "width": 0.32930555555555563, "height": 0.011320707070706981, "page": 2}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.39417810457516345, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.3503267973856209, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3133"}, {"text": "However, these approaches only take the input from a few users, causing fatigue [21] and increasing potential bias.", "label": "Novelty", "bboxes": [{"left": 0.4264705882352941, "top": 0.48315909090909087, "width": 0.055607843137254864, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.4969962121212121, "width": 0.3950326797385621, "height": 0.011320707070707037, "page": 2}, {"left": 0.08790522875816993, "top": 0.5108333333333334, "width": 0.2365833333333333, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Interactive UI Design Optimization", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3134"}, {"text": "In this example, Spacewalker uses either \" nav-1 \" or \" nav-2 \" at a time, while the rest children are unaffected.", "label": "Novelty", "bboxes": [{"left": 0.5195343137254902, "top": 0.5240025252525252, "width": 0.39255555555555544, "height": 0.011988636363636451, "page": 2}, {"left": 0.5195343137254902, "top": 0.5385075757575758, "width": 0.27861601307189554, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3135"}, {"text": "However, when these shared options are less desirable, conventional GA performs poorly as those options that are not compared also receive positive responses.", "label": "Novelty", "bboxes": [{"left": 0.7414885620915033, "top": 0.8014103535353535, "width": 0.17059967320261438, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8152474747474748, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8290845959595959, "width": 0.3948153594771241, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3136"}, {"text": "To address the issue, we enhance traditional genetic algorithms, for each iteration, by directing rater feedback to genes that participate in a comparison while allowing the rest genes in a sequence to remain stochastic in the downstream evolution.", "label": "Novelty", "bboxes": [{"left": 0.51909477124183, "top": 0.8429217171717173, "width": 0.39327941176470604, "height": 0.011320707070706981, "page": 4}, {"left": 0.5195343137254902, "top": 0.8567588383838384, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8705959595959595, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.2844787581699346, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3137"}, {"text": "Spacewalker instead presents each rater a pair of different candidate designs at a time, and asks the rater to select the preferred design, i.e., a twoalternative forced-choice (2AFC) method.", "label": "Novelty", "bboxes": [{"left": 0.8387401960784313, "top": 0.22025631313131314, "width": 0.07362745098039225, "height": 0.011320707070707037, "page": 4}, {"left": 0.5195343137254902, "top": 0.23409343434343433, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.24793055555555554, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.26176767676767676, "width": 0.24217647058823533, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3138"}, {"text": "Although presenting more than two examples in a gallery design can be another appealing alternative [1, 13], the viewing area for each example would be too small in our case of web design, and may prevent raters from noticing design details that matter.", "label": "Novelty", "bboxes": [{"left": 0.5190212418300654, "top": 0.2894419191919192, "width": 0.39307352941176465, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.30327777777777776, "width": 0.39283823529411754, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.317114898989899, "width": 0.3925637254901959, "height": 0.011320707070707037, "page": 4}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.35701470588235296, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3139"}, {"text": "However, pairwise comparison raises challenges for genetic programming.", "label": "Novelty", "bboxes": [{"left": 0.80340522875817, "top": 0.6768775252525252, "width": 0.10868954248366014, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6907146464646465, "width": 0.3308251633986927, "height": 0.011320707070706981, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3140"}, {"text": "On the other hand, with a limited number of worker feedback, which is often the case in reality, a search very likely ends up with a sub-optimal design, as shown in our experiments later.", "label": "Novelty", "bboxes": [{"left": 0.4618790849673203, "top": 0.36862500000000004, "width": 0.01858986928104578, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.38246212121212125, "width": 0.3925653594771242, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.39629924242424247, "width": 0.39256372549019614, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.4101363636363636, "width": 0.32421895424836605, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3141"}, {"text": "The goal of our initialization process is to maximally cover possible options for each attribute, but meanwhile limit the total number of designs to be compared by human raters.", "label": "Novelty", "bboxes": [{"left": 0.19848039215686275, "top": 0.3586262626262626, "width": 0.2844591503267974, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.37246338383838384, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.38630050505050506, "width": 0.35276143790849673, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3142"}, {"text": "Therefore, we treat each attribute independently during initialization, and generate design variations by sampling the options of one attribute at a time while affixing the rest exploratory attributes at a random value.", "label": "Novelty", "bboxes": [{"left": 0.44373856209150325, "top": 0.38630050505050506, "width": 0.03919771241830072, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.4001376262626263, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.4139747474747475, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.42781186868686866, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 5}, {"left": 0.08753921568627451, "top": 0.4416489898989899, "width": 0.034862745098039216, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3143"}, {"text": "The diff mask represents where we would likely gain knowledge by comparing these two designs: in the part where they differ.", "label": "Novelty", "bboxes": [{"left": 0.241890522875817, "top": 0.20641919191919192, "width": 0.23857189542483662, "height": 0.011320707070707037, "page": 5}, {"left": 0.08790522875816993, "top": 0.22025631313131314, "width": 0.3925571895424837, "height": 0.011320707070707037, "page": 5}, {"left": 0.08736437908496732, "top": 0.23409343434343433, "width": 0.10527941176470589, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3144"}, {"text": "We conduct a user study to investigate whether Spacewalker markup extension to HTML is easy to understand and use by designers and developers, and how they react to the overall support of Spacewalker for design exploration.", "label": "Novelty", "bboxes": [{"left": 0.828156862745098, "top": 0.3218320707070707, "width": 0.08394117647058819, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.3356691919191919, "width": 0.39256045751633983, "height": 0.011320707070707037, "page": 5}, {"left": 0.5195343137254902, "top": 0.34950631313131314, "width": 0.39416993464052286, "height": 0.011320707070707037, "page": 5}, {"left": 0.5195343137254902, "top": 0.3633434343434343, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.3771805555555555, "width": 0.07033986928104574, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3145"}, {"text": "The back-end server is responsible for scheduling workers for different web UI pairs without conflicts and supporting multiple workers submitting results at the same time, using database read/write lock.", "label": "Novelty", "bboxes": [{"left": 0.8893807189542484, "top": 0.1649078282828283, "width": 0.02271405228758161, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.39255718954248375, "height": 0.011320707070707092, "page": 5}, {"left": 0.5189918300653594, "top": 0.1925820707070707, "width": 0.3931013071895425, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.20641919191919192, "width": 0.3948153594771242, "height": 0.011320707070707037, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.3 Architectures", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3146"}, {"text": "In the first experiment, we examine the effect of different search space sizes on the techniques.", "label": "Novelty", "bboxes": [{"left": 0.46800980392156866, "top": 0.5432411616161616, "width": 0.012450980392156752, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5570782828282829, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5709154040404041, "width": 0.14541993464052283, "height": 0.011320707070706981, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3147"}, {"text": "In the second experiment, we test these search methods on different types of web pages.", "label": "Novelty", "bboxes": [{"left": 0.23743464052287583, "top": 0.5709154040404041, "width": 0.24303431372549028, "height": 0.011320707070706981, "page": 6}, {"left": 0.08790522875816993, "top": 0.5847525252525253, "width": 0.2873496732026144, "height": 0.011320707070706981, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3148"}, {"text": "On average, participants were able to understand and create a search specification in 28 minutes (SD = 13.8), exploring different options for five to eight attributes.", "label": "Novelty", "bboxes": [{"left": 0.23340522875816994, "top": 0.2756376262626263, "width": 0.24953921568627452, "height": 0.011320707070707037, "page": 6}, {"left": 0.08790522875816993, "top": 0.2894747474747475, "width": 0.39416830065359476, "height": 0.011320707070707037, "page": 6}, {"left": 0.08790522875816993, "top": 0.3033118686868687, "width": 0.323078431372549, "height": 0.011320707070707037, "page": 6}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3149"}, {"text": "In sum, the only difference between the two conditions is the method used for searching the design space, while the rest aspects including the feedback mechanism is the same.", "label": "Novelty", "bboxes": [{"left": 0.3238921568627451, "top": 0.7737361111111111, "width": 0.15657352941176472, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.7875732323232324, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8014103535353535, "width": 0.39503594771241834, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.10501143790849672, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3150"}, {"text": "In this experiment, we varied the search space size by using a different number of attributes and options in the design search specification.", "label": "Novelty", "bboxes": [{"left": 0.8265310457516339, "top": 0.2894419191919192, "width": 0.08803431372549031, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.30327777777777776, "width": 0.39283986928104575, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.317114898989899, "width": 0.3679967320261438, "height": 0.011320707070707037, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3151"}, {"text": "Although our participants did not encounter much difficulty, this can be challenging as the design space becomes convoluted.", "label": "Novelty", "bboxes": [{"left": 0.6168954248366013, "top": 0.42046464646464643, "width": 0.2951993464052288, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.43430176767676765, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.44813888888888886, "width": 0.07110294117647065, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3152"}, {"text": "First, in complex design spaces, designers may want to maintain dependencies between several sets of elements, while style options for different elements can also be dependent, where only certain options or elements can be combined together.", "label": "Novelty", "bboxes": [{"left": 0.791687908496732, "top": 0.5311603535353535, "width": 0.12287745098039238, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.5449974747474747, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.558834595959596, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.5726717171717172, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.5865088383838384, "width": 0.13454738562091495, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3153"}, {"text": "However, we note that Spacewalker performed well on rather large search spaces in our experiment, and that other approaches, such as uniform sampling, would suffer more in these cases as the probability of encountering one \"good\" example would be minuscule.", "label": "Novelty", "bboxes": [{"left": 0.6266960784313725, "top": 0.7110416666666667, "width": 0.2853937908496732, "height": 0.011320707070707092, "page": 7}, {"left": 0.5189918300653594, "top": 0.7248787878787879, "width": 0.3933790849673203, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7387159090909091, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7525530303030303, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7663901515151516, "width": 0.08202124183006543, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3154"}, {"text": "However, we observed", "label": "Novelty", "bboxes": [{"left": 0.778423202614379, "top": 0.8770858585858586, "width": 0.13367810457516338, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3155"}, {"text": "For the experiment where Spacewalker is used to search for different web page types, we find that crowd raters, from the crossmethod evaluation, preferred the results produced by Spacewalker genetic method in all cases we tested when they are compared with those from the uniform sampling method (see Table 3).", "label": "Novelty", "bboxes": [{"left": 0.10418464052287582, "top": 0.43430176767676765, "width": 0.3765588235294117, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.44813888888888886, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4619760101010101, "width": 0.3928333333333333, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4758131313131313, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4896502525252525, "width": 0.3386454248366013, "height": 0.011320707070707092, "page": 7}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3156"}, {"text": "Note that depending on the search options specified in a design, the difference between the outcome designs from the two methods can be subtle sometime, e.g., the Dashboard case in Figure 7.", "label": "Novelty", "bboxes": [{"left": 0.35779901960784316, "top": 0.517324494949495, "width": 0.12266666666666665, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.5311603535353535, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.5449974747474747, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.558834595959596, "width": 0.25667810457516343, "height": 0.011320707070707092, "page": 7}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3157"}, {"text": "This indicates that the benefit of Spacewalker is well demonstrated across different web page types.", "label": "Novelty", "bboxes": [{"left": 0.1891062091503268, "top": 0.5865088383838384, "width": 0.2913627450980393, "height": 0.011320707070707092, "page": 7}, {"left": 0.08736437908496732, "top": 0.6003459595959596, "width": 0.3052973856209151, "height": 0.011320707070707092, "page": 7}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3158"}, {"text": "The Spacewalker markup extension is easy to understand and use for specifying design exploration.", "label": "Novelty", "bboxes": [{"left": 0.10418464052287582, "top": 0.8014103535353535, "width": 0.3762794117647058, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.23391830065359476, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3159"}, {"text": "\"This tool provides (a) useful way to compare my designs. I used to use the Inspect tool in Chrome to try out different values of the styles of my attributes, but the limitation is that I can only modify one item at a time. With this tool I could manage my HTML/CSS code and potential designs of the whole page efficiently. It improves my productivity and experience significantly.\"", "label": "Novelty", "bboxes": [{"left": 0.10418464052287582, "top": 0.7184002525252525, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7322373737373737, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7460732323232323, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7599103535353536, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7737474747474747, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7875845959595961, "width": 0.2415947712418301, "height": 0.011320707070706981, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3160"}, {"text": "However, it might not be always clear to the designer what the reasonable values are for these parameters.", "label": "Novelty", "bboxes": [{"left": 0.4264656862745098, "top": 0.37246338383838384, "width": 0.055607843137254864, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.38630050505050506, "width": 0.395031045751634, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.4001376262626263, "width": 0.1688496732026144, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3161"}, {"text": "Although searching over the entire space would be ideal, it is impractical to do so.", "label": "Novelty", "bboxes": [{"left": 0.0873921568627451, "top": 0.24793055555555554, "width": 0.3955490196078431, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.26176767676767676, "width": 0.10300326797385621, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3162"}, {"text": "However, it tremendously enhances A/B testing by allowing designers to explore a much larger design space with minimal effort.", "label": "Novelty", "bboxes": [{"left": 0.3009313725490196, "top": 0.6353661616161616, "width": 0.1820032679738562, "height": 0.011320707070707203, "page": 9}, {"left": 0.08790522875816993, "top": 0.6492032828282828, "width": 0.39284150326797385, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.20268137254901963, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3163"}, {"text": "It allows designers to easily instrument a design exploration using our simple markup extension, seamlessly distribute design critique tasks to crowd workers, and quickly receive exploration results with genetic algorithms.", "label": "Novelty", "bboxes": [{"left": 0.29425816993464055, "top": 0.6630404040404041, "width": 0.18867810457516343, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3941732026143791, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6907146464646465, "width": 0.392562091503268, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.3647826797385621, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3164"}, {"text": "Our experiments indicate that Spacewalker is well received by designers, and Spacewalkers genetic search algorithm significantly outperformed a uniform sampling baseline under different search space sizes and web design types.", "label": "Novelty", "bboxes": [{"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.39293790849673205, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.39293464052287597, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.20023529411764707, "height": 0.011320707070707092, "page": 9}], "section": "7 CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3165"}, {"text": "An expressive and easy-to-use HTML markup extension that allows designers to easily specify various alternatives for design search, which requires negligible learning effort;  An enhanced genetic algorithm that can efficiently explore a large design space using crowd worker responses from pairwise comparison of UI designs;  Integrated general tool support that allows designers to easily obtain an improved design from a large range of options within a short period of time (e.g., 1 hour) at a small amount of cost (e.g., 35 US dollars).", "label": "Novelty", "bboxes": [{"left": 0.11399509803921569, "top": 0.5902285353535354, "width": 0.36646895424836595, "height": 0.012238636363636313, "page": 1}, {"left": 0.1277892156862745, "top": 0.6049835858585858, "width": 0.3529477124183007, "height": 0.011320707070707092, "page": 1}, {"left": 0.1277892156862745, "top": 0.618820707070707, "width": 0.33253267973856204, "height": 0.011320707070707092, "page": 1}, {"left": 0.11399509803921569, "top": 0.631739898989899, "width": 0.3664722222222222, "height": 0.012238636363636313, "page": 1}, {"left": 0.1277892156862745, "top": 0.6464949494949495, "width": 0.3526781045751634, "height": 0.011320707070707092, "page": 1}, {"left": 0.1277892156862745, "top": 0.6603320707070707, "width": 0.20980228758169933, "height": 0.011320707070707092, "page": 1}, {"left": 0.11399509803921569, "top": 0.6732512626262627, "width": 0.36894607843137256, "height": 0.012238636363636313, "page": 1}, {"left": 0.1277892156862745, "top": 0.6880063131313131, "width": 0.3526830065359477, "height": 0.011320707070707092, "page": 1}, {"left": 0.12724673202614378, "top": 0.7018434343434343, "width": 0.3532205882352941, "height": 0.011320707070707092, "page": 1}, {"left": 0.1277892156862745, "top": 0.7156792929292929, "width": 0.1601503267973856, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3166"}, {"text": "Their optimization objectives (or fitness functions [12]) are designed based on user click behaviors of specific interaction tasks.", "label": "Objective", "bboxes": [{"left": 0.821516339869281, "top": 0.4765, "width": 0.09305228758169937, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.4903371212121212, "width": 0.39255882352941185, "height": 0.011320707070707037, "page": 0}, {"left": 0.5195343137254902, "top": 0.5041742424242424, "width": 0.2844509803921569, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3167"}, {"text": "There lacks a tool for general-purpose design space exploration that is seamlessly integrated into current design practice.", "label": "Objective", "bboxes": [{"left": 0.8076535947712418, "top": 0.5041742424242424, "width": 0.10443790849673218, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5180113636363636, "width": 0.39293790849673205, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5318484848484848, "width": 0.23321405228758174, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3168"}, {"text": "In Spacewalker, we aim to address these issues by providing an integrated support for designers to explore a large design space and improve their design.", "label": "Objective", "bboxes": [{"left": 0.6606797385620915, "top": 0.7258472222222222, "width": 0.2514183006535947, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7396843434343435, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7535214646464646, "width": 0.2761127450980392, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3169"}, {"text": "We also aim to minimize the effort and cost of the designer to perform the task.", "label": "Objective", "bboxes": [{"left": 0.4326192810457516, "top": 0.3309520202020202, "width": 0.04784967320261446, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.3447891414141414, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.3586262626262626, "width": 0.027763071895424832, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Crowd-Powered UI Design & Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3170"}, {"text": "During initialization , the first generation is randomly generated, with the goal of covering as many configurations as possible.", "label": "Objective", "bboxes": [{"left": 0.33396732026143794, "top": 0.514195707070707, "width": 0.14650163398692811, "height": 0.011332070707070763, "page": 4}, {"left": 0.08790522875816993, "top": 0.5280328282828283, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5418699494949495, "width": 0.18922385620915033, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3171"}, {"text": "The goal of our initialization process is to maximally cover possible options for each attribute, but meanwhile limit the total number of designs to be compared by human raters.", "label": "Objective", "bboxes": [{"left": 0.19848039215686275, "top": 0.3586262626262626, "width": 0.2844591503267974, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.37246338383838384, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.38630050505050506, "width": 0.35276143790849673, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3172"}, {"text": "The goal is to test whether web designers are able to learn and use our markups to specify search criteria and launch a design exploration task, and to gather feedback of the Spacewalker system.", "label": "Objective", "bboxes": [{"left": 0.8885424836601308, "top": 0.46491919191919195, "width": 0.02354901960784317, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.47875631313131317, "width": 0.39283660130718956, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.4925934343434344, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.5064305555555556, "width": 0.33367647058823524, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3173"}, {"text": "To address challenge 1), We designed the HTML annotation as a simple extension of the existing HTML and CSS grammar, where instead of specifying a single value for an attribute, a designer can provide multiple candidate values for it, which are to be explored by Spacewalker.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.7255656565656566, "width": 0.3762794117647059, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7394027777777777, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7532398989898991, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7670770202020202, "width": 0.39255392156862734, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7809141414141415, "width": 0.09825653594771244, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3174"}, {"text": "In this paper, we present Spacewalker, a tool that allows designers to rapidly search a design space of a web UI for an optimal design within that space (see Figure 1).", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.545685606060606, "width": 0.3787565359477124, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5595227272727272, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5733598484848484, "width": 0.24064379084967324, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3175"}, {"text": "In Spacewalker, we aim to address these issues by providing an integrated support for designers to explore a large design space and improve their design.", "label": "Method", "bboxes": [{"left": 0.6606797385620915, "top": 0.7258472222222222, "width": 0.2514183006535947, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7396843434343435, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7535214646464646, "width": 0.2761127450980392, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3176"}, {"text": "We evaluated Spacewalker by asking interaction designers to use it for exploring a set of UI design tasks, and Spacewalker received positive feedback.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.46294444444444444, "width": 0.3762794117647058, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.47678156565656565, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.49061742424242427, "width": 0.10406699346405228, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3177"}, {"text": "Our work is related to three areas of the literature, including UI evaluation methods, crowdsourcing-based design support, and interactive UI design optimization.", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.761003787878788, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.7748409090909091, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.7886780303030303, "width": 0.19503431372549018, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3178"}, {"text": "Similar to previous work, we also embed the crowd in the loop of the design and evaluation process.", "label": "Method", "bboxes": [{"left": 0.4210065359477124, "top": 0.26176767676767676, "width": 0.05946241830065363, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.275604797979798, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.2894419191919192, "width": 0.16637254901960785, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Crowd-Powered UI Design & Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3179"}, {"text": "Although we employ GA-based algorithms and crowd in our work, Spacewalker is designed to address a general web UI design scenario.", "label": "Method", "bboxes": [{"left": 0.40123856209150327, "top": 0.6492032828282828, "width": 0.07922385620915029, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3373758169934641, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Interactive UI Design Optimization", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3180"}, {"text": "We here describe how UI designers or developers would use Spacewalker to explore the design space of their user interfaces.", "label": "Method", "bboxes": [{"left": 0.08720261437908497, "top": 0.8014103535353535, "width": 0.3957352941176471, "height": 0.011320707070707092, "page": 2}, {"left": 0.08736437908496732, "top": 0.8152474747474748, "width": 0.341718954248366, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3181"}, {"text": "In this section, we discuss the system design and algorithmic details that underline the Spacewalker.", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.8100050505050506, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.8238421717171717, "width": 0.18967810457516343, "height": 0.011320707070707092, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3182"}, {"text": "As shown in the above example, Spacewalker supports a rich set of methods for exploring a design space through simple HTML extensions, which are intuitive to designers as shown in our experiments.", "label": "Method", "bboxes": [{"left": 0.5190212418300654, "top": 0.469530303030303, "width": 0.39307843137254905, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.48336742424242424, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.49720454545454545, "width": 0.3948153594771242, "height": 0.011320707070707037, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3183"}, {"text": "In our early exploration, we found conventional GA sensitive to the random initialization of design options.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.75989898989899, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7737361111111111, "width": 0.27940849673202606, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3184"}, {"text": "Thus, our fitness function outputs 1 for the preferred design and 0 for the less preferred one.", "label": "Method", "bboxes": [{"left": 0.765313725490196, "top": 0.26176767676767676, "width": 0.1467777777777779, "height": 0.011332070707070707, "page": 4}, {"left": 0.5195343137254902, "top": 0.2749368686868687, "width": 0.394812091503268, "height": 0.011988636363636396, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3185"}, {"text": "On the other hand, with a limited number of worker feedback, which is often the case in reality, a search very likely ends up with a sub-optimal design, as shown in our experiments later.", "label": "Method", "bboxes": [{"left": 0.4618790849673203, "top": 0.36862500000000004, "width": 0.01858986928104578, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.38246212121212125, "width": 0.3925653594771242, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.39629924242424247, "width": 0.39256372549019614, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.4101363636363636, "width": 0.32421895424836605, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3186"}, {"text": "For simplicity, we did not use these features in this paper.", "label": "Method", "bboxes": [{"left": 0.46042156862745104, "top": 0.12339772727272727, "width": 0.020320261437908438, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.1372348484848485, "width": 0.32019771241830064, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3187"}, {"text": "We refer an instance of a UI design, which is acquired by selecting a specific option for each attribute to be explored, as a configuration of the design.", "label": "Method", "bboxes": [{"left": 0.4615653594771242, "top": 0.7427878787878788, "width": 0.018895424836601227, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.756625, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.7704621212121212, "width": 0.39256045751633994, "height": 0.011332070707070763, "page": 4}, {"left": 0.08790522875816993, "top": 0.7842992424242424, "width": 0.06349019607843136, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3188"}, {"text": "The goal of our initialization process is to maximally cover possible options for each attribute, but meanwhile limit the total number of designs to be compared by human raters.", "label": "Method", "bboxes": [{"left": 0.19848039215686275, "top": 0.3586262626262626, "width": 0.2844591503267974, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.37246338383838384, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.38630050505050506, "width": 0.35276143790849673, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3189"}, {"text": "To do so, we introduce a bit mask, named feedback mask , for each genetic sequencethat corresponds a design instance, which has the same length as a genetic sequence.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.10956060606060607, "width": 0.3762859477124183, "height": 0.011332070707070707, "page": 5}, {"left": 0.08790522875816993, "top": 0.12339772727272727, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.1372348484848485, "width": 0.22974673202614382, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3190"}, {"text": "We enhanced the single-point crossover operation of genetic algorithms, by assigning random option values to positions in the sequence (corresponding attributes in the design) where the feedback mask is 0 in the descendants.", "label": "Method", "bboxes": [{"left": 0.17576143790849674, "top": 0.5108333333333334, "width": 0.30470424836601306, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.5246704545454546, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.5385075757575758, "width": 0.39256699346405227, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.5516767676767677, "width": 0.2360114379084967, "height": 0.011988636363636451, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3191"}, {"text": "We invited five participants for this remote user study.", "label": "Method", "bboxes": [{"left": 0.7086290849673202, "top": 0.5293876262626263, "width": 0.20346732026143788, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.5432247474747475, "width": 0.10915196078431366, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3192"}, {"text": "We evaluate Spacewalker in multiple dimensions.", "label": "Method", "bboxes": [{"left": 0.5188316993464052, "top": 0.3218320707070707, "width": 0.3049624183006536, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3193"}, {"text": "Our front end includes a task authoring interface for a designer to create and launch a task (see Figure 2), a monitor interface for the designer to monitor the task progress and export results (see Figure 4), and a worker interface for the worker to compare a pair of designs (see Figure 3).", "label": "Method", "bboxes": [{"left": 0.15269117647058825, "top": 0.8048611111111111, "width": 0.32805065359477126, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8186982323232324, "width": 0.39283333333333337, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8325353535353536, "width": 0.39255555555555555, "height": 0.011320707070706981, "page": 5}, {"left": 0.08790522875816993, "top": 0.8463724747474748, "width": 0.395031045751634, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8602095959595959, "width": 0.21625000000000005, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.3 Architectures", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3194"}, {"text": "Mutation is handled in a similar way to the traditional approach, where we alter one attribute in a genetic sequence based on a mutation rate.", "label": "Method", "bboxes": [{"left": 0.17194281045751633, "top": 0.6215290404040403, "width": 0.30852450980392154, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.6353661616161616, "width": 0.39256045751633994, "height": 0.011320707070707203, "page": 5}, {"left": 0.08790522875816993, "top": 0.6492032828282828, "width": 0.11604084967320262, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3195"}, {"text": "In this study, we evaluate the usability of our proposed HTML extensions by gather informal feedback from web designers.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.45108333333333334, "width": 0.39293790849673205, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.46491919191919195, "width": 0.36536274509803923, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3196"}, {"text": "Finally, we consider nested designs (where one option value depend on another parent value).", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.6907146464646465, "width": 0.37627450980392163, "height": 0.011320707070706981, "page": 5}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.20579411764705885, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3197"}, {"text": "We asked each participant to add markups to one template HTML web page (the Cover example in Section 5.2.1), specifying exploration options for attributes or style sheet entries that they would like to change.", "label": "Method", "bboxes": [{"left": 0.8247287581699346, "top": 0.6677575757575758, "width": 0.08736274509803932, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.681594696969697, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.6954318181818182, "width": 0.39283496732026146, "height": 0.011332070707070652, "page": 5}, {"left": 0.5195343137254902, "top": 0.7092689393939394, "width": 0.3948202614379086, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3198"}, {"text": "All participants were able to learn the Spacewalker markup syntax using the description we provided and were able to create syntactically correct specifications.", "label": "Method", "bboxes": [{"left": 0.6833088235294118, "top": 0.8429217171717173, "width": 0.22878104575163383, "height": 0.011320707070706981, "page": 5}, {"left": 0.5195343137254902, "top": 0.8567588383838384, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 5}, {"left": 0.5189918300653594, "top": 0.8705959595959595, "width": 0.33919934640522875, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3199"}, {"text": "We conducted two experiments to evaluate whether Spacewalker was able to efficiently search a design space and generate better designs by utilizing the responses from the crowd workers.", "label": "Method", "bboxes": [{"left": 0.08720261437908497, "top": 0.4878926767676768, "width": 0.39354901960784316, "height": 0.011320707070707092, "page": 6}, {"left": 0.08736437908496732, "top": 0.501729797979798, "width": 0.3933856209150327, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5155669191919192, "width": 0.36811601307189545, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3200"}, {"text": "After a task was finished, we selected the five designs that received most votes from raters for each method.", "label": "Method", "bboxes": [{"left": 0.3887303921568627, "top": 0.8567588383838384, "width": 0.09173366013071899, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8844318181818183, "width": 0.13575490196078432, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3201"}, {"text": "We received largely positive feedback from the participants.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.34482323232323236, "width": 0.3582385620915032, "height": 0.011320707070707037, "page": 6}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3202"}, {"text": "We conducted both experiments following the same procedure.", "label": "Method", "bboxes": [{"left": 0.3279264705882353, "top": 0.6076919191919191, "width": 0.1550081699346405, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.6215290404040403, "width": 0.22123692810457518, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3203"}, {"text": "For each search specification, we calculate the percentage of votes received by Spacewalker genetic method (the rest of the votes are received by uniform sampling).", "label": "Method", "bboxes": [{"left": 0.689547385620915, "top": 0.7958598484848485, "width": 0.22502287581699354, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8096969696969697, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8235340909090909, "width": 0.3948202614379086, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3204"}, {"text": "We then deployed another task to a separate group of crowd workers for evaluating the quality of these selected designs.", "label": "Method", "bboxes": [{"left": 0.5799232026143791, "top": 0.1372348484848485, "width": 0.332174836601307, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.3948186274509803, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3205"}, {"text": "We added exploration attributes and options to five additional web page templates, which are also based on Bootstrap examples 8 .", "label": "Method", "bboxes": [{"left": 0.816795751633987, "top": 0.4001376262626263, "width": 0.09777777777777785, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.4139747474747475, "width": 0.39416830065359487, "height": 0.011320707070707092, "page": 6}, {"left": 0.5189918300653594, "top": 0.4253421717171717, "width": 0.26498856209150334, "height": 0.013790404040404047, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3206"}, {"text": "In this experiment, we varied the search space size by using a different number of attributes and options in the design search specification.", "label": "Method", "bboxes": [{"left": 0.8265310457516339, "top": 0.2894419191919192, "width": 0.08803431372549031, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.30327777777777776, "width": 0.39283986928104575, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.317114898989899, "width": 0.3679967320261438, "height": 0.011320707070707037, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3207"}, {"text": "Although our participants did not encounter much difficulty, this can be challenging as the design space becomes convoluted.", "label": "Method", "bboxes": [{"left": 0.6168954248366013, "top": 0.42046464646464643, "width": 0.2951993464052288, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.43430176767676765, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.44813888888888886, "width": 0.07110294117647065, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3208"}, {"text": "In order to support more advanced dependency, we believe automatic tooling for identifying option dependencies and detecting potential inconsistency would be necessary, which provides opportunity for future work.", "label": "Method", "bboxes": [{"left": 0.6577320261437908, "top": 0.5865088383838384, "width": 0.25683169934640526, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6003459595959596, "width": 0.39503431372549025, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6141830808080808, "width": 0.39417156862745095, "height": 0.011320707070707092, "page": 7}, {"left": 0.5189918300653594, "top": 0.628020202020202, "width": 0.27204084967320274, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3209"}, {"text": "In addition, we observe a trend of increased preference for Spacewalker genetic method as the size of the search space increases (see Figure 6).", "label": "Method", "bboxes": [{"left": 0.3930261437908497, "top": 0.36511742424242427, "width": 0.0874395424836601, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.3789532828282828, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.392790404040404, "width": 0.363328431372549, "height": 0.011320707070707092, "page": 7}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3210"}, {"text": "Our quantitative experiments for examining the performance of Spacewalker algorithms for searching a design space indicate that it improves designs over time by producing better design candidates, particularly when the search space is large.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.8355744949494949, "width": 0.3762794117647058, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8494116161616162, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8632487373737373, "width": 0.39417320261437905, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8770858585858586, "width": 0.25522549019607843, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3211"}, {"text": "For the experiment where Spacewalker is used to search for different web page types, we find that crowd raters, from the crossmethod evaluation, preferred the results produced by Spacewalker genetic method in all cases we tested when they are compared with those from the uniform sampling method (see Table 3).", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.43430176767676765, "width": 0.3765588235294117, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.44813888888888886, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4619760101010101, "width": 0.3928333333333333, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4758131313131313, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4896502525252525, "width": 0.3386454248366013, "height": 0.011320707070707092, "page": 7}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3212"}, {"text": "Our participants gave us several useful suggestions for improvements.", "label": "Method", "bboxes": [{"left": 0.3268300653594771, "top": 0.8152474747474748, "width": 0.15363888888888894, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.2787058823529412, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3213"}, {"text": "In this section, we discuss the strengths and limitation of our work, and our plan for future work.", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.6492032828282828, "width": 0.3941683006535948, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.17506535947712415, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3214"}, {"text": "We plan to infer these hyperparamters (the number of workers and iterations) needed automatically.", "label": "Method", "bboxes": [{"left": 0.2549166666666666, "top": 0.4416489898989899, "width": 0.22554901960784324, "height": 0.011320707070707092, "page": 9}, {"left": 0.0874656862745098, "top": 0.45548484848484855, "width": 0.3695114379084968, "height": 0.011320707070707037, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3215"}, {"text": "For each of the tasks, our GA-based algorithm only visited a small portion of the design space.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.17874494949494948, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.1925820707070707, "width": 0.19990686274509806, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3216"}, {"text": "Finally, we want to emphasize that Spacewalker is able to identify an optimal design in a defined design space, instead of finding a globally optimal design.", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.5385075757575758, "width": 0.3787516339869281, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.552344696969697, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5661818181818182, "width": 0.15059640522875817, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3217"}, {"text": "effectively utilize crowd worker feedback, our system can quickly explore the search space of a web design, which provides realtime feedback to the designer about the progress of the search.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.10956060606060607, "width": 0.39294281045751633, "height": 0.011320707070707065, "page": 9}, {"left": 0.5195343137254902, "top": 0.12339772727272727, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1372348484848485, "width": 0.3948202614379086, "height": 0.011320707070707092, "page": 9}], "section": "7 CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3218"}, {"text": "Our HTML markup for creating exploration specification provides a lightweight and familiar language for designers to specify complex designs and search requirements.", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.3929183006535948, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8844318181818183, "width": 0.19304248366013071, "height": 0.011320707070707092, "page": 9}], "section": "7 CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3219"}, {"text": "We would like to thank anonymous reviewers for their insightful feedback for improving the paper.", "label": "Method", "bboxes": [{"left": 0.5188316993464052, "top": 0.24478535353535355, "width": 0.39325816993464047, "height": 0.011320707070707065, "page": 9}, {"left": 0.5195343137254902, "top": 0.25862247474747474, "width": 0.2007859477124183, "height": 0.011320707070707037, "page": 9}], "section": "ACKNOWLEDGEMENTS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3220"}, {"text": "recruit user participants, moderate a study session, and observe and analyze findings from the study [4].", "label": "Result", "bboxes": [{"left": 0.5195343137254902, "top": 0.46294444444444444, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.47678156565656565, "width": 0.2392875816993465, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3221"}, {"text": "In addition, we observe a trend of increased preference for Spacewalker genetic method as the size of the search space increases (see Figure 6).", "label": "Result", "bboxes": [{"left": 0.3930261437908497, "top": 0.36511742424242427, "width": 0.0874395424836601, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.3789532828282828, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.392790404040404, "width": 0.363328431372549, "height": 0.011320707070707092, "page": 7}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3222"}, {"text": "For the experiment where Spacewalker is used to search for different web page types, we find that crowd raters, from the crossmethod evaluation, preferred the results produced by Spacewalker genetic method in all cases we tested when they are compared with those from the uniform sampling method (see Table 3).", "label": "Result", "bboxes": [{"left": 0.10418464052287582, "top": 0.43430176767676765, "width": 0.3765588235294117, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.44813888888888886, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4619760101010101, "width": 0.3928333333333333, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4758131313131313, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4896502525252525, "width": 0.3386454248366013, "height": 0.011320707070707092, "page": 7}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3223"}, {"text": "Our experiments show that the concept of Spacewalker is well received by the designers and developers.", "label": "Result", "bboxes": [{"left": 0.26665196078431375, "top": 0.6630404040404041, "width": 0.21628594771241827, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3948153594771242, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3224"}, {"text": "Various commercial tools also exist to support A/B testing of UI designs.", "label": "Conclusion", "bboxes": [{"left": 0.5190506535947712, "top": 0.33813005050505046, "width": 0.3930424836601307, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.3519671717171717, "width": 0.04832843137254905, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3225"}, {"text": "3) creating a tool that can provide integrated support for design exploration.", "label": "Conclusion", "bboxes": [{"left": 0.7549330065359477, "top": 0.6978926767676767, "width": 0.15716503267973847, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7117297979797981, "width": 0.29933496732026155, "height": 0.011320707070706981, "page": 0}], "section": "1 INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3226"}, {"text": "For example, a designer may consider multiple color schemes or layout choices for a UI.", "label": "Conclusion", "bboxes": [{"left": 0.1620718954248366, "top": 0.6984633838383838, "width": 0.31866666666666665, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.7123005050505051, "width": 0.20808496732026144, "height": 0.011320707070706981, "page": 0}], "section": "1 INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3227"}, {"text": "CHI 21, May 813, 2021, Yokohama, Japan ACM ISBN 978-1-4503-8096-6/21/05.", "label": "Conclusion", "bboxes": [{"left": 0.08790522875816993, "top": 0.8551742424242424, "width": 0.19444607843137257, "height": 0.008805555555555622, "page": 0}, {"left": 0.08750653594771242, "top": 0.8762979797979799, "width": 0.16908006535947712, "height": 0.008805555555555511, "page": 0}], "section": "1 INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3228"}, {"text": "Nevertheless, A/B testing still struggles to support a large design space when there are hundreds or thousands of design alternatives.", "label": "Conclusion", "bboxes": [{"left": 0.6520833333333333, "top": 0.6981729797979798, "width": 0.2600114379084967, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7120101010101011, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7258472222222222, "width": 0.13622875816993463, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3229"}, {"text": "In Spacewalker, we aim to address these issues by providing an integrated support for designers to explore a large design space and improve their design.", "label": "Conclusion", "bboxes": [{"left": 0.6606797385620915, "top": 0.7258472222222222, "width": 0.2514183006535947, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7396843434343435, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7535214646464646, "width": 0.2761127450980392, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3230"}, {"text": "Various tools or platforms are available to support A/B testing of UI designs, such as GoodUI 2 , Optimizely 3 and VWO 4 .", "label": "Conclusion", "bboxes": [{"left": 0.6605555555555556, "top": 0.5321287878787879, "width": 0.2515343137254902, "height": 0.011320707070706981, "page": 1}, {"left": 0.5195343137254902, "top": 0.5434962121212121, "width": 0.39174673202614374, "height": 0.013790404040403992, "page": 1}, {"left": 0.5195343137254902, "top": 0.5573333333333333, "width": 0.06890032679738567, "height": 0.013790404040403992, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3231"}, {"text": "The experiments indicate that UI designs obtained by Spacewalker were significantly more preferred by human evaluators 1 than those from a baseline method.", "label": "Conclusion", "bboxes": [{"left": 0.17180065359477123, "top": 0.5321287878787879, "width": 0.30866830065359485, "height": 0.011320707070706981, "page": 1}, {"left": 0.08790522875816993, "top": 0.545965909090909, "width": 0.3950326797385621, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5573333333333333, "width": 0.26154738562091506, "height": 0.013790404040403992, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3232"}, {"text": "Our work is related to three areas of the literature, including UI evaluation methods, crowdsourcing-based design support, and interactive UI design optimization.", "label": "Conclusion", "bboxes": [{"left": 0.08790522875816993, "top": 0.761003787878788, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.7748409090909091, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.7886780303030303, "width": 0.19503431372549018, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3233"}, {"text": "To do so, she adds an explore-child-id attribute to the parent div node of the navigation bar, which instructs Spacewalker to only use one out of multiple candidates when generating and evaluating a design.", "label": "Conclusion", "bboxes": [{"left": 0.606437908496732, "top": 0.37179545454545454, "width": 0.3056568627450982, "height": 0.011988636363636396, "page": 2}, {"left": 0.5195343137254902, "top": 0.38563257575757576, "width": 0.39503594771241834, "height": 0.011988636363636396, "page": 2}, {"left": 0.5189918300653594, "top": 0.4001376262626263, "width": 0.3931062091503267, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.4139747474747475, "width": 0.14450326797385626, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3234"}, {"text": "Lastly, instead of exploring a design space based on elements, a designer can explore style options with CSS selectors using the same format as a regular CSS file, and by again prefixing the \" explore\" tag, and by using a line of dashes to indicate alternative styles (see examples in Section 3).", "label": "Conclusion", "bboxes": [{"left": 0.6850098039215686, "top": 0.8152474747474748, "width": 0.2270882352941176, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.8290845959595959, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.8429217171717173, "width": 0.39294281045751644, "height": 0.011320707070706981, "page": 3}, {"left": 0.5195343137254902, "top": 0.8560909090909091, "width": 0.39255718954248375, "height": 0.01198863636363634, "page": 3}, {"left": 0.5195343137254902, "top": 0.8705959595959595, "width": 0.3399885620915033, "height": 0.011320707070707092, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3235"}, {"text": "Because of this, powerful CSS features, such as CSS variables (which can", "label": "Conclusion", "bboxes": [{"left": 0.863187908496732, "top": 0.8705959595959595, "width": 0.04890196078431375, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3236"}, {"text": "As a raters judgement can be dominated by the early examples and may drift over time [1], it is generally difficult for a user to rate the goodness of a design with an absolute scale.", "label": "Conclusion", "bboxes": [{"left": 0.5782892156862746, "top": 0.1925820707070707, "width": 0.3362761437908497, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.20641919191919192, "width": 0.39283823529411765, "height": 0.011320707070707037, "page": 4}, {"left": 0.5195343137254902, "top": 0.22025631313131314, "width": 0.3161764705882353, "height": 0.011320707070707037, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3237"}, {"text": "Thus, our fitness function outputs 1 for the preferred design and 0 for the less preferred one.", "label": "Conclusion", "bboxes": [{"left": 0.765313725490196, "top": 0.26176767676767676, "width": 0.1467777777777779, "height": 0.011332070707070707, "page": 4}, {"left": 0.5195343137254902, "top": 0.2749368686868687, "width": 0.394812091503268, "height": 0.011988636363636396, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3238"}, {"text": "Although presenting more than two examples in a gallery design can be another appealing alternative [1, 13], the viewing area for each example would be too small in our case of web design, and may prevent raters from noticing design details that matter.", "label": "Conclusion", "bboxes": [{"left": 0.5190212418300654, "top": 0.2894419191919192, "width": 0.39307352941176465, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.30327777777777776, "width": 0.39283823529411754, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.317114898989899, "width": 0.3925637254901959, "height": 0.011320707070707037, "page": 4}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.35701470588235296, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3239"}, {"text": "To search for an optimal design in the space, it is prohibitively expensive to examine every possible design configuration with worker evaluation.", "label": "Conclusion", "bboxes": [{"left": 0.3831830065359477, "top": 0.3409507575757576, "width": 0.09728431372549018, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.3547878787878788, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.36862500000000004, "width": 0.3697745098039216, "height": 0.011320707070707037, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3240"}, {"text": "On the other hand, with a limited number of worker feedback, which is often the case in reality, a search very likely ends up with a sub-optimal design, as shown in our experiments later.", "label": "Conclusion", "bboxes": [{"left": 0.4618790849673203, "top": 0.36862500000000004, "width": 0.01858986928104578, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.38246212121212125, "width": 0.3925653594771242, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.39629924242424247, "width": 0.39256372549019614, "height": 0.011320707070707037, "page": 4}, {"left": 0.08790522875816993, "top": 0.4101363636363636, "width": 0.32421895424836605, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3241"}, {"text": "be used to store values in custom properties) 6 , can be adopted to streamline the specification of possible values for properties.", "label": "Conclusion", "bboxes": [{"left": 0.08790522875816993, "top": 0.1070909090909091, "width": 0.3925653594771242, "height": 0.013790404040404033, "page": 4}, {"left": 0.08790522875816993, "top": 0.12339772727272727, "width": 0.36884477124183007, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3242"}, {"text": "We refer an instance of a UI design, which is acquired by selecting a specific option for each attribute to be explored, as a configuration of the design.", "label": "Conclusion", "bboxes": [{"left": 0.4615653594771242, "top": 0.7427878787878788, "width": 0.018895424836601227, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.756625, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.7704621212121212, "width": 0.39256045751633994, "height": 0.011332070707070763, "page": 4}, {"left": 0.08790522875816993, "top": 0.7842992424242424, "width": 0.06349019607843136, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3243"}, {"text": "Note that attributes that are not marked for exploration do not appear in a configuration for genetic programming because they are already determined by the designer.", "label": "Conclusion", "bboxes": [{"left": 0.15491339869281046, "top": 0.7842992424242424, "width": 0.3255522875816994, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.7981363636363636, "width": 0.39256699346405227, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8119734848484849, "width": 0.26708006535947715, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3244"}, {"text": "During initialization , the first generation is randomly generated, with the goal of covering as many configurations as possible.", "label": "Conclusion", "bboxes": [{"left": 0.33396732026143794, "top": 0.514195707070707, "width": 0.14650163398692811, "height": 0.011332070707070763, "page": 4}, {"left": 0.08790522875816993, "top": 0.5280328282828283, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5418699494949495, "width": 0.18922385620915033, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3245"}, {"text": "The goal of our initialization process is to maximally cover possible options for each attribute, but meanwhile limit the total number of designs to be compared by human raters.", "label": "Conclusion", "bboxes": [{"left": 0.19848039215686275, "top": 0.3586262626262626, "width": 0.2844591503267974, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.37246338383838384, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.38630050505050506, "width": 0.35276143790849673, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3246"}, {"text": "Therefore, we treat each attribute independently during initialization, and generate design variations by sampling the options of one attribute at a time while affixing the rest exploratory attributes at a random value.", "label": "Conclusion", "bboxes": [{"left": 0.44373856209150325, "top": 0.38630050505050506, "width": 0.03919771241830072, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.4001376262626263, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.4139747474747475, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.42781186868686866, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 5}, {"left": 0.08753921568627451, "top": 0.4416489898989899, "width": 0.034862745098039216, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3247"}, {"text": "The diff mask represents where we would likely gain knowledge by comparing these two designs: in the part where they differ.", "label": "Conclusion", "bboxes": [{"left": 0.241890522875817, "top": 0.20641919191919192, "width": 0.23857189542483662, "height": 0.011320707070707037, "page": 5}, {"left": 0.08790522875816993, "top": 0.22025631313131314, "width": 0.3925571895424837, "height": 0.011320707070707037, "page": 5}, {"left": 0.08736437908496732, "top": 0.23409343434343433, "width": 0.10527941176470589, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3248"}, {"text": "We conduct a user study to investigate whether Spacewalker markup extension to HTML is easy to understand and use by designers and developers, and how they react to the overall support of Spacewalker for design exploration.", "label": "Conclusion", "bboxes": [{"left": 0.828156862745098, "top": 0.3218320707070707, "width": 0.08394117647058819, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.3356691919191919, "width": 0.39256045751633983, "height": 0.011320707070707037, "page": 5}, {"left": 0.5195343137254902, "top": 0.34950631313131314, "width": 0.39416993464052286, "height": 0.011320707070707037, "page": 5}, {"left": 0.5195343137254902, "top": 0.3633434343434343, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.3771805555555555, "width": 0.07033986928104574, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3249"}, {"text": "Therefore, each design search task needed 50 raters.", "label": "Conclusion", "bboxes": [{"left": 0.1688513071895425, "top": 0.6907146464646465, "width": 0.3138709150326797, "height": 0.011320707070706981, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3250"}, {"text": "To account for raters that may not be responsive after accepting the tasks, we distributed the tasks to 70 raters.", "label": "Conclusion", "bboxes": [{"left": 0.0874656862745098, "top": 0.7045517676767676, "width": 0.3930065359477124, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.718388888888889, "width": 0.25572549019607843, "height": 0.011320707070706981, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3251"}, {"text": "For uniform sampling, to ensure that it receives the same number of rater responses as the genetic method, we randomly deployed 500 samples, which amounts to 250 pairs thus 250 rater responses.", "label": "Conclusion", "bboxes": [{"left": 0.3472794117647059, "top": 0.718388888888889, "width": 0.13480392156862753, "height": 0.011320707070706981, "page": 6}, {"left": 0.08790522875816993, "top": 0.7322260101010101, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.7460618686868686, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.75989898989899, "width": 0.2881356209150327, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3252"}, {"text": "First, in complex design spaces, designers may want to maintain dependencies between several sets of elements, while style options for different elements can also be dependent, where only certain options or elements can be combined together.", "label": "Conclusion", "bboxes": [{"left": 0.791687908496732, "top": 0.5311603535353535, "width": 0.12287745098039238, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.5449974747474747, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.558834595959596, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.5726717171717172, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.5865088383838384, "width": 0.13454738562091495, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "3253"}, {"text": "In order to support more advanced dependency, we believe automatic tooling for identifying option dependencies and detecting potential inconsistency would be necessary, which provides opportunity for future work.", "label": "Conclusion", "bboxes": [{"left": 0.6577320261437908, "top": 0.5865088383838384, "width": 0.25683169934640526, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6003459595959596, "width": 0.39503431372549025, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6141830808080808, "width": 0.39417156862745095, "height": 0.011320707070707092, "page": 7}, {"left": 0.5189918300653594, "top": 0.628020202020202, "width": 0.27204084967320274, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "3254"}, {"text": "Therefore, even our enhanced genetic algorithm may not converge to an optimal solution with a small number of comparisons.", "label": "Conclusion", "bboxes": [{"left": 0.8170081699346405, "top": 0.6556944444444444, "width": 0.09508333333333341, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.669530303030303, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6833674242424242, "width": 0.279437908496732, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "3255"}, {"text": "Our quantitative experiments for examining the performance of Spacewalker algorithms for searching a design space indicate that it improves designs over time by producing better design candidates, particularly when the search space is large.", "label": "Conclusion", "bboxes": [{"left": 0.5358137254901961, "top": 0.8355744949494949, "width": 0.3762794117647058, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8494116161616162, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8632487373737373, "width": 0.39417320261437905, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8770858585858586, "width": 0.25522549019607843, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "3256"}, {"text": "P1 suggested that Spacewalker should provide suggestions for possible options to explore for a specific property, and warns designers when an option value is out of a reasonable range for a good design.", "label": "Conclusion", "bboxes": [{"left": 0.37136274509803924, "top": 0.8290845959595959, "width": 0.10910620915032682, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.39256372549019614, "height": 0.011320707070706981, "page": 7}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 7}, {"left": 0.08753921568627451, "top": 0.8705959595959595, "width": 0.2949264705882353, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "3257"}, {"text": "\"This tool provides (a) useful way to compare my designs. I used to use the Inspect tool in Chrome to try out different values of the styles of my attributes, but the limitation is that I can only modify one item at a time. With this tool I could manage my HTML/CSS code and potential designs of the whole page efficiently. It improves my productivity and experience significantly.\"", "label": "Conclusion", "bboxes": [{"left": 0.10418464052287582, "top": 0.7184002525252525, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7322373737373737, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7460732323232323, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7599103535353536, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7737474747474747, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7875845959595961, "width": 0.2415947712418301, "height": 0.011320707070706981, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "3258"}, {"text": "there is a research opportunity to better support designers by providing more guidance when creating a design search task.", "label": "Conclusion", "bboxes": [{"left": 0.1755326797385621, "top": 0.30327777777777776, "width": 0.3074084967320262, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.317114898989899, "width": 0.39255882352941174, "height": 0.011320707070707037, "page": 9}, {"left": 0.08790522875816993, "top": 0.3309520202020202, "width": 0.027209150326797382, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "3259"}, {"text": "The streamlined end-to-end support of Spacewalker is useful for both novice and experienced designers.", "label": "Conclusion", "bboxes": [{"left": 0.456828431372549, "top": 0.7045517676767676, "width": 0.023640522875817027, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.718388888888889, "width": 0.39255882352941174, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.7322260101010101, "width": 0.19746568627450983, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "3260"}, {"text": "that the quality of these designs could further improved with more iterations and workers.", "label": "Conclusion", "bboxes": [{"left": 0.08790522875816993, "top": 0.10956060606060607, "width": 0.39255882352941174, "height": 0.011320707070707065, "page": 9}, {"left": 0.08790522875816993, "top": 0.12339772727272727, "width": 0.13865522875816993, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "3261"}, {"text": "These improvements can be easily achievable, particularly because these were achieved by only using 50 workers and a small monetary budget (around 35 US Dollars), which gives a lot of room to scale up.", "label": "Conclusion", "bboxes": [{"left": 0.2302156862745098, "top": 0.12339772727272727, "width": 0.2527205882352942, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.1372348484848485, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 9}, {"left": 0.08736437908496732, "top": 0.1510719696969697, "width": 0.3931062091503268, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.1649078282828283, "width": 0.1822140522875817, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "3262"}, {"text": "Our experiments indicate that Spacewalker is well received by designers, and Spacewalkers genetic search algorithm significantly outperformed a uniform sampling baseline under different search space sizes and web design types.", "label": "Conclusion", "bboxes": [{"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.39293790849673205, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.39293464052287597, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.20023529411764707, "height": 0.011320707070707092, "page": 9}], "section": "7 CONCLUSION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "3263"}, {"text": "Spacewalker provides integrated support to enable designers to rapidly explore a large design space to improve their web UI design.", "label": "Conclusion", "bboxes": [{"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.3948153594771242, "height": 0.011320707070706981, "page": 9}], "section": "7 CONCLUSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "3264"}, {"text": "An expressive and easy-to-use HTML markup extension that allows designers to easily specify various alternatives for design search, which requires negligible learning effort;  An enhanced genetic algorithm that can efficiently explore a large design space using crowd worker responses from pairwise comparison of UI designs;  Integrated general tool support that allows designers to easily obtain an improved design from a large range of options within a short period of time (e.g., 1 hour) at a small amount of cost (e.g., 35 US dollars).", "label": "Conclusion", "bboxes": [{"left": 0.11399509803921569, "top": 0.5902285353535354, "width": 0.36646895424836595, "height": 0.012238636363636313, "page": 1}, {"left": 0.1277892156862745, "top": 0.6049835858585858, "width": 0.3529477124183007, "height": 0.011320707070707092, "page": 1}, {"left": 0.1277892156862745, "top": 0.618820707070707, "width": 0.33253267973856204, "height": 0.011320707070707092, "page": 1}, {"left": 0.11399509803921569, "top": 0.631739898989899, "width": 0.3664722222222222, "height": 0.012238636363636313, "page": 1}, {"left": 0.1277892156862745, "top": 0.6464949494949495, "width": 0.3526781045751634, "height": 0.011320707070707092, "page": 1}, {"left": 0.1277892156862745, "top": 0.6603320707070707, "width": 0.20980228758169933, "height": 0.011320707070707092, "page": 1}, {"left": 0.11399509803921569, "top": 0.6732512626262627, "width": 0.36894607843137256, "height": 0.012238636363636313, "page": 1}, {"left": 0.1277892156862745, "top": 0.6880063131313131, "width": 0.3526830065359477, "height": 0.011320707070707092, "page": 1}, {"left": 0.12724673202614378, "top": 0.7018434343434343, "width": 0.3532205882352941, "height": 0.011320707070707092, "page": 1}, {"left": 0.1277892156862745, "top": 0.7156792929292929, "width": 0.1601503267973856, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3265"}, {"text": "Note that the CSS attributes of elements for each node can be further explored, which enables recursive exploration.", "label": "Future Work", "bboxes": [{"left": 0.8023202614379085, "top": 0.5385075757575758, "width": 0.10977777777777775, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.552344696969697, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.5661818181818182, "width": 0.17852287581699355, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3266"}, {"text": "If multiple properties need to be explored jointly (e.g., height and width), Spacewalker allows a designer to combine multiple properties for exploration by joining their names using \" -and\" and optional values using a semicolon ( ; ):", "label": "Future Work", "bboxes": [{"left": 0.6537908496732027, "top": 0.6400492424242424, "width": 0.25830718954248344, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.6538863636363637, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.6677234848484849, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.6808926767676768, "width": 0.3375130718954249, "height": 0.01198863636363623, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3267"}, {"text": "To address the issue, we enhance traditional genetic algorithms, for each iteration, by directing rater feedback to genes that participate in a comparison while allowing the rest genes in a sequence to remain stochastic in the downstream evolution.", "label": "Future Work", "bboxes": [{"left": 0.51909477124183, "top": 0.8429217171717173, "width": 0.39327941176470604, "height": 0.011320707070706981, "page": 4}, {"left": 0.5195343137254902, "top": 0.8567588383838384, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8705959595959595, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.2844787581699346, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3268"}, {"text": "The search spaces specified ranged from 480 to 3888, which indicated the need of designers to explore large design spaces.", "label": "Future Work", "bboxes": [{"left": 0.4146356209150327, "top": 0.3033118686868687, "width": 0.06582843137254896, "height": 0.011320707070707037, "page": 6}, {"left": 0.08790522875816993, "top": 0.3171489898989899, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.33098611111111115, "width": 0.25747712418300656, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3269"}, {"text": "In order to support more advanced dependency, we believe automatic tooling for identifying option dependencies and detecting potential inconsistency would be necessary, which provides opportunity for future work.", "label": "Future Work", "bboxes": [{"left": 0.6577320261437908, "top": 0.5865088383838384, "width": 0.25683169934640526, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6003459595959596, "width": 0.39503431372549025, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.6141830808080808, "width": 0.39417156862745095, "height": 0.011320707070707092, "page": 7}, {"left": 0.5189918300653594, "top": 0.628020202020202, "width": 0.27204084967320274, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3270"}, {"text": "With a large enough search space, the effectiveness of any algorithm will be impacted.", "label": "Future Work", "bboxes": [{"left": 0.6052303921568628, "top": 0.7663901515151516, "width": 0.3068643790849672, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.7802272727272728, "width": 0.20738888888888896, "height": 0.011320707070706981, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3271"}, {"text": "This will require Spacewalker to encode certain design knowledge to make proper", "label": "Future Work", "bboxes": [{"left": 0.38509313725490196, "top": 0.8705959595959595, "width": 0.0953660130718954, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8844318181818183, "width": 0.3928333333333333, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3272"}, {"text": "In this section, we discuss the strengths and limitation of our work, and our plan for future work.", "label": "Future Work", "bboxes": [{"left": 0.08790522875816993, "top": 0.6492032828282828, "width": 0.3941683006535948, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.17506535947712415, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3273"}, {"text": "To further our understanding, we intend to compare the automatic process enabled by Spacewalker with the existing manual process for design search in realistic design projects in the future.", "label": "Future Work", "bboxes": [{"left": 0.28804084967320265, "top": 0.7322260101010101, "width": 0.19242483660130716, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.7460618686868686, "width": 0.39283496732026146, "height": 0.011320707070707092, "page": 9}, {"left": 0.08736437908496732, "top": 0.75989898989899, "width": 0.3955816993464052, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.7737361111111111, "width": 0.15783496732026142, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3274"}, {"text": "that the quality of these designs could further improved with more iterations and workers.", "label": "Future Work", "bboxes": [{"left": 0.08790522875816993, "top": 0.10956060606060607, "width": 0.39255882352941174, "height": 0.011320707070707065, "page": 9}, {"left": 0.08790522875816993, "top": 0.12339772727272727, "width": 0.13865522875816993, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3275"}, {"text": "The goal is to test whether web designers are able to learn and use our markups to specify search criteria and launch a design exploration task, and to gather feedback of the Spacewalker system.", "label": "Objective", "bboxes": [{"left": 0.8885424836601308, "top": 0.46491919191919195, "width": 0.02354901960784317, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.47875631313131317, "width": 0.39283660130718956, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.4925934343434344, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.5064305555555556, "width": 0.33367647058823524, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": 0.857783317565918, "is_author_statement": true, "is_in_expected_section": true, "id": "3276"}, {"text": "In Spacewalker, we aim to address these issues by providing an integrated support for designers to explore a large design space and improve their design.", "label": "Objective", "bboxes": [{"left": 0.6606797385620915, "top": 0.7258472222222222, "width": 0.2514183006535947, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7396843434343435, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.7535214646464646, "width": 0.2761127450980392, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "prob": 0.7050750255584717, "is_author_statement": true, "is_in_expected_section": true, "id": "3277"}, {"text": "Spacewalker instead presents each rater a pair of different candidate designs at a time, and asks the rater to select the preferred design, i.e., a twoalternative forced-choice (2AFC) method.", "label": "Method", "bboxes": [{"left": 0.8387401960784313, "top": 0.22025631313131314, "width": 0.07362745098039225, "height": 0.011320707070707037, "page": 4}, {"left": 0.5195343137254902, "top": 0.23409343434343433, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.24793055555555554, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.26176767676767676, "width": 0.24217647058823533, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.8018777370452881, "is_author_statement": false, "is_in_expected_section": true, "id": "3278"}, {"text": "After selecting the parents, the next generation is generated through the crossover operation.", "label": "Method", "bboxes": [{"left": 0.17700163398692811, "top": 0.5972171717171717, "width": 0.30346732026143797, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6110542929292929, "width": 0.2649019607843138, "height": 0.011332070707070763, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.7630525827407837, "is_author_statement": false, "is_in_expected_section": true, "id": "3279"}, {"text": "One method is single-point crossover, where a crossover point is randomly selected from both parents, and all the genetic representation to the right of the crossover point are swapped, forming two children (Figure 5).", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.6525656565656566, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6664027777777779, "width": 0.3950326797385621, "height": 0.011320707070706981, "page": 4}, {"left": 0.08790522875816993, "top": 0.680239898989899, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6940770202020201, "width": 0.11091503267973855, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.7614638209342957, "is_author_statement": false, "is_in_expected_section": true, "id": "3280"}, {"text": "For each search specification, we calculate the percentage of votes received by Spacewalker genetic method (the rest of the votes are received by uniform sampling).", "label": "Method", "bboxes": [{"left": 0.689547385620915, "top": 0.7958598484848485, "width": 0.22502287581699354, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8096969696969697, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8235340909090909, "width": 0.3948202614379086, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.7476945519447327, "is_author_statement": true, "is_in_expected_section": false, "id": "3281"}, {"text": "For uniform sampling, to ensure that it receives the same number of rater responses as the genetic method, we randomly deployed 500 samples, which amounts to 250 pairs thus 250 rater responses.", "label": "Method", "bboxes": [{"left": 0.3472794117647059, "top": 0.718388888888889, "width": 0.13480392156862753, "height": 0.011320707070706981, "page": 6}, {"left": 0.08790522875816993, "top": 0.7322260101010101, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.7460618686868686, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.75989898989899, "width": 0.2881356209150327, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.67348712682724, "is_author_statement": true, "is_in_expected_section": false, "id": "3282"}, {"text": "In sum, the only difference between the two conditions is the method used for searching the design space, while the rest aspects including the feedback mechanism is the same.", "label": "Method", "bboxes": [{"left": 0.3238921568627451, "top": 0.7737361111111111, "width": 0.15657352941176472, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.7875732323232324, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8014103535353535, "width": 0.39503594771241834, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.10501143790849672, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.6181893348693848, "is_author_statement": false, "is_in_expected_section": false, "id": "3283"}, {"text": "To address challenge 1), We designed the HTML annotation as a simple extension of the existing HTML and CSS grammar, where instead of specifying a single value for an attribute, a designer can provide multiple candidate values for it, which are to be explored by Spacewalker.", "label": "Method", "bboxes": [{"left": 0.5358137254901961, "top": 0.7255656565656566, "width": 0.3762794117647059, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7394027777777777, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7532398989898991, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7670770202020202, "width": 0.39255392156862734, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7809141414141415, "width": 0.09825653594771244, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": 0.5985918641090393, "is_author_statement": true, "is_in_expected_section": true, "id": "3284"}, {"text": "The back-end server is responsible for scheduling workers for different web UI pairs without conflicts and supporting multiple workers submitting results at the same time, using database read/write lock.", "label": "Method", "bboxes": [{"left": 0.8893807189542484, "top": 0.1649078282828283, "width": 0.02271405228758161, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.39255718954248375, "height": 0.011320707070707092, "page": 5}, {"left": 0.5189918300653594, "top": 0.1925820707070707, "width": 0.3931013071895425, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.20641919191919192, "width": 0.3948153594771242, "height": 0.011320707070707037, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.3 Architectures", "prob": 0.542984664440155, "is_author_statement": false, "is_in_expected_section": true, "id": "3285"}, {"text": "The genetic algorithm generates design instances from the options, which are sent to the crowdsourcing frontend to collect worker feedback.", "label": "Method", "bboxes": [{"left": 0.5850539215686275, "top": 0.35407954545454545, "width": 0.32704248366013067, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.36791666666666667, "width": 0.39503758169934644, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.3817537878787879, "width": 0.13140196078431365, "height": 0.011320707070707092, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM", "prob": 0.5316394567489624, "is_author_statement": false, "is_in_expected_section": true, "id": "3286"}, {"text": "Therefore, we treat each attribute independently during initialization, and generate design variations by sampling the options of one attribute at a time while affixing the rest exploratory attributes at a random value.", "label": "Method", "bboxes": [{"left": 0.44373856209150325, "top": 0.38630050505050506, "width": 0.03919771241830072, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.4001376262626263, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.4139747474747475, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.42781186868686866, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 5}, {"left": 0.08753921568627451, "top": 0.4416489898989899, "width": 0.034862745098039216, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.4651779532432556, "is_author_statement": true, "is_in_expected_section": true, "id": "3287"}, {"text": "For each pair of designs shown to a crowd worker, we compute a diff mask to capture the differences between their genetic sequences where the corresponding bits for the differences are set to 1 .", "label": "Method", "bboxes": [{"left": 0.2854297385620915, "top": 0.1649078282828283, "width": 0.19503758169934643, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.17874494949494948, "width": 0.392562091503268, "height": 0.011332070707070735, "page": 5}, {"left": 0.08790522875816993, "top": 0.1925820707070707, "width": 0.3928398692810458, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.20575126262626264, "width": 0.15151470588235294, "height": 0.011988636363636312, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.4629864990711212, "is_author_statement": true, "is_in_expected_section": true, "id": "3288"}, {"text": "Our tool then parses the annotated HTML or CSS specification, and intelligently generates and distributes various configurations of the web UI to crowd workers for evaluation.", "label": "Method", "bboxes": [{"left": 0.8893267973856209, "top": 0.6010328282828282, "width": 0.023044117647058715, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.6148699494949494, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.6287070707070707, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 0}, {"left": 0.5189918300653594, "top": 0.6425441919191919, "width": 0.24420261437908508, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": 0.46187323331832886, "is_author_statement": true, "is_in_expected_section": true, "id": "3289"}, {"text": "Once enough feedback is received for one iteration, the genetic algorithm generates the next generation of designs, and the process is repeated until the specified number of iterations is reached.", "label": "Method", "bboxes": [{"left": 0.6550457516339869, "top": 0.3817537878787879, "width": 0.2570522875816993, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.3955909090909091, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.4094280303030303, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.42326515151515154, "width": 0.12396241830065358, "height": 0.011320707070707092, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM", "prob": 0.45221537351608276, "is_author_statement": false, "is_in_expected_section": true, "id": "3290"}, {"text": "After a task was finished, we selected the five designs that received most votes from raters for each method.", "label": "Method", "bboxes": [{"left": 0.3887303921568627, "top": 0.8567588383838384, "width": 0.09173366013071899, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8844318181818183, "width": 0.13575490196078432, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.44145771861076355, "is_author_statement": true, "is_in_expected_section": false, "id": "3291"}, {"text": "Similar to previous work, we also embed the crowd in the loop of the design and evaluation process.", "label": "Method", "bboxes": [{"left": 0.4210065359477124, "top": 0.26176767676767676, "width": 0.05946241830065363, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.275604797979798, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.2894419191919192, "width": 0.16637254901960785, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Crowd-Powered UI Design & Evaluation", "prob": 0.4389514625072479, "is_author_statement": true, "is_in_expected_section": false, "id": "3292"}, {"text": "In a typical HTML page or a CSS specification, designers first annotate each attribute they want to explore using a simple markup extension we designed.", "label": "Method", "bboxes": [{"left": 0.7644967320261438, "top": 0.5733598484848484, "width": 0.14760130718954245, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5871957070707071, "width": 0.3929411764705881, "height": 0.011320707070706981, "page": 0}, {"left": 0.5189918300653594, "top": 0.6010328282828282, "width": 0.36693137254901953, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": 0.4354220926761627, "is_author_statement": true, "is_in_expected_section": true, "id": "3293"}, {"text": "A crowd worker first signs in the worker interface by entering their worker ID, and then performs a sequence of evaluation tasks in which each trial involves indicating their preference over a pair of designs.", "label": "Method", "bboxes": [{"left": 0.8621895424836601, "top": 0.12339772727272727, "width": 0.04990849673202613, "height": 0.011320707070707092, "page": 5}, {"left": 0.5189918300653594, "top": 0.1372348484848485, "width": 0.3933839869281046, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.3664575163398692, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.3 Architectures", "prob": 0.42137783765792847, "is_author_statement": false, "is_in_expected_section": true, "id": "3294"}, {"text": "We plan to infer these hyperparamters (the number of workers and iterations) needed automatically.", "label": "Method", "bboxes": [{"left": 0.2549166666666666, "top": 0.4416489898989899, "width": 0.22554901960784324, "height": 0.011320707070707092, "page": 9}, {"left": 0.0874656862745098, "top": 0.45548484848484855, "width": 0.3695114379084968, "height": 0.011320707070707037, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.4105229675769806, "is_author_statement": true, "is_in_expected_section": false, "id": "3295"}, {"text": "Each rater in the evaluation was asked to compare designs from Spacewalker genetic method and those from uniform sampling side by side.", "label": "Method", "bboxes": [{"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.04548529411764701, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.4048495888710022, "is_author_statement": false, "is_in_expected_section": false, "id": "3296"}, {"text": "To preserve the hierarchical relationship of the HTML tree structure, the parser also maintains the hierarchical layout of the elements to be explored in a separate tree structure.", "label": "Method", "bboxes": [{"left": 0.31594444444444447, "top": 0.2412941919191919, "width": 0.16452777777777772, "height": 0.011320707070707065, "page": 4}, {"left": 0.08790522875816993, "top": 0.2551313131313131, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.2689684343434343, "width": 0.39256699346405227, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.28280555555555553, "width": 0.0840375816993464, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "prob": 0.4030432403087616, "is_author_statement": false, "is_in_expected_section": true, "id": "3297"}, {"text": "Participants then edited the provided the HTML documents in their preferred code or text editors, and we recorded the time used for them to experiment with the markups and complete each task.", "label": "Method", "bboxes": [{"left": 0.747452614379085, "top": 0.7369431818181819, "width": 0.16464542483660127, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.7507790404040403, "width": 0.39503267973856215, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.7646161616161616, "width": 0.39255718954248375, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.7784532828282827, "width": 0.22053431372549026, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": 0.4013301730155945, "is_author_statement": true, "is_in_expected_section": false, "id": "3298"}, {"text": "We then deployed another task to a separate group of crowd workers for evaluating the quality of these selected designs.", "label": "Method", "bboxes": [{"left": 0.5799232026143791, "top": 0.1372348484848485, "width": 0.332174836601307, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.3948186274509803, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.3853801488876343, "is_author_statement": true, "is_in_expected_section": false, "id": "3299"}, {"text": "We enhanced the single-point crossover operation of genetic algorithms, by assigning random option values to positions in the sequence (corresponding attributes in the design) where the feedback mask is 0 in the descendants.", "label": "Method", "bboxes": [{"left": 0.17576143790849674, "top": 0.5108333333333334, "width": 0.30470424836601306, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.5246704545454546, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.5385075757575758, "width": 0.39256699346405227, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.5516767676767677, "width": 0.2360114379084967, "height": 0.011988636363636451, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.38030529022216797, "is_author_statement": true, "is_in_expected_section": true, "id": "3300"}, {"text": "To systematically examine how well Spacewalker algorithms can evolve a design by quickly searching a design space, we tested it on six design tasks that range in search space sizes and design types.", "label": "Method", "bboxes": [{"left": 0.19482843137254902, "top": 0.49061742424242427, "width": 0.2859101307189542, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5044545454545455, "width": 0.3941683006535948, "height": 0.011320707070707092, "page": 1}, {"left": 0.08736437908496732, "top": 0.5182916666666667, "width": 0.39310620915032685, "height": 0.011320707070706981, "page": 1}, {"left": 0.08790522875816993, "top": 0.5321287878787879, "width": 0.0798300653594771, "height": 0.011320707070706981, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.37762466073036194, "is_author_statement": true, "is_in_expected_section": true, "id": "3301"}, {"text": "We based our study on the Cover example provided by Bootstrap 8 , and we added Spacewalker markups to create the specifications used in the study.", "label": "Method", "bboxes": [{"left": 0.8923725490196077, "top": 0.317114898989899, "width": 0.019725490196078388, "height": 0.011320707070707037, "page": 6}, {"left": 0.5195343137254902, "top": 0.32848358585858584, "width": 0.39256045751633983, "height": 0.013800505050505063, "page": 6}, {"left": 0.5189918300653594, "top": 0.3447891414141414, "width": 0.3931062091503268, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.3586262626262626, "width": 0.05772875816993461, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.36922788619995117, "is_author_statement": true, "is_in_expected_section": false, "id": "3302"}, {"text": "Our work is related to three areas of the literature, including UI evaluation methods, crowdsourcing-based design support, and interactive UI design optimization.", "label": "Method", "bboxes": [{"left": 0.08790522875816993, "top": 0.761003787878788, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.7748409090909091, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.7886780303030303, "width": 0.19503431372549018, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK", "prob": 0.3580133020877838, "is_author_statement": true, "is_in_expected_section": false, "id": "3303"}, {"text": "We can train a model to predict a reasonable worker and iteration size by incorporating design complexities.", "label": "Method", "bboxes": [{"left": 0.21833823529411764, "top": 0.4969962121212121, "width": 0.26212091503267976, "height": 0.011320707070707037, "page": 9}, {"left": 0.08736437908496732, "top": 0.5108333333333334, "width": 0.3953578431372549, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.3482823073863983, "is_author_statement": true, "is_in_expected_section": false, "id": "3304"}, {"text": "Behind the scene, Spacewalker creates three page designs with each using a different background choice.", "label": "Method", "bboxes": [{"left": 0.8086650326797385, "top": 0.26176767676767676, "width": 0.10503758169934652, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.275604797979798, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.2894419191919192, "width": 0.11716339869281045, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "prob": 0.3404093384742737, "is_author_statement": false, "is_in_expected_section": true, "id": "3305"}, {"text": "Alex can monitor the progress of the task in the Progress Viewer (see Figure 4), which allows her to see the sample designs of the current generation (iteration).", "label": "Method", "bboxes": [{"left": 0.10418464052287582, "top": 0.6829886363636364, "width": 0.3765669934640523, "height": 0.011320707070707092, "page": 3}, {"left": 0.0874656862745098, "top": 0.6968257575757575, "width": 0.3929967320261438, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.7106628787878788, "width": 0.17824509803921568, "height": 0.011320707070707092, "page": 3}], "section": "3 USING SPACEWALKER", "prob": 0.33725398778915405, "is_author_statement": false, "is_in_expected_section": true, "id": "3306"}, {"text": "Once a designer submits an HTML specification, the parser extracts the attributes", "label": "Method", "bboxes": [{"left": 0.38698202614379085, "top": 0.8515164141414141, "width": 0.09375653594771244, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.8653535353535353, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM", "prob": 0.3275061547756195, "is_author_statement": false, "is_in_expected_section": true, "id": "3307"}, {"text": "5.2.2 Performance Results.", "label": "Result", "bboxes": [{"left": 0.5195343137254902, "top": 0.795905303030303, "width": 0.16432026143790845, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.8903476595878601, "is_author_statement": false, "is_in_expected_section": true, "id": "3308"}, {"text": "As a result, it is necessary to use a more intelligent algorithm.", "label": "Result", "bboxes": [{"left": 0.41580718954248364, "top": 0.4101363636363636, "width": 0.06627124183006539, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.42397222222222225, "width": 0.30301960784313725, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.888299286365509, "is_author_statement": false, "is_in_expected_section": false, "id": "3309"}, {"text": "We evaluate Spacewalker in multiple dimensions.", "label": "Result", "bboxes": [{"left": 0.5188316993464052, "top": 0.3218320707070707, "width": 0.3049624183006536, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS", "prob": 0.8294211626052856, "is_author_statement": true, "is_in_expected_section": true, "id": "3310"}, {"text": "summarizes the results.", "label": "Result", "bboxes": [{"left": 0.08790522875816993, "top": 0.2756376262626263, "width": 0.14185620915032682, "height": 0.011320707070707037, "page": 6}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": 0.8259625434875488, "is_author_statement": false, "is_in_expected_section": true, "id": "3311"}, {"text": "Crowd raters, from the cross-method evaluation, showed significant preference for the designs generated by Spacewalker for all the search space sizes in Experiment 1 (see Table 2).", "label": "Result", "bboxes": [{"left": 0.10418464052287582, "top": 0.33744318181818184, "width": 0.3787630718954249, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.35128030303030305, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.36511742424242427, "width": 0.30173202614379085, "height": 0.011320707070707092, "page": 7}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.8083546161651611, "is_author_statement": false, "is_in_expected_section": true, "id": "3312"}, {"text": "All participants appreciated the time savings and improved efficiency when working with Spacewalker.", "label": "Result", "bboxes": [{"left": 0.38968137254901963, "top": 0.3863333333333333, "width": 0.09078431372549023, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.4001704545454545, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 6}, {"left": 0.08736437908496732, "top": 0.41400757575757574, "width": 0.10556045751633984, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": 0.7931019067764282, "is_author_statement": false, "is_in_expected_section": true, "id": "3313"}, {"text": "Our experiments indicate that Spacewalker is well received by designers, and Spacewalkers genetic search algorithm significantly outperformed a uniform sampling baseline under different search space sizes and web design types.", "label": "Result", "bboxes": [{"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.39293790849673205, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1649078282828283, "width": 0.39293464052287597, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.20023529411764707, "height": 0.011320707070707092, "page": 9}], "section": "7 CONCLUSION", "prob": 0.7878656983375549, "is_author_statement": true, "is_in_expected_section": true, "id": "3314"}, {"text": "On average, each task took about 1 hour to finish.", "label": "Result", "bboxes": [{"left": 0.10418464052287582, "top": 0.8567588383838384, "width": 0.28203267973856205, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.7679852247238159, "is_author_statement": false, "is_in_expected_section": true, "id": "3315"}, {"text": "This indicates that the benefit of Spacewalker is well demonstrated across different web page types.", "label": "Result", "bboxes": [{"left": 0.1891062091503268, "top": 0.5865088383838384, "width": 0.2913627450980393, "height": 0.011320707070707092, "page": 7}, {"left": 0.08736437908496732, "top": 0.6003459595959596, "width": 0.3052973856209151, "height": 0.011320707070707092, "page": 7}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.7483528852462769, "is_author_statement": false, "is_in_expected_section": true, "id": "3316"}, {"text": "The experiments indicate that UI designs obtained by Spacewalker were significantly more preferred by human evaluators 1 than those from a baseline method.", "label": "Result", "bboxes": [{"left": 0.17180065359477123, "top": 0.5321287878787879, "width": 0.30866830065359485, "height": 0.011320707070706981, "page": 1}, {"left": 0.08790522875816993, "top": 0.545965909090909, "width": 0.3950326797385621, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.5573333333333333, "width": 0.26154738562091506, "height": 0.013790404040403992, "page": 1}], "section": "1 INTRODUCTION", "prob": 0.7057240009307861, "is_author_statement": false, "is_in_expected_section": true, "id": "3317"}, {"text": "Therefore, each design search task needed 50 raters.", "label": "Result", "bboxes": [{"left": 0.1688513071895425, "top": 0.6907146464646465, "width": 0.3138709150326797, "height": 0.011320707070706981, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.6992048621177673, "is_author_statement": false, "is_in_expected_section": true, "id": "3318"}, {"text": "We believe the above extensions can provide a good starting point for designers to understand the search space.", "label": "Result", "bboxes": [{"left": 0.5944493464052287, "top": 0.44813888888888886, "width": 0.3176486928104574, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.4619760101010101, "width": 0.3746160130718954, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.6951524615287781, "is_author_statement": true, "is_in_expected_section": true, "id": "3319"}, {"text": "Reinecke et al. evaluated a set of 430 web designs through 40,000 online participants, demonstrating the feasibility of largescale design evaluations through the crowd [17].", "label": "Result", "bboxes": [{"left": 0.14784477124183007, "top": 0.1372348484848485, "width": 0.3326176470588236, "height": 0.011320707070707092, "page": 2}, {"left": 0.08756862745098039, "top": 0.1510719696969697, "width": 0.39537418300653593, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.1649078282828283, "width": 0.29515522875816996, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Crowd-Powered UI Design & Evaluation", "prob": 0.6907240748405457, "is_author_statement": false, "is_in_expected_section": true, "id": "3320"}, {"text": "P1 suggested that Spacewalker should provide suggestions for possible options to explore for a specific property, and warns designers when an option value is out of a reasonable range for a good design.", "label": "Result", "bboxes": [{"left": 0.37136274509803924, "top": 0.8290845959595959, "width": 0.10910620915032682, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.39256372549019614, "height": 0.011320707070706981, "page": 7}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 7}, {"left": 0.08753921568627451, "top": 0.8705959595959595, "width": 0.2949264705882353, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.6677184700965881, "is_author_statement": false, "is_in_expected_section": true, "id": "3321"}, {"text": "We invited five participants for this remote user study.", "label": "Result", "bboxes": [{"left": 0.7086290849673202, "top": 0.5293876262626263, "width": 0.20346732026143788, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.5432247474747475, "width": 0.10915196078431366, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": 0.6457144021987915, "is_author_statement": true, "is_in_expected_section": true, "id": "3322"}, {"text": "5.1.2 Results & Feedback.", "label": "Result", "bboxes": [{"left": 0.5195343137254902, "top": 0.8429671717171717, "width": 0.15807679738562097, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": 0.6328845620155334, "is_author_statement": false, "is_in_expected_section": true, "id": "3323"}, {"text": "The setup of the two experiments is the following.", "label": "Result", "bboxes": [{"left": 0.8884575163398692, "top": 0.26176767676767676, "width": 0.02364052287581697, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.275604797979798, "width": 0.2745441176470589, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.6318126320838928, "is_author_statement": false, "is_in_expected_section": true, "id": "3324"}, {"text": "As shown in the above example, Spacewalker supports a rich set of methods for exploring a design space through simple HTML extensions, which are intuitive to designers as shown in our experiments.", "label": "Result", "bboxes": [{"left": 0.5190212418300654, "top": 0.469530303030303, "width": 0.39307843137254905, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.48336742424242424, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.49720454545454545, "width": 0.3948153594771242, "height": 0.011320707070707037, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "prob": 0.6254032850265503, "is_author_statement": true, "is_in_expected_section": false, "id": "3325"}, {"text": "In our early exploration, we found conventional GA sensitive to the random initialization of design options.", "label": "Result", "bboxes": [{"left": 0.5358137254901961, "top": 0.75989898989899, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7737361111111111, "width": 0.27940849673202606, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.6225062012672424, "is_author_statement": true, "is_in_expected_section": false, "id": "3326"}, {"text": "Our experiments show that the concept of Spacewalker is well received by the designers and developers.", "label": "Result", "bboxes": [{"left": 0.26665196078431375, "top": 0.6630404040404041, "width": 0.21628594771241827, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3948153594771242, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.6214061975479126, "is_author_statement": true, "is_in_expected_section": true, "id": "3327"}, {"text": "250 comparisons were made for each design.", "label": "Result", "bboxes": [{"left": 0.291468954248366, "top": 0.1925820707070707, "width": 0.18927614379084973, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.20641919191919192, "width": 0.07451307189542485, "height": 0.011320707070707037, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.6174710988998413, "is_author_statement": false, "is_in_expected_section": true, "id": "3328"}, {"text": "The pairwise comparison of designs eases the rater task and yields more reliable feedback than rating a design individually on an absolute scale.", "label": "Result", "bboxes": [{"left": 0.7451078431372549, "top": 0.6492032828282828, "width": 0.16697875816993468, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6768775252525252, "width": 0.28088398692810457, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.6154784560203552, "is_author_statement": false, "is_in_expected_section": false, "id": "3329"}, {"text": "Based on the result of each pairwise comparison, Spacewalker learns which design was preferred by a human rater to which the option contributes, and sets the corresponding mask value as 1 , and the rest as 0 .", "label": "Result", "bboxes": [{"left": 0.35172712418300656, "top": 0.45548484848484855, "width": 0.1287418300653595, "height": 0.011320707070707037, "page": 5}, {"left": 0.08790522875816993, "top": 0.46932196969696965, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.48315909090909087, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.4963282828282828, "width": 0.34125, "height": 0.01198863636363634, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.610658586025238, "is_author_statement": false, "is_in_expected_section": false, "id": "3330"}, {"text": "It allows designers to easily instrument a design exploration using our simple markup extension, seamlessly distribute design critique tasks to crowd workers, and quickly receive exploration results with genetic algorithms.", "label": "Result", "bboxes": [{"left": 0.29425816993464055, "top": 0.6630404040404041, "width": 0.18867810457516343, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3941732026143791, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6907146464646465, "width": 0.392562091503268, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.3647826797385621, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5756027102470398, "is_author_statement": true, "is_in_expected_section": true, "id": "3331"}, {"text": "However, we observed", "label": "Result", "bboxes": [{"left": 0.778423202614379, "top": 0.8770858585858586, "width": 0.13367810457516338, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5721525549888611, "is_author_statement": true, "is_in_expected_section": true, "id": "3332"}, {"text": "\"This tool provides (a) useful way to compare my designs. I used to use the Inspect tool in Chrome to try out different values of the styles of my attributes, but the limitation is that I can only modify one item at a time. With this tool I could manage my HTML/CSS code and potential designs of the whole page efficiently. It improves my productivity and experience significantly.\"", "label": "Result", "bboxes": [{"left": 0.10418464052287582, "top": 0.7184002525252525, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7322373737373737, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7460732323232323, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7599103535353536, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7737474747474747, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.7875845959595961, "width": 0.2415947712418301, "height": 0.011320707070706981, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5684016942977905, "is_author_statement": false, "is_in_expected_section": true, "id": "3333"}, {"text": "that the quality of these designs could further improved with more iterations and workers.", "label": "Result", "bboxes": [{"left": 0.08790522875816993, "top": 0.10956060606060607, "width": 0.39255882352941174, "height": 0.011320707070707065, "page": 9}, {"left": 0.08790522875816993, "top": 0.12339772727272727, "width": 0.13865522875816993, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "prob": 0.5514519214630127, "is_author_statement": false, "is_in_expected_section": true, "id": "3334"}, {"text": "human raters 5 and 10 iterations (see Figure 2).", "label": "Result", "bboxes": [{"left": 0.08790522875816993, "top": 0.1070909090909091, "width": 0.273812091503268, "height": 0.013790404040404033, "page": 3}], "section": "3 USING SPACEWALKER", "prob": 0.5473469495773315, "is_author_statement": false, "is_in_expected_section": false, "id": "3335"}, {"text": "In the second experiment, we test these search methods on different types of web pages.", "label": "Result", "bboxes": [{"left": 0.23743464052287583, "top": 0.5709154040404041, "width": 0.24303431372549028, "height": 0.011320707070706981, "page": 6}, {"left": 0.08790522875816993, "top": 0.5847525252525253, "width": 0.2873496732026144, "height": 0.011320707070706981, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.5375047922134399, "is_author_statement": true, "is_in_expected_section": true, "id": "3336"}, {"text": "The specifications for these designs were created so that their search spaces are similar in size (between 972 and 1215, mean=1050) for all the tasks.", "label": "Result", "bboxes": [{"left": 0.7870392156862746, "top": 0.42781186868686866, "width": 0.1253300653594771, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.4416489898989899, "width": 0.39283986928104575, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.45548484848484855, "width": 0.3685343137254903, "height": 0.011320707070707037, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.49764779210090637, "is_author_statement": false, "is_in_expected_section": true, "id": "3337"}, {"text": "Our front end includes a task authoring interface for a designer to create and launch a task (see Figure 2), a monitor interface for the designer to monitor the task progress and export results (see Figure 4), and a worker interface for the worker to compare a pair of designs (see Figure 3).", "label": "Result", "bboxes": [{"left": 0.15269117647058825, "top": 0.8048611111111111, "width": 0.32805065359477126, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8186982323232324, "width": 0.39283333333333337, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8325353535353536, "width": 0.39255555555555555, "height": 0.011320707070706981, "page": 5}, {"left": 0.08790522875816993, "top": 0.8463724747474748, "width": 0.395031045751634, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8602095959595959, "width": 0.21625000000000005, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.3 Architectures", "prob": 0.48122304677963257, "is_author_statement": true, "is_in_expected_section": false, "id": "3338"}, {"text": "If one configuration selected option 3 for attribute A , option 1 for attribute B , and option 6 for attribute C , then the resulted genetic representation is [3,1,6] .", "label": "Result", "bboxes": [{"left": 0.7472287581699346, "top": 0.12339772727272727, "width": 0.1648725490196079, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.13656691919191918, "width": 0.39283823529411754, "height": 0.011988636363636396, "page": 4}, {"left": 0.5195343137254902, "top": 0.1504040404040404, "width": 0.37965032679738575, "height": 0.011988636363636396, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.47155892848968506, "is_author_statement": false, "is_in_expected_section": false, "id": "3339"}, {"text": "Thus, our fitness function outputs 1 for the preferred design and 0 for the less preferred one.", "label": "Result", "bboxes": [{"left": 0.765313725490196, "top": 0.26176767676767676, "width": 0.1467777777777779, "height": 0.011332070707070707, "page": 4}, {"left": 0.5195343137254902, "top": 0.2749368686868687, "width": 0.394812091503268, "height": 0.011988636363636396, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.4573056995868683, "is_author_statement": true, "is_in_expected_section": false, "id": "3340"}, {"text": "Experiment #1: Effects of Search Space Sizes.", "label": "Result", "bboxes": [{"left": 0.5195343137254902, "top": 0.2894191919191919, "width": 0.3021029411764705, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "prob": 0.4468315541744232, "is_author_statement": false, "is_in_expected_section": true, "id": "3341"}, {"text": "As a result, we introduce variations to asequencewhere we haveyet to acquire raterfeedback.", "label": "Result", "bboxes": [{"left": 0.32783169934640527, "top": 0.552344696969697, "width": 0.15263888888888882, "height": 0.011320707070707092, "page": 5}, {"left": 0.08753921568627451, "top": 0.5661818181818182, "width": 0.3951879084967319, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.4458581507205963, "is_author_statement": true, "is_in_expected_section": false, "id": "3342"}, {"text": "In this study, we evaluate the usability of our proposed HTML extensions by gather informal feedback from web designers.", "label": "Result", "bboxes": [{"left": 0.5195343137254902, "top": 0.45108333333333334, "width": 0.39293790849673205, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.46491919191919195, "width": 0.36536274509803923, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": 0.43933790922164917, "is_author_statement": true, "is_in_expected_section": true, "id": "3343"}, {"text": "Note that the CSS attributes of elements for each node can be further explored, which enables recursive exploration.", "label": "Result", "bboxes": [{"left": 0.8023202614379085, "top": 0.5385075757575758, "width": 0.10977777777777775, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.552344696969697, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.5661818181818182, "width": 0.17852287581699355, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "prob": 0.42253145575523376, "is_author_statement": false, "is_in_expected_section": false, "id": "3344"}, {"text": "To ease the effort for exploring a design space, previous work has extensively investigated using crowdsourcing as an essential component in UI design and evaluation [3, 911, 14, 15, 17, 25, 26], which lowers the threshold for acquiring user feedback at scale.", "label": "Result", "bboxes": [{"left": 0.5358137254901961, "top": 0.2827828282828283, "width": 0.3766633986928105, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.29661868686868686, "width": 0.39255718954248375, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.3104558080808081, "width": 0.39417156862745095, "height": 0.011320707070707092, "page": 0}, {"left": 0.5189918300653594, "top": 0.32429292929292924, "width": 0.39535784313725497, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "prob": 0.41846993565559387, "is_author_statement": false, "is_in_expected_section": true, "id": "3345"}, {"text": "See Section 3 for an example.", "label": "Result", "bboxes": [{"left": 0.8913856209150327, "top": 0.8014103535353535, "width": 0.020712418300653512, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.8152474747474748, "width": 0.1600653594771243, "height": 0.011320707070707092, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "prob": 0.40571844577789307, "is_author_statement": false, "is_in_expected_section": false, "id": "3346"}, {"text": "AB4Web addresses this problem through randomized split testing, and successfully analyzed users preferences for a task with 49 designs [23].", "label": "Result", "bboxes": [{"left": 0.5947598039215686, "top": 0.6704987373737373, "width": 0.31733823529411764, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6843358585858587, "width": 0.39293464052287586, "height": 0.011320707070707092, "page": 1}, {"left": 0.5189918300653594, "top": 0.6981729797979798, "width": 0.12854901960784315, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "prob": 0.3931276202201843, "is_author_statement": false, "is_in_expected_section": true, "id": "3347"}, {"text": "We reviewed their completed HTML specifications to check if they were correct.", "label": "Result", "bboxes": [{"left": 0.7664983660130719, "top": 0.8061275252525252, "width": 0.14806535947712418, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.8199646464646465, "width": 0.3432287581699346, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "prob": 0.37065815925598145, "is_author_statement": true, "is_in_expected_section": true, "id": "3348"}, {"text": "When an attribute is mutated, its mask is set to 0 .", "label": "Result", "bboxes": [{"left": 0.1857156862745098, "top": 0.676209595959596, "width": 0.29634477124183006, "height": 0.01198863636363634, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.3 Architectures", "prob": 0.35344037413597107, "is_author_statement": false, "is_in_expected_section": false, "id": "3349"}, {"text": "To adapt the genetic algorithm for searching an optimal UI configuration, we first encode each configuration as a genetic sequence, which is an ordered list of valued attributes whose value is denoted by the index to an", "label": "Result", "bboxes": [{"left": 0.35863071895424836, "top": 0.8119734848484849, "width": 0.12183496732026144, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8258093434343435, "width": 0.3925604575163399, "height": 0.011320707070706981, "page": 4}, {"left": 0.08790522875816993, "top": 0.8396464646464646, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8534835858585857, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "prob": 0.3401680886745453, "is_author_statement": true, "is_in_expected_section": false, "id": "3350"}], "uist-1": [{"text": "This paper is about enabling blind people to use the touchscreens they encounter in-the-wild , despite the fact that nothing about how these systems are designed is intended for their use.", "label": "Author", "bboxes": [{"left": 0.657235294117647, "top": 0.8015012626262626, "width": 0.26460130718954245, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8150492424242425, "width": 0.39717320261437916, "height": 0.012868686868686807, "page": 0}, {"left": 0.5246633986928104, "top": 0.8291742424242424, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8430113636363636, "width": 0.1428627450980393, "height": 0.012579545454545538, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3351"}, {"text": "In a formative study, we rst identied key challenges and design considerations for a system to provide access to dy namic touchscreen interfaces in the real world.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.6149621212121212, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6287992424242425, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6426363636363637, "width": 0.3039575163398692, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3352"}, {"text": "Our technical evaluation showed that StateLens can accurately reconstruct interface structures from stationary, hand-held, and web usage videos.", "label": "Author", "bboxes": [{"left": 0.8336830065359477, "top": 0.6426363636363637, "width": 0.08815032679738566, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6564734848484849, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.670310606060606, "width": 0.39717647058823546, "height": 0.012579545454545538, "page": 1}, {"left": 0.5242565359477125, "top": 0.6841477272727273, "width": 0.04652614379084963, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3353"}, {"text": "Then through a user study with 14 blind participants, we showed that the conversational agent, the iOS application, and the 3D-printed accessories collectively helped blind users access otherwise inaccessible dynamic touchscreen devices effectively.", "label": "Author", "bboxes": [{"left": 0.5820686274509804, "top": 0.711820707070707, "width": 0.3418039215686274, "height": 0.012579545454545538, "page": 1}, {"left": 0.5240784313725491, "top": 0.7256578282828283, "width": 0.39979575163398673, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7394949494949494, "width": 0.39717973856209166, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.7533320707070708, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7671691919191919, "width": 0.0730261437908497, "height": 0.012579545454545538, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3354"}, {"text": "This paper introduces StateLens , a reverse engineering solution for making existing dynamic touchscreens accessible.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.6423472222222222, "width": 0.39768300653594774, "height": 0.012868686868686918, "page": 1}, {"left": 0.08811928104575163, "top": 0.6564734848484849, "width": 0.35609477124183003, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3355"}, {"text": "Our work is related to prior work on", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.24262091503267968, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3356"}, {"text": "To do this, we introduce a set of simple 3D-printed accessories that allow users to explore without touching the screen with their finger, and perform a gesture to activate touch at a desired position.", "label": "Author", "bboxes": [{"left": 0.9050098039215686, "top": 0.5105555555555555, "width": 0.016833333333333367, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.5243926767676768, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.538229797979798, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.5520669191919192, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.5659040404040404, "width": 0.056763071895424844, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3357"}, {"text": "Our work goes beyond VizLens by enabling access to dynamic touchscreens.", "label": "Author", "bboxes": [{"left": 0.6258823529411766, "top": 0.6236161616161616, "width": 0.2959542483660129, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.6374532828282828, "width": 0.2148921568627451, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3358"}, {"text": "Without the 3D-printed ac cessories introduced in this paper, VizLens would not work for touchscreens.", "label": "Author", "bboxes": [{"left": 0.7448692810457517, "top": 0.6374532828282828, "width": 0.17967483660130723, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6512891414141414, "width": 0.3974591503267976, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6651262626262626, "width": 0.08797385620915033, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3359"}, {"text": "We conducted a formative study to identify the key challenges and design considerations for a system to provide access to dy namic touchscreen interfaces in the real world.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.13697853535353535, "width": 0.39794281045751634, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.15081565656565657, "width": 0.3998839869281046, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.16465151515151516, "width": 0.3008382352941177, "height": 0.012579545454545454, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3360"}, {"text": "We conducted semi-structured interviews with 16 blind people about their ex periences and challenges with public touchscreen appliances, and their strategies for overcoming these challenges.", "label": "Author", "bboxes": [{"left": 0.39401307189542484, "top": 0.16465151515151516, "width": 0.09128594771241827, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.17848863636363635, "width": 0.3998839869281046, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.19232575757575757, "width": 0.3992107843137255, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.20616287878787878, "width": 0.33799673202614383, "height": 0.012579545454545454, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3361"}, {"text": "Then us ing a Wizard-of-Oz approach, we asked two participants to try using a touchscreen coffee machine with verbal instructions given by the researchers.", "label": "Author", "bboxes": [{"left": 0.431171568627451, "top": 0.20616287878787878, "width": 0.05683169934640525, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.22, "width": 0.39774673202614386, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.23383712121212122, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.2476742424242424, "width": 0.1667238562091503, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3362"}, {"text": "We extracted key insights that re ected participants challenges and strategies, which we used in the design of StateLens.", "label": "Author", "bboxes": [{"left": 0.26244771241830067, "top": 0.2476742424242424, "width": 0.22555228758169937, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.2615113636363636, "width": 0.3971797385620915, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.27534848484848484, "width": 0.17317320261437913, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3363"}, {"text": "Informed by the participants feedback to our initial prototype, we designed variations of 3D-printed accessories (Figure 3DG) that focus on improving stability and comfort during use.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.3992140522875818, "height": 0.012579545454545427, "page": 3}, {"left": 0.5240735294117647, "top": 0.7813737373737373, "width": 0.39884803921568623, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.7952108585858586, "width": 0.36716176470588247, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3364"}, {"text": "We also focused on capacitive touchscreens rather than resistive touch screens, since resistive screens usually require some pressure to activate so the issue of accidental activation is not as severe compared to capacitive touchscreens.", "label": "Author", "bboxes": [{"left": 0.8682385620915032, "top": 0.8367222222222221, "width": 0.05358169934640533, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.24255392156862743, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3365"}, {"text": "We rst conducted an exploratory search on Thingiverse to understand what openly available solutions exist for people to interact with touchscreens and see if they can enable risk-free exploration for blind people.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.16255555555555556, "width": 0.39793790849673205, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.17639267676767678, "width": 0.39717810457516345, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.190229797979798, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.20406691919191922, "width": 0.19151307189542488, "height": 0.012579545454545454, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3366"}, {"text": "We created a list of 11 search terms including: touchscreen accessibility; touchscreen stylus; screen stylus; capacitive screen input; resistive screen; input assistive; assistive finger cap; finger cap; 3D printed acces sibility; conductive PLA accessibility; and prosthetic finger.", "label": "Author", "bboxes": [{"left": 0.7232173202614379, "top": 0.20406691919191922, "width": 0.19861928104575155, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.21790404040404043, "width": 0.3985277777777778, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.2317411616161616, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.2455770202020202, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.2594141414141414, "width": 0.4000130718954249, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3367"}, {"text": "We then ltered results that were not related to accessibility or assistive technology ( e.g. , raspberry pi and/or touchscreen cases), leading to a total of 39 relevant items.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.2870883838383838, "width": 0.39850163398692817, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.30063636363636365, "width": 0.39717810457516345, "height": 0.012868686868686807, "page": 3}, {"left": 0.5246633986928104, "top": 0.31476262626262624, "width": 0.2939624183006536, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3368"}, {"text": "We tested this design in a pilot study with two blind partic ipants (one female, age 48; one male, age 57).", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.5763712121212121, "width": 0.4006437908496734, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246617647058823, "top": 0.5902083333333333, "width": 0.32037908496732026, "height": 0.012579545454545538, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3369"}, {"text": "While the 3D-printed finger ring enabled our participants to explore without accidental triggers, participants also identied issues related with the design and suggested other solutions.", "label": "Author", "bboxes": [{"left": 0.855186274509804, "top": 0.5902083333333333, "width": 0.06665032679738547, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246617647058823, "top": 0.6040454545454546, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 3}, {"left": 0.5240784313725491, "top": 0.6178825757575758, "width": 0.3977565359477123, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.631719696969697, "width": 0.3650767973856208, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3370"}, {"text": "Our solution should support more uid interactions to reduce blind users cognitive effort in exploring the interface layout and accessing functions on complex and unfamiliar touchscreen devices.", "label": "Author", "bboxes": [{"left": 0.3557222222222222, "top": 0.6400113636363637, "width": 0.1295767973856209, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6538484848484848, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.6676856060606061, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6815227272727272, "width": 0.31383823529411764, "height": 0.012579545454545538, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3371"}, {"text": "Inspired by the finger cap designs from Thingiverse, we rst created a 3D-printed ring that allows users to explore without touching the screen, and tilt their finger forward to perform a touch at a desired position (Figure 3A-C).", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.5134760101010101, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5273131313131313, "width": 0.39717973856209154, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.5411502525252525, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5549873737373737, "width": 0.2726372549019608, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3372"}, {"text": "Using an approach akin to afnity diagramming [5, 10, 21], we classied these items into ve main categories of devices: styluses, prosthetic accessories, finger caps, buttons and joy sticks.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.33614646464646464, "width": 0.3991993464052288, "height": 0.012579545454545427, "page": 3}, {"left": 0.5240784313725491, "top": 0.34998358585858586, "width": 0.40002614379084966, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3638207070707071, "width": 0.3998823529411766, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3776578282828283, "width": 0.041316993464052265, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3373"}, {"text": "We show each of these categories with example items in Table 1.", "label": "Author", "bboxes": [{"left": 0.571031045751634, "top": 0.3776578282828283, "width": 0.3508071895424836, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3914949494949495, "width": 0.07058496732026143, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3374"}, {"text": "Although the Thingiverse designs are closely re lated to assistive usage for touchscreens, none of them satisfy our need to enable blind users risk-free access to an existing touchscreen device.", "label": "Author", "bboxes": [{"left": 0.6011584967320261, "top": 0.3914949494949495, "width": 0.32338562091503276, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4053320707070707, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.41916919191919194, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4330063131313131, "width": 0.12513725490196081, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3375"}, {"text": "We used these categories to inspire design ideas for prototypes that take on familiar forms used in the Thingiverse accessibility community but also support risk-free exploration (Figure 3).", "label": "Author", "bboxes": [{"left": 0.6547303921568627, "top": 0.4330063131313131, "width": 0.2671062091503269, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4468434343434343, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 3}, {"left": 0.5241601307189543, "top": 0.46068055555555554, "width": 0.3976830065359477, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.47451767676767675, "width": 0.14759967320261447, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3376"}, {"text": "Therefore, our solution should support risk-free exploration to enable blind users freely explore without accidentally trig gering functions on the screen.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.8643964646464646, "width": 0.40057679738562096, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.20027777777777778, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3377"}, {"text": "We use the coffee machine in Figure 4 as a running example.", "label": "Author", "bboxes": [{"left": 0.6811013071895425, "top": 0.45580934343434343, "width": 0.24073529411764694, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.46964646464646465, "width": 0.14261764705882352, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3378"}, {"text": "We had success with both techniques, though conductive PLA was more durable, while conductive paint can come off after repeated use.", "label": "Author", "bboxes": [{"left": 0.6704983660130719, "top": 0.329939393939394, "width": 0.25336437908496734, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.3437765151515152, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.3576136363636364, "width": 0.24413235294117652, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3379"}, {"text": "Our design variations consist of a finger cap (Figure 3D) and a conductive stylus (Figure 3G).", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.329939393939394, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.3437765151515152, "width": 0.21460947712418305, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3380"}, {"text": "We represent a state diagram as a directed graph G = ( V , E , S , T ) where S is the start state and T = { T 1 , T 2 , ..., T n } con tains the end states where tasks are accomplished.", "label": "Author", "bboxes": [{"left": 0.8299983660130719, "top": 0.8367222222222221, "width": 0.09183823529411761, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.850270202020202, "width": 0.39846405228758164, "height": 0.012868686868686918, "page": 4}, {"left": 0.5240784313725491, "top": 0.8641073232323233, "width": 0.40046241830065343, "height": 0.01406186868686865, "page": 4}, {"left": 0.5246601307189542, "top": 0.8782335858585859, "width": 0.4000130718954248, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3381"}, {"text": "In the current work, we demon strate StateLens with videos captured from stationary cameras, hand-held mobile phones and web video repositories.", "label": "Author", "bboxes": [{"left": 0.719436274509804, "top": 0.7437941919191919, "width": 0.20510784313725483, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7576300505050505, "width": 0.3992140522875818, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7714671717171716, "width": 0.34768137254901965, "height": 0.012579545454545538, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3382"}, {"text": "We do so because the rst few candidates often include transition residuals from the previous state, such as animations.", "label": "Author", "bboxes": [{"left": 0.6470702614379085, "top": 0.7675378787878788, "width": 0.274766339869281, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7813737373737373, "width": 0.39717647058823535, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.7952108585858586, "width": 0.0960882352941177, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3383"}, {"text": "We use a time window of 1 second for this process.", "label": "Author", "bboxes": [{"left": 0.6292875816993464, "top": 0.7952108585858586, "width": 0.2925490196078431, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.05394771241830065, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3384"}, {"text": "Since there is no existing models for detecting touchscreen interfaces, we re-purpose state-of-the-art object detection models output for this task.", "label": "Author", "bboxes": [{"left": 0.39716503267973857, "top": 0.5631666666666667, "width": 0.08813071895424829, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5770037878787879, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5908409090909091, "width": 0.3974509803921568, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6046780303030302, "width": 0.059609477124183005, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3385"}, {"text": "coffee_drinks gourmet_drinks hot_beverages to the coffee drink type state V 1 can be represented as: E 01 = V 0  V 1 = ( { b coffee_drinks } , V 0 , V 1 ) , stating that by interacting with button Coffee Drinks in the initial state, we could get to the desired state for coffee drinks type selection.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.41869191919191917, "width": 0.39944117647058824, "height": 0.023061868686868714, "page": 5}, {"left": 0.08811928104575163, "top": 0.4415290404040404, "width": 0.39774183006535946, "height": 0.01414520202020203, "page": 5}, {"left": 0.08811928104575163, "top": 0.4556540404040404, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4694911616161616, "width": 0.4000245098039216, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3386"}, {"text": "Following our running example, the transition from the initial state S = V 0 = ( { b , b , b } , other metadata )", "label": "Author", "bboxes": [{"left": 0.38922222222222225, "top": 0.38646969696969696, "width": 0.09635457516339868, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4000176767676768, "width": 0.397187908496732, "height": 0.01406186868686865, "page": 5}, {"left": 0.08749019607843136, "top": 0.413854797979798, "width": 0.3823349673202615, "height": 0.012868686868686807, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3387"}, {"text": "Inspired by our formative study, the goal of the conversational agent is to reduce the time and effort of the blind users to explore, understand, and activate functions on inaccessible and unfamiliar touchscreen interfaces.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.37084343434343436, "width": 0.39717647058823535, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.3846805555555555, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.39851767676767674, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.41235479797979796, "width": 0.25273529411764706, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3388"}, {"text": "These tech niques effectively reduces the search space, speeds up the state detection process, and improves the robustness of state detection, which we will validate in technical evaluation.", "label": "Author", "bboxes": [{"left": 0.8461715686274509, "top": 0.17831565656565657, "width": 0.07837254901960788, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.19215277777777778, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.205989898989899, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.21982702020202022, "width": 0.36121405228758185, "height": 0.012579545454545454, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3389"}, {"text": "To make sure the touchpoint does not change from exploration to activation ( i.e. , the problems Slide Rule [24] addressed with split tap, and VizLens [17] addressed with shifting the interaction point), we measured the ground truth touchpoint location and placed the color marker on the accessory accordingly.", "label": "Author", "bboxes": [{"left": 0.3681830065359477, "top": 0.6631313131313132, "width": 0.11710947712418307, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6766780303030303, "width": 0.3992140522875817, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.6908042929292929, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 7}, {"left": 0.08753267973856209, "top": 0.7046414141414141, "width": 0.39776307189542476, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.397171568627451, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.27905065359477127, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3390"}, {"text": "We collected a total of 28 videos from a diverse set of eight dy namic touchscreen interfaces, in different lighting conditions, and with both stationary and hand-held cameras, resulting in a total of 40,140 video frames.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.38839141414141415, "width": 0.40065032679738566, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.40222853535353537, "width": 0.399202614379085, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4160656565656566, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4299027777777778, "width": 0.1881176470588235, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3391"}, {"text": "We also manually selected web videos of four touchscreen interfaces, resulting in a total of 32,610 video frames.", "label": "Author", "bboxes": [{"left": 0.717859477124183, "top": 0.4299027777777778, "width": 0.20397385620915043, "height": 0.012579545454545482, "page": 7}, {"left": 0.5242565359477125, "top": 0.443739898989899, "width": 0.3975718954248365, "height": 0.012579545454545427, "page": 7}, {"left": 0.524671568627451, "top": 0.45757702020202023, "width": 0.14098692810457503, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3392"}, {"text": "All of these videos for our evaluation were collected by sighted people.", "label": "Author", "bboxes": [{"left": 0.6721503267973856, "top": 0.45757702020202023, "width": 0.24969444444444444, "height": 0.012579545454545427, "page": 7}, {"left": 0.5240898692810457, "top": 0.47141414141414145, "width": 0.22857516339869277, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3393"}, {"text": "We rst evaluated the effectiveness of StateLens in reconstruct ing interface structures from stationary, hand-held, and web usage videos.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.531050505050505, "width": 0.40065032679738566, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5448876262626262, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.5587247474747474, "width": 0.0854624183006536, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3394"}, {"text": "Regarding the effect of our screen detection approach, a com bination of Screen Detection+SURF+OCR features generally yielded higher performance compared to SURF+OCR fea tures.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.7952108585858586, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39774346405228767, "height": 0.012579545454545538, "page": 7}, {"left": 0.5242565359477125, "top": 0.822885101010101, "width": 0.40027941176470594, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.0361078431372549, "height": 0.012579545454545538, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3395"}, {"text": "We conducted a multi-part technical evaluation in order to understand how each key component of StateLens performs across a wide range of interfaces and usage scenarios.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.3149179292929293, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246617647058823, "top": 0.32875505050505055, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.34259217171717177, "width": 0.35015686274509805, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3396"}, {"text": "For each interface and video source, we computed the preci sion, recall, and F1 scores for the extracted states using four congurations of features:", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.6631313131313132, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6769684343434343, "width": 0.39745588235294127, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6908042929292929, "width": 0.16960784313725497, "height": 0.012579545454545538, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3397"}, {"text": "We then evaluated the robustness of our techniques in identify ing states compared to the baseline approach.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.5751654040404041, "width": 0.40065032679738566, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.5890025252525253, "width": 0.30075326797385626, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3398"}, {"text": "We varied the total number of states involved from one to all 14, and plotted the percentage of errors in identifying the current state.", "label": "Author", "bboxes": [{"left": 0.8305310457516341, "top": 0.5890025252525253, "width": 0.09130555555555542, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6028396464646465, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6166767676767677, "width": 0.36595751633986917, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3399"}, {"text": "We evaluated the efciency of our techniques in identifying states compared to the naive approach in VizLens::State De tection [17] which compares against every possible reference image.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.822885101010101, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811764705882352, "top": 0.8367222222222221, "width": 0.3998807189542483, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.04297875816993464, "height": 0.012579545454545538, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3400"}, {"text": "We varied the total number of states involved from one to all 14, and plotted the amount of processing time required for identifying the current state.", "label": "Author", "bboxes": [{"left": 0.13607516339869283, "top": 0.8643964646464646, "width": 0.3492173202614379, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.20961764705882358, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3401"}, {"text": "We next evaluated the effectiveness of using state diagrams to reduce latency and prevent errors in the state detection process.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.7727929292929293, "width": 0.3979362745098039, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7866300505050504, "width": 0.40002777777777776, "height": 0.012579545454545538, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3402"}, {"text": "In order to enable repeated testing without wasting coffee, we built a simulated interactive prototype of the coffee machine in Figure 4 with InVision [23], which we displayed on an iPad tablet of similar size as the coffee machines interface (iPad Pro 3rd generation, 11-inch, running iOS 12.2 without VoiceOver enabled).", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.49769191919191924, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5115277777777778, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.525364898989899, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.5392020202020202, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 9}, {"left": 0.08758169934640524, "top": 0.5530391414141415, "width": 0.3977140522875816, "height": 0.012579545454545538, "page": 9}, {"left": 0.08753267973856209, "top": 0.5668762626262627, "width": 0.13685294117647062, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3403"}, {"text": "We recruited 14 visually impaired users (9 female, 5 male, age 34-85).", "label": "Author", "bboxes": [{"left": 0.3804035947712418, "top": 0.6083876262626263, "width": 0.10488398692810458, "height": 0.012579545454545427, "page": 9}, {"left": 0.0877124183006536, "top": 0.6222247474747475, "width": 0.36434640522875816, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3404"}, {"text": "The demographics of our participants are shown in Table 3.", "label": "Author", "bboxes": [{"left": 0.4594656862745098, "top": 0.6222247474747475, "width": 0.025818627450980458, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6360618686868688, "width": 0.3577091503267974, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3405"}, {"text": "The goal of our user study was to evaluate how the components of StateLens (the 3D-printed accessories, the conversational agent, and the iOS application) perform in enabling blind people to accomplish realistic tasks that involve otherwise inaccessible dynamic touchscreen interfaces.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.4014368686868687, "width": 0.39768300653594774, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4152739898989899, "width": 0.39716830065359476, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4291111111111111, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4429482323232323, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.45678535353535354, "width": 0.2918611111111111, "height": 0.012579545454545482, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3406"}, {"text": "After each step of the study, we collected Likert scale ratings and subjective feedback from the participants.", "label": "Author", "bboxes": [{"left": 0.5240784313725491, "top": 0.3041060606060606, "width": 0.397766339869281, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.3179431818181818, "width": 0.3123758169934642, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3407"}, {"text": "Finally, we ended the study with a semi-structured interview asking for the participants comments and suggestions on the StateLens system.", "label": "Author", "bboxes": [{"left": 0.846437908496732, "top": 0.3179431818181818, "width": 0.07539869281045763, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.331780303030303, "width": 0.39745588235294127, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.3456174242424242, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.3594545454545454, "width": 0.04835620915032679, "height": 0.012579545454545482, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3408"}, {"text": "Next in user evaluation, we further demonstrate how the generated state diagrams power interactive applications to assist blind users access existing dynamic touchscreen devices.", "label": "Author", "bboxes": [{"left": 0.43666339869281046, "top": 0.3190189393939394, "width": 0.048632352941176404, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.3328560606060606, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.3466931818181818, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.36053030303030303, "width": 0.33298202614379085, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3409"}, {"text": "We now detail our user study results and summarize user feedback and preferences.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.425885101010101, "width": 0.3982189542483662, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.43972222222222224, "width": 0.17071895424836614, "height": 0.012579545454545482, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3410"}, {"text": "Because our smoothing approach requires a new state to be seen continuously across multiple frames in order to determine a state transition, there may be a delay in determining if a button press was successful.", "label": "Author", "bboxes": [{"left": 0.6467140522875817, "top": 0.515804292929293, "width": 0.27512908496732025, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.5296414141414142, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 10}, {"left": 0.5246633986928104, "top": 0.5434785353535354, "width": 0.3977385620915034, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.5573156565656566, "width": 0.3146339869281046, "height": 0.012579545454545538, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3411"}, {"text": "Our 3D-printed accessories elegantly add risk-free explo ration to existing capacitive touchscreen devices without modifying the underlying hardware or software, which has been a major hurdle for past efforts.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.4037411616161616, "width": 0.3998807189542485, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.4175782828282828, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.431415404040404, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.44525252525252523, "width": 0.2448627450980393, "height": 0.012579545454545482, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3412"}, {"text": "In our user study, we discovered issues around holding the accessories in certain angles, and the last meter problem to accurately activate the exact button once.", "label": "Author", "bboxes": [{"left": 0.7782598039215687, "top": 0.44525252525252523, "width": 0.1435767973856209, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.45908964646464645, "width": 0.39717320261437905, "height": 0.012578282828282827, "page": 11}, {"left": 0.5246633986928104, "top": 0.47292676767676767, "width": 0.39716830065359476, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.4867638888888889, "width": 0.12132679738562091, "height": 0.012579545454545482, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3413"}, {"text": "As future work, we have started to design hardware proxies that can locate and actuate external touchscreens automatically.", "label": "Author", "bboxes": [{"left": 0.6935457516339869, "top": 0.5006010101010101, "width": 0.22829084967320257, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.5144381313131313, "width": 0.39716830065359465, "height": 0.012579545454545538, "page": 11}, {"left": 0.5246633986928104, "top": 0.5282739898989899, "width": 0.17782026143790852, "height": 0.012579545454545427, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3414"}, {"text": "People who are blind were involved throughout the research, including several people with visual impairments on our ex tended research team, and multiple sessions of design and study with a total of 30 outside participants.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.64325, "width": 0.3992009803921569, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.6570871212121212, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.6709242424242424, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.6847613636363636, "width": 0.28585457516339874, "height": 0.012579545454545427, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3415"}, {"text": "While we strove to make this paper self-contained, it builds on our long his tory of work involving thousands of blind people as students, researchers, participants, and users.", "label": "Author", "bboxes": [{"left": 0.3790702614379085, "top": 0.6847613636363636, "width": 0.10621895424836603, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.6985984848484849, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.7124356060606061, "width": 0.39921732026143797, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.7262727272727272, "width": 0.2305392156862745, "height": 0.012579545454545538, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3416"}, {"text": "As with most systems, StateLens currently has some limita tions, which we believe could be explored in future work.", "label": "Author", "bboxes": [{"left": 0.5240784313725491, "top": 0.6216199494949495, "width": 0.40046078431372534, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.6354570707070707, "width": 0.37029901960784317, "height": 0.012579545454545538, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3417"}, {"text": "In this paper, we developed a hybrid crowd-computer vision system to enable access to dynamic touchscreens in-the-wild.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.7675366161616162, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.7813737373737373, "width": 0.40002777777777776, "height": 0.012579545454545538, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3418"}, {"text": "One unique contribution of this work is that we demonstrated the possibility of extracting state diagrams from existing point of-view videos instead of screenshots or screencast videos [2, 26, 37].", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.39717156862745095, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.3998839869281046, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3992107843137255, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.049869281045751626, "height": 0.012579545454545538, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3419"}, {"text": "For existing physical devices whose underlying hard ware or software cannot be modied, point-of-view videos are more prevalent and easier to acquire compared to screencast videos, which makes our approach generalizable to a large variety of devices and scenarios.", "label": "Author", "bboxes": [{"left": 0.14306372549019608, "top": 0.8367222222222221, "width": 0.3449264705882353, "height": 0.012579545454545538, "page": 11}, {"left": 0.08753267973856209, "top": 0.8505593434343435, "width": 0.39776307189542476, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 11}, {"left": 0.0877124183006536, "top": 0.8782335858585859, "width": 0.3975718954248366, "height": 0.012579545454545427, "page": 11}, {"left": 0.0877124183006536, "top": 0.892070707070707, "width": 0.21120098039215687, "height": 0.012579545454545427, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3420"}, {"text": "We motivated our approach as a benet to improve accessibil ity for blind users.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.0814570707070707, "width": 0.40064215686274507, "height": 0.012579545454545468, "page": 11}, {"left": 0.5246633986928104, "top": 0.09529419191919192, "width": 0.11714542483660129, "height": 0.012579545454545468, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3421"}, {"text": "Through understanding of the state diagrams of devices with readily available or user-taken point-of-view videos, our approach can provide additional information to the user as they interact with the devices ( e.g. , augmented reality applications for translation services, interactive tutorials).", "label": "Author", "bboxes": [{"left": 0.6310996732026144, "top": 0.15064141414141416, "width": 0.2907418300653595, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.16447853535353535, "width": 0.39776143790849683, "height": 0.012579545454545454, "page": 11}, {"left": 0.5242565359477125, "top": 0.17831565656565657, "width": 0.3975866013071895, "height": 0.012579545454545454, "page": 11}, {"left": 0.5246633986928104, "top": 0.19186363636363635, "width": 0.39774019607843136, "height": 0.01286868686868689, "page": 11}, {"left": 0.5246633986928104, "top": 0.205989898989899, "width": 0.3741503267973856, "height": 0.012579545454545454, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3422"}, {"text": "Using StateLens, we envision building a queryable map of state diagrams for many of the devices in the world using existing point-of-view videos that have been shared online.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.2273737373737374, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 11}, {"left": 0.5246633986928104, "top": 0.24121085858585856, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.2550479797979798, "width": 0.3745196078431372, "height": 0.012579545454545482, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3423"}, {"text": "In this section, we discuss how the approaches used in StateLens might generalize to extract information from existing online videos to, for instance, assist sighted users and con struct a queryable map of devices.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.3868825757575758, "width": 0.399874183006536, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.40071969696969695, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.41455681818181817, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.4283939393939394, "width": 0.21855555555555561, "height": 0.012579545454545427, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3424"}, {"text": "We also discuss limitations of our work, which represent opportunities for future research.", "label": "Author", "bboxes": [{"left": 0.3117140522875817, "top": 0.4283939393939394, "width": 0.17357352941176468, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.4422310606060606, "width": 0.3999477124183007, "height": 0.012579545454545427, "page": 11}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3425"}, {"text": "We evaluated StateLens across a number of touchscreen inter faces and with blind users in the lab, but we did not deeply study how StateLens works in the real world, which is often much more complicated and messier than in-lab studies.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.40064542483660137, "height": 0.012579545454545468, "page": 12}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3977385620915033, "height": 0.012579545454545468, "page": 12}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 12}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.36700326797385624, "height": 0.01257954545454544, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3426"}, {"text": "Our next step is to harden our implementation to scale to many users, and deploy it to understand how it performs in the ev eryday lives of blind people.", "label": "Author", "bboxes": [{"left": 0.46021241830065357, "top": 0.12296717171717171, "width": 0.02536437908496736, "height": 0.01257954545454544, "page": 12}, {"left": 0.08811928104575163, "top": 0.13680429292929294, "width": 0.3977385620915033, "height": 0.012579545454545454, "page": 12}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.185171568627451, "height": 0.012579545454545454, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3427"}, {"text": "We have presented StateLens , a reverse engineering solution that makes existing dynamic touchscreens accessible.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.2045378787878788, "width": 0.3979362745098039, "height": 0.01286868686868689, "page": 12}, {"left": 0.08811928104575163, "top": 0.2186641414141414, "width": 0.3426601307189543, "height": 0.012579545454545482, "page": 12}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3428"}, {"text": "Our formative study identied challenges and re quirements, which informed the design and architecture of StateLens.", "label": "Author", "bboxes": [{"left": 0.15770751633986926, "top": 0.3293598484848485, "width": 0.33029248366013075, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.3431969696969697, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.35703409090909094, "width": 0.0696388888888889, "height": 0.012579545454545427, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3429"}, {"text": "Our evaluations demonstrated the feasibility of StateLens in accurately reconstructing the state diagram, iden tifying interface states, and giving effective feedback and guid ance.", "label": "Author", "bboxes": [{"left": 0.16712254901960782, "top": 0.35703409090909094, "width": 0.3181699346405229, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.37087121212121216, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.3847083333333334, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.3985441919191919, "width": 0.034468954248366004, "height": 0.012579545454545482, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3430"}, {"text": "This work has been supported by the National Science Foun dation (#IIS-1816012), Google, and the National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR).", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.4665669191919192, "width": 0.4003839869281045, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.48040404040404044, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.49424116161616166, "width": 0.39717483660130726, "height": 0.012579545454545371, "page": 12}, {"left": 0.08758169934640524, "top": 0.5080770202020202, "width": 0.07978921568627452, "height": 0.012579545454545427, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3431"}, {"text": "We thank the participants who contributed to our studies for their time, and the reviewers for their valuable feed back and suggestions.", "label": "Author", "bboxes": [{"left": 0.17146405228758171, "top": 0.5080770202020202, "width": 0.31409803921568624, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.5219141414141414, "width": 0.3998839869281046, "height": 0.012579545454545538, "page": 12}, {"left": 0.08811928104575163, "top": 0.5357512626262626, "width": 0.1461503267973856, "height": 0.012579545454545427, "page": 12}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3432"}, {"text": "These videos can be collected in many ways, including through existing IoT and surveillance cameras, through motivating sighted volunteers to contribute videos using mobile and wearable cameras, by encouraging manufacturers to share videos as a low-cost way to make their systems accessible to more people, and by mining existing demo and tutorial videos in online repositories.", "label": "Contribution", "bboxes": [{"left": 0.7041388888888889, "top": 0.6330972222222222, "width": 0.2176977124183006, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6469343434343434, "width": 0.39717483660130715, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.6607714646464646, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 4}, {"left": 0.5242565359477125, "top": 0.6746085858585859, "width": 0.3975718954248365, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.6884457070707071, "width": 0.39745424836601306, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7022828282828283, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.7161199494949495, "width": 0.3083562091503268, "height": 0.012579545454545427, "page": 4}], "section": "Capturing Point-of-View Usage Video", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3433"}, {"text": "Finally, StateLens captures the interaction component that triggered a state transition, e.g. , a button b n that contributes to the transition E i j = V i  V j = ( { b n } , V i , V j ) .", "label": "Contribution", "bboxes": [{"left": 0.08811928104575163, "top": 0.3897929292929293, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.40334090909090914, "width": 0.3971748366013072, "height": 0.013834595959595908, "page": 6}, {"left": 0.08812091503267974, "top": 0.41717803030303025, "width": 0.3142107843137255, "height": 0.014047979797979837, "page": 6}], "section": "Recognizing User Interaction", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3434"}, {"text": "One unique contribution of this work is that we demonstrated the possibility of extracting state diagrams from existing point of-view videos instead of screenshots or screencast videos [2, 26, 37].", "label": "Contribution", "bboxes": [{"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.39717156862745095, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.3998839869281046, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3992107843137255, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.049869281045751626, "height": 0.012579545454545538, "page": 11}], "section": "Technical Approach to Accessibility", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3435"}, {"text": "(iii) a blind person does not have the option to choose a different touchscreen platform that would be more accessible and cannot get access to the software or hardware to make it work better.", "label": "Novelty", "bboxes": [{"left": 0.6087303921568628, "top": 0.7597007575757576, "width": 0.3131062091503267, "height": 0.012868686868686807, "page": 0}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.7876641414141414, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8015012626262626, "width": 0.12751470588235292, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3436"}, {"text": "This paper is about enabling blind people to use the touchscreens they encounter in-the-wild , despite the fact that nothing about how these systems are designed is intended for their use.", "label": "Objective", "bboxes": [{"left": 0.657235294117647, "top": 0.8015012626262626, "width": 0.26460130718954245, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8150492424242425, "width": 0.39717320261437916, "height": 0.012868686868686807, "page": 0}, {"left": 0.5246633986928104, "top": 0.8291742424242424, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8430113636363636, "width": 0.1428627450980393, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3437"}, {"text": "This approach can work for many static interfaces, but struggles when the interface changes dynamically (as most touchscreens do), and cannot solve the problem of how a blind user could interact with a touchscreen without accidentally triggering touches.", "label": "Novelty", "bboxes": [{"left": 0.3932630718954248, "top": 0.5659040404040404, "width": 0.09202777777777776, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5797411616161616, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5935782828282828, "width": 0.3971797385620915, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.6074141414141414, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.08753267973856209, "top": 0.6212512626262626, "width": 0.3847483660130719, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3438"}, {"text": "the notion of risk-free exploration to counter this problem [24], but their solution (requiring multiple taps instead of just one) requires being able to modify how the touchscreen oper ates.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.4552083333333333, "width": 0.39716830065359465, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.46904419191919194, "width": 0.39717973856209154, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.48288131313131316, "width": 0.399875816993464, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.4967184343434344, "width": 0.029970588235294082, "height": 0.012579545454545371, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3439"}, {"text": "In contrast to prior work, StateLens is a solution for reverse engineering existing physical inter faces through much noisier point-of-view videos rather than screenshots or prototyped GUIs.", "label": "Novelty", "bboxes": [{"left": 0.24365196078431375, "top": 0.7689040404040404, "width": 0.24164052287581703, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7827411616161617, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7965782828282828, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8104154040404041, "width": 0.2106797385620915, "height": 0.012579545454545427, "page": 2}], "section": "Reverse Engineering User Interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3440"}, {"text": "However, because of changing display states and screen layouts, explor ing and activating UI components across multiple screens is difcult (analogous to nding ones way in a new city).", "label": "Novelty", "bboxes": [{"left": 0.860563725490196, "top": 0.7833699494949494, "width": 0.06329575163398704, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.7972070707070708, "width": 0.3998692810457516, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.8110441919191919, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.8248813131313132, "width": 0.3715702614379085, "height": 0.012579545454545427, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3441"}, {"text": "Users often ask questions about interfaces [8], but it can be difcult to map the answers received, e.g. , the stop button is in the middle of the bottom row of buttons, to actually using the interface because doing so requires locating the referenced object in space ( e.g. , place a finger on the button).", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.1997007575757576, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.21324747474747474, "width": 0.39717647058823535, "height": 0.01286868686868689, "page": 2}, {"left": 0.5246633986928104, "top": 0.2273737373737374, "width": 0.39717647058823535, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.24121085858585856, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.2547588383838384, "width": 0.32424019607843135, "height": 0.012868686868686863, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3442"}, {"text": "These systems could more easily assist blind users with using an interface, but assisting in this way is likely to be cumbersome and slow.", "label": "Novelty", "bboxes": [{"left": 0.598937908496732, "top": 0.331780303030303, "width": 0.3229019607843139, "height": 0.012579545454545482, "page": 2}, {"left": 0.5240784313725491, "top": 0.3456174242424242, "width": 0.39831862745098034, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246601307189542, "top": 0.3594545454545454, "width": 0.19219281045751635, "height": 0.012579545454545482, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3443"}, {"text": "This can help users understand the relative positions of elements, but they still have the challenge of physically locating the elements in space on the real interface in order to use it.", "label": "Novelty", "bboxes": [{"left": 0.7884722222222222, "top": 0.38712878787878785, "width": 0.13336437908496734, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.40096590909090907, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4148030303030303, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4286401515151515, "width": 0.24440849673202625, "height": 0.012579545454545482, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3444"}, {"text": "While the 3D-printed finger ring enabled our participants to explore without accidental triggers, participants also identied issues related with the design and suggested other solutions.", "label": "Novelty", "bboxes": [{"left": 0.855186274509804, "top": 0.5902083333333333, "width": 0.06665032679738547, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246617647058823, "top": 0.6040454545454546, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 3}, {"left": 0.5240784313725491, "top": 0.6178825757575758, "width": 0.3977565359477123, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.631719696969697, "width": 0.3650767973856208, "height": 0.012579545454545427, "page": 3}], "section": "Finger Ring Prototype", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3445"}, {"text": "For example, the location of the ring on the finger may vary for different users and different sessions during use, thus changing the actual position of touch.", "label": "Novelty", "bboxes": [{"left": 0.8992892156862745, "top": 0.631719696969697, "width": 0.02283006535947707, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6455568181818182, "width": 0.39745588235294127, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.6593939393939394, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.673229797979798, "width": 0.1830261437908497, "height": 0.012579545454545427, "page": 3}], "section": "Finger Ring Prototype", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3446"}, {"text": "Although the Thingiverse designs are closely re lated to assistive usage for touchscreens, none of them satisfy our need to enable blind users risk-free access to an existing touchscreen device.", "label": "Novelty", "bboxes": [{"left": 0.6011584967320261, "top": 0.3914949494949495, "width": 0.32338562091503276, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4053320707070707, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.41916919191919194, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4330063131313131, "width": 0.12513725490196081, "height": 0.012579545454545482, "page": 3}], "section": "Thingiverse Survey", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3447"}, {"text": "We used these categories to inspire design ideas for prototypes that take on familiar forms used in the Thingiverse accessibility community but also support risk-free exploration (Figure 3).", "label": "Novelty", "bboxes": [{"left": 0.6547303921568627, "top": 0.4330063131313131, "width": 0.2671062091503269, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4468434343434343, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 3}, {"left": 0.5241601307189543, "top": 0.46068055555555554, "width": 0.3976830065359477, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.47451767676767675, "width": 0.14759967320261447, "height": 0.012579545454545482, "page": 3}], "section": "Thingiverse Survey", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3448"}, {"text": "When attempting to use existing inaccessible touchscreen de vices, participants found holding their fingers in mid-air while trying to explore and locate the buttons to be very awkward and unusable, which also often resulted in accidental touches.", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.8090479797979797, "width": 0.4006372549019608, "height": 0.012579545454545538, "page": 3}, {"left": 0.0877124183006536, "top": 0.822885101010101, "width": 0.3975751633986928, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.4000212418300654, "height": 0.012579545454545427, "page": 3}], "section": "Supporting Independence", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3449"}, {"text": "We had success with both techniques, though conductive PLA was more durable, while conductive paint can come off after repeated use.", "label": "Novelty", "bboxes": [{"left": 0.6704983660130719, "top": 0.329939393939394, "width": 0.25336437908496734, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.3437765151515152, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.3576136363636364, "width": 0.24413235294117652, "height": 0.012579545454545427, "page": 4}], "section": "Design Variations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3450"}, {"text": "This finger-worn design also incorporates a slit so that when 3D printed with a exible material ( e.g. , thermoplastic polyurethane  TPU), it can t around fingers of different sizes.", "label": "Novelty", "bboxes": [{"left": 0.2033529411764706, "top": 0.440635101010101, "width": 0.28193954248366015, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.45418308080808084, "width": 0.3992156862745098, "height": 0.012868686868686807, "page": 4}, {"left": 0.08811928104575163, "top": 0.46830934343434344, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.48214646464646466, "width": 0.11593137254901963, "height": 0.012578282828282827, "page": 4}], "section": "Design Variations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3451"}, {"text": "On the other hand, if both the matched inlier ratio and the OCR similarity score are below a certain threshold, StateLens determines it as not a match.", "label": "Novelty", "bboxes": [{"left": 0.7213235294117647, "top": 0.560060606060606, "width": 0.20051307189542478, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.5738977272727273, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.5877348484848485, "width": 0.37164379084967325, "height": 0.012579545454545538, "page": 5}], "section": "Representing State Diagram", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3452"}, {"text": "On the other hand, if continuous unmatched states in the pool do not reach the window size to qualify as a new state, they are considered noise and the candidate pool will be cleared.", "label": "Novelty", "bboxes": [{"left": 0.5853513071895425, "top": 0.8090479797979797, "width": 0.33648529411764705, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.39775490196078434, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.39717810457516356, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.05152614379084963, "height": 0.012579545454545427, "page": 5}], "section": "Adding New States", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3453"}, {"text": "( { b 1 , b 2 , ..., b n } , descriptions, coordinates, other metadata ) , where b n is one of the interactive elements ( e.g. , but tons) in state V i .", "label": "Novelty", "bboxes": [{"left": 0.08748366013071895, "top": 0.30315909090909093, "width": 0.38307516339869285, "height": 0.014060606060605996, "page": 5}, {"left": 0.08753267973856209, "top": 0.3169949494949495, "width": 0.4004640522875817, "height": 0.013835858585858507, "page": 5}, {"left": 0.08811764705882352, "top": 0.3308320707070707, "width": 0.12622549019607843, "height": 0.013835858585858563, "page": 5}], "section": "Representing State Diagram", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3454"}, {"text": "To do this, StateLens transforms all the possible paths (interaction traces) from S to T in the generated state diagram into different intents ( e.g. , to make coffee drinks, to make gourmet drinks), and the in teractive element values in the edges E i along the path into required entities for the intent and their attributes/values ( e.g. , size: large/medium/small).", "label": "Novelty", "bboxes": [{"left": 0.7824248366013071, "top": 0.41235479797979796, "width": 0.13940686274509806, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.4259027777777778, "width": 0.39704411764705894, "height": 0.012868686868686863, "page": 6}, {"left": 0.5246633986928104, "top": 0.439739898989899, "width": 0.3992058823529413, "height": 0.012868686868686863, "page": 6}, {"left": 0.5246633986928104, "top": 0.4538661616161616, "width": 0.3998807189542485, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.46741414141414145, "width": 0.39716993464052297, "height": 0.013834595959595908, "page": 6}, {"left": 0.5246633986928104, "top": 0.48125126262626267, "width": 0.3992075163398693, "height": 0.012868686868686863, "page": 6}, {"left": 0.5246633986928104, "top": 0.49537626262626266, "width": 0.17242810457516344, "height": 0.012579545454545427, "page": 6}], "section": "Enabling Natural Language Queries", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3455"}, {"text": "StateLens heuristically generates training samples for the intents and prompts to the required entities from the descriptive texts along different paths aforementioned.", "label": "Novelty", "bboxes": [{"left": 0.65875, "top": 0.5507247474747474, "width": 0.26308660130718964, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5645618686868686, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5783989898989899, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 6}], "section": "Enabling Natural Language Queries", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3456"}, {"text": "However, in the future, these processes can be sped up and the produced bounding boxes can be tracked across frames to offer better performance.", "label": "Novelty", "bboxes": [{"left": 0.7354395424836602, "top": 0.27517424242424243, "width": 0.1864003267973856, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.28901136363636365, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.30284848484848487, "width": 0.3705212418300653, "height": 0.012579545454545427, "page": 6}], "section": "Accessing the State Diagram", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3457"}, {"text": "We collected a total of 28 videos from a diverse set of eight dy namic touchscreen interfaces, in different lighting conditions, and with both stationary and hand-held cameras, resulting in a total of 40,140 video frames.", "label": "Novelty", "bboxes": [{"left": 0.5238986928104574, "top": 0.38839141414141415, "width": 0.40065032679738566, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.40222853535353537, "width": 0.399202614379085, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4160656565656566, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4299027777777778, "width": 0.1881176470588235, "height": 0.012579545454545482, "page": 7}], "section": "Dataset", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3458"}, {"text": "Regarding the different video sources, stationary videos generally performed better com pared to hand-held ones for the same interface, because state matching is more robust with less camera blur, changing back ground noise and other uncertainty from camera motion.", "label": "Novelty", "bboxes": [{"left": 0.2857761437908497, "top": 0.6011300505050505, "width": 0.19951633986928102, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6149671717171716, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6288030303030303, "width": 0.3971781045751634, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6426401515151515, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6564772727272726, "width": 0.3680130718954249, "height": 0.012579545454545538, "page": 8}], "section": "Generating the State Diagram", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3459"}, {"text": "Furthermore, using the coffee machine with all 14 states, StateLens can still maintain suf cient speed for audio-guided interaction (~5fps), while the baseline approach dropped to ~2fps and became unusable.", "label": "Novelty", "bboxes": [{"left": 0.7247614379084967, "top": 0.49672348484848483, "width": 0.1970751633986929, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.510560606060606, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.5243977272727273, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.5382335858585858, "width": 0.37950653594771244, "height": 0.012579545454545427, "page": 8}], "section": "Using State Diagram to Reduce Search Time", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3460"}, {"text": "Duplicate states require more manual effort to clean up, but have less impact on user experience compared to missing states.", "label": "Novelty", "bboxes": [{"left": 0.42232026143790846, "top": 0.7055366161616161, "width": 0.0629754901960784, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.7193724747474748, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.733209595959596, "width": 0.35420915032679734, "height": 0.012579545454545427, "page": 8}], "section": "Generating the State Diagram", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3461"}, {"text": "On the other hand, the average attempts of using the stylus ( M = 2 . 48 , SD = 1 . 07 ) was more than that from using the finger cap ( M = 1 . 90 , SD = 1 . 01 ) .", "label": "Novelty", "bboxes": [{"left": 0.524656862745098, "top": 0.6002348484848485, "width": 0.39717483660130704, "height": 0.012579545454545427, "page": 9}, {"left": 0.524029411764706, "top": 0.6137828282828283, "width": 0.3978039215686274, "height": 0.012868686868686918, "page": 9}, {"left": 0.5246601307189542, "top": 0.6276199494949495, "width": 0.22434640522875826, "height": 0.012868686868686918, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3462"}, {"text": "However, there were differences across the various screen placements.", "label": "Novelty", "bboxes": [{"left": 0.6321290849673202, "top": 0.6908042929292929, "width": 0.28971078431372554, "height": 0.012579545454545538, "page": 9}, {"left": 0.5242565359477125, "top": 0.7046414141414141, "width": 0.17608006535947707, "height": 0.012579545454545427, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3463"}, {"text": "On the other hand, participants preferred the n ger cap much more than the stylus (65% vs. 35%) in the 45 and 0 screen placements, since the finger cap became more comfortable to use in these positions ( M = 6 . 3 , SD = 0 . 8 ) .", "label": "Novelty", "bboxes": [{"left": 0.5991993464052288, "top": 0.7876641414141414, "width": 0.32534477124183014, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8015012626262626, "width": 0.39912908496732025, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8153383838383839, "width": 0.39717647058823535, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8288851010101009, "width": 0.38015849673202606, "height": 0.012868686868686918, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3464"}, {"text": "from a general category ( e.g. , coffee drinks), but can freely choose the other properties ( e.g. , coffee type, strength, size).", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.08116792929292929, "width": 0.39773856209150327, "height": 0.012868686868686877, "page": 9}, {"left": 0.5246633986928104, "top": 0.09500505050505051, "width": 0.40002941176470597, "height": 0.012868686868686877, "page": 9}], "section": "Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3465"}, {"text": "While we strove to make this paper self-contained, it builds on our long his tory of work involving thousands of blind people as students, researchers, participants, and users.", "label": "Novelty", "bboxes": [{"left": 0.3790702614379085, "top": 0.6847613636363636, "width": 0.10621895424836603, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.6985984848484849, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.7124356060606061, "width": 0.39921732026143797, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.7262727272727272, "width": 0.2305392156862745, "height": 0.012579545454545538, "page": 11}], "section": "Technical Approach to Accessibility", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3466"}, {"text": "In a perfect world, posthoc xes like StateLens would not be needed (because all technologies would be inherently accessible), but in practice access technology like StateLens plays a vital role.", "label": "Novelty", "bboxes": [{"left": 0.3260751633986928, "top": 0.4834962121212121, "width": 0.1619248366013072, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.4973333333333333, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.511169191919192, "width": 0.3971764705882353, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.5250063131313132, "width": 0.3267418300653595, "height": 0.012579545454545427, "page": 11}], "section": "Technical Approach to Accessibility", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3467"}, {"text": "For example, a vending ma chine could be labeled with Braille, but the checkout credit card machine is not accessible.", "label": "Novelty", "bboxes": [{"left": 0.3031486928104575, "top": 0.5526805555555556, "width": 0.18484150326797388, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.5665176767676768, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.580354797979798, "width": 0.19509313725490196, "height": 0.012579545454545427, "page": 11}], "section": "Technical Approach to Accessibility", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3468"}, {"text": "A low vision user (P12) mentioned that even though he might not always need assistance, if the interfaces contrast or bright ness is poor, a system like StateLens would be greatly helpful as a conrmation.", "label": "Novelty", "bboxes": [{"left": 0.08753267973856209, "top": 0.22108459595959595, "width": 0.3977598039215686, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.23492171717171717, "width": 0.3998839869281046, "height": 0.012579545454545454, "page": 11}, {"left": 0.08811928104575163, "top": 0.2487588383838384, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.2625959595959596, "width": 0.12067973856209151, "height": 0.012579545454545427, "page": 11}], "section": "Completing Realistic Tasks", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3469"}, {"text": "He would nd it useful if, for example, a button for a coffee selection labeled Rainbows End could further be described as a coffee blend containing tasting notes of nuts and citrus even though the display does not provide that information.", "label": "Novelty", "bboxes": [{"left": 0.31690686274509805, "top": 0.29027020202020204, "width": 0.1704199346405229, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.3041060606060606, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.3179431818181818, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.331780303030303, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.3456174242424242, "width": 0.1878562091503268, "height": 0.012579545454545482, "page": 11}], "section": "Completing Realistic Tasks", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3470"}, {"text": "However, this approach could be benecial to sighted people and people with cognitive disabilities in many ways as well.", "label": "Novelty", "bboxes": [{"left": 0.6468333333333334, "top": 0.09529419191919192, "width": 0.2750032679738561, "height": 0.012579545454545468, "page": 11}, {"left": 0.5246633986928104, "top": 0.10913131313131313, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 11}, {"left": 0.5246633986928104, "top": 0.12296717171717171, "width": 0.12469117647058825, "height": 0.01257954545454544, "page": 11}], "section": "Technical Approach to Accessibility", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3471"}, {"text": "Furthermore, similar but slightly different models of a device may reuse another state diagram and enable transfer learning.", "label": "Novelty", "bboxes": [{"left": 0.8377467320261438, "top": 0.32423358585858586, "width": 0.0861307189542484, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.3380707070707071, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.3519078282828283, "width": 0.3295473856209151, "height": 0.012579545454545427, "page": 11}], "section": "Technical Approach to Accessibility", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3472"}, {"text": "We evaluated StateLens across a number of touchscreen inter faces and with blind users in the lab, but we did not deeply study how StateLens works in the real world, which is often much more complicated and messier than in-lab studies.", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.40064542483660137, "height": 0.012579545454545468, "page": 12}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3977385620915033, "height": 0.012579545454545468, "page": 12}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 12}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.36700326797385624, "height": 0.01257954545454544, "page": 12}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3473"}, {"text": "VizWiz lets blind people take a picture, speak a question, and get answers back from the crowd within approximately 30 seconds [6].", "label": "Objective", "bboxes": [{"left": 0.7103986928104575, "top": 0.14435227272727272, "width": 0.21143790849673194, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.15818939393939394, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 2}, {"left": 0.5240784313725491, "top": 0.17202651515151515, "width": 0.2404673202614378, "height": 0.012579545454545454, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3474"}, {"text": "More than 10,000 users have asked more than 100,000 questions using VizWiz [20].", "label": "Objective", "bboxes": [{"left": 0.7695898692810457, "top": 0.17202651515151515, "width": 0.152248366013072, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.18586363636363637, "width": 0.4000130718954249, "height": 0.012579545454545454, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3475"}, {"text": "Users often ask questions about interfaces [8], but it can be difcult to map the answers received, e.g. , the stop button is in the middle of the bottom row of buttons, to actually using the interface because doing so requires locating the referenced object in space ( e.g. , place a finger on the button).", "label": "Objective", "bboxes": [{"left": 0.5246633986928104, "top": 0.1997007575757576, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.21324747474747474, "width": 0.39717647058823535, "height": 0.01286868686868689, "page": 2}, {"left": 0.5246633986928104, "top": 0.2273737373737374, "width": 0.39717647058823535, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.24121085858585856, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.2547588383838384, "width": 0.32424019607843135, "height": 0.012868686868686863, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3476"}, {"text": "The designs aim to reduce the change of touchpoint when the user moves from exploration to interaction ( i.e. , touch acti vation), and maintain consistency across sessions.", "label": "Objective", "bboxes": [{"left": 0.8968823529411766, "top": 0.7952108585858586, "width": 0.024959150326797297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.8225959595959595, "width": 0.3998627450980392, "height": 0.012868686868686918, "page": 3}, {"left": 0.5242483660130719, "top": 0.8367222222222221, "width": 0.3358872549019608, "height": 0.012579545454545538, "page": 3}], "section": "Design Variations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3477"}, {"text": "Since there is no existing models for detecting touchscreen interfaces, we re-purpose state-of-the-art object detection models output for this task.", "label": "Objective", "bboxes": [{"left": 0.39716503267973857, "top": 0.5631666666666667, "width": 0.08813071895424829, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5770037878787879, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5908409090909091, "width": 0.3974509803921568, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6046780303030302, "width": 0.059609477124183005, "height": 0.012579545454545538, "page": 5}], "section": "Representing State Diagram", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3478"}, {"text": "Inspired by our formative study, the goal of the conversational agent is to reduce the time and effort of the blind users to explore, understand, and activate functions on inaccessible and unfamiliar touchscreen interfaces.", "label": "Objective", "bboxes": [{"left": 0.5246633986928104, "top": 0.37084343434343436, "width": 0.39717647058823535, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.3846805555555555, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.39851767676767674, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.41235479797979796, "width": 0.25273529411764706, "height": 0.012579545454545482, "page": 6}], "section": "Enabling Natural Language Queries", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3479"}, {"text": "To address this problem, special-purpose models for detecting screens could be built.", "label": "Objective", "bboxes": [{"left": 0.17465032679738562, "top": 0.46904924242424245, "width": 0.31064215686274516, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.48288636363636367, "width": 0.23205392156862747, "height": 0.012579545454545427, "page": 8}], "section": "Generating the State Diagram", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3480"}, {"text": "The goal of our user study was to evaluate how the components of StateLens (the 3D-printed accessories, the conversational agent, and the iOS application) perform in enabling blind people to accomplish realistic tasks that involve otherwise inaccessible dynamic touchscreen interfaces.", "label": "Objective", "bboxes": [{"left": 0.08761437908496732, "top": 0.4014368686868687, "width": 0.39768300653594774, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4152739898989899, "width": 0.39716830065359476, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4291111111111111, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4429482323232323, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.45678535353535354, "width": 0.2918611111111111, "height": 0.012579545454545482, "page": 9}], "section": "USER EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3481"}, {"text": "Following a brief introduction of the study and demographic questions, participants first completed tasks using the 3D printed accessories.", "label": "Objective", "bboxes": [{"left": 0.08811928104575163, "top": 0.6769671717171717, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6908042929292929, "width": 0.3998807189542484, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.12411601307189545, "height": 0.012579545454545427, "page": 9}], "section": "Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3482"}, {"text": "For all Likert scale questions, par ticipants rated along a scale of 1 to 7, where 1 was extremely negative and 7 was extremely positive.", "label": "Objective", "bboxes": [{"left": 0.7004199346405229, "top": 0.43972222222222224, "width": 0.22411437908496745, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.45355934343434345, "width": 0.39775000000000016, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.4673964646464646, "width": 0.2511486928104576, "height": 0.012579545454545482, "page": 9}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3483"}, {"text": "This paper is about enabling blind people to use the touchscreens they encounter in-the-wild , despite the fact that nothing about how these systems are designed is intended for their use.", "label": "Method", "bboxes": [{"left": 0.657235294117647, "top": 0.8015012626262626, "width": 0.26460130718954245, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8150492424242425, "width": 0.39717320261437916, "height": 0.012868686868686807, "page": 0}, {"left": 0.5246633986928104, "top": 0.8291742424242424, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8430113636363636, "width": 0.1428627450980393, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3484"}, {"text": "In a formative study, we first identied key challenges and design considerations for a system to provide access to dy namic touchscreen interfaces in the real world.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.6149621212121212, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6287992424242425, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6426363636363637, "width": 0.3039575163398692, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3485"}, {"text": "This paper introduces StateLens , a reverse engineering solution for making existing dynamic touchscreens accessible.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.6423472222222222, "width": 0.39768300653594774, "height": 0.012868686868686918, "page": 1}, {"left": 0.08811928104575163, "top": 0.6564734848484849, "width": 0.35609477124183003, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3486"}, {"text": "Our work is related to prior work on", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.24262091503267968, "height": 0.012579545454545427, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3487"}, {"text": "To do this, we introduce a set of simple 3D-printed accessories that allow users to explore without touching the screen with their finger, and perform a gesture to activate touch at a desired position.", "label": "Method", "bboxes": [{"left": 0.9050098039215686, "top": 0.5105555555555555, "width": 0.016833333333333367, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.5243926767676768, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.538229797979798, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.5520669191919192, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.5659040404040404, "width": 0.056763071895424844, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3488"}, {"text": "Our work goes beyond VizLens by enabling access to dynamic touchscreens.", "label": "Novelty", "bboxes": [{"left": 0.6258823529411766, "top": 0.6236161616161616, "width": 0.2959542483660129, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.6374532828282828, "width": 0.2148921568627451, "height": 0.012579545454545427, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3489"}, {"text": "We conducted a formative study to identify the key challenges and design considerations for a system to provide access to dy namic touchscreen interfaces in the real world.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.13697853535353535, "width": 0.39794281045751634, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.15081565656565657, "width": 0.3998839869281046, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.16465151515151516, "width": 0.3008382352941177, "height": 0.012579545454545454, "page": 3}], "section": "FORMATIVE STUDY", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3490"}, {"text": "Informed by the participants feedback to our initial prototype, we designed variations of 3D-printed accessories (Figure 3DG) that focus on improving stability and comfort during use.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.3992140522875818, "height": 0.012579545454545427, "page": 3}, {"left": 0.5240735294117647, "top": 0.7813737373737373, "width": 0.39884803921568623, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.7952108585858586, "width": 0.36716176470588247, "height": 0.012579545454545427, "page": 3}], "section": "Design Variations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3491"}, {"text": "We first conducted an exploratory search on Thingiverse to understand what openly available solutions exist for people to interact with touchscreens and see if they can enable risk-free exploration for blind people.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.16255555555555556, "width": 0.39793790849673205, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.17639267676767678, "width": 0.39717810457516345, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.190229797979798, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.20406691919191922, "width": 0.19151307189542488, "height": 0.012579545454545454, "page": 3}], "section": "Thingiverse Survey", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3492"}, {"text": "Our solution should enable blind people to independently access touchscreen devices without needing sighted assistance.", "label": "Objective", "bboxes": [{"left": 0.45975490196078433, "top": 0.506195707070707, "width": 0.025820261437908554, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5200328282828283, "width": 0.39716830065359476, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.5338699494949495, "width": 0.36399346405228755, "height": 0.012579545454545427, "page": 3}], "section": "Supporting Independence", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3493"}, {"text": "We tested this design in a pilot study with two blind partic ipants (one female, age 48; one male, age 57).", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.5763712121212121, "width": 0.4006437908496734, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246617647058823, "top": 0.5902083333333333, "width": 0.32037908496732026, "height": 0.012579545454545538, "page": 3}], "section": "Finger Ring Prototype", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3494"}, {"text": "Our solution should support more uid interactions to reduce blind users cognitive effort in exploring the interface layout and accessing functions on complex and unfamiliar touchscreen devices.", "label": "Objective", "bboxes": [{"left": 0.3557222222222222, "top": 0.6400113636363637, "width": 0.1295767973856209, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6538484848484848, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.6676856060606061, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6815227272727272, "width": 0.31383823529411764, "height": 0.012579545454545538, "page": 3}], "section": "Supporting Independence", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3495"}, {"text": "Inspired by the finger cap designs from Thingiverse, we first created a 3D-printed ring that allows users to explore without touching the screen, and tilt their finger forward to perform a touch at a desired position (Figure 3A-C).", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.5134760101010101, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5273131313131313, "width": 0.39717973856209154, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.5411502525252525, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5549873737373737, "width": 0.2726372549019608, "height": 0.012579545454545427, "page": 3}], "section": "Finger Ring Prototype", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3496"}, {"text": "Using an approach akin to afnity diagramming [5, 10, 21], we classied these items into ve main categories of devices: styluses, prosthetic accessories, finger caps, buttons and joy sticks.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.33614646464646464, "width": 0.3991993464052288, "height": 0.012579545454545427, "page": 3}, {"left": 0.5240784313725491, "top": 0.34998358585858586, "width": 0.40002614379084966, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3638207070707071, "width": 0.3998823529411766, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3776578282828283, "width": 0.041316993464052265, "height": 0.012579545454545427, "page": 3}], "section": "Thingiverse Survey", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3497"}, {"text": "Therefore, our solution should support risk-free exploration to enable blind users freely explore without accidentally trig gering functions on the screen.", "label": "Objective", "bboxes": [{"left": 0.08761437908496732, "top": 0.8643964646464646, "width": 0.40057679738562096, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.20027777777777778, "height": 0.012579545454545427, "page": 3}], "section": "Supporting Independence", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3498"}, {"text": "We use the coffee machine in Figure 4 as a running example.", "label": "Method", "bboxes": [{"left": 0.6811013071895425, "top": 0.45580934343434343, "width": 0.24073529411764694, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.46964646464646465, "width": 0.14261764705882352, "height": 0.012579545454545482, "page": 4}], "section": "STATELENS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3499"}, {"text": "We had success with both techniques, though conductive PLA was more durable, while conductive paint can come off after repeated use.", "label": "Method", "bboxes": [{"left": 0.6704983660130719, "top": 0.329939393939394, "width": 0.25336437908496734, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.3437765151515152, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.3576136363636364, "width": 0.24413235294117652, "height": 0.012579545454545427, "page": 4}], "section": "Design Variations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3500"}, {"text": "Our design variations consist of a finger cap (Figure 3D) and a conductive stylus (Figure 3G).", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.329939393939394, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.3437765151515152, "width": 0.21460947712418305, "height": 0.012579545454545427, "page": 4}], "section": "Design Variations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3501"}, {"text": "In the current work, we demon strate StateLens with videos captured from stationary cameras, hand-held mobile phones and web video repositories.", "label": "Method", "bboxes": [{"left": 0.719436274509804, "top": 0.7437941919191919, "width": 0.20510784313725483, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7576300505050505, "width": 0.3992140522875818, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7714671717171716, "width": 0.34768137254901965, "height": 0.012579545454545538, "page": 4}], "section": "Capturing Point-of-View Usage Video", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3502"}, {"text": "Since there is no existing models for detecting touchscreen interfaces, we re-purpose state-of-the-art object detection models output for this task.", "label": "Method", "bboxes": [{"left": 0.39716503267973857, "top": 0.5631666666666667, "width": 0.08813071895424829, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5770037878787879, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5908409090909091, "width": 0.3974509803921568, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6046780303030302, "width": 0.059609477124183005, "height": 0.012579545454545538, "page": 5}], "section": "Representing State Diagram", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3503"}, {"text": "coffee_drinks gourmet_drinks hot_beverages to the coffee drink type state V 1 can be represented as: E 01 = V 0  V 1 = ( { b coffee_drinks } , V 0 , V 1 ) , stating that by interacting with button Coffee Drinks in the initial state, we could get to the desired state for coffee drinks type selection.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.41869191919191917, "width": 0.39944117647058824, "height": 0.023061868686868714, "page": 5}, {"left": 0.08811928104575163, "top": 0.4415290404040404, "width": 0.39774183006535946, "height": 0.01414520202020203, "page": 5}, {"left": 0.08811928104575163, "top": 0.4556540404040404, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4694911616161616, "width": 0.4000245098039216, "height": 0.012579545454545482, "page": 5}], "section": "Representing State Diagram", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3504"}, {"text": "Inspired by our formative study, the goal of the conversational agent is to reduce the time and effort of the blind users to explore, understand, and activate functions on inaccessible and unfamiliar touchscreen interfaces.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.37084343434343436, "width": 0.39717647058823535, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.3846805555555555, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.39851767676767674, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.41235479797979796, "width": 0.25273529411764706, "height": 0.012579545454545482, "page": 6}], "section": "Enabling Natural Language Queries", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3505"}, {"text": "These tech niques effectively reduces the search space, speeds up the state detection process, and improves the robustness of state detection, which we will validate in technical evaluation.", "label": "Method", "bboxes": [{"left": 0.8461715686274509, "top": 0.17831565656565657, "width": 0.07837254901960788, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.19215277777777778, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.205989898989899, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.21982702020202022, "width": 0.36121405228758185, "height": 0.012579545454545454, "page": 6}], "section": "Accessing the State Diagram", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3506"}, {"text": "StateLens identies the current state of the dynamic interface, and recognizes the users touchpoint location to provide realtime feedback and guidance for blind users through the iOS application.", "bboxes": [{"left": 0.08811928104575163, "top": 0.593945707070707, "width": 0.3992075163398694, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6077828282828283, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6216199494949495, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6354570707070707, "width": 0.07793954248366013, "height": 0.012579545454545538, "page": 7}], "section": "Providing Interactive Feedback and Guidance", "label": "Method", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3507"}, {"text": "We collected a total of 28 videos from a diverse set of eight dy namic touchscreen interfaces, in different lighting conditions, and with both stationary and hand-held cameras, resulting in a total of 40,140 video frames.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.38839141414141415, "width": 0.40065032679738566, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.40222853535353537, "width": 0.399202614379085, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4160656565656566, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4299027777777778, "width": 0.1881176470588235, "height": 0.012579545454545482, "page": 7}], "section": "Dataset", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3508"}, {"text": "We first evaluated the effectiveness of StateLens in reconstruct ing interface structures from stationary, hand-held, and web usage videos.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.531050505050505, "width": 0.40065032679738566, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5448876262626262, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.5587247474747474, "width": 0.0854624183006536, "height": 0.012579545454545427, "page": 7}], "section": "Generating the State Diagram", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3509"}, {"text": "Regarding the effect of our screen detection approach, a com bination of Screen Detection+SURF+OCR features generally yielded higher performance compared to SURF+OCR fea tures.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.7952108585858586, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39774346405228767, "height": 0.012579545454545538, "page": 7}, {"left": 0.5242565359477125, "top": 0.822885101010101, "width": 0.40027941176470594, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.0361078431372549, "height": 0.012579545454545538, "page": 7}], "section": "Generating the State Diagram", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3510"}, {"text": "We conducted a multi-part technical evaluation in order to understand how each key component of StateLens performs across a wide range of interfaces and usage scenarios.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.3149179292929293, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246617647058823, "top": 0.32875505050505055, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.34259217171717177, "width": 0.35015686274509805, "height": 0.012579545454545427, "page": 7}], "section": "TECHNICAL EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3511"}, {"text": "For each interface and video source, we computed the preci sion, recall, and F1 scores for the extracted states using four congurations of features:", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.6631313131313132, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6769684343434343, "width": 0.39745588235294127, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6908042929292929, "width": 0.16960784313725497, "height": 0.012579545454545538, "page": 7}], "section": "Generating the State Diagram", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3512"}, {"text": "We then evaluated the robustness of our techniques in identifying states compared to the baseline approach.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.5751654040404041, "width": 0.40065032679738566, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.5890025252525253, "width": 0.30075326797385626, "height": 0.012579545454545427, "page": 8}], "section": "Using State Diagram to Reduce Search Error", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3513"}, {"text": "We evaluated the efciency of our techniques in identifying states compared to the naive approach in VizLens::State De tection [17] which compares against every possible reference image.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.822885101010101, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811764705882352, "top": 0.8367222222222221, "width": 0.3998807189542483, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.04297875816993464, "height": 0.012579545454545538, "page": 8}], "section": "Using State Diagram to Reduce Search Time", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3514"}, {"text": "We next evaluated the effectiveness of using state diagrams to reduce latency and prevent errors in the state detection process.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.7727929292929293, "width": 0.3979362745098039, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7866300505050504, "width": 0.40002777777777776, "height": 0.012579545454545538, "page": 8}], "section": "Accessing the State Diagram", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3515"}, {"text": "In order to enable repeated testing without wasting coffee, we built a simulated interactive prototype of the coffee machine in Figure 4 with InVision [23], which we displayed on an iPad tablet of similar size as the coffee machines interface (iPad Pro 3rd generation, 11-inch, running iOS 12.2 without VoiceOver enabled).", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.49769191919191924, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5115277777777778, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.525364898989899, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.5392020202020202, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 9}, {"left": 0.08758169934640524, "top": 0.5530391414141415, "width": 0.3977140522875816, "height": 0.012579545454545538, "page": 9}, {"left": 0.08753267973856209, "top": 0.5668762626262627, "width": 0.13685294117647062, "height": 0.012579545454545427, "page": 9}], "section": "Apparatus and Participants", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3516"}, {"text": "The goal of our user study was to evaluate how the components of StateLens (the 3D-printed accessories, the conversational agent, and the iOS application) perform in enabling blind people to accomplish realistic tasks that involve otherwise inaccessible dynamic touchscreen interfaces.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.4014368686868687, "width": 0.39768300653594774, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4152739898989899, "width": 0.39716830065359476, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4291111111111111, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4429482323232323, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.45678535353535354, "width": 0.2918611111111111, "height": 0.012579545454545482, "page": 9}], "section": "USER EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3517"}, {"text": "After each step of the study, we collected Likert scale ratings and subjective feedback from the participants.", "label": "Method", "bboxes": [{"left": 0.5240784313725491, "top": 0.3041060606060606, "width": 0.397766339869281, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.3179431818181818, "width": 0.3123758169934642, "height": 0.012579545454545427, "page": 9}], "section": "Procedure", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3518"}, {"text": "Next in user evaluation, we further demonstrate how the generated state diagrams power interactive applications to assist blind users access existing dynamic touchscreen devices.", "label": "Method", "bboxes": [{"left": 0.43666339869281046, "top": 0.3190189393939394, "width": 0.048632352941176404, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.3328560606060606, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.3466931818181818, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.36053030303030303, "width": 0.33298202614379085, "height": 0.012579545454545427, "page": 9}], "section": "Using State Diagram to Reduce Search Error", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3519"}, {"text": "We now detail our user study results and summarize user feedback and preferences.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.425885101010101, "width": 0.3982189542483662, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.43972222222222224, "width": 0.17071895424836614, "height": 0.012579545454545482, "page": 9}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3520"}, {"text": "We observed that participants sometimes held the accessories in awkward postures, likely due to unfamiliarity.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.8505593434343435, "width": 0.39794607843137264, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.33000980392156853, "height": 0.012579545454545538, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3521"}, {"text": "Because our smoothing approach requires a new state to be seen continuously across multiple frames in order to determine a state transition, there may be a delay in determining if a button press was successful.", "label": "Method", "bboxes": [{"left": 0.6467140522875817, "top": 0.515804292929293, "width": 0.27512908496732025, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.5296414141414142, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 10}, {"left": 0.5246633986928104, "top": 0.5434785353535354, "width": 0.3977385620915034, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.5573156565656566, "width": 0.3146339869281046, "height": 0.012579545454545538, "page": 10}], "section": "Completing Realistic Tasks", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3522"}, {"text": "Our 3D-printed accessories elegantly add risk-free explo ration to existing capacitive touchscreen devices without modifying the underlying hardware or software, which has been a major hurdle for past efforts.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.4037411616161616, "width": 0.3998807189542485, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.4175782828282828, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.431415404040404, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.44525252525252523, "width": 0.2448627450980393, "height": 0.012579545454545482, "page": 11}], "section": "Assistive Hardware for Automatic Screen Actuation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3523"}, {"text": "People who are blind were involved throughout the research, including several people with visual impairments on our ex tended research team, and multiple sessions of design and study with a total of 30 outside participants.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.64325, "width": 0.3992009803921569, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.6570871212121212, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.6709242424242424, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.6847613636363636, "width": 0.28585457516339874, "height": 0.012579545454545427, "page": 11}], "section": "Technical Approach to Accessibility", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3524"}, {"text": "As with most systems, StateLens currently has some limita tions, which we believe could be explored in future work.", "label": "Method", "bboxes": [{"left": 0.5240784313725491, "top": 0.6216199494949495, "width": 0.40046078431372534, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.6354570707070707, "width": 0.37029901960784317, "height": 0.012579545454545538, "page": 11}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3525"}, {"text": "In this paper, we developed a hybrid crowd-computer vision system to enable access to dynamic touchscreens in-the-wild.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.7675366161616162, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.7813737373737373, "width": 0.40002777777777776, "height": 0.012579545454545538, "page": 11}], "section": "Technical Approach to Accessibility", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3526"}, {"text": "We motivated our approach as a benet to improve accessibil ity for blind users.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.0814570707070707, "width": 0.40064215686274507, "height": 0.012579545454545468, "page": 11}, {"left": 0.5246633986928104, "top": 0.09529419191919192, "width": 0.11714542483660129, "height": 0.012579545454545468, "page": 11}], "section": "Technical Approach to Accessibility", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3527"}, {"text": "Using StateLens, we envision building a queryable map of state diagrams for many of the devices in the world using existing point-of-view videos that have been shared online.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.2273737373737374, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 11}, {"left": 0.5246633986928104, "top": 0.24121085858585856, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.2550479797979798, "width": 0.3745196078431372, "height": 0.012579545454545482, "page": 11}], "section": "Technical Approach to Accessibility", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3528"}, {"text": "In this section, we discuss how the approaches used in StateLens might generalize to extract information from existing online videos to, for instance, assist sighted users and con struct a queryable map of devices.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.3868825757575758, "width": 0.399874183006536, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.40071969696969695, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.41455681818181817, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.4283939393939394, "width": 0.21855555555555561, "height": 0.012579545454545427, "page": 11}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3529"}, {"text": "We evaluated StateLens across a number of touchscreen inter faces and with blind users in the lab, but we did not deeply study how StateLens works in the real world, which is often much more complicated and messier than in-lab studies.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.40064542483660137, "height": 0.012579545454545468, "page": 12}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3977385620915033, "height": 0.012579545454545468, "page": 12}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 12}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.36700326797385624, "height": 0.01257954545454544, "page": 12}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3530"}, {"text": "We have presented StateLens , a reverse engineering solution that makes existing dynamic touchscreens accessible.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.2045378787878788, "width": 0.3979362745098039, "height": 0.01286868686868689, "page": 12}, {"left": 0.08811928104575163, "top": 0.2186641414141414, "width": 0.3426601307189543, "height": 0.012579545454545482, "page": 12}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3531"}, {"text": "This work has been supported by the National Science Foun dation (#IIS-1816012), Google, and the National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR).", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.4665669191919192, "width": 0.4003839869281045, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.48040404040404044, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.49424116161616166, "width": 0.39717483660130726, "height": 0.012579545454545371, "page": 12}, {"left": 0.08758169934640524, "top": 0.5080770202020202, "width": 0.07978921568627452, "height": 0.012579545454545427, "page": 12}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3532"}, {"text": "StateLens is the first system to enable access to dynamic touchscreens in-the-wild, that addresses the very hard case in which blind users encounter a touchscreen that is inaccessible and unfamiliar, which they cannot modify the hardware or software, and whose screen updates dynamically to show new information and interface components.", "label": "Novelty", "bboxes": [{"left": 0.16649183006535948, "top": 0.7671691919191919, "width": 0.3188006535947713, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.7810063131313132, "width": 0.3977385620915034, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7948434343434343, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.8086805555555556, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8225176767676767, "width": 0.3977352941176471, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.8363547979797981, "width": 0.3376535947712418, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3533"}, {"text": "We show each of these categories with example items in Table 1.", "label": "Result", "bboxes": [{"left": 0.571031045751634, "top": 0.3776578282828283, "width": 0.3508071895424836, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3914949494949495, "width": 0.07058496732026143, "height": 0.012579545454545427, "page": 3}], "section": "Thingiverse Survey", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3534"}, {"text": "The results show that as the number of states increases, StateLens achieved a relatively stable error rate of ~5% compared to the", "label": "Result", "bboxes": [{"left": 0.8960179738562093, "top": 0.6166767676767677, "width": 0.02581862745098029, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6305138888888889, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6443510101010101, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 8}], "section": "Using State Diagram to Reduce Search Error", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3535"}, {"text": "The results show that as the", "label": "Result", "bboxes": [{"left": 0.30278104575163395, "top": 0.892070707070707, "width": 0.18251143790849678, "height": 0.012579545454545427, "page": 8}], "section": "Using State Diagram to Reduce Search Time", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3536"}, {"text": "I would welcome more opportunities to use interfaces with [StateLens], like operating the cable company box. It would be great if interfaces could also show up on my phone screen and read it to me or let me explore it there.  P12", "label": "Result", "bboxes": [{"left": 0.10439869281045752, "top": 0.1440631313131313, "width": 0.3646127450980392, "height": 0.012579545454545454, "page": 11}, {"left": 0.10439869281045752, "top": 0.15790025252525253, "width": 0.3674673202614378, "height": 0.012579545454545454, "page": 11}, {"left": 0.10412091503267974, "top": 0.17173737373737372, "width": 0.3648954248366013, "height": 0.012579545454545482, "page": 11}, {"left": 0.10439869281045752, "top": 0.18557449494949493, "width": 0.3682336601307189, "height": 0.012579545454545454, "page": 11}, {"left": 0.10195588235294117, "top": 0.1997007575757576, "width": 0.037539215686274496, "height": 0.012579545454545454, "page": 11}], "section": "Completing Realistic Tasks", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3537"}, {"text": "Imagine sitting down for a 12-hour ight only to realize that the entertainment center on the seatback in front of you can only be controlled by its inaccessible touchscreen; imagine checking out at the grocery store and being required to tell the cashier your pin number out loud because the checkout kiosk is an inaccessible touchscreen; and, imagine not being able to independently make yourself a coffee at your workplace because the fancy new coffee machine is controlled only by an inaccessible touchscreen.", "label": "Conclusion", "bboxes": [{"left": 0.5664444444444444, "top": 0.4895391414141414, "width": 0.35539215686274517, "height": 0.012579545454545482, "page": 0}, {"left": 0.5246633986928104, "top": 0.5033762626262626, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.5172133838383838, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.531050505050505, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.5448876262626262, "width": 0.397578431372549, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.5587247474747474, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.5725618686868686, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.5863989898989899, "width": 0.39773856209150327, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.6002361111111111, "width": 0.19109803921568624, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3538"}, {"text": "StateLens represents an important step for solv ing this long-standing accessibility problem, and its technical approach may nd applications broadly for augmenting how people interact with the touchscreens they encounter.", "label": "Conclusion", "bboxes": [{"left": 0.6044133986928105, "top": 0.7671691919191919, "width": 0.3201307189542484, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.7810063131313132, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7948434343434343, "width": 0.3977696078431373, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.8086805555555556, "width": 0.344702614379085, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3539"}, {"text": "Slide Rule developed multi-touch gestures that could control touchscreens non-visually [24], which have informed the popular VoiceOver screen reader on the iPhone.", "label": "Conclusion", "bboxes": [{"left": 0.14760947712418301, "top": 0.46904419191919194, "width": 0.33768300653594774, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.48288131313131316, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.4967184343434344, "width": 0.3276486928104575, "height": 0.012579545454545371, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3540"}, {"text": "This approach can work for many static interfaces, but struggles when the interface changes dynamically (as most touchscreens do), and cannot solve the problem of how a blind user could interact with a touchscreen without accidentally triggering touches.", "label": "Conclusion", "bboxes": [{"left": 0.3932630718954248, "top": 0.5659040404040404, "width": 0.09202777777777776, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5797411616161616, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5935782828282828, "width": 0.3971797385620915, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.6074141414141414, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.08753267973856209, "top": 0.6212512626262626, "width": 0.3847483660130719, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3541"}, {"text": "StateLens is intended to work on touchscreens already installed in the world that are not possible to be modied.", "label": "Conclusion", "bboxes": [{"left": 0.5603954248366013, "top": 0.4967184343434344, "width": 0.36200653594771237, "height": 0.012579545454545371, "page": 1}, {"left": 0.5246633986928104, "top": 0.5105555555555555, "width": 0.37530555555555556, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3542"}, {"text": "VizLens::State Detection is able to do limited adaptation to dynamic interfaces by matching against every possible state and providing feedback based on the best match.", "label": "Conclusion", "bboxes": [{"left": 0.5240784313725491, "top": 0.755695707070707, "width": 0.39775490196078434, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246601307189542, "top": 0.7695328282828283, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246601307189542, "top": 0.7833699494949494, "width": 0.32848202614379096, "height": 0.012579545454545538, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3543"}, {"text": "However, because of changing display states and screen layouts, explor ing and activating UI components across multiple screens is difcult (analogous to nding ones way in a new city).", "label": "Conclusion", "bboxes": [{"left": 0.860563725490196, "top": 0.7833699494949494, "width": 0.06329575163398704, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.7972070707070708, "width": 0.3998692810457516, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.8110441919191919, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.8248813131313132, "width": 0.3715702614379085, "height": 0.012579545454545427, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3544"}, {"text": "Users often ask questions about interfaces [8], but it can be difcult to map the answers received, e.g. , the stop button is in the middle of the bottom row of buttons, to actually using the interface because doing so requires locating the referenced object in space ( e.g. , place a finger on the button).", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.1997007575757576, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.21324747474747474, "width": 0.39717647058823535, "height": 0.01286868686868689, "page": 2}, {"left": 0.5246633986928104, "top": 0.2273737373737374, "width": 0.39717647058823535, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.24121085858585856, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.2547588383838384, "width": 0.32424019607843135, "height": 0.012868686868686863, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3545"}, {"text": "Other systems provide more continuous support.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.2764330808080808, "width": 0.3103709150326798, "height": 0.012579545454545427, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3546"}, {"text": "These systems could more easily assist blind users with using an interface, but assisting in this way is likely to be cumbersome and slow.", "label": "Conclusion", "bboxes": [{"left": 0.598937908496732, "top": 0.331780303030303, "width": 0.3229019607843139, "height": 0.012579545454545482, "page": 2}, {"left": 0.5240784313725491, "top": 0.3456174242424242, "width": 0.39831862745098034, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246601307189542, "top": 0.3594545454545454, "width": 0.19219281045751635, "height": 0.012579545454545482, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3547"}, {"text": "Participants also mentioned sighted peo ple giving incorrect or incomplete information because of a lack of patience or experience helping blind people.", "label": "Conclusion", "bboxes": [{"left": 0.2248888888888889, "top": 0.4785214646464646, "width": 0.2631029411764706, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.49235858585858583, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.506195707070707, "width": 0.3633169934640523, "height": 0.012579545454545427, "page": 3}], "section": "Supporting Independence", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3548"}, {"text": "For example, the location of the ring on the finger may vary for different users and different sessions during use, thus changing the actual position of touch.", "label": "Conclusion", "bboxes": [{"left": 0.8992892156862745, "top": 0.631719696969697, "width": 0.02283006535947707, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6455568181818182, "width": 0.39745588235294127, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.6593939393939394, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.673229797979798, "width": 0.1830261437908497, "height": 0.012579545454545427, "page": 3}], "section": "Finger Ring Prototype", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3549"}, {"text": "Our solution should support more uid interactions to reduce blind users cognitive effort in exploring the interface layout and accessing functions on complex and unfamiliar touchscreen devices.", "label": "Conclusion", "bboxes": [{"left": 0.3557222222222222, "top": 0.6400113636363637, "width": 0.1295767973856209, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6538484848484848, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.6676856060606061, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6815227272727272, "width": 0.31383823529411764, "height": 0.012579545454545538, "page": 3}], "section": "Supporting Independence", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3550"}, {"text": "We used these categories to inspire design ideas for prototypes that take on familiar forms used in the Thingiverse accessibility community but also support risk-free exploration (Figure 3).", "label": "Conclusion", "bboxes": [{"left": 0.6547303921568627, "top": 0.4330063131313131, "width": 0.2671062091503269, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4468434343434343, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 3}, {"left": 0.5241601307189543, "top": 0.46068055555555554, "width": 0.3976830065359477, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.47451767676767675, "width": 0.14759967320261447, "height": 0.012579545454545482, "page": 3}], "section": "Thingiverse Survey", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3551"}, {"text": "Therefore, our solution should support risk-free exploration to enable blind users freely explore without accidentally trig gering functions on the screen.", "label": "Conclusion", "bboxes": [{"left": 0.08761437908496732, "top": 0.8643964646464646, "width": 0.40057679738562096, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.20027777777777778, "height": 0.012579545454545427, "page": 3}], "section": "Supporting Independence", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3552"}, {"text": "accessories make exploration possible by bringing risk-free exploration to touchscreens.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.0814570707070707, "width": 0.39717320261437916, "height": 0.012579545454545468, "page": 3}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.18285947712418305, "height": 0.012579545454545468, "page": 3}], "section": "Improving Accessibility for Physical Interfaces", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3553"}, {"text": "We do so because the first few candidates often include transition residuals from the previous state, such as animations.", "label": "Conclusion", "bboxes": [{"left": 0.6470702614379085, "top": 0.7675378787878788, "width": 0.274766339869281, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7813737373737373, "width": 0.39717647058823535, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.7952108585858586, "width": 0.0960882352941177, "height": 0.012579545454545427, "page": 5}], "section": "Adding New States", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3554"}, {"text": "coffee_drinks gourmet_drinks hot_beverages to the coffee drink type state V 1 can be represented as: E 01 = V 0  V 1 = ( { b coffee_drinks } , V 0 , V 1 ) , stating that by interacting with button Coffee Drinks in the initial state, we could get to the desired state for coffee drinks type selection.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.41869191919191917, "width": 0.39944117647058824, "height": 0.023061868686868714, "page": 5}, {"left": 0.08811928104575163, "top": 0.4415290404040404, "width": 0.39774183006535946, "height": 0.01414520202020203, "page": 5}, {"left": 0.08811928104575163, "top": 0.4556540404040404, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4694911616161616, "width": 0.4000245098039216, "height": 0.012579545454545482, "page": 5}], "section": "Representing State Diagram", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3555"}, {"text": "In the next section of Accessing the State Diagram, using the user interaction information, StateLens predicts the state that the interface could be transitioning to, and reduces the process ing latency and errors by narrowing down the search space.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.5080366161616162, "width": 0.3971699346405229, "height": 0.012578282828282772, "page": 6}, {"left": 0.08811928104575163, "top": 0.5218737373737374, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5357108585858587, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5495479797979799, "width": 0.4000130718954249, "height": 0.012579545454545427, "page": 6}], "section": "Recognizing User Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3556"}, {"text": "Note that recognizing finger touchpoint locations in naturalistic us age videos is not always possible or accurate, such as under extreme lighting conditions, or when users are wearing gloves.", "label": "Conclusion", "bboxes": [{"left": 0.45393300653594776, "top": 0.5772209595959595, "width": 0.03136111111111106, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.5910580808080808, "width": 0.39987254901960784, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.604895202020202, "width": 0.39745588235294127, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.6187323232323232, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 6}], "section": "Recognizing User Interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3557"}, {"text": "To do this, StateLens transforms all the possible paths (interaction traces) from S to T in the generated state diagram into different intents ( e.g. , to make coffee drinks, to make gourmet drinks), and the in teractive element values in the edges E i along the path into required entities for the intent and their attributes/values ( e.g. , size: large/medium/small).", "label": "Conclusion", "bboxes": [{"left": 0.7824248366013071, "top": 0.41235479797979796, "width": 0.13940686274509806, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.4259027777777778, "width": 0.39704411764705894, "height": 0.012868686868686863, "page": 6}, {"left": 0.5246633986928104, "top": 0.439739898989899, "width": 0.3992058823529413, "height": 0.012868686868686863, "page": 6}, {"left": 0.5246633986928104, "top": 0.4538661616161616, "width": 0.3998807189542485, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.46741414141414145, "width": 0.39716993464052297, "height": 0.013834595959595908, "page": 6}, {"left": 0.5246633986928104, "top": 0.48125126262626267, "width": 0.3992075163398693, "height": 0.012868686868686863, "page": 6}, {"left": 0.5246633986928104, "top": 0.49537626262626266, "width": 0.17242810457516344, "height": 0.012579545454545427, "page": 6}], "section": "Enabling Natural Language Queries", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3558"}, {"text": "Because Dialogow only requires a small number of user ut terance samples for training, StateLens uses a random sample of entity values and concatenates with phrases such as Select ... to create training sentences.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.5922361111111111, "width": 0.3998758169934641, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6060732323232323, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6199103535353535, "width": 0.39716830065359476, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6337474747474747, "width": 0.20086111111111116, "height": 0.012579545454545427, "page": 6}], "section": "Enabling Natural Language Queries", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3559"}, {"text": "This is because the screen detection and OCR processes have longer delays (~1 second).", "label": "Conclusion", "bboxes": [{"left": 0.5663807189542484, "top": 0.2613383838383838, "width": 0.35545588235294123, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.27517424242424243, "width": 0.20573856209150332, "height": 0.012579545454545482, "page": 6}], "section": "Accessing the State Diagram", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3560"}, {"text": "Crowd workers are first asked to rate the image quality, segment the interface region (with the generated screen bounding box as a start when available), indicate the approximate number of interaction components, and additionally provide a description of the interface state.", "label": "Conclusion", "bboxes": [{"left": 0.354766339869281, "top": 0.15127020202020203, "width": 0.13052614379084976, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.16510732323232322, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.17894444444444443, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.19278156565656565, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.20661868686868687, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.22045580808080809, "width": 0.09475816993464052, "height": 0.012579545454545454, "page": 6}], "section": "Adding New States", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3561"}, {"text": "StateLens uses the state diagram and the associated aggrega tion of interaction traces to automatically generate a natural language summary of the devices popular use cases.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.3918510101010101, "width": 0.39987418300653593, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.4056881313131313, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4195252525252525, "width": 0.34787254901960785, "height": 0.012579545454545427, "page": 7}], "section": "Generating Natural Language Summary", "prob": 1, "is_author_statement": false, "is_in_expected_section": false, "id": "3562"}, {"text": "This natural lan guage summary is also integrated in the conversational agent (Figure 5), and users can simply ask, e.g. , tell me a summary.", "label": "Conclusion", "bboxes": [{"left": 0.37791339869281043, "top": 0.5302209595959596, "width": 0.11008660130718961, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.5440580808080808, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 7}, {"left": 0.08758169934640524, "top": 0.5576060606060607, "width": 0.40060294117647055, "height": 0.012868686868686807, "page": 7}], "section": "Generating Natural Language Summary", "prob": 1, "is_author_statement": false, "is_in_expected_section": false, "id": "3563"}, {"text": "Regarding the different video sources, stationary videos generally performed better com pared to hand-held ones for the same interface, because state matching is more robust with less camera blur, changing back ground noise and other uncertainty from camera motion.", "label": "Conclusion", "bboxes": [{"left": 0.2857761437908497, "top": 0.6011300505050505, "width": 0.19951633986928102, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6149671717171716, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6288030303030303, "width": 0.3971781045751634, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6426401515151515, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6564772727272726, "width": 0.3680130718954249, "height": 0.012579545454545538, "page": 8}], "section": "Generating the State Diagram", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3564"}, {"text": "We evaluated the efciency of our techniques in identifying states compared to the naive approach in VizLens::State De tection [17] which compares against every possible reference image.", "label": "Conclusion", "bboxes": [{"left": 0.08735457516339869, "top": 0.822885101010101, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811764705882352, "top": 0.8367222222222221, "width": 0.3998807189542483, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.04297875816993464, "height": 0.012579545454545538, "page": 8}], "section": "Using State Diagram to Reduce Search Time", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3565"}, {"text": "To address this problem, special-purpose models for detecting screens could be built.", "label": "Conclusion", "bboxes": [{"left": 0.17465032679738562, "top": 0.46904924242424245, "width": 0.31064215686274516, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.48288636363636367, "width": 0.23205392156862747, "height": 0.012579545454545427, "page": 8}], "section": "Generating the State Diagram", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3566"}, {"text": "This is likely because the conductive material is less sensitive compared to fingers.", "label": "Conclusion", "bboxes": [{"left": 0.7542205882352941, "top": 0.6279090909090909, "width": 0.16762091503267973, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.6417462121212121, "width": 0.36944607843137267, "height": 0.012579545454545427, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3567"}, {"text": "We now detail our user study results and summarize user feedback and preferences.", "label": "Conclusion", "bboxes": [{"left": 0.5238986928104574, "top": 0.425885101010101, "width": 0.3982189542483662, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.43972222222222224, "width": 0.17071895424836614, "height": 0.012579545454545482, "page": 9}], "section": "Results", "prob": 1, "is_author_statement": true, "is_in_expected_section": false, "id": "3568"}, {"text": "We observed that participants sometimes held the accessories in awkward postures, likely due to unfamiliarity.", "label": "Conclusion", "bboxes": [{"left": 0.5238986928104574, "top": 0.8505593434343435, "width": 0.39794607843137264, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.33000980392156853, "height": 0.012579545454545538, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3569"}, {"text": "Because our smoothing approach requires a new state to be seen continuously across multiple frames in order to determine a state transition, there may be a delay in determining if a button press was successful.", "label": "Conclusion", "bboxes": [{"left": 0.6467140522875817, "top": 0.515804292929293, "width": 0.27512908496732025, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.5296414141414142, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 10}, {"left": 0.5246633986928104, "top": 0.5434785353535354, "width": 0.3977385620915034, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.5573156565656566, "width": 0.3146339869281046, "height": 0.012579545454545538, "page": 10}], "section": "Completing Realistic Tasks", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3570"}, {"text": "In this case, some users may accidentally press again at the same location triggering an incorrect selection on the next screen state.", "label": "Conclusion", "bboxes": [{"left": 0.8448431372549019, "top": 0.5573156565656566, "width": 0.07901960784313733, "height": 0.012579545454545538, "page": 10}, {"left": 0.5246633986928104, "top": 0.5711527777777778, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.5849898989898991, "width": 0.363514705882353, "height": 0.012579545454545427, "page": 10}], "section": "Completing Realistic Tasks", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3571"}, {"text": "This issue may be alleviated by providing more immediate feed back such as a tentative audio conrmation that a button press has been successful.", "label": "Conclusion", "bboxes": [{"left": 0.893217320261438, "top": 0.5849898989898991, "width": 0.028625816993464115, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.5988270202020202, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.6126641414141414, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.6265, "width": 0.13154901960784315, "height": 0.012579545454545538, "page": 10}], "section": "Completing Realistic Tasks", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3572"}, {"text": "Note that the task completion time is likely to reduce in prac tice, since the agents speaking rate is dependent on the users screen reader setting, and after repeated usage, the users will get familiar with the functions.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.3508207070707071, "width": 0.3998709150326799, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.36465782828282833, "width": 0.3998692810457516, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.37849494949494944, "width": 0.39717810457516356, "height": 0.012579545454545482, "page": 10}, {"left": 0.5246633986928104, "top": 0.39233207070707066, "width": 0.20104248366013078, "height": 0.012579545454545482, "page": 10}], "section": "Prespecifying Tasks with the Conversational Agent", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3573"}, {"text": "Almost all of this subset of participants (7) had a strong familiarity with using VoiceOver on an iPhone or iPad, suggesting their habitual use of this tech nology may inuence their interactions using the accessories.", "label": "Conclusion", "bboxes": [{"left": 0.3083872549019608, "top": 0.44139015151515154, "width": 0.17691013071895428, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.45522727272727276, "width": 0.3974493464052288, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.469064393939394, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.4829015151515152, "width": 0.3996764705882353, "height": 0.012579545454545427, "page": 10}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3574"}, {"text": "Better affordances could further improve learnability as one participant (P14) noted that a conductive stylus design which incorporates a physical button to trigger, instead of a conduc tive region, would be benecial.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.3369835858585859, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.3508207070707071, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.36465782828282833, "width": 0.3998709150326797, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.37849494949494944, "width": 0.20872549019607844, "height": 0.012579545454545482, "page": 10}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3575"}, {"text": "Overall, participants were very excited about the potential of StateLens, and felt that it could help them access other inaccessible interface in the future ( M = 6 . 6 , SD = 0 . 9 ) :", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.7522916666666667, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.7661287878787879, "width": 0.39745588235294116, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.7796767676767677, "width": 0.3654738562091504, "height": 0.012868686868686807, "page": 10}], "section": "Completing Realistic Tasks", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3576"}, {"text": "If the screen is cluttered, it could still be quite difcult to operate.", "label": "Conclusion", "bboxes": [{"left": 0.6522173202614379, "top": 0.4867638888888889, "width": 0.2696192810457517, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.5006010101010101, "width": 0.1636846405228758, "height": 0.012579545454545427, "page": 11}], "section": "Assistive Hardware for Automatic Screen Actuation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3577"}, {"text": "Blind users could brush a phone case on the external touchscreen, then the built-in camera would capture, recognize, and instruct actuators contacting the external screen to trigger functions at the right place and time.", "label": "Conclusion", "bboxes": [{"left": 0.7075228758169935, "top": 0.5282739898989899, "width": 0.2143202614379085, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.5421111111111111, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 11}, {"left": 0.5240784313725491, "top": 0.5559482323232323, "width": 0.3977630718954249, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.5697853535353535, "width": 0.4000081699346405, "height": 0.012579545454545538, "page": 11}], "section": "Assistive Hardware for Automatic Screen Actuation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3578"}, {"text": "As with most systems, StateLens currently has some limita tions, which we believe could be explored in future work.", "label": "Conclusion", "bboxes": [{"left": 0.5240784313725491, "top": 0.6216199494949495, "width": 0.40046078431372534, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.6354570707070707, "width": 0.37029901960784317, "height": 0.012579545454545538, "page": 11}], "section": "Limitations", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3579"}, {"text": "One solution may be to detect and factor in UI widgets that are expected to change using approaches like those in PreFab [12] and TapShoe [32].", "label": "Conclusion", "bboxes": [{"left": 0.6546732026143791, "top": 0.6769671717171717, "width": 0.2674411764705882, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.6908042929292929, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 11}, {"left": 0.5246633986928104, "top": 0.7046414141414141, "width": 0.3016486928104575, "height": 0.012579545454545427, "page": 11}], "section": "Limitations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3580"}, {"text": "Even if videos only capture a subset of possible tasks, these would likely be frequently used paths of action, thus still providing reasonable functionality in many cases.", "label": "Conclusion", "bboxes": [{"left": 0.8249787581699346, "top": 0.7813737373737373, "width": 0.09685294117647059, "height": 0.012579545454545538, "page": 11}, {"left": 0.5246633986928104, "top": 0.7952108585858586, "width": 0.39717483660130726, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 11}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.1804068627450981, "height": 0.012579545454545427, "page": 11}], "section": "Limitations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3581"}, {"text": "If a blind user needs to access an unseen state, StateLens could add it to the state diagram onthe-y, asking the user to wait for that screen to be labeled and then added to the full state diagram.", "label": "Conclusion", "bboxes": [{"left": 0.7101519607843138, "top": 0.822885101010101, "width": 0.21168954248366012, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.3998807189542485, "height": 0.012579545454545538, "page": 11}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.22912091503267973, "height": 0.012579545454545538, "page": 11}], "section": "Limitations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3582"}, {"text": "In a perfect world, posthoc xes like StateLens would not be needed (because all technologies would be inherently accessible), but in practice access technology like StateLens plays a vital role.", "label": "Conclusion", "bboxes": [{"left": 0.3260751633986928, "top": 0.4834962121212121, "width": 0.1619248366013072, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.4973333333333333, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.511169191919192, "width": 0.3971764705882353, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.5250063131313132, "width": 0.3267418300653595, "height": 0.012579545454545427, "page": 11}], "section": "Technical Approach to Accessibility", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3583"}, {"text": "For example, a vending ma chine could be labeled with Braille, but the checkout credit card machine is not accessible.", "label": "Conclusion", "bboxes": [{"left": 0.3031486928104575, "top": 0.5526805555555556, "width": 0.18484150326797388, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.5665176767676768, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.580354797979798, "width": 0.19509313725490196, "height": 0.012579545454545427, "page": 11}], "section": "Technical Approach to Accessibility", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3584"}, {"text": "StateLens is a stopgap measure to make access possible (as are many access technologies), and introduces ideas that might nd purchase in other access and accessible technologies.", "label": "Conclusion", "bboxes": [{"left": 0.288062091503268, "top": 0.580354797979798, "width": 0.19723039215686278, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.5941919191919193, "width": 0.39919934640522875, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.6080290404040405, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.6218661616161616, "width": 0.1839983660130719, "height": 0.012579545454545427, "page": 11}], "section": "Technical Approach to Accessibility", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3585"}, {"text": "He would nd it useful if, for example, a button for a coffee selection labeled Rainbows End could further be described as a coffee blend containing tasting notes of nuts and citrus even though the display does not provide that information.", "label": "Conclusion", "bboxes": [{"left": 0.31690686274509805, "top": 0.29027020202020204, "width": 0.1704199346405229, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.3041060606060606, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.3179431818181818, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.331780303030303, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.3456174242424242, "width": 0.1878562091503268, "height": 0.012579545454545482, "page": 11}], "section": "Completing Realistic Tasks", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3586"}, {"text": "However, this approach could be benecial to sighted people and people with cognitive disabilities in many ways as well.", "label": "Conclusion", "bboxes": [{"left": 0.6468333333333334, "top": 0.09529419191919192, "width": 0.2750032679738561, "height": 0.012579545454545468, "page": 11}, {"left": 0.5246633986928104, "top": 0.10913131313131313, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 11}, {"left": 0.5246633986928104, "top": 0.12296717171717171, "width": 0.12469117647058825, "height": 0.01257954545454544, "page": 11}], "section": "Technical Approach to Accessibility", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3587"}, {"text": "Furthermore, similar but slightly different models of a device may reuse another state diagram and enable transfer learning.", "label": "Conclusion", "bboxes": [{"left": 0.8377467320261438, "top": 0.32423358585858586, "width": 0.0861307189542484, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.3380707070707071, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.3519078282828283, "width": 0.3295473856209151, "height": 0.012579545454545427, "page": 11}], "section": "Technical Approach to Accessibility", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3588"}, {"text": "Special thanks to Patrick Carrington, Meredith Ringel Morris, Zheng Yao, and Xu Wang for their help and support.", "label": "Conclusion", "bboxes": [{"left": 0.2408954248366013, "top": 0.5357512626262626, "width": 0.24642320261437906, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.5495883838383838, "width": 0.39745588235294127, "height": 0.012579545454545538, "page": 12}, {"left": 0.08811928104575163, "top": 0.563425505050505, "width": 0.11258333333333333, "height": 0.012579545454545427, "page": 12}], "section": "CONCLUSION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "3589"}, {"text": "I would welcome more opportunities to use interfaces with [StateLens], like operating the cable company box. It would be great if interfaces could also show up on my phone screen and read it to me or let me explore it there.  P12", "label": "Conclusion", "bboxes": [{"left": 0.10439869281045752, "top": 0.1440631313131313, "width": 0.3646127450980392, "height": 0.012579545454545454, "page": 11}, {"left": 0.10439869281045752, "top": 0.15790025252525253, "width": 0.3674673202614378, "height": 0.012579545454545454, "page": 11}, {"left": 0.10412091503267974, "top": 0.17173737373737372, "width": 0.3648954248366013, "height": 0.012579545454545482, "page": 11}, {"left": 0.10439869281045752, "top": 0.18557449494949493, "width": 0.3682336601307189, "height": 0.012579545454545454, "page": 11}, {"left": 0.10195588235294117, "top": 0.1997007575757576, "width": 0.037539215686274496, "height": 0.012579545454545454, "page": 11}], "section": "Completing Realistic Tasks", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3590"}, {"text": "VizLens users would also need to take pictures when the screen changes, which is difcult.", "label": "Future Work", "bboxes": [{"left": 0.6176732026143791, "top": 0.6651262626262626, "width": 0.3041617647058824, "height": 0.012579545454545538, "page": 2}, {"left": 0.5240784313725491, "top": 0.6789633838383838, "width": 0.29479248366013067, "height": 0.012579545454545427, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3591"}, {"text": "Although the Thingiverse designs are closely re lated to assistive usage for touchscreens, none of them satisfy our need to enable blind users risk-free access to an existing touchscreen device.", "label": "Future Work", "bboxes": [{"left": 0.6011584967320261, "top": 0.3914949494949495, "width": 0.32338562091503276, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4053320707070707, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.41916919191919194, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4330063131313131, "width": 0.12513725490196081, "height": 0.012579545454545482, "page": 3}], "section": "Thingiverse Survey", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3592"}, {"text": "On the other hand, if continuous unmatched states in the pool do not reach the window size to qualify as a new state, they are considered noise and the candidate pool will be cleared.", "label": "Future Work", "bboxes": [{"left": 0.5853513071895425, "top": 0.8090479797979797, "width": 0.33648529411764705, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.39775490196078434, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.39717810457516356, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.05152614379084963, "height": 0.012579545454545427, "page": 5}], "section": "Adding New States", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3593"}, {"text": "If such bounding boxes exist and their sizes are above 10% of the image size (aiming to lter out objects that are not the one of interest), StateLens crops the image using the bounding box to remove background noises for further processing.", "label": "Future Work", "bboxes": [{"left": 0.37351960784313726, "top": 0.6323510101010101, "width": 0.11176797385620912, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6461881313131314, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 5}, {"left": 0.08758823529411765, "top": 0.6600252525252526, "width": 0.39974673202614386, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6738623737373737, "width": 0.397171568627451, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.687699494949495, "width": 0.27855228758169936, "height": 0.012579545454545427, "page": 5}], "section": "Representing State Diagram", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3594"}, {"text": "If so, the full video frame is retained and used for further processing.", "label": "Future Work", "bboxes": [{"left": 0.39769934640522875, "top": 0.7153737373737374, "width": 0.0875866013071896, "height": 0.012579545454545427, "page": 5}, {"left": 0.0877124183006536, "top": 0.7292108585858585, "width": 0.3553284313725491, "height": 0.012579545454545538, "page": 5}], "section": "Representing State Diagram", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3595"}, {"text": "In those cases, StateLens will fallback to only using the state transition without the detailed interaction component as the triggering event, e.g. , E i j = V i  V j = ( 0/ , V i , V j ) .", "label": "Future Work", "bboxes": [{"left": 0.08811928104575163, "top": 0.6325694444444444, "width": 0.3971748366013072, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.6464065656565656, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.6599545454545455, "width": 0.3166421568627451, "height": 0.013834595959595908, "page": 6}], "section": "Recognizing User Interaction", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3596"}, {"text": "These tech niques effectively reduces the search space, speeds up the state detection process, and improves the robustness of state detection, which we will validate in technical evaluation.", "label": "Future Work", "bboxes": [{"left": 0.8461715686274509, "top": 0.17831565656565657, "width": 0.07837254901960788, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.19215277777777778, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.205989898989899, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.21982702020202022, "width": 0.36121405228758185, "height": 0.012579545454545454, "page": 6}], "section": "Accessing the State Diagram", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3597"}, {"text": "However, in the future, these processes can be sped up and the produced bounding boxes can be tracked across frames to offer better performance.", "label": "Future Work", "bboxes": [{"left": 0.7354395424836602, "top": 0.27517424242424243, "width": 0.1864003267973856, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.28901136363636365, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.30284848484848487, "width": 0.3705212418300653, "height": 0.012579545454545427, "page": 6}], "section": "Accessing the State Diagram", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3598"}, {"text": "Once all required entities are fullled, the StateLens iOS application will proceed to guiding the users to activate each button on the predened interaction trace.", "label": "Future Work", "bboxes": [{"left": 0.21928758169934642, "top": 0.31428914141414144, "width": 0.2660049019607843, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.3281262626262626, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.3419633838383838, "width": 0.380812091503268, "height": 0.012579545454545482, "page": 7}], "section": "Enabling Natural Language Queries", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3599"}, {"text": "Parameters can be chosen to further maximize recall (sacri cing some precision), as post-hoc crowd validation can be applied in the future to further lter out duplicates.", "label": "Future Work", "bboxes": [{"left": 0.08811928104575163, "top": 0.6778623737373737, "width": 0.3998807189542484, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.691699494949495, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7055366161616161, "width": 0.3291143790849673, "height": 0.012579545454545538, "page": 8}], "section": "Generating the State Diagram", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3600"}, {"text": "The whole study was video and audio recorded for further analysis.", "label": "Future Work", "bboxes": [{"left": 0.710248366013072, "top": 0.37329166666666663, "width": 0.21158823529411763, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.38712878787878785, "width": 0.2291879084967321, "height": 0.012579545454545482, "page": 9}], "section": "Procedure", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3601"}, {"text": "Next in user evaluation, we further demonstrate how the generated state diagrams power interactive applications to assist blind users access existing dynamic touchscreen devices.", "label": "Future Work", "bboxes": [{"left": 0.43666339869281046, "top": 0.3190189393939394, "width": 0.048632352941176404, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.3328560606060606, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.3466931818181818, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.36053030303030303, "width": 0.33298202614379085, "height": 0.012579545454545427, "page": 9}], "section": "Using State Diagram to Reduce Search Error", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3602"}, {"text": "Note that the task completion time is likely to reduce in prac tice, since the agents speaking rate is dependent on the users screen reader setting, and after repeated usage, the users will get familiar with the functions.", "label": "Future Work", "bboxes": [{"left": 0.5246633986928104, "top": 0.3508207070707071, "width": 0.3998709150326799, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.36465782828282833, "width": 0.3998692810457516, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.37849494949494944, "width": 0.39717810457516356, "height": 0.012579545454545482, "page": 10}, {"left": 0.5246633986928104, "top": 0.39233207070707066, "width": 0.20104248366013078, "height": 0.012579545454545482, "page": 10}], "section": "Prespecifying Tasks with the Conversational Agent", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3603"}, {"text": "Better affordances could further improve learnability as one participant (P14) noted that a conductive stylus design which incorporates a physical button to trigger, instead of a conduc tive region, would be benecial.", "label": "Future Work", "bboxes": [{"left": 0.08811928104575163, "top": 0.3369835858585859, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.3508207070707071, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.36465782828282833, "width": 0.3998709150326797, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.37849494949494944, "width": 0.20872549019607844, "height": 0.012579545454545482, "page": 10}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3604"}, {"text": "Overall, participants were very excited about the potential of StateLens, and felt that it could help them access other inaccessible interface in the future ( M = 6 . 6 , SD = 0 . 9 ) :", "label": "Future Work", "bboxes": [{"left": 0.5246633986928104, "top": 0.7522916666666667, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.7661287878787879, "width": 0.39745588235294116, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.7796767676767677, "width": 0.3654738562091504, "height": 0.012868686868686807, "page": 10}], "section": "Completing Realistic Tasks", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3605"}, {"text": "Itll be a thing, I will actually use it.  P1", "label": "Future Work", "bboxes": [{"left": 0.5409264705882353, "top": 0.8010618686868687, "width": 0.2852516339869281, "height": 0.012867424242424153, "page": 10}], "section": "Completing Realistic Tasks", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3606"}, {"text": "As future work, we have started to design hardware proxies that can locate and actuate external touchscreens automatically.", "label": "Future Work", "bboxes": [{"left": 0.6935457516339869, "top": 0.5006010101010101, "width": 0.22829084967320257, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.5144381313131313, "width": 0.39716830065359465, "height": 0.012579545454545538, "page": 11}, {"left": 0.5246633986928104, "top": 0.5282739898989899, "width": 0.17782026143790852, "height": 0.012579545454545427, "page": 11}], "section": "Assistive Hardware for Automatic Screen Actuation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3607"}, {"text": "As with most systems, StateLens currently has some limita tions, which we believe could be explored in future work.", "label": "Future Work", "bboxes": [{"left": 0.5240784313725491, "top": 0.6216199494949495, "width": 0.40046078431372534, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.6354570707070707, "width": 0.37029901960784317, "height": 0.012579545454545538, "page": 11}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3608"}, {"text": "A low vision user (P12) mentioned that even though he might not always need assistance, if the interfaces contrast or bright ness is poor, a system like StateLens would be greatly helpful as a conrmation.", "label": "Future Work", "bboxes": [{"left": 0.08753267973856209, "top": 0.22108459595959595, "width": 0.3977598039215686, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.23492171717171717, "width": 0.3998839869281046, "height": 0.012579545454545454, "page": 11}, {"left": 0.08811928104575163, "top": 0.2487588383838384, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.2625959595959596, "width": 0.12067973856209151, "height": 0.012579545454545427, "page": 11}], "section": "Completing Realistic Tasks", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3609"}, {"text": "He would nd it useful if, for example, a button for a coffee selection labeled Rainbows End could further be described as a coffee blend containing tasting notes of nuts and citrus even though the display does not provide that information.", "label": "Future Work", "bboxes": [{"left": 0.31690686274509805, "top": 0.29027020202020204, "width": 0.1704199346405229, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.3041060606060606, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.3179431818181818, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.331780303030303, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.3456174242424242, "width": 0.1878562091503268, "height": 0.012579545454545482, "page": 11}], "section": "Completing Realistic Tasks", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3610"}, {"text": "We also discuss limitations of our work, which represent opportunities for future research.", "label": "Future Work", "bboxes": [{"left": 0.3117140522875817, "top": 0.4283939393939394, "width": 0.17357352941176468, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.4422310606060606, "width": 0.3999477124183007, "height": 0.012579545454545427, "page": 11}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3611"}, {"text": "Inspired by our formative study, the goal of the conversational agent is to reduce the time and effort of the blind users to explore, understand, and activate functions on inaccessible and unfamiliar touchscreen interfaces.", "label": "Objective", "bboxes": [{"left": 0.5246633986928104, "top": 0.37084343434343436, "width": 0.39717647058823535, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.3846805555555555, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.39851767676767674, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.41235479797979796, "width": 0.25273529411764706, "height": 0.012579545454545482, "page": 6}], "section": "Enabling Natural Language Queries", "prob": 0.7896773815155029, "is_author_statement": true, "is_in_expected_section": false, "id": "3612"}, {"text": "The designs aim to reduce the change of touchpoint when the user moves from exploration to interaction ( i.e. , touch acti vation), and maintain consistency across sessions.", "label": "Objective", "bboxes": [{"left": 0.8968823529411766, "top": 0.7952108585858586, "width": 0.024959150326797297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.8225959595959595, "width": 0.3998627450980392, "height": 0.012868686868686918, "page": 3}, {"left": 0.5242483660130719, "top": 0.8367222222222221, "width": 0.3358872549019608, "height": 0.012579545454545538, "page": 3}], "section": "Design Variations", "prob": 0.6999070048332214, "is_author_statement": false, "is_in_expected_section": false, "id": "3613"}, {"text": "If such bounding boxes exist and their sizes are above 10% of the image size (aiming to lter out objects that are not the one of interest), StateLens crops the image using the bounding box to remove background noises for further processing.", "label": "Method", "bboxes": [{"left": 0.37351960784313726, "top": 0.6323510101010101, "width": 0.11176797385620912, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6461881313131314, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 5}, {"left": 0.08758823529411765, "top": 0.6600252525252526, "width": 0.39974673202614386, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6738623737373737, "width": 0.397171568627451, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.687699494949495, "width": 0.27855228758169936, "height": 0.012579545454545427, "page": 5}], "section": "Representing State Diagram", "prob": 0.6297030448913574, "is_author_statement": false, "is_in_expected_section": true, "id": "3614"}, {"text": "In order to enable repeated testing without wasting coffee, we built a simulated interactive prototype of the coffee machine in Figure 4 with InVision [23], which we displayed on an iPad tablet of similar size as the coffee machines interface (iPad Pro 3rd generation, 11-inch, running iOS 12.2 without VoiceOver enabled).", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.49769191919191924, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5115277777777778, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.525364898989899, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.5392020202020202, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 9}, {"left": 0.08758169934640524, "top": 0.5530391414141415, "width": 0.3977140522875816, "height": 0.012579545454545538, "page": 9}, {"left": 0.08753267973856209, "top": 0.5668762626262627, "width": 0.13685294117647062, "height": 0.012579545454545427, "page": 9}], "section": "Apparatus and Participants", "prob": 0.5746822953224182, "is_author_statement": true, "is_in_expected_section": true, "id": "3615"}, {"text": "StateLens uses a hybrid crowd-computer vision pipeline to dynamically generate state diagrams about interface structures from point-of-view usage videos, and to provide interactive feedback and guidance to help blind users access the interfaces through these diagrams.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.4004621212121212, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.41429924242424243, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.42813510101010105, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.4419722222222222, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.45580934343434343, "width": 0.15161928104575162, "height": 0.012579545454545482, "page": 4}], "section": "STATELENS", "prob": 0.5534073710441589, "is_author_statement": false, "is_in_expected_section": true, "id": "3616"}, {"text": "StateLens builds upon the crowdsourcing workow in VizLens [17], and uses a two-step workow to label the area of the image that contains the interface assisted with screen detection results, and then label the individual interaction components assisted with OCR output (Figure 2).", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.0959229797979798, "width": 0.39717320261437916, "height": 0.01257954545454544, "page": 6}, {"left": 0.08811928104575163, "top": 0.10976010101010102, "width": 0.39717320261437916, "height": 0.01257954545454544, "page": 6}, {"left": 0.08811928104575163, "top": 0.12359722222222222, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.13743434343434344, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.15127020202020203, "width": 0.2553725490196079, "height": 0.012579545454545454, "page": 6}], "section": "Adding New States", "prob": 0.5262104272842407, "is_author_statement": false, "is_in_expected_section": true, "id": "3617"}, {"text": "Inspired by the finger cap designs from Thingiverse, we first created a 3D-printed ring that allows users to explore without touching the screen, and tilt their finger forward to perform a touch at a desired position (Figure 3A-C).", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.5134760101010101, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5273131313131313, "width": 0.39717973856209154, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.5411502525252525, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5549873737373737, "width": 0.2726372549019608, "height": 0.012579545454545427, "page": 3}], "section": "Finger Ring Prototype", "prob": 0.5145944356918335, "is_author_statement": true, "is_in_expected_section": true, "id": "3618"}, {"text": "StateLens first uses SURF feature detectors to compute key points and feature vectors in both the existing state reference images (Figure 4) and the input image.", "label": "Method", "bboxes": [{"left": 0.18161601307189543, "top": 0.8367222222222221, "width": 0.3036764705882353, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.34170261437908495, "height": 0.012579545454545538, "page": 5}], "section": "Representing State Diagram", "prob": 0.5030547976493835, "is_author_statement": false, "is_in_expected_section": false, "id": "3619"}, {"text": "StateLens uses simple heuristic templatebased generation methods that concatenate words like I want ... with most frequently selected button options, i.e. entities, as well as the descriptive text of the intent.", "label": "Method", "bboxes": [{"left": 0.21045915032679738, "top": 0.48870959595959595, "width": 0.2775408496732027, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.5025467171717172, "width": 0.3971781045751634, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.516094696969697, "width": 0.3992107843137255, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.5302209595959596, "width": 0.2840163398692811, "height": 0.012579545454545538, "page": 7}], "section": "Generating Natural Language Summary", "prob": 0.5024548172950745, "is_author_statement": false, "is_in_expected_section": true, "id": "3620"}, {"text": "Then us ing a Wizard-of-Oz approach, we asked two participants to try using a touchscreen coffee machine with verbal instructions given by the researchers.", "label": "Method", "bboxes": [{"left": 0.431171568627451, "top": 0.20616287878787878, "width": 0.05683169934640525, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.22, "width": 0.39774673202614386, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.23383712121212122, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.2476742424242424, "width": 0.1667238562091503, "height": 0.012579545454545482, "page": 3}], "section": "FORMATIVE STUDY", "prob": 0.49616512656211853, "is_author_statement": true, "is_in_expected_section": false, "id": "3621"}, {"text": "Using an approach akin to afnity diagramming [5, 10, 21], we classied these items into ve main categories of devices: styluses, prosthetic accessories, finger caps, buttons and joy sticks.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.33614646464646464, "width": 0.3991993464052288, "height": 0.012579545454545427, "page": 3}, {"left": 0.5240784313725491, "top": 0.34998358585858586, "width": 0.40002614379084966, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3638207070707071, "width": 0.3998823529411766, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3776578282828283, "width": 0.041316993464052265, "height": 0.012579545454545427, "page": 3}], "section": "Thingiverse Survey", "prob": 0.4757940173149109, "is_author_statement": true, "is_in_expected_section": true, "id": "3622"}, {"text": "To make sure the touchpoint does not change from exploration to activation ( i.e. , the problems Slide Rule [24] addressed with split tap, and VizLens [17] addressed with shifting the interaction point), we measured the ground truth touchpoint location and placed the color marker on the accessory accordingly.", "label": "Method", "bboxes": [{"left": 0.3681830065359477, "top": 0.6631313131313132, "width": 0.11710947712418307, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6766780303030303, "width": 0.3992140522875817, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.6908042929292929, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 7}, {"left": 0.08753267973856209, "top": 0.7046414141414141, "width": 0.39776307189542476, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.397171568627451, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.27905065359477127, "height": 0.012579545454545427, "page": 7}], "section": "Providing Interactive Feedback and Guidance", "prob": 0.46096840500831604, "is_author_statement": true, "is_in_expected_section": true, "id": "3623"}, {"text": "StateLens uses the state diagram and the associated aggrega tion of interaction traces to automatically generate a natural language summary of the devices popular use cases.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.3918510101010101, "width": 0.39987418300653593, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.4056881313131313, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4195252525252525, "width": 0.34787254901960785, "height": 0.012579545454545427, "page": 7}], "section": "Generating Natural Language Summary", "prob": 0.45064833760261536, "is_author_statement": false, "is_in_expected_section": true, "id": "3624"}, {"text": "For each of the three screen placements (in the order of 90 vertical at chest-level, 45 tilted at chest-level, and 0 at on the table), participants completed ve trials using both the finger cap and the conductive stylus.", "label": "Method", "bboxes": [{"left": 0.21699019607843137, "top": 0.7046414141414141, "width": 0.2683022875816994, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.3992140522875817, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.305202614379085, "height": 0.012579545454545538, "page": 9}], "section": "Procedure", "prob": 0.4480595290660858, "is_author_statement": false, "is_in_expected_section": true, "id": "3625"}, {"text": "Participants were then asked to activate a touch.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.8150492424242425, "width": 0.3133006535947713, "height": 0.012868686868686807, "page": 9}], "section": "Procedure", "prob": 0.4469131529331207, "is_author_statement": false, "is_in_expected_section": true, "id": "3626"}, {"text": "Essentially, StateLens uses the last image of the previous state V i before the state transition, transforms the input image to the reference image frame through warping, and detects the touchpoint lo cation using skin color thresholding and other standard image processing techniques [36].", "label": "Method", "bboxes": [{"left": 0.4113643790849673, "top": 0.41746717171717174, "width": 0.07596568627450978, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811437908496732, "top": 0.43101515151515146, "width": 0.39717973856209154, "height": 0.013834595959596019, "page": 6}, {"left": 0.08811928104575163, "top": 0.44514141414141417, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.4589785353535354, "width": 0.3998709150326797, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.4728156565656566, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.48665151515151517, "width": 0.17857843137254903, "height": 0.012579545454545482, "page": 6}], "section": "Recognizing User Interaction", "prob": 0.4455307424068451, "is_author_statement": false, "is_in_expected_section": true, "id": "3627"}, {"text": "StateLens employs several techniques to enable efcient searching of states to reduce latency and prevent errors.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.7813737373737373, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.36030392156862745, "height": 0.012579545454545427, "page": 6}], "section": "Accessing the State Diagram", "prob": 0.69, "is_author_statement": false, "is_in_expected_section": true, "id": "3628"}, {"text": "By ltering matches and nding the perspective transformation [11] between the reference-input image pairs using RANSAC (Random Sample Consensus) [13], StateLens is able to compute the ratios of inliers to the number of good matches for each existing state.", "label": "Method", "bboxes": [{"left": 0.6797630718954248, "top": 0.3034482323232323, "width": 0.24207352941176463, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3172840909090909, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3311212121212121, "width": 0.39826960784313736, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3449583333333333, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.35879545454545453, "width": 0.31680228758169937, "height": 0.012579545454545482, "page": 5}], "section": "Representing State Diagram", "prob": 0.4322412610054016, "is_author_statement": false, "is_in_expected_section": true, "id": "3629"}, {"text": "[25] apply a crowdsourcing workow to extract step-by-step structure from existing online tutorial videos.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.6921717171717172, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7060088383838383, "width": 0.2945637254901961, "height": 0.012579545454545427, "page": 2}], "section": "Reverse Engineering User Interfaces", "prob": 0.42148590087890625, "is_author_statement": false, "is_in_expected_section": true, "id": "3630"}, {"text": "To do this, StateLens ranks the aggregated interaction traces, then generates prompts for each trace based on the involved state and transition metadata as well as the corresponding interac tion components.", "label": "Method", "bboxes": [{"left": 0.46887581699346403, "top": 0.43336237373737374, "width": 0.01641666666666669, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.44719949494949496, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4610366161616162, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.47487247474747474, "width": 0.39986928104575165, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.48870959595959595, "width": 0.11505065359477125, "height": 0.012579545454545427, "page": 7}], "section": "Generating Natural Language Summary", "prob": 0.4089290499687195, "is_author_statement": false, "is_in_expected_section": true, "id": "3631"}, {"text": "Using the Amazon Rekognition Object Detection API [1], StateLens first detects bounding boxes of object categories related to electronics and machines.", "label": "Method", "bboxes": [{"left": 0.15471895424836601, "top": 0.6046780303030302, "width": 0.33057352941176477, "height": 0.012579545454545538, "page": 5}, {"left": 0.08753267973856209, "top": 0.6185138888888889, "width": 0.4004591503267974, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6323510101010101, "width": 0.28031699346405226, "height": 0.012579545454545427, "page": 5}], "section": "Representing State Diagram", "prob": 0.4073125123977661, "is_author_statement": false, "is_in_expected_section": true, "id": "3632"}, {"text": "To do this, we introduce a set of simple 3D-printed accessories that allow users to explore without touching the screen with their finger, and perform a gesture to activate touch at a desired position.", "label": "Method", "bboxes": [{"left": 0.9050098039215686, "top": 0.5105555555555555, "width": 0.016833333333333367, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.5243926767676768, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.538229797979798, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.5520669191919192, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.5659040404040404, "width": 0.056763071895424844, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.40199631452560425, "is_author_statement": true, "is_in_expected_section": true, "id": "3633"}, {"text": "Next, according to the three interaction traces prespecied through the conversational agent, participants were asked to use the 3D-printed accessories to perform the tasks following the guidance and feedback of the iOS application.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.1305151515151515, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.14435227272727272, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.15818939393939394, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.17202651515151515, "width": 0.3137369281045752, "height": 0.012579545454545454, "page": 9}], "section": "Procedure", "prob": 0.3767695426940918, "is_author_statement": false, "is_in_expected_section": true, "id": "3634"}, {"text": "Finally, we ended the study with a semi-structured interview asking for the participants comments and suggestions on the StateLens system.", "label": "Method", "bboxes": [{"left": 0.846437908496732, "top": 0.3179431818181818, "width": 0.07539869281045763, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.331780303030303, "width": 0.39745588235294127, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.3456174242424242, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.3594545454545454, "width": 0.04835620915032679, "height": 0.012579545454545482, "page": 9}], "section": "Procedure", "prob": 0.31554561853408813, "is_author_statement": true, "is_in_expected_section": true, "id": "3635"}, {"text": "Regarding the effect of our screen detection approach, a com bination of Screen Detection+SURF+OCR features generally yielded higher performance compared to SURF+OCR fea tures.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.7952108585858586, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39774346405228767, "height": 0.012579545454545538, "page": 7}, {"left": 0.5242565359477125, "top": 0.822885101010101, "width": 0.40027941176470594, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.0361078431372549, "height": 0.012579545454545538, "page": 7}], "section": "Generating the State Diagram", "prob": 0.891990602016449, "is_author_statement": true, "is_in_expected_section": false, "id": "3636"}, {"text": "In general, participants found both accessories to be com fortable to use ( M = 5 . 9 , SD = 1 . 1 ) and highly useful ( M = 6 . 4 , SD = 0 . 8 ) .", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.6631313131313132, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.6766780303030303, "width": 0.39718790849673213, "height": 0.012869949494949462, "page": 9}, {"left": 0.5246699346405229, "top": 0.6905151515151515, "width": 0.09928921568627447, "height": 0.012868686868686918, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": 0.69, "is_author_statement": true, "is_in_expected_section": true, "id": "3637"}, {"text": "The results show that as the", "label": "Result", "bboxes": [{"left": 0.30278104575163395, "top": 0.892070707070707, "width": 0.18251143790849678, "height": 0.012579545454545427, "page": 8}], "section": "Using State Diagram to Reduce Search Time", "prob": 0.8043178915977478, "is_author_statement": false, "is_in_expected_section": false, "id": "3638"}, {"text": "Overall, the combination of Screen Detection+SURF+OCR features achieved high performances across a wide range of interfaces, and were often the best in the four feature congurations.", "label": "Result", "bboxes": [{"left": 0.6986013071895425, "top": 0.7323156565656566, "width": 0.2232352941176471, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.39717810457516356, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.759989898989899, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.20162908496732024, "height": 0.012579545454545427, "page": 7}], "section": "Generating the State Diagram", "prob": 0.7907593250274658, "is_author_statement": false, "is_in_expected_section": false, "id": "3639"}, {"text": "If the highest matched ratio across existing reference images is not high enough, meaning the match using only SURF features is not so condent, StateLens then uses the Google Cloud Vision API [16] to compute OCR results for the input image and compares to the pre-computed OCR results of the state reference image.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.40785353535353536, "width": 0.39718137254901964, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.4216906565656566, "width": 0.3976225490196078, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.43552777777777774, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.44936489898989895, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.46320202020202017, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.4770391414141414, "width": 0.14830882352941177, "height": 0.012579545454545482, "page": 5}], "section": "Representing State Diagram", "prob": 0.7797395586967468, "is_author_statement": false, "is_in_expected_section": false, "id": "3640"}, {"text": "Regarding OCR features, a combination of Screen Detec tion+SURF+OCR features generally had better performance compared to Screen Detection+SURF features.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.504270202020202, "width": 0.3998807189542484, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.5181073232323232, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5319444444444444, "width": 0.31607843137254904, "height": 0.012579545454545427, "page": 8}], "section": "Generating the State Diagram", "prob": 0.7721295952796936, "is_author_statement": false, "is_in_expected_section": false, "id": "3641"}, {"text": "number of states increases, StateLens achieved a relatively stable processing time compared to the linear increase in the baseline approach (Figure 7).", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.46904924242424245, "width": 0.39773856209150327, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.48288636363636367, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.49672348484848483, "width": 0.19470098039215689, "height": 0.012579545454545538, "page": 8}], "section": "Using State Diagram to Reduce Search Time", "prob": 0.7711287140846252, "is_author_statement": false, "is_in_expected_section": false, "id": "3642"}, {"text": "However, there were differences across the various screen placements.", "label": "Result", "bboxes": [{"left": 0.6321290849673202, "top": 0.6908042929292929, "width": 0.28971078431372554, "height": 0.012579545454545538, "page": 9}, {"left": 0.5242565359477125, "top": 0.7046414141414141, "width": 0.17608006535947707, "height": 0.012579545454545427, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": 0.7663044929504395, "is_author_statement": false, "is_in_expected_section": false, "id": "3643"}, {"text": "The results show that as the number of states increases, StateLens achieved a relatively stable error rate of ~5% compared to the", "label": "Result", "bboxes": [{"left": 0.8960179738562093, "top": 0.6166767676767677, "width": 0.02581862745098029, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6305138888888889, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6443510101010101, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 8}], "section": "Using State Diagram to Reduce Search Error", "prob": 0.7372595071792603, "is_author_statement": false, "is_in_expected_section": false, "id": "3644"}, {"text": "The overall task completion rate was 94.7%.", "label": "Result", "bboxes": [{"left": 0.8458235294117648, "top": 0.47429292929292927, "width": 0.07601307189542472, "height": 0.012579545454545482, "page": 10}, {"left": 0.5246633986928104, "top": 0.4881300505050505, "width": 0.21380228758169928, "height": 0.012579545454545482, "page": 10}], "section": "Completing Realistic Tasks", "prob": 0.7371533513069153, "is_author_statement": false, "is_in_expected_section": false, "id": "3645"}, {"text": "In subjective ratings, participants found the StateLens iOS ap plication to be easy to learn ( M = 5 . 5 , SD = 0 . 9 ) , comfortable to use ( M = 5 . 6 , SD = 1 . 2 ) , and very useful ( M = 6 . 1 , SD = 1 . 1 ) .", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.647885101010101, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.6614330808080808, "width": 0.39717483660130715, "height": 0.012868686868686807, "page": 10}, {"left": 0.5246633986928104, "top": 0.6752702020202019, "width": 0.39717647058823546, "height": 0.012868686868686918, "page": 10}, {"left": 0.5242516339869281, "top": 0.6891199494949495, "width": 0.031259803921568685, "height": 0.012856060606060593, "page": 10}], "section": "Completing Realistic Tasks", "prob": 0.6770530939102173, "is_author_statement": false, "is_in_expected_section": false, "id": "3646"}, {"text": "coffee_drinks gourmet_drinks hot_beverages to the coffee drink type state V 1 can be represented as: E 01 = V 0  V 1 = ( { b coffee_drinks } , V 0 , V 1 ) , stating that by interacting with button Coffee Drinks in the initial state, we could get to the desired state for coffee drinks type selection.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.41869191919191917, "width": 0.39944117647058824, "height": 0.023061868686868714, "page": 5}, {"left": 0.08811928104575163, "top": 0.4415290404040404, "width": 0.39774183006535946, "height": 0.01414520202020203, "page": 5}, {"left": 0.08811928104575163, "top": 0.4556540404040404, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4694911616161616, "width": 0.4000245098039216, "height": 0.012579545454545482, "page": 5}], "section": "Representing State Diagram", "prob": 0.6388972401618958, "is_author_statement": true, "is_in_expected_section": false, "id": "3647"}, {"text": "This can be improved with practice, as participants generally found the accessories to be very easy to learn ( M = 6 . 2 , SD = 0 . 9 ) .", "label": "Result", "bboxes": [{"left": 0.8638218954248366, "top": 0.8643964646464646, "width": 0.058014705882353024, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8917815656565657, "width": 0.4000228758169936, "height": 0.012868686868686696, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "prob": 0.6313470602035522, "is_author_statement": false, "is_in_expected_section": false, "id": "3648"}, {"text": "Then through a user study with 14 blind participants, we showed that the conversational agent, the iOS application, and the 3D-printed accessories collectively helped blind users access otherwise inaccessible dynamic touchscreen devices effectively.", "label": "Result", "bboxes": [{"left": 0.5820686274509804, "top": 0.711820707070707, "width": 0.3418039215686274, "height": 0.012579545454545538, "page": 1}, {"left": 0.5240784313725491, "top": 0.7256578282828283, "width": 0.39979575163398673, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7394949494949494, "width": 0.39717973856209166, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.7533320707070708, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7671691919191919, "width": 0.0730261437908497, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "prob": 0.6266377568244934, "is_author_statement": true, "is_in_expected_section": true, "id": "3649"}, {"text": "Our evaluations demonstrated the feasibility of StateLens in accurately reconstructing the state diagram, iden tifying interface states, and giving effective feedback and guid ance.", "label": "Result", "bboxes": [{"left": 0.16712254901960782, "top": 0.35703409090909094, "width": 0.3181699346405229, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.37087121212121216, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.3847083333333334, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.3985441919191919, "width": 0.034468954248366004, "height": 0.012579545454545482, "page": 12}], "section": "CONCLUSION", "prob": 0.6236369013786316, "is_author_statement": true, "is_in_expected_section": true, "id": "3650"}, {"text": "We show each of these categories with example items in Table 1.", "label": "Result", "bboxes": [{"left": 0.571031045751634, "top": 0.3776578282828283, "width": 0.3508071895424836, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3914949494949495, "width": 0.07058496732026143, "height": 0.012579545454545427, "page": 3}], "section": "Thingiverse Survey", "prob": 0.6212817430496216, "is_author_statement": true, "is_in_expected_section": false, "id": "3651"}, {"text": "We evaluated StateLens across a number of touchscreen inter faces and with blind users in the lab, but we did not deeply study how StateLens works in the real world, which is often much more complicated and messier than in-lab studies.", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.40064542483660137, "height": 0.012579545454545468, "page": 12}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3977385620915033, "height": 0.012579545454545468, "page": 12}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 12}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.36700326797385624, "height": 0.01257954545454544, "page": 12}], "section": "Limitations", "prob": 0.6148879528045654, "is_author_statement": true, "is_in_expected_section": false, "id": "3652"}, {"text": "Participants spent an average of 53.7 seconds ( SD = 11 . 6 ) to prespecify tasks with the conversational agent, with an overall task completion rate of 100%, and found it to be ex tremely easy to learn ( M = 6 . 6 , SD = 0 . 6 ) , comfortable to use ( M = 6 . 8 , SD = 0 . 4 ) , and useful ( M = 6 . 7 , SD = 0 . 6 ) .", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.5241161616161616, "width": 0.3984640522875817, "height": 0.012868686868686918, "page": 10}, {"left": 0.08811928104575163, "top": 0.5382424242424243, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.5520795454545455, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.5656262626262626, "width": 0.39717156862745095, "height": 0.012868686868686807, "page": 10}, {"left": 0.08748366013071895, "top": 0.5794633838383838, "width": 0.345702614379085, "height": 0.012868686868686918, "page": 10}], "section": "Prespecifying Tasks with the Conversational Agent", "prob": 0.5923879742622375, "is_author_statement": false, "is_in_expected_section": false, "id": "3653"}, {"text": "These tech niques effectively reduces the search space, speeds up the state detection process, and improves the robustness of state detection, which we will validate in technical evaluation.", "label": "Result", "bboxes": [{"left": 0.8461715686274509, "top": 0.17831565656565657, "width": 0.07837254901960788, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.19215277777777778, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.205989898989899, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.21982702020202022, "width": 0.36121405228758185, "height": 0.012579545454545454, "page": 6}], "section": "Accessing the State Diagram", "prob": 0.578676164150238, "is_author_statement": true, "is_in_expected_section": false, "id": "3654"}, {"text": "Task completion rate and time were recorded.", "label": "Result", "bboxes": [{"left": 0.5241601307189543, "top": 0.10913131313131313, "width": 0.29750980392156867, "height": 0.012579545454545454, "page": 9}], "section": "Procedure", "prob": 0.4835681617259979, "is_author_statement": false, "is_in_expected_section": false, "id": "3655"}, {"text": "Since there is no existing models for detecting touchscreen interfaces, we re-purpose state-of-the-art object detection models output for this task.", "label": "Method", "bboxes": [{"left": 0.39716503267973857, "top": 0.5631666666666667, "width": 0.08813071895424829, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5770037878787879, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5908409090909091, "width": 0.3974509803921568, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6046780303030302, "width": 0.059609477124183005, "height": 0.012579545454545538, "page": 5}], "section": "Representing State Diagram", "prob": 0.466082364320755, "is_author_statement": true, "is_in_expected_section": false, "id": "3656"}, {"text": "We had success with both techniques, though conductive PLA was more durable, while conductive paint can come off after repeated use.", "label": "Result", "bboxes": [{"left": 0.6704983660130719, "top": 0.329939393939394, "width": 0.25336437908496734, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.3437765151515152, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.3576136363636364, "width": 0.24413235294117652, "height": 0.012579545454545427, "page": 4}], "section": "Design Variations", "prob": 0.4523179829120636, "is_author_statement": true, "is_in_expected_section": false, "id": "3657"}, {"text": "We next evaluated the effectiveness of using state diagrams to reduce latency and prevent errors in the state detection process.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.7727929292929293, "width": 0.3979362745098039, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7866300505050504, "width": 0.40002777777777776, "height": 0.012579545454545538, "page": 8}], "section": "Accessing the State Diagram", "prob": 0.44525274634361267, "is_author_statement": true, "is_in_expected_section": false, "id": "3658"}, {"text": "We use a time window of 1 second for this process.", "label": "Result", "bboxes": [{"left": 0.6292875816993464, "top": 0.7952108585858586, "width": 0.2925490196078431, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.05394771241830065, "height": 0.012579545454545538, "page": 5}], "section": "Adding New States", "prob": 0.44513511657714844, "is_author_statement": true, "is_in_expected_section": false, "id": "3659"}, {"text": "In our user study, we discovered issues around holding the accessories in certain angles, and the last meter problem to accurately activate the exact button once.", "label": "Result", "bboxes": [{"left": 0.7782598039215687, "top": 0.44525252525252523, "width": 0.1435767973856209, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.45908964646464645, "width": 0.39717320261437905, "height": 0.012578282828282827, "page": 11}, {"left": 0.5246633986928104, "top": 0.47292676767676767, "width": 0.39716830065359476, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.4867638888888889, "width": 0.12132679738562091, "height": 0.012579545454545482, "page": 11}], "section": "Assistive Hardware for Automatic Screen Actuation", "prob": 0.43469762802124023, "is_author_statement": true, "is_in_expected_section": false, "id": "3660"}, {"text": "for SURF descriptors.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.3034482323232323, "width": 0.14764542483660126, "height": 0.012579545454545482, "page": 5}], "section": "Representing State Diagram", "prob": 0.4097450375556946, "is_author_statement": false, "is_in_expected_section": false, "id": "3661"}, {"text": "StateLens represents the interface structure with a state diagram, as shown in Figure 2 and the instantiation of the coffee machine shown in Figure 4.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.28474673202614387, "height": 0.012579545454545538, "page": 4}], "section": "Representing State Diagram", "prob": 0.4081430435180664, "is_author_statement": false, "is_in_expected_section": true, "id": "3662"}, {"text": "Detecting the Screen", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.5219785353535353, "width": 0.1368104575163399, "height": 0.011321969696969747, "page": 5}], "section": "Representing State Diagram", "prob": 0.4046679437160492, "is_author_statement": false, "is_in_expected_section": false, "id": "3663"}, {"text": "StateLens extracts two kinds of features and intelligently com bines them (Figure 2): SURF (Speeded-Up Robust Features) [3] and OCR.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.3998839869281046, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.39826307189542487, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.08942810457516341, "height": 0.012579545454545538, "page": 5}], "section": "Representing State Diagram", "prob": 0.3872804045677185, "is_author_statement": false, "is_in_expected_section": true, "id": "3664"}, {"text": "Next in user evaluation, we further demonstrate how the generated state diagrams power interactive applications to assist blind users access existing dynamic touchscreen devices.", "label": "Result", "bboxes": [{"left": 0.43666339869281046, "top": 0.3190189393939394, "width": 0.048632352941176404, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.3328560606060606, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.3466931818181818, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.36053030303030303, "width": 0.33298202614379085, "height": 0.012579545454545427, "page": 9}], "section": "Using State Diagram to Reduce Search Error", "prob": 0.37746530771255493, "is_author_statement": true, "is_in_expected_section": false, "id": "3665"}, {"text": "Parameters can be chosen to further maximize recall (sacri cing some precision), as post-hoc crowd validation can be applied in the future to further lter out duplicates.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.6778623737373737, "width": 0.3998807189542484, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.691699494949495, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7055366161616161, "width": 0.3291143790849673, "height": 0.012579545454545538, "page": 8}], "section": "Generating the State Diagram", "prob": 0.3698083460330963, "is_author_statement": false, "is_in_expected_section": false, "id": "3666"}, {"text": "We tested this design in a pilot study with two blind partic ipants (one female, age 48; one male, age 57).", "label": "Result", "bboxes": [{"left": 0.5238986928104574, "top": 0.5763712121212121, "width": 0.4006437908496734, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246617647058823, "top": 0.5902083333333333, "width": 0.32037908496732026, "height": 0.012579545454545538, "page": 3}], "section": "Finger Ring Prototype", "prob": 0.36617761850357056, "is_author_statement": true, "is_in_expected_section": false, "id": "3667"}, {"text": "The created agent then guides", "label": "Result", "bboxes": [{"left": 0.7306078431372549, "top": 0.6337474747474747, "width": 0.19123856209150325, "height": 0.012579545454545427, "page": 6}], "section": "Enabling Natural Language Queries", "prob": 0.348479688167572, "is_author_statement": false, "is_in_expected_section": false, "id": "3668"}, {"text": "Slide Rule developed", "label": "Result", "bboxes": [{"left": 0.3475653594771242, "top": 0.8854128787878788, "width": 0.13772549019607844, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "prob": 0.31179091334342957, "is_author_statement": false, "is_in_expected_section": true, "id": "3669"}, {"text": "These approaches have looked to automatically extract GUI components from screenshot images in order to decouple GUI element representation from predened image templates [2, 9, 12, 22, 38], to augment existing interfaces through understanding of GUI components [2, 12], and to extract interaction ows from screencast videos and screen metadata [25, 28, 29, 39].", "label": "Result", "bboxes": [{"left": 0.33530065359477124, "top": 0.2971881313131313, "width": 0.14999673202614383, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.31102525252525254, "width": 0.3998807189542484, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.32486237373737376, "width": 0.39717647058823524, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.338699494949495, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3525366161616162, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3663737373737374, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3802095959595959, "width": 0.2490964052287582, "height": 0.012579545454545482, "page": 2}], "section": "Reverse Engineering User Interfaces", "prob": 0.2722269594669342, "is_author_statement": false, "is_in_expected_section": false, "id": "3670"}], "1602.06979": [{"text": "As we gain access to ever larger and more diverse datasets, it becomes important to scale our ability to conduct such analyses with breadth and accuracy.", "label": "Author", "bboxes": [{"left": 0.6014673202614379, "top": 0.759989898989899, "width": 0.3203660130718955, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.7876641414141414, "width": 0.26721568627450976, "height": 0.012579545454545427, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3671"}, {"text": "Copyrights for components of this work owned by others than ACM must be honored.", "label": "Author", "bboxes": [{"left": 0.19487418300653594, "top": 0.8307777777777777, "width": 0.28717973856209145, "height": 0.008804292929292856, "page": 0}, {"left": 0.0913611111111111, "top": 0.8408421717171717, "width": 0.10639542483660132, "height": 0.008804292929292856, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3672"}, {"text": "To build Empath, we extend a deep learning skip-gram network to capture words in a neural embedding [23].", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.75, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7638371212121211, "width": 0.33259150326797393, "height": 0.012579545454545538, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3673"}, {"text": "We can then use similarity comparisons in the resulting vector space to map a vocabulary of 59,690 words onto Empaths 200 categories (and beyond, onto user-dened categories).", "label": "Author", "bboxes": [{"left": 0.31392156862745096, "top": 0.791510101010101, "width": 0.17136764705882357, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.8053472222222222, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8191843434343434, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8330214646464646, "width": 0.23734477124183007, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3674"}, {"text": "Finally, we demonstrate how we can lter these relationships through the crowd to efciently construct new, human validated dictionaries.", "label": "Author", "bboxes": [{"left": 0.27136274509803926, "top": 0.8606957070707071, "width": 0.21393464052287575, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8745328282828283, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8883699494949495, "width": 0.28928921568627447, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3675"}, {"text": "Our evaluation validates Empath by comparing its analyses against LIWC, a lexicon of gold standard categories that have been psychometrically validated.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.34746843434343433, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.36130555555555555, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.37514267676767676, "width": 0.21515686274509804, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3676"}, {"text": "We nd the correlation between Empath and LIWC across a mixed-corpus dataset is high (r=0.906), and remains high even without the crowd lter (0.90), which suggests Empaths data-driven word counts are very similar to those made by a heavily validated dictionary.", "label": "Author", "bboxes": [{"left": 0.7461029411764706, "top": 0.37514267676767676, "width": 0.1757303921568628, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.388979797979798, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.4028169191919192, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.4166540404040404, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.43049116161616163, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.44432828282828285, "width": 0.04912908496732027, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3677"}, {"text": "When we instead train Empath on the 100 billion word Google News corpus, its unsupervised model shows less agreement (0.84) with LIWC.", "label": "Author", "bboxes": [{"left": 0.5850735294117647, "top": 0.44432828282828285, "width": 0.3367598039215687, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.4581641414141414, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.47200126262626263, "width": 0.20043954248366014, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3678"}, {"text": "While Empath presents an approach that can be trained on any text corpora, in this paper we use 1.8 billion words of modern amateur ction.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.562570707070707, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5764078282828282, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5902449494949494, "width": 0.10133496732026144, "height": 0.012579545454545538, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3679"}, {"text": "To address these problems, we present Empath : a living lexicon mined from modern text on the web.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.34717929292929295, "width": 0.39716830065359476, "height": 0.012868686868686863, "page": 1}, {"left": 0.08811928104575163, "top": 0.36130555555555555, "width": 0.30892156862745096, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3680"}, {"text": "For example, using the seed terms twitter and facebook, we can generate and validate a category for social media .", "label": "Author", "bboxes": [{"left": 0.19640522875816993, "top": 0.4028169191919192, "width": 0.2888839869281046, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4166540404040404, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.43020202020202025, "width": 0.08919117647058823, "height": 0.012868686868686807, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3681"}, {"text": "We show how the open-ended nature of Empaths model can replicate and extend classic work in classifying deceptive language [31], identifying the patterns of language in movie reviews [32], and analyzing mood on twitter [13].", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.21538888888888888, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.22922474747474747, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.2430618686868687, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.2568989898989899, "width": 0.3074411764705882, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3682"}, {"text": "In the movie review dataset, we nd positive reviews are more strongly connected with intellectual categories like philosophy, politics, and law.", "label": "Author", "bboxes": [{"left": 0.7384362745098039, "top": 0.29841035353535356, "width": 0.18339705882352952, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.3122474747474748, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.326084595959596, "width": 0.3638464052287582, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3683"}, {"text": "Through Empath, we aim to empower researchers with the ability to generate and validate these categories.", "label": "Author", "bboxes": [{"left": 0.15673366013071896, "top": 0.649729797979798, "width": 0.3285555555555555, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6635669191919191, "width": 0.3687140522875817, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3684"}, {"text": "To motivate the opportunities that Empath creates, we rst present three example analyses that illustrate its breadth and exibility.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.6780328282828282, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.6918699494949495, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7057070707070706, "width": 0.06609150326797386, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3685"}, {"text": "Following this section, we explain the techniques behind Empaths model in more detail.", "label": "Author", "bboxes": [{"left": 0.6753071895424837, "top": 0.7472171717171717, "width": 0.2465261437908497, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7610542929292929, "width": 0.32691013071895425, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3686"}, {"text": "What kinds of words accompany our lies?", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.2762336601307189, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3687"}, {"text": "In our rst example, we use Empath to analyze a dataset of deceptive hotel reviews reported previously by Ott el al.", "label": "Author", "bboxes": [{"left": 0.8072794117647059, "top": 0.8090479797979797, "width": 0.11455392156862754, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.26894117647058824, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3688"}, {"text": "In Empath, we adapt these techniques to mine natural language for its relation to emotional and topical categories.", "label": "Author", "bboxes": [{"left": 0.7879395424836602, "top": 0.4426111111111111, "width": 0.13389379084967323, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.45644823232323234, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4702840909090909, "width": 0.21496078431372545, "height": 0.012579545454545482, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3689"}, {"text": "We draw on some of this knowledge, like the ConceptNet hierarchy, when seeding Empaths categories.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.5470176767676768, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.5608535353535354, "width": 0.2852042483660131, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3690"}, {"text": "We nd that ction offers a richer source of affective signal.", "label": "Author", "bboxes": [{"left": 0.869562091503268, "top": 0.6162020202020202, "width": 0.052271241830065374, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6300391414141414, "width": 0.3362222222222222, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3691"}, {"text": "We borrow from the last of these approaches in constructing of Empaths unsupervised model.", "label": "Author", "bboxes": [{"left": 0.6961274509803921, "top": 0.2690189393939394, "width": 0.2257058823529413, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.2828560606060606, "width": 0.39651797385620924, "height": 0.012579545454545482, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3692"}, {"text": "In our second example, we show how Empath can help us discover trends in a dataset of movie reviews collected by Pang et al.", "label": "Author", "bboxes": [{"left": 0.7693169934640522, "top": 0.7227095959595959, "width": 0.15251633986928115, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.7365467171717172, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.7503838383838384, "width": 0.27245751633986937, "height": 0.012579545454545538, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3693"}, {"text": "We ran Empaths full set of categories over the truthful and deceptive reviews, and produced aggregate statistics for each.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.40651767676767675, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.42035479797979797, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3694"}, {"text": "Using normalized means of the category counts for each group, we then computed odds ratios and p-values for the categories most likely to appear in deceptive and truthful reviews.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.4341919191919192, "width": 0.39716993464052286, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.4480277777777778, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.461864898989899, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.4757020202020202, "width": 0.041640522875816974, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3695"}, {"text": "All the results we report are signicant after a Bonferroni correction (  = 2 . 5 e  5 ).", "label": "Author", "bboxes": [{"left": 0.13472549019607843, "top": 0.4757020202020202, "width": 0.35056372549019604, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.4860391414141414, "width": 0.18570424836601307, "height": 0.016909090909090874, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3696"}, {"text": "Our results provide new evidence in support of the Ott et al. study, suggesting that deceptive reviews convey stronger sentiment across both positively and negatively charged categories, and tend towards exaggerated language (Figure 4).", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.5109242424242424, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5247613636363636, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.5385972222222223, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5524343434343435, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3697"}, {"text": "The truthtellers more often discuss concrete ideas and phenomena like the ocean (1.6 odds,), vehicles (1.7 odds) or noises (1.7 odds), for example It seemed like a nice enough place with reasonably close beach access or they took forever to Valet our car .", "label": "Author", "bboxes": [{"left": 0.4201160130718954, "top": 0.6077828282828283, "width": 0.06518137254901962, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.6216199494949495, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6351679292929293, "width": 0.3971748366013072, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.6492941919191919, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6630441919191918, "width": 0.39717647058823524, "height": 0.012666666666666826, "page": 3}, {"left": 0.08811928104575163, "top": 0.6768800505050506, "width": 0.05961437908496732, "height": 0.012666666666666493, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3698"}, {"text": "We see a tendency towards more mundane activities among the truth-tellers through categories like eating (1.3 odds), cleaning (1.3 odds), or hygiene (1.2 odds).", "label": "Author", "bboxes": [{"left": 0.15605228758169937, "top": 0.6769671717171717, "width": 0.32923692810457517, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6905151515151515, "width": 0.3971748366013072, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.7043522727272727, "width": 0.3268039215686274, "height": 0.012868686868686807, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3699"}, {"text": "Using Empath, we can generate a new set of human validated terms that capture this idea, creating a new spatial category.", "label": "Author", "bboxes": [{"left": 0.6614542483660131, "top": 0.4325037878787879, "width": 0.2603790849673203, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4463409090909091, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4598888888888889, "width": 0.15626143790849678, "height": 0.012868686868686863, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3700"}, {"text": "To do so, we tell Empath to seed the category with the terms big, small, and circular.", "label": "Author", "bboxes": [{"left": 0.6929215686274509, "top": 0.46017803030303034, "width": 0.22891176470588248, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.47401515151515156, "width": 0.3971699346405231, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3701"}, {"text": "When we then add the new spatial category to our analysis, we nd it favors truthful reviews by 1.2 odds ( p < 0 . 001 ).", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.5856792929292929, "width": 0.39717320261437905, "height": 0.012868686868686918, "page": 3}, {"left": 0.5246633986928104, "top": 0.5970883838383838, "width": 0.39717483660130715, "height": 0.0161262626262626, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3702"}, {"text": "Truth-tellers use more spatial language, for example, the room that we originally were in had a huge square cut out of the wall that had exposed pipes, bricks, dirt and dust.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.6136426767676768, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6273926767676767, "width": 0.39717483660130715, "height": 0.012666666666666826, "page": 3}, {"left": 0.5246633986928104, "top": 0.6413169191919192, "width": 0.3763807189542483, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3703"}, {"text": "In our nal example, we use Empath to investigate the relationship between mood on twitter and time of day, replicating the work of Golder and Macy [13].", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.39716993464052286, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.2584248366013072, "height": 0.012579545454545538, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3704"}, {"text": "While the corpus of tweets analyzed by the original paper is not publicly available, we reproduce the papers ndings on a smaller corpus of 591,520 tweets from the PST time-zone, running LIWC on our data as an additional benchmark (Figure 6).", "label": "Author", "bboxes": [{"left": 0.35401633986928105, "top": 0.8367222222222221, "width": 0.13127287581699348, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.30923202614379086, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3705"}, {"text": "We nd a similar relationship on our data with both Empath and LIWC: a low in the morning (around 8am), peaking to a high around 11pm.", "label": "Author", "bboxes": [{"left": 0.8090539215686274, "top": 0.09529419191919192, "width": 0.11277941176470596, "height": 0.012579545454545468, "page": 4}, {"left": 0.5246633986928104, "top": 0.10913131313131313, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.12296717171717171, "width": 0.39716993464052297, "height": 0.01257954545454544, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3706"}, {"text": "Using ve shared emotional categories as features in a logistic regression to predict positive and negative movie reviews, we compare Empath and LIWC under a 10fold cross-validation t-test that exhibits low Type II error [8].", "label": "Author", "bboxes": [{"left": 0.18559640522875817, "top": 0.7096161616161616, "width": 0.2996928104575164, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7234532828282828, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.737290404040404, "width": 0.39716993464052286, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7511275252525252, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3707"}, {"text": "We nd no signicant difference between tools ( p = 0 . 43 ).", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.7622474747474748, "width": 0.3821666666666667, "height": 0.01612626262626249, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3708"}, {"text": "We adopt a data-driven approach using the ConceptNet knowledge base [21].", "label": "Author", "bboxes": [{"left": 0.8446552287581699, "top": 0.5159558080808081, "width": 0.0771781045751635, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.5297929292929293, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.5436300505050505, "width": 0.031189542483660126, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3709"}, {"text": "We prefer this approach to a purely manual one as it can potentially scale to thousands of other new categories.", "label": "Author", "bboxes": [{"left": 0.8039084967320261, "top": 0.5851401515151515, "width": 0.11792320261437894, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.5989772727272727, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.612814393939394, "width": 0.2249395424836601, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3710"}, {"text": "Specically, to generate Empaths category names and seed terms, we selected 200 common dependency relationships in ConceptNet, conditioned on 10,000 common words in our corpus.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.759989898989899, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7876641414141414, "width": 0.04746895424836595, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3711"}, {"text": "We then manually rened this list, eliminating redundant or sparse categories.", "label": "Author", "bboxes": [{"left": 0.5834133986928104, "top": 0.7876641414141414, "width": 0.338419934640523, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8015012626262626, "width": 0.19075490196078437, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3712"}, {"text": "For some categories we added additional seed terms to better represent the concept, resulting in a nal set of two to ve seed terms for each category.", "label": "Author", "bboxes": [{"left": 0.7224983660130718, "top": 0.8015012626262626, "width": 0.19933496732026157, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8153383838383839, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8291742424242424, "width": 0.3627238562091505, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3713"}, {"text": "Beyond these obviously polarized categories, we nd interesting trends in the topics associated with positive and negative reviews.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.43916540404040405, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.45300252525252527, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.4668396464646465, "width": 0.09200816993464052, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3714"}, {"text": "For example, no matter how shiny the supercial sheen is, this is still trash , and, like all garbage , it stinks , or a punchdrunk mess of a movie, or no free popcorn coupon can ever restore to us the time weve spent or wash the awful images from our mind.", "label": "Author", "bboxes": [{"left": 0.22314869281045752, "top": 0.5913724747474748, "width": 0.26215196078431374, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.6051224747474747, "width": 0.3971748366013072, "height": 0.012666666666666715, "page": 4}, {"left": 0.08811928104575163, "top": 0.6189595959595959, "width": 0.3971666666666667, "height": 0.012666666666666715, "page": 4}, {"left": 0.08811928104575163, "top": 0.6328838383838384, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.6466338383838384, "width": 0.2771290849673203, "height": 0.012666666666666715, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3715"}, {"text": "To train Empaths model, we adapt the deep learning skipgram architecture introduced by Mikolov et al.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.6823926767676768, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.696229797979798, "width": 0.30761928104575165, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3716"}, {"text": "We can then borrow these representations, called neural embeddings, to map words onto a vector space.", "label": "Author", "bboxes": [{"left": 0.43541013071895424, "top": 0.7792512626262627, "width": 0.0498790849673203, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7930883838383838, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.8069255050505051, "width": 0.22157026143790848, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3717"}, {"text": "We train our skip-gram network on the ction corpus from Wattpad, lemmatizing all words through a preprocessing step.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.23107323232323232, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.24490909090909088, "width": 0.39716993464052297, "height": 0.01257954545454551, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3718"}, {"text": "We dene and ignore stopwords as words with logadjusted probability greater than -8, according to the spaCy NLP toolkit.", "label": "Author", "bboxes": [{"left": 0.5748676470588235, "top": 0.32793181818181816, "width": 0.34696568627450985, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3417689393939394, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3556060606060606, "width": 0.08191503267973854, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3719"}, {"text": "The space is M ( n  h ) where n is the size of our vocabulary (40,000), and h the number of hidden nodes in the network (150).", "label": "Author", "bboxes": [{"left": 0.839624183006536, "top": 0.5702083333333333, "width": 0.08220915032679743, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.5813282828282827, "width": 0.39716666666666667, "height": 0.01612626262626271, "page": 5}, {"left": 0.5246633986928104, "top": 0.5978825757575758, "width": 0.3353104575163399, "height": 0.013145202020202085, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3720"}, {"text": "We use the neural embeddings created by our skip-gram network to construct a vector space model (VSM).", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.41045454545454546, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.4242916666666667, "width": 0.30719607843137264, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3721"}, {"text": "In our case, VSMs allow Empath to discover member terms for categories.", "label": "Author", "bboxes": [{"left": 0.796861111111111, "top": 0.4796388888888889, "width": 0.12497222222222237, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.4934760101010101, "width": 0.36054248366013075, "height": 0.012579545454545371, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3722"}, {"text": "While Empaths topical and emotional categories stem from different sources of knowledge, we generate member terms for both kinds of categories in the same way.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.2587462121212121, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.27258333333333334, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.28642045454545456, "width": 0.3085800653594772, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3723"}, {"text": "How do we connect a term like rampage with the category violent ?", "label": "Author", "bboxes": [{"left": 0.3428660130718954, "top": 0.3583371212121212, "width": 0.14242320261437913, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.371885101010101, "width": 0.30437254901960786, "height": 0.012868686868686863, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3724"}, {"text": "We can also search the vector spaces on multiple terms by querying on the vector sum of those terms  a kind of reasoning by analogy.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.8230176767676768, "width": 0.39717320261437905, "height": 0.012868686868686918, "page": 5}, {"left": 0.5246633986928104, "top": 0.8371439393939394, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8509810606060606, "width": 0.12503758169934642, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3725"}, {"text": "To generate the query vector for one of Empaths categories, we add the vector corresponding to the", "label": "Author", "bboxes": [{"left": 0.658718954248366, "top": 0.8509810606060606, "width": 0.2631143790849674, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.8648181818181818, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3726"}, {"text": "While Empath can be trained on any text corpus, for the analyses in this paper we use a dataset of modern ction from Wattpad, 1 a community of amateur writers.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.5885340909090909, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6023712121212121, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6146022727272727, "width": 0.2821045751633987, "height": 0.014185606060606148, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3727"}, {"text": "Similar relationships hold for the other primary and secondary emotions in Parrotts hierarchy, which we use to bootstrap Empaths base set of 32 emotional categories.", "label": "Author", "bboxes": [{"left": 0.3758954248366013, "top": 0.1958510101010101, "width": 0.10939379084967321, "height": 0.012579545454545454, "page": 5}, {"left": 0.08811928104575163, "top": 0.20968813131313133, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 5}, {"left": 0.08811928104575163, "top": 0.22352525252525254, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 5}, {"left": 0.08811928104575163, "top": 0.23736237373737376, "width": 0.19913725490196077, "height": 0.012579545454545454, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3728"}, {"text": "As we have discussed, each of Empaths categories is dened by seed words (e.g., lust : desire, passion; clothing : shirt, hat; social media : facebook, twitter).", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.42123232323232324, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.434780303030303, "width": 0.3971732026143791, "height": 0.012868686868686918, "page": 5}, {"left": 0.08811928104575163, "top": 0.44861742424242423, "width": 0.213718954248366, "height": 0.012868686868686918, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3729"}, {"text": "Empaths model uses these seed words to generate a candidate set of member terms for its categories, which we validate through paid crowdsourcing.", "label": "Author", "bboxes": [{"left": 0.30815522875817, "top": 0.44890656565656567, "width": 0.17714542483660128, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4627436868686869, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4765808080808081, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3730"}, {"text": "Using an embedding function v that maps a word to the vector space, we can nd the eight terms nearest to v ( depressed ) , by comparing its cosine similarity with all other terms in the space, and selecting the ones that are most similar:", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.725189393939394, "width": 0.39716830065359465, "height": 0.013145202020201863, "page": 5}, {"left": 0.5246633986928104, "top": 0.7390265151515151, "width": 0.39717483660130715, "height": 0.013409090909090926, "page": 5}, {"left": 0.5246633986928104, "top": 0.7528636363636364, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7667007575757575, "width": 0.3295964052287582, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3731"}, {"text": "Finally, to help researchers analyze text over new kinds of categories, we have released Empath as a web service and open source library.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.6740404040404041, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6878775252525252, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7017146464646465, "width": 0.09282189542483665, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3732"}, {"text": "The web service 3 allows users to analyze documents across Empaths built-in categories (Figure 1), generate new unsupervised categories, and request new categories be validated using our crowdsourcing pipeline.", "label": "Author", "bboxes": [{"left": 0.6224820261437909, "top": 0.7001073232323233, "width": 0.2993627450980392, "height": 0.014186868686868692, "page": 6}, {"left": 0.5246633986928104, "top": 0.7155517676767676, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7293876262626262, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.7432247474747475, "width": 0.3194215686274511, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3733"}, {"text": "We have validated Empaths 200 categories (with a vocabulary of more than 10,000 words) at a total cost of $840.", "label": "Author", "bboxes": [{"left": 0.7668431372549019, "top": 0.5251704545454545, "width": 0.15499019607843145, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5390075757575757, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.552844696969697, "width": 0.19379901960784318, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3734"}, {"text": "We divide the total number of words to be ltered across many separate tasks, where each task consists of twenty words to be rated for a given category.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.5291906565656566, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5430277777777778, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.556864898989899, "width": 0.2661895424836601, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3735"}, {"text": "We ask three independent workers to complete each task at a cost of $0.14 per task, resulting in an hourly wage in line with Ethical Guidelines for AMT research [35].", "label": "Author", "bboxes": [{"left": 0.4379166666666667, "top": 0.5845391414141414, "width": 0.0473725490196078, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5983762626262626, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.6122121212121212, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.6260492424242424, "width": 0.25033496732026145, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3736"}, {"text": "So, if we want to lter a category of 200 words, we would have 200 / 20 = 10 tasks, which must be completed by three workers, at a total cost of 10  3  0 .", "label": "Author", "bboxes": [{"left": 0.3728676470588235, "top": 0.6537234848484849, "width": 0.11242156862745101, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.6648434343434344, "width": 0.3971748366013072, "height": 0.01612626262626249, "page": 6}, {"left": 0.08811928104575163, "top": 0.6813977272727273, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.6925176767676768, "width": 0.11168627450980394, "height": 0.01612626262626249, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3737"}, {"text": "We limit tasks to Masters workers to ensure quality [26] and we aggregate crowdworker feedback by majority vote.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.7166186868686869, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.730455808080808, "width": 0.3532156862745098, "height": 0.012579545454545538, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3738"}, {"text": "If two of three workers believe a word is at least weakly related to the category, then Empath will keep the word, otherwise we remove it from the category.", "label": "Author", "bboxes": [{"left": 0.4463169934640523, "top": 0.730455808080808, "width": 0.03897222222222224, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7442929292929293, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.7581300505050504, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7719671717171718, "width": 0.1886388888888889, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3739"}, {"text": "We chose this lower threshold for term inclusion as it showed the highest agreement with LIWC in our benchmarks.", "label": "Author", "bboxes": [{"left": 0.28527124183006536, "top": 0.7719671717171718, "width": 0.20001797385620917, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.7858042929292929, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7996414141414142, "width": 0.17528921568627454, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3740"}, {"text": "We provide a sample of terms accepted and rejected by the crowd in Table 2.", "label": "Author", "bboxes": [{"left": 0.7258856209150327, "top": 0.3515795454545455, "width": 0.19594771241830067, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.3654166666666667, "width": 0.30161274509803926, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3741"}, {"text": "Our experience conrms the ndings of prior work that category construction is somewhat subjective.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.5742285353535354, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5880656565656566, "width": 0.28999019607843135, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3742"}, {"text": "Here we evaluate Empaths crowd ltered and unsupervised predictions against similar gold standard categories in LIWC.", "label": "Author", "bboxes": [{"left": 0.6799460784313726, "top": 0.829199494949495, "width": 0.24188725490196084, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.8430366161616161, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.8568737373737374, "width": 0.1652124183006537, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3743"}, {"text": "By ltering Empaths categories through the crowd, we offer the benets of both modern NLP and human validation: increasing category precision, and more carefully validating category contents.", "label": "Author", "bboxes": [{"left": 0.2706862745098039, "top": 0.3191199494949495, "width": 0.21460294117647066, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3329570707070707, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3467941919191919, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.36063131313131314, "width": 0.2864248366013072, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3744"}, {"text": "To validate each of Empaths categories, we have created a crowdsourcing pipeline on Amazon Mechanical Turk.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.3820151515151515, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3958522727272727, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3745"}, {"text": "Specically, we ask crowdworkers:", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.4096893939393939, "width": 0.22967647058823532, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3746"}, {"text": "We plot Empaths best and worst category correlations with LIWC in Figure 7.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.5788207070707071, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.5926578282828283, "width": 0.12689215686274513, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3747"}, {"text": "Finally, to test the importance of choosing seed terms, we re-ran our evaluation while permuting the seed words in Empaths categories.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.3475542929292929, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.3613914141414141, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.37522853535353534, "width": 0.11367483660130717, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3748"}, {"text": "Over one trial, we dropped one seed term from each category.", "label": "Author", "bboxes": [{"left": 0.6461029411764706, "top": 0.37522853535353534, "width": 0.2757303921568628, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.38906565656565656, "width": 0.12734803921568638, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3749"}, {"text": "Over another, we replaced one term from each category with a similar alternative (e.g., church to chapel, or kill to murder).", "label": "Author", "bboxes": [{"left": 0.6568790849673203, "top": 0.38906565656565656, "width": 0.2649542483660131, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4029027777777778, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.416739898989899, "width": 0.2106470588235294, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3750"}, {"text": "We then ran all tools over the documents in the test corpus, recorded their category word counts, then used these counts to compute Pearson correlations between all shared categories, as well as aggregate overall correlations.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.6077828282828283, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6216199494949495, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6354570707070707, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6492941919191919, "width": 0.2732222222222222, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3751"}, {"text": "In our experiment, these correlations speak to how well one tool approximates another.", "label": "Author", "bboxes": [{"left": 0.44550326797385625, "top": 0.6908042929292929, "width": 0.039785947712418224, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.12840686274509805, "height": 0.012579545454545538, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3752"}, {"text": "To compare all tools, we created a mixed textual dataset evenly divided among tweets [28], StackExchange opinions [5], movie reviews [32], hotel reviews [31], and chapters sampled from four classic novels on Project Gutenberg (David Coppereld, Moby Dick, Anna Karenina, and The Count of Monte Cristo) [1].", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.3574583333333334, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.37129545454545454, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.3851325757575757, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.3989696969696969, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.41280681818181814, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.42664393939393935, "width": 0.12111437908496729, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3753"}, {"text": "In permuting Empaths seed terms, we found it retained high unsupervised agreement with LIWC (between 0.82 and 0.88).", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.641715909090909, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6555530303030302, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3754"}, {"text": "The correlation between tools was most strongly affected when we dropped seeds that added a unique meaning to a category.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.6693901515151516, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6832272727272727, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.697064393939394, "width": 0.039833333333333276, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3755"}, {"text": "When we removed kill from the death s seed list, Empath lost the adversarial aspects of death (embodied in words like war, execute, or murder) and fell to 0.82 correlation with LIWC for that category.", "label": "Author", "bboxes": [{"left": 0.7227434640522876, "top": 0.7109015151515151, "width": 0.19908986928104577, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.724449494949495, "width": 0.39716993464052297, "height": 0.012868686868686918, "page": 7}, {"left": 0.5246633986928104, "top": 0.738574494949495, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7524116161616161, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7662487373737374, "width": 0.032851307189542545, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3756"}, {"text": "The broad reach of our dataset allows Empath to classify documents among a large number of categories.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.0959229797979798, "width": 0.3971699346405229, "height": 0.01257954545454544, "page": 7}, {"left": 0.08811928104575163, "top": 0.10976010101010102, "width": 0.29339215686274517, "height": 0.01257954545454544, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3757"}, {"text": "Human inspection and crowd ltering of Empaths categories (Table 2) provide some evidence, but ideally we would like to answer this question in a more quantitative way.", "label": "Author", "bboxes": [{"left": 0.3405212418300654, "top": 0.12359722222222222, "width": 0.14476797385620915, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.13743434343434344, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.15127020202020203, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.16510732323232322, "width": 0.15907679738562092, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3758"}, {"text": "Next we selected two parameters for Empath: the minimum cosine similarity for category inclusion and the seed words for each category (we xed the size of each category at a maximum of 200 words).", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.461864898989899, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4757020202020202, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.4895391414141414, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.5033762626262626, "width": 0.13587908496732026, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3759"}, {"text": "To choose these parameters, we divided our mixed text dataset into a training corpus of 900 documents and a test corpus of 3500 documents.", "label": "Author", "bboxes": [{"left": 0.22888235294117645, "top": 0.5033762626262626, "width": 0.25640686274509805, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.5172133838383838, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.531050505050505, "width": 0.2797009803921569, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3760"}, {"text": "We selected up to ve seed words that best approximated each LIWC category, and found that a minimum cosine similarity of 0.5 offered the best performance.", "label": "Author", "bboxes": [{"left": 0.37262254901960784, "top": 0.531050505050505, "width": 0.11266666666666669, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.5448876262626262, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.5587247474747474, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.5725618686868686, "width": 0.11598692810457516, "height": 0.012579545454545538, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3761"}, {"text": "We created crowd ltered versions of these categories as described in the previous section.", "label": "Author", "bboxes": [{"left": 0.20893954248366012, "top": 0.5725618686868686, "width": 0.2763496732026144, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.5863977272727273, "width": 0.3045098039215686, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3762"}, {"text": "Here we discuss our results and the limitations of our approach.", "label": "Author", "bboxes": [{"left": 0.8667777777777778, "top": 0.8782335858585859, "width": 0.055055555555555635, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3558366013071895, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3763"}, {"text": "Fortunately, LIWC has been extensively validated by researchers [33], so we can use it to benchmark Empaths predictions across the categories that they share in common.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.18649242424242424, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.20032954545454545, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.21416666666666667, "width": 0.3784003267973856, "height": 0.012579545454545454, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3764"}, {"text": "If we can demonstrate that Empath provides very similar results across these categories, this would suggest that Empaths predictions are close to achieving gold standard accuracy.", "label": "Author", "bboxes": [{"left": 0.47444607843137254, "top": 0.21416666666666667, "width": 0.010843137254901991, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.2280037878787879, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.24183964646464648, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.25567676767676767, "width": 0.3536241830065359, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3765"}, {"text": "To anchor this analysis, we collected benchmark Pearson correlations against LIWC for GI and EmoLex (two existing human validated lexicons).", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.7398636363636364, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7537007575757576, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7675366161616162, "width": 0.16062254901960782, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3766"}, {"text": "We found a benchmark correlation of 0.876 between GI and LIWC over positive emotion , negative emotion , religion , work , and achievement , and a correlation of 0.899 between EmoLex and LIWC over positive emotion , negative emotion , anger , and sadness .", "label": "Author", "bboxes": [{"left": 0.25631209150326795, "top": 0.7675366161616162, "width": 0.2289771241830066, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.781084595959596, "width": 0.3971732026143791, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.7949217171717171, "width": 0.3971764705882353, "height": 0.012868686868686918, "page": 7}, {"left": 0.08811928104575163, "top": 0.8087588383838384, "width": 0.3971748366013072, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.8225959595959595, "width": 0.28770261437908495, "height": 0.012868686868686918, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3767"}, {"text": "We take this as evidence that gold standard lexicons can disagree: if Empath approximates their performance against LIWC, it agrees with LIWC as well as other carefully-validated dictionaries agree with LIWC.", "label": "Author", "bboxes": [{"left": 0.29355718954248367, "top": 0.8505593434343435, "width": 0.19173202614379087, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.3797826797385621, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3768"}, {"text": "Here we compare the predictions of Empath and LIWC over 12 shared categories: sadness , anger , positive emotion , negative emotion , sexual , money , death , achievement , home , religion , work , and health .", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.27706186868686866, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.2906098484848485, "width": 0.3971732026143791, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.3044469696969697, "width": 0.3971748366013072, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.3182840909090909, "width": 0.1501225490196078, "height": 0.012868686868686918, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3769"}, {"text": "As we see in our results, this scenario does not happen often, and when it does happen the effect size is small.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.7128661616161617, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7267032828282828, "width": 0.364171568627451, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3770"}, {"text": "We suggest that crowd validation offers the qualitative benet of removing false positives from analyses, while on the whole performing almost identically to (and usually slightly better than) the unltered version of Empath.", "label": "Author", "bboxes": [{"left": 0.4639950980392156, "top": 0.7267032828282828, "width": 0.021294117647058908, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7405404040404041, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7543762626262627, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7682133838383838, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.7820505050505051, "width": 0.25204411764705886, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3771"}, {"text": "When we inspected the output of the crowd ltering step to determine what had caused this effect, we found in a small number of cases in which the crowd was overzealous.", "label": "Author", "bboxes": [{"left": 0.24402124183006538, "top": 0.5392739898989899, "width": 0.24126797385620916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5531111111111111, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5669482323232323, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5807853535353535, "width": 0.08132843137254901, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3772"}, {"text": "In our case, when the crowd lters out a common word shared by LIWC (like semester), this causes overall agreement across the corpus to decrease (through additional false negatives), despite the appropriate removal of many other less common words.", "label": "Author", "bboxes": [{"left": 0.16008823529411764, "top": 0.6361338383838384, "width": 0.3252009803921569, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6499709595959596, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6638068181818182, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6776439393939393, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.6914810606060606, "width": 0.2107450980392157, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3773"}, {"text": "Contrary to this critique, we have found that ction is a useful training dataset for Empath given its abundance of concrete descriptors and emotional terms.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.4770075757575758, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.490844696969697, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.5046818181818182, "width": 0.24756699346405242, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3774"}, {"text": "When we replaced the word embeddings learned by our model with alternative embeddings trained on Google News [23], we found its average unsupervised correlation with LIWC decreased to 0.84.", "label": "Author", "bboxes": [{"left": 0.7772287581699346, "top": 0.5046818181818182, "width": 0.14460457516339875, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.5185189393939393, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.5323560606060606, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.5461931818181818, "width": 0.3648071895424836, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3775"}, {"text": "We created these categories in a data-driven manner.", "label": "Author", "bboxes": [{"left": 0.8474869281045753, "top": 0.8153383838383839, "width": 0.07434640522875813, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.8291742424242424, "width": 0.27227777777777784, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3776"}, {"text": "We have not evaluated Empath over these more qualitative aspects of usability.", "label": "Author", "bboxes": [{"left": 0.7721813725490196, "top": 0.8430113636363636, "width": 0.14965196078431375, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.8568484848484849, "width": 0.36221895424836614, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3777"}, {"text": "Second, we have not tested how well Empaths categories generalize beyond the core set it shares with LIWC.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3360424836601307, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3778"}, {"text": "Empaths design decisions suggest a set of limitations, many of which we hope to address in future work.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.6832575757575757, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6970946969696971, "width": 0.2869133986928105, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3779"}, {"text": "In this paper, we used Bonferroni correction to handle the issue, but there are more mature methods available.", "label": "Author", "bboxes": [{"left": 0.21896732026143792, "top": 0.3937436868686869, "width": 0.26632189542483664, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.40758080808080804, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.42141792929292926, "width": 0.0621029411764706, "height": 0.012579545454545482, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3780"}, {"text": "These questions are ever changing, as is our use of language.", "label": "Author", "bboxes": [{"left": 0.4464150326797386, "top": 0.6292979797979797, "width": 0.038874183006535945, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.643135101010101, "width": 0.36145424836601314, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3781"}, {"text": "Finally, while ction provides a powerful model for generating lexical categories, we have also seen that, for certain topics (e.g. death in Google News), other corpora may have even greater potential.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.17202651515151515, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.18586363636363637, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.19941161616161615, "width": 0.397171568627451, "height": 0.01286868686868689, "page": 9}, {"left": 0.08811928104575163, "top": 0.21353661616161618, "width": 0.1446699346405229, "height": 0.012579545454545454, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3782"}, {"text": "We see potential for training Empath on other text beyond ction.", "label": "Author", "bboxes": [{"left": 0.30272712418300657, "top": 0.268885101010101, "width": 0.18256535947712416, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.2827222222222222, "width": 0.2382565359477124, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3783"}, {"text": "While Empaths categories are all generated and validated in the same way, we have seen though our evaluation that choice of seed words can be important.", "label": "Author", "bboxes": [{"left": 0.37949346405228757, "top": 0.0814570707070707, "width": 0.10579575163398702, "height": 0.012579545454545468, "page": 9}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3971699346405229, "height": 0.012579545454545468, "page": 9}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.11450490196078432, "height": 0.01257954545454544, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3784"}, {"text": "And how do we best discover them?", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.13680429292929294, "width": 0.2356683006535948, "height": 0.012579545454545454, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3785"}, {"text": "In future work, we hope to investigate these questions more closely.", "label": "Author", "bboxes": [{"left": 0.3288169934640523, "top": 0.13680429292929294, "width": 0.15647222222222223, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.2801078431372549, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3786"}, {"text": "Special thanks to our reviewers and colleagues at Stanford for their helpful feedback.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.6988080808080809, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.712645202020202, "width": 0.17174019607843138, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3787"}, {"text": "This work is supported by a NSF Graduate Fellowship and a NIH and Stanford Medical Scientist Training Grant.", "label": "Author", "bboxes": [{"left": 0.26668137254901964, "top": 0.712645202020202, "width": 0.2186078431372549, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.7264823232323233, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7403194444444444, "width": 0.12467973856209151, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3788"}, {"text": "This papers contributions include:", "label": "Contribution", "bboxes": [{"left": 0.5246633986928104, "top": 0.5210606060606061, "width": 0.22620915032679734, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3789"}, {"text": "Empath contributes a different perspective, that ction can be an appropriate tool for learning a breadth of topical and emotional categories, to the benet of social science.", "label": "Contribution", "bboxes": [{"left": 0.7263741830065359, "top": 0.37342550505050504, "width": 0.1954591503267975, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.38726262626262625, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.40109974747474747, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4149368686868687, "width": 0.13583006535947706, "height": 0.012579545454545482, "page": 2}], "section": "Data Mining and Modeling", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3790"}, {"text": "But like other popular lexicons, LIWC is small: it has only 40 topical and emotional categories, many of which contain fewer than 100 words.", "label": "Novelty", "bboxes": [{"left": 0.3958888888888889, "top": 0.2292260101010101, "width": 0.08940359477124182, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.2430618686868687, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.2568989898989899, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.2707361111111111, "width": 0.043692810457516315, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3791"}, {"text": "While Empath presents an approach that can be trained on any text corpora, in this paper we use 1.8 billion words of modern amateur ction.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.562570707070707, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5764078282828282, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5902449494949494, "width": 0.10133496732026144, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3792"}, {"text": "On the other hand, amateur ction tends to be explicit about both scenesetting and emotion, with a higher density of adjective descriptors (e.g., the broken vending machine perplexed her.).", "label": "Novelty", "bboxes": [{"left": 0.39973202614379083, "top": 0.6594305555555555, "width": 0.08556209150326799, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6732676767676767, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.687104797979798, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7006515151515152, "width": 0.3971748366013072, "height": 0.012868686868686696, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3793"}, {"text": "And like LIWC (but unlike other machine learning models), Empaths contents are validated by humans.", "label": "Novelty", "bboxes": [{"left": 0.15048366013071895, "top": 0.5273497474747475, "width": 0.3348055555555556, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5411868686868687, "width": 0.3468349673202615, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3794"}, {"text": "We show how the open-ended nature of Empaths model can replicate and extend classic work in classifying deceptive language [31], identifying the patterns of language in movie reviews [32], and analyzing mood on twitter [13].", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.21538888888888888, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.22922474747474747, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.2430618686868687, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.2568989898989899, "width": 0.3074411764705882, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3795"}, {"text": "(e.g., power, weakness), but fewer emotions [40].", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.241344696969697, "width": 0.31965032679738564, "height": 0.012579545454545454, "page": 2}], "section": "Extracting Signal from Text", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3796"}, {"text": "While Empaths analyses are similarly driven by dictionary-based word counts, Empath operates over a more extensive set of categories, and can generate and validate new categories on demand using unsupervised language modeling.", "label": "Novelty", "bboxes": [{"left": 0.44550326797385625, "top": 0.2690189393939394, "width": 0.03978594771241828, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.2828560606060606, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.29669318181818183, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.31053030303030305, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.32436742424242426, "width": 0.31333333333333335, "height": 0.012579545454545482, "page": 2}], "section": "Extracting Signal from Text", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3797"}, {"text": "Augur likewise mines ction, but it does so to learn human activities for interactive systems [10].", "label": "Novelty", "bboxes": [{"left": 0.6884771241830066, "top": 0.3180782828282828, "width": 0.23335620915032684, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.3319141414141414, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 2}], "section": "Data Mining and Modeling", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3798"}, {"text": "Empath contributes a different perspective, that ction can be an appropriate tool for learning a breadth of topical and emotional categories, to the benet of social science.", "label": "Novelty", "bboxes": [{"left": 0.7263741830065359, "top": 0.37342550505050504, "width": 0.1954591503267975, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.38726262626262625, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.40109974747474747, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4149368686868687, "width": 0.13583006535947706, "height": 0.012579545454545482, "page": 2}], "section": "Data Mining and Modeling", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3799"}, {"text": "Work in sentiment analysis, in combination with deep learning, has developed powerful techniques to classify text across positive and negative polarity [39], but has also beneted from simpler, transparent models and rules [15].", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.3457512626262626, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3595883838383838, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.37342550505050504, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.38726262626262625, "width": 0.3325751633986928, "height": 0.012579545454545482, "page": 2}], "section": "Extracting Signal from Text", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3800"}, {"text": "Truthful reviews, on the other hand, display higher odds ratios for none of Empaths emotional categories.", "label": "Novelty", "bboxes": [{"left": 0.6400637254901961, "top": 0.3461654040404041, "width": 0.2817696078431373, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3600025252525253, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3801"}, {"text": "While the original study provided some evidence that liars use less spatially descriptive language, it wasnt able to test the theory directly.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.40482954545454547, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4186666666666667, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4325037878787879, "width": 0.12611111111111117, "height": 0.012579545454545427, "page": 3}], "section": "Spatial language in lies", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3802"}, {"text": "While the corpus of tweets analyzed by the original paper is not publicly available, we reproduce the papers ndings on a smaller corpus of 591,520 tweets from the PST time-zone, running LIWC on our data as an additional benchmark (Figure 6).", "label": "Novelty", "bboxes": [{"left": 0.35401633986928105, "top": 0.8367222222222221, "width": 0.13127287581699348, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.30923202614379086, "height": 0.012579545454545427, "page": 4}], "section": "Exploring the movie dataset", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3803"}, {"text": "However, Empath covers a broader set of categories than other tools, and users can generate and validate new categories with a few seed words.", "label": "Novelty", "bboxes": [{"left": 0.7421470588235295, "top": 0.31568813131313134, "width": 0.1796862745098039, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.3295252525252525, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.3433623737373737, "width": 0.3681437908496733, "height": 0.012579545454545482, "page": 4}], "section": "EMPATH", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3804"}, {"text": "Using a 1-way ANOVA to test for changes in mean negative affect by hour, Empath reports a highly signicant difference ( F (23 , 591520) = 17 .", "label": "Novelty", "bboxes": [{"left": 0.719063725490196, "top": 0.15064141414141416, "width": 0.20276960784313736, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.16447853535353535, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.17559848484848486, "width": 0.3512222222222223, "height": 0.016126262626262627, "page": 4}], "section": "Exploring the movie dataset", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3805"}, {"text": "We nd no signicant difference between tools ( p = 0 . 43 ).", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.7622474747474748, "width": 0.3821666666666667, "height": 0.01612626262626249, "page": 4}], "section": "Exploring the movie dataset", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3806"}, {"text": "For example, the network might learn that death predicts a nearby occurrence of the word carrion, but not of incest.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.7377398989898989, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.7515770202020202, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3807"}, {"text": "While Empaths topical and emotional categories stem from different sources of knowledge, we generate member terms for both kinds of categories in the same way.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.2587462121212121, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.27258333333333334, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.28642045454545456, "width": 0.3085800653594772, "height": 0.012579545454545427, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3808"}, {"text": "While Empath can be trained on any text corpus, for the analyses in this paper we use a dataset of modern ction from Wattpad, 1 a community of amateur writers.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.5885340909090909, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6023712121212121, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6146022727272727, "width": 0.2821045751633987, "height": 0.014185606060606148, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3809"}, {"text": "Finally, to test the importance of choosing seed terms, we re-ran our evaluation while permuting the seed words in Empaths categories.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.3475542929292929, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.3613914141414141, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.37522853535353534, "width": 0.11367483660130717, "height": 0.012579545454545482, "page": 7}], "section": "Method", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3810"}, {"text": "On the other hand, replacing seeds with alternative forms or synonyms (e.g., hate to hatred, or kill to murder) usually had little impact on Empaths correlations with LIWC.", "label": "Novelty", "bboxes": [{"left": 0.658328431372549, "top": 0.7800858585858585, "width": 0.26350490196078435, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7939229797979799, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.807760101010101, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8215972222222223, "width": 0.13880882352941182, "height": 0.012579545454545427, "page": 7}], "section": "Results", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3811"}, {"text": "But how accurate are these categorical associations?", "label": "Novelty", "bboxes": [{"left": 0.38908169934640524, "top": 0.10976010101010102, "width": 0.09620751633986929, "height": 0.01257954545454544, "page": 7}, {"left": 0.08811928104575163, "top": 0.12359722222222222, "width": 0.24764869281045754, "height": 0.012579545454545454, "page": 7}], "section": "EVALUATION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3812"}, {"text": "Human inspection and crowd ltering of Empaths categories (Table 2) provide some evidence, but ideally we would like to answer this question in a more quantitative way.", "label": "Novelty", "bboxes": [{"left": 0.3405212418300654, "top": 0.12359722222222222, "width": 0.14476797385620915, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.13743434343434344, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.15127020202020203, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.16510732323232322, "width": 0.15907679738562092, "height": 0.012579545454545482, "page": 7}], "section": "EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3813"}, {"text": "While EmoLex and GI are commonly regarded as gold standards, they correlate imperfectly with LIWC.", "label": "Novelty", "bboxes": [{"left": 0.3844330065359477, "top": 0.822885101010101, "width": 0.10086274509803922, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.20052124183006537, "height": 0.012579545454545427, "page": 7}], "section": "Method", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3814"}, {"text": "We suggest that crowd validation offers the qualitative benet of removing false positives from analyses, while on the whole performing almost identically to (and usually slightly better than) the unltered version of Empath.", "label": "Novelty", "bboxes": [{"left": 0.4639950980392156, "top": 0.7267032828282828, "width": 0.021294117647058908, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7405404040404041, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7543762626262627, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7682133838383838, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.7820505050505051, "width": 0.25204411764705886, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3815"}, {"text": "While ction allows Empath to learn an approximation of the gold-standard categories that dene tools like LIWC, its data-driven reasoning may succeed less well on corner cases of analysis and connotation.", "label": "Novelty", "bboxes": [{"left": 0.3780277777777778, "top": 0.8367222222222221, "width": 0.10726143790849674, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.08093790849673203, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3816"}, {"text": "While adding a crowd lter to Empath improves its overall correlations with LIWC, the improvement is not statistically signicant.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.45625252525252524, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.47008964646464646, "width": 0.39716993464052286, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.4839267676767677, "width": 0.07190359477124182, "height": 0.012579545454545482, "page": 8}], "section": "The role of human validation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3817"}, {"text": "For example, the word semester appears in LIWCs work category, but the crowd removed it from Empath.", "label": "Novelty", "bboxes": [{"left": 0.18035457516339867, "top": 0.5807853535353535, "width": 0.3049346405228759, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5943333333333333, "width": 0.39718137254901964, "height": 0.01286868686868703, "page": 8}, {"left": 0.08811928104575163, "top": 0.608459595959596, "width": 0.03210130718954249, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3818"}, {"text": "In our case, when the crowd lters out a common word shared by LIWC (like semester), this causes overall agreement across the corpus to decrease (through additional false negatives), despite the appropriate removal of many other less common words.", "label": "Novelty", "bboxes": [{"left": 0.16008823529411764, "top": 0.6361338383838384, "width": 0.3252009803921569, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6499709595959596, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6638068181818182, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6776439393939393, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.6914810606060606, "width": 0.2107450980392157, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3819"}, {"text": "Contrary to this critique, we have found that ction is a useful training dataset for Empath given its abundance of concrete descriptors and emotional terms.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.4770075757575758, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.490844696969697, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.5046818181818182, "width": 0.24756699346405242, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3820"}, {"text": "Fiction may suffer from the overly fanciful plot events and motifs that surround death (e.g. suffocation, torture), but it captures more relevant words around most categories.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.6153775252525253, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6292146464646465, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6430517676767677, "width": 0.3500424836601309, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3821"}, {"text": "First, while Empath reports high Pearson correlations with LIWCs categories, it is possible that other more qualitative properties are important to lexical categories.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.3006029411764706, "height": 0.012579545454545538, "page": 8}], "section": "Limitations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3822"}, {"text": "In this paper, we used Bonferroni correction to handle the issue, but there are more mature methods available.", "label": "Novelty", "bboxes": [{"left": 0.21896732026143792, "top": 0.3937436868686869, "width": 0.26632189542483664, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.40758080808080804, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.42141792929292926, "width": 0.0621029411764706, "height": 0.012579545454545482, "page": 9}], "section": "Statistical false positives", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3823"}, {"text": "Finally, while ction provides a powerful model for generating lexical categories, we have also seen that, for certain topics (e.g. death in Google News), other corpora may have even greater potential.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.17202651515151515, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.18586363636363637, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.19941161616161615, "width": 0.397171568627451, "height": 0.01286868686868689, "page": 9}, {"left": 0.08811928104575163, "top": 0.21353661616161618, "width": 0.1446699346405229, "height": 0.012579545454545454, "page": 9}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3824"}, {"text": "Could different datasets be targeted at specic categories?", "label": "Novelty", "bboxes": [{"left": 0.23816013071895423, "top": 0.21353661616161618, "width": 0.2471290849673203, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.12734803921568633, "height": 0.012579545454545454, "page": 9}], "section": "Limitations", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3825"}, {"text": "While Empaths categories are all generated and validated in the same way, we have seen though our evaluation that choice of seed words can be important.", "label": "Novelty", "bboxes": [{"left": 0.37949346405228757, "top": 0.0814570707070707, "width": 0.10579575163398702, "height": 0.012579545454545468, "page": 9}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3971699346405229, "height": 0.012579545454545468, "page": 9}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.11450490196078432, "height": 0.01257954545454544, "page": 9}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3826"}, {"text": "Through Empath, we aim to empower researchers with the ability to generate and validate these categories.", "label": "Objective", "bboxes": [{"left": 0.15673366013071896, "top": 0.649729797979798, "width": 0.3285555555555555, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6635669191919191, "width": 0.3687140522875817, "height": 0.012579545454545538, "page": 2}], "section": "Applications for Text Analysis", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3827"}, {"text": "One of Empaths goals is to embed modern NLP techniques in a way that offers the transparency of dictionaries like LIWC.", "label": "Objective", "bboxes": [{"left": 0.4085114379084967, "top": 0.4287739898989899, "width": 0.07678758169934641, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4426111111111111, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.45644823232323234, "width": 0.3527124183006536, "height": 0.012579545454545482, "page": 2}], "section": "Extracting Signal from Text", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3828"}, {"text": "For exploratory research questions, Empath provides a highlevel view over many potential categories, some of which a researcher may not have thought to investigate.", "label": "Objective", "bboxes": [{"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3195522875816994, "height": 0.012579545454545427, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3829"}, {"text": "Prior work in human-validated category construction has typically relied upon less efcient approaches, for example using crowd workers to annotate the 10,000 most common dictionary words with scores over all categories in question [27].", "label": "Objective", "bboxes": [{"left": 0.8486764705882353, "top": 0.4006376262626263, "width": 0.07315686274509814, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.4144747474747475, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.4283118686868687, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.44214898989898993, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.45598611111111115, "width": 0.3606078431372548, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3830"}, {"text": "Prior work has adopted a similar question and scale [27].", "label": "Objective", "bboxes": [{"left": 0.08811928104575163, "top": 0.5078068181818182, "width": 0.37057026143790855, "height": 0.012579545454545538, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3831"}, {"text": "Human inspection and crowd ltering of Empaths categories (Table 2) provide some evidence, but ideally we would like to answer this question in a more quantitative way.", "label": "Objective", "bboxes": [{"left": 0.3405212418300654, "top": 0.12359722222222222, "width": 0.14476797385620915, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.13743434343434344, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.15127020202020203, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.16510732323232322, "width": 0.15907679738562092, "height": 0.012579545454545482, "page": 7}], "section": "EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3832"}, {"text": "For example, Holms method and FDR are often used in statistical genomics to test thousands of hypotheses.", "label": "Objective", "bboxes": [{"left": 0.15963235294117645, "top": 0.42141792929292926, "width": 0.3256568627450981, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4352550505050505, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 9}], "section": "Statistical false positives", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3833"}, {"text": "In depth, its user-dened categories provide a exible means by which researchers can ask domain-specic questions.", "label": "Objective", "bboxes": [{"left": 0.4717287581699346, "top": 0.6016237373737373, "width": 0.013560457516339941, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.6154608585858585, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6292979797979797, "width": 0.3509869281045752, "height": 0.012579545454545538, "page": 9}], "section": "CONCLUSION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3834"}, {"text": "In future work, we hope to investigate these questions more closely.", "label": "Objective", "bboxes": [{"left": 0.3288169934640523, "top": 0.13680429292929294, "width": 0.15647222222222223, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.2801078431372549, "height": 0.012579545454545427, "page": 9}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3835"}, {"text": "As we gain access to ever larger and more diverse datasets, it becomes important to scale our ability to conduct such analyses with breadth and accuracy.", "label": "Objective", "bboxes": [{"left": 0.6014673202614379, "top": 0.759989898989899, "width": 0.3203660130718955, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.7876641414141414, "width": 0.26721568627450976, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3836"}, {"text": "To build Empath, we extend a deep learning skip-gram network to capture words in a neural embedding [23].", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.75, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7638371212121211, "width": 0.33259150326797393, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3837"}, {"text": "Our evaluation validates Empath by comparing its analyses against LIWC, a lexicon of gold standard categories that have been psychometrically validated.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.34746843434343433, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.36130555555555555, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.37514267676767676, "width": 0.21515686274509804, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3838"}, {"text": "While Empath presents an approach that can be trained on any text corpora, in this paper we use 1.8 billion words of modern amateur ction.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.562570707070707, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5764078282828282, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5902449494949494, "width": 0.10133496732026144, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3839"}, {"text": "To address these problems, we present Empath : a living lexicon mined from modern text on the web.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.34717929292929295, "width": 0.39716830065359476, "height": 0.012868686868686863, "page": 1}, {"left": 0.08811928104575163, "top": 0.36130555555555555, "width": 0.30892156862745096, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3840"}, {"text": "We show how the open-ended nature of Empaths model can replicate and extend classic work in classifying deceptive language [31], identifying the patterns of language in movie reviews [32], and analyzing mood on twitter [13].", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.21538888888888888, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.22922474747474747, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.2430618686868687, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.2568989898989899, "width": 0.3074411764705882, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3841"}, {"text": "Through Empath, we aim to empower researchers with the ability to generate and validate these categories.", "label": "Method", "bboxes": [{"left": 0.15673366013071896, "top": 0.649729797979798, "width": 0.3285555555555555, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6635669191919191, "width": 0.3687140522875817, "height": 0.012579545454545538, "page": 2}], "section": "Applications for Text Analysis", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3842"}, {"text": "To motivate the opportunities that Empath creates, we rst present three example analyses that illustrate its breadth and exibility.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.6780328282828282, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.6918699494949495, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7057070707070706, "width": 0.06609150326797386, "height": 0.012579545454545538, "page": 2}], "section": "EMPATH APPLICATIONS", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3843"}, {"text": "What kinds of words accompany our lies?", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.2762336601307189, "height": 0.012579545454545538, "page": 2}], "section": "Example 1: Understanding deception in hotel reviews", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3844"}, {"text": "In Empath, we adapt these techniques to mine natural language for its relation to emotional and topical categories.", "label": "Method", "bboxes": [{"left": 0.7879395424836602, "top": 0.4426111111111111, "width": 0.13389379084967323, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.45644823232323234, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4702840909090909, "width": 0.21496078431372545, "height": 0.012579545454545482, "page": 2}], "section": "Data Mining and Modeling", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3845"}, {"text": "We draw on some of this knowledge, like the ConceptNet hierarchy, when seeding Empaths categories.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.5470176767676768, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.5608535353535354, "width": 0.2852042483660131, "height": 0.012579545454545427, "page": 2}], "section": "Data Mining and Modeling", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3846"}, {"text": "We borrow from the last of these approaches in constructing of Empaths unsupervised model.", "label": "Novelty", "bboxes": [{"left": 0.6961274509803921, "top": 0.2690189393939394, "width": 0.2257058823529413, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.2828560606060606, "width": 0.39651797385620924, "height": 0.012579545454545482, "page": 2}], "section": "Data Mining and Modeling", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3847"}, {"text": "In our second example, we show how Empath can help us discover trends in a dataset of movie reviews collected by Pang et al.", "label": "Method", "bboxes": [{"left": 0.7693169934640522, "top": 0.7227095959595959, "width": 0.15251633986928115, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.7365467171717172, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.7503838383838384, "width": 0.27245751633986937, "height": 0.012579545454545538, "page": 3}], "section": "Example 2: Understanding language in movie reviews", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3848"}, {"text": "We ran Empaths full set of categories over the truthful and deceptive reviews, and produced aggregate statistics for each.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.40651767676767675, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.42035479797979797, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3849"}, {"text": "Our results provide new evidence in support of the Ott et al. study, suggesting that deceptive reviews convey stronger sentiment across both positively and negatively charged categories, and tend towards exaggerated language (Figure 4).", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.5109242424242424, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5247613636363636, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.5385972222222223, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5524343434343435, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3850"}, {"text": "Using Empath, we can generate a new set of human validated terms that capture this idea, creating a new spatial category.", "label": "Method", "bboxes": [{"left": 0.6614542483660131, "top": 0.4325037878787879, "width": 0.2603790849673203, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4463409090909091, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4598888888888889, "width": 0.15626143790849678, "height": 0.012868686868686863, "page": 3}], "section": "Spatial language in lies", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3851"}, {"text": "When we then add the new spatial category to our analysis, we nd it favors truthful reviews by 1.2 odds ( p < 0 . 001 ).", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.5856792929292929, "width": 0.39717320261437905, "height": 0.012868686868686918, "page": 3}, {"left": 0.5246633986928104, "top": 0.5970883838383838, "width": 0.39717483660130715, "height": 0.0161262626262626, "page": 3}], "section": "Spatial language in lies", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3852"}, {"text": "In our nal example, we use Empath to investigate the relationship between mood on twitter and time of day, replicating the work of Golder and Macy [13].", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.39716993464052286, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.2584248366013072, "height": 0.012579545454545538, "page": 4}], "section": "Exploring the movie dataset", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3853"}, {"text": "We nd a similar relationship on our data with both Empath and LIWC: a low in the morning (around 8am), peaking to a high around 11pm.", "label": "Method", "bboxes": [{"left": 0.8090539215686274, "top": 0.09529419191919192, "width": 0.11277941176470596, "height": 0.012579545454545468, "page": 4}, {"left": 0.5246633986928104, "top": 0.10913131313131313, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.12296717171717171, "width": 0.39716993464052297, "height": 0.01257954545454544, "page": 4}], "section": "Exploring the movie dataset", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3854"}, {"text": "Using ve shared emotional categories as features in a logistic regression to predict positive and negative movie reviews, we compare Empath and LIWC under a 10fold cross-validation t-test that exhibits low Type II error [8].", "label": "Method", "bboxes": [{"left": 0.18559640522875817, "top": 0.7096161616161616, "width": 0.2996928104575164, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7234532828282828, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.737290404040404, "width": 0.39716993464052286, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7511275252525252, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}], "section": "Exploring the movie dataset", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3855"}, {"text": "We adopt a data-driven approach using the ConceptNet knowledge base [21].", "label": "Method", "bboxes": [{"left": 0.8446552287581699, "top": 0.5159558080808081, "width": 0.0771781045751635, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.5297929292929293, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.5436300505050505, "width": 0.031189542483660126, "height": 0.012579545454545427, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3856"}, {"text": "Specically, to generate Empaths category names and seed terms, we selected 200 common dependency relationships in ConceptNet, conditioned on 10,000 common words in our corpus.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.759989898989899, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7876641414141414, "width": 0.04746895424836595, "height": 0.012579545454545427, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3857"}, {"text": "Beyond these obviously polarized categories, we nd interesting trends in the topics associated with positive and negative reviews.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.43916540404040405, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.45300252525252527, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.4668396464646465, "width": 0.09200816993464052, "height": 0.012579545454545427, "page": 4}], "section": "Exploring the movie dataset", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3858"}, {"text": "To train Empaths model, we adapt the deep learning skipgram architecture introduced by Mikolov et al.", "label": "Objective", "bboxes": [{"left": 0.08811928104575163, "top": 0.6823926767676768, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.696229797979798, "width": 0.30761928104575165, "height": 0.012579545454545427, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3859"}, {"text": "We train our skip-gram network on the ction corpus from Wattpad, lemmatizing all words through a preprocessing step.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.23107323232323232, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.24490909090909088, "width": 0.39716993464052297, "height": 0.01257954545454551, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3860"}, {"text": "The space is M ( n  h ) where n is the size of our vocabulary (40,000), and h the number of hidden nodes in the network (150).", "label": "Method", "bboxes": [{"left": 0.839624183006536, "top": 0.5702083333333333, "width": 0.08220915032679743, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.5813282828282827, "width": 0.39716666666666667, "height": 0.01612626262626271, "page": 5}, {"left": 0.5246633986928104, "top": 0.5978825757575758, "width": 0.3353104575163399, "height": 0.013145202020202085, "page": 5}], "section": "Building categories with a vector space", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3861"}, {"text": "We use the neural embeddings created by our skip-gram network to construct a vector space model (VSM).", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.41045454545454546, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.4242916666666667, "width": 0.30719607843137264, "height": 0.012579545454545482, "page": 5}], "section": "Building categories with a vector space", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3862"}, {"text": "While Empaths topical and emotional categories stem from different sources of knowledge, we generate member terms for both kinds of categories in the same way.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.2587462121212121, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.27258333333333334, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.28642045454545456, "width": 0.3085800653594772, "height": 0.012579545454545427, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3863"}, {"text": "How do we connect a term like rampage with the category violent ?", "label": "Method", "bboxes": [{"left": 0.3428660130718954, "top": 0.3583371212121212, "width": 0.14242320261437913, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.371885101010101, "width": 0.30437254901960786, "height": 0.012868686868686863, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3864"}, {"text": "We can also search the vector spaces on multiple terms by querying on the vector sum of those terms  a kind of reasoning by analogy.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.8230176767676768, "width": 0.39717320261437905, "height": 0.012868686868686918, "page": 5}, {"left": 0.5246633986928104, "top": 0.8371439393939394, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8509810606060606, "width": 0.12503758169934642, "height": 0.012579545454545538, "page": 5}], "section": "Building categories with a vector space", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3865"}, {"text": "While Empath can be trained on any text corpus, for the analyses in this paper we use a dataset of modern ction from Wattpad, 1 a community of amateur writers.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.5885340909090909, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6023712121212121, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6146022727272727, "width": 0.2821045751633987, "height": 0.014185606060606148, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3866"}, {"text": "Similar relationships hold for the other primary and secondary emotions in Parrotts hierarchy, which we use to bootstrap Empaths base set of 32 emotional categories.", "label": "Method", "bboxes": [{"left": 0.3758954248366013, "top": 0.1958510101010101, "width": 0.10939379084967321, "height": 0.012579545454545454, "page": 5}, {"left": 0.08811928104575163, "top": 0.20968813131313133, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 5}, {"left": 0.08811928104575163, "top": 0.22352525252525254, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 5}, {"left": 0.08811928104575163, "top": 0.23736237373737376, "width": 0.19913725490196077, "height": 0.012579545454545454, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3867"}, {"text": "As we have discussed, each of Empaths categories is dened by seed words (e.g., lust : desire, passion; clothing : shirt, hat; social media : facebook, twitter).", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.42123232323232324, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.434780303030303, "width": 0.3971732026143791, "height": 0.012868686868686918, "page": 5}, {"left": 0.08811928104575163, "top": 0.44861742424242423, "width": 0.213718954248366, "height": 0.012868686868686918, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3868"}, {"text": "Using an embedding function v that maps a word to the vector space, we can nd the eight terms nearest to v ( depressed ) , by comparing its cosine similarity with all other terms in the space, and selecting the ones that are most similar:", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.725189393939394, "width": 0.39716830065359465, "height": 0.013145202020201863, "page": 5}, {"left": 0.5246633986928104, "top": 0.7390265151515151, "width": 0.39717483660130715, "height": 0.013409090909090926, "page": 5}, {"left": 0.5246633986928104, "top": 0.7528636363636364, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7667007575757575, "width": 0.3295964052287582, "height": 0.012579545454545538, "page": 5}], "section": "Building categories with a vector space", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3869"}, {"text": "Finally, to help researchers analyze text over new kinds of categories, we have released Empath as a web service and open source library.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.6740404040404041, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6878775252525252, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7017146464646465, "width": 0.09282189542483665, "height": 0.012579545454545427, "page": 6}], "section": "Empath API and web service", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3870"}, {"text": "We have validated Empaths 200 categories (with a vocabulary of more than 10,000 words) at a total cost of $840.", "label": "Method", "bboxes": [{"left": 0.7668431372549019, "top": 0.5251704545454545, "width": 0.15499019607843145, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5390075757575757, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.552844696969697, "width": 0.19379901960784318, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3871"}, {"text": "We divide the total number of words to be ltered across many separate tasks, where each task consists of twenty words to be rated for a given category.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.5291906565656566, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5430277777777778, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.556864898989899, "width": 0.2661895424836601, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3872"}, {"text": "We limit tasks to Masters workers to ensure quality [26] and we aggregate crowdworker feedback by majority vote.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.7166186868686869, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.730455808080808, "width": 0.3532156862745098, "height": 0.012579545454545538, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3873"}, {"text": "We provide a sample of terms accepted and rejected by the crowd in Table 2.", "label": "Method", "bboxes": [{"left": 0.7258856209150327, "top": 0.3515795454545455, "width": 0.19594771241830067, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.3654166666666667, "width": 0.30161274509803926, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3874"}, {"text": "Our experience conrms the ndings of prior work that category construction is somewhat subjective.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.5742285353535354, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5880656565656566, "width": 0.28999019607843135, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3875"}, {"text": "Here we evaluate Empaths crowd ltered and unsupervised predictions against similar gold standard categories in LIWC.", "label": "Method", "bboxes": [{"left": 0.6799460784313726, "top": 0.829199494949495, "width": 0.24188725490196084, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.8430366161616161, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.8568737373737374, "width": 0.1652124183006537, "height": 0.012579545454545427, "page": 6}], "section": "EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3876"}, {"text": "By ltering Empaths categories through the crowd, we offer the benets of both modern NLP and human validation: increasing category precision, and more carefully validating category contents.", "label": "Method", "bboxes": [{"left": 0.2706862745098039, "top": 0.3191199494949495, "width": 0.21460294117647066, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3329570707070707, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3467941919191919, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.36063131313131314, "width": 0.2864248366013072, "height": 0.012579545454545482, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3877"}, {"text": "To validate each of Empaths categories, we have created a crowdsourcing pipeline on Amazon Mechanical Turk.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.3820151515151515, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3958522727272727, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3878"}, {"text": "We plot Empaths best and worst category correlations with LIWC in Figure 7.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.5788207070707071, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.5926578282828283, "width": 0.12689215686274513, "height": 0.012579545454545427, "page": 7}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3879"}, {"text": "Finally, to test the importance of choosing seed terms, we re-ran our evaluation while permuting the seed words in Empaths categories.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.3475542929292929, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.3613914141414141, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.37522853535353534, "width": 0.11367483660130717, "height": 0.012579545454545482, "page": 7}], "section": "Method", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3880"}, {"text": "We then ran all tools over the documents in the test corpus, recorded their category word counts, then used these counts to compute Pearson correlations between all shared categories, as well as aggregate overall correlations.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.6077828282828283, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6216199494949495, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6354570707070707, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6492941919191919, "width": 0.2732222222222222, "height": 0.012579545454545427, "page": 7}], "section": "Method", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3881"}, {"text": "To compare all tools, we created a mixed textual dataset evenly divided among tweets [28], StackExchange opinions [5], movie reviews [32], hotel reviews [31], and chapters sampled from four classic novels on Project Gutenberg (David Coppereld, Moby Dick, Anna Karenina, and The Count of Monte Cristo) [1].", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.3574583333333334, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.37129545454545454, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.3851325757575757, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.3989696969696969, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.41280681818181814, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.42664393939393935, "width": 0.12111437908496729, "height": 0.012579545454545482, "page": 7}], "section": "Method", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3882"}, {"text": "In permuting Empaths seed terms, we found it retained high unsupervised agreement with LIWC (between 0.82 and 0.88).", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.641715909090909, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6555530303030302, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}], "section": "Results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3883"}, {"text": "The broad reach of our dataset allows Empath to classify documents among a large number of categories.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.0959229797979798, "width": 0.3971699346405229, "height": 0.01257954545454544, "page": 7}, {"left": 0.08811928104575163, "top": 0.10976010101010102, "width": 0.29339215686274517, "height": 0.01257954545454544, "page": 7}], "section": "EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3884"}, {"text": "Next we selected two parameters for Empath: the minimum cosine similarity for category inclusion and the seed words for each category (we xed the size of each category at a maximum of 200 words).", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.461864898989899, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4757020202020202, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.4895391414141414, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.5033762626262626, "width": 0.13587908496732026, "height": 0.012579545454545427, "page": 7}], "section": "Method", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3885"}, {"text": "Here we discuss our results and the limitations of our approach.", "label": "Method", "bboxes": [{"left": 0.8667777777777778, "top": 0.8782335858585859, "width": 0.055055555555555635, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3558366013071895, "height": 0.012579545454545427, "page": 7}], "section": "DISCUSSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3886"}, {"text": "Fortunately, LIWC has been extensively validated by researchers [33], so we can use it to benchmark Empaths predictions across the categories that they share in common.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.18649242424242424, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.20032954545454545, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.21416666666666667, "width": 0.3784003267973856, "height": 0.012579545454545454, "page": 7}], "section": "EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3887"}, {"text": "To anchor this analysis, we collected benchmark Pearson correlations against LIWC for GI and EmoLex (two existing human validated lexicons).", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.7398636363636364, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7537007575757576, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7675366161616162, "width": 0.16062254901960782, "height": 0.012579545454545427, "page": 7}], "section": "Method", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3888"}, {"text": "Here we compare the predictions of Empath and LIWC over 12 shared categories: sadness , anger , positive emotion , negative emotion , sexual , money , death , achievement , home , religion , work , and health .", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.27706186868686866, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.2906098484848485, "width": 0.3971732026143791, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.3044469696969697, "width": 0.3971748366013072, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.3182840909090909, "width": 0.1501225490196078, "height": 0.012868686868686918, "page": 7}], "section": "EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3889"}, {"text": "As we see in our results, this scenario does not happen often, and when it does happen the effect size is small.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.7128661616161617, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7267032828282828, "width": 0.364171568627451, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3890"}, {"text": "When we inspected the output of the crowd ltering step to determine what had caused this effect, we found in a small number of cases in which the crowd was overzealous.", "label": "Method", "bboxes": [{"left": 0.24402124183006538, "top": 0.5392739898989899, "width": 0.24126797385620916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5531111111111111, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5669482323232323, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5807853535353535, "width": 0.08132843137254901, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3891"}, {"text": "Contrary to this critique, we have found that ction is a useful training dataset for Empath given its abundance of concrete descriptors and emotional terms.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.4770075757575758, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.490844696969697, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.5046818181818182, "width": 0.24756699346405242, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3892"}, {"text": "We created these categories in a data-driven manner.", "label": "Method", "bboxes": [{"left": 0.8474869281045753, "top": 0.8153383838383839, "width": 0.07434640522875813, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.8291742424242424, "width": 0.27227777777777784, "height": 0.012579545454545427, "page": 8}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3893"}, {"text": "Second, we have not tested how well Empaths categories generalize beyond the core set it shares with LIWC.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3360424836601307, "height": 0.012579545454545427, "page": 8}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3894"}, {"text": "Empaths design decisions suggest a set of limitations, many of which we hope to address in future work.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.6832575757575757, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6970946969696971, "width": 0.2869133986928105, "height": 0.012579545454545427, "page": 8}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3895"}, {"text": "In this paper, we used Bonferroni correction to handle the issue, but there are more mature methods available.", "label": "Method", "bboxes": [{"left": 0.21896732026143792, "top": 0.3937436868686869, "width": 0.26632189542483664, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.40758080808080804, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.42141792929292926, "width": 0.0621029411764706, "height": 0.012579545454545482, "page": 9}], "section": "Statistical false positives", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3896"}, {"text": "These questions are ever changing, as is our use of language.", "label": "Method", "bboxes": [{"left": 0.4464150326797386, "top": 0.6292979797979797, "width": 0.038874183006535945, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.643135101010101, "width": 0.36145424836601314, "height": 0.012579545454545427, "page": 9}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3897"}, {"text": "Finally, while ction provides a powerful model for generating lexical categories, we have also seen that, for certain topics (e.g. death in Google News), other corpora may have even greater potential.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.17202651515151515, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.18586363636363637, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.19941161616161615, "width": 0.397171568627451, "height": 0.01286868686868689, "page": 9}, {"left": 0.08811928104575163, "top": 0.21353661616161618, "width": 0.1446699346405229, "height": 0.012579545454545454, "page": 9}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3898"}, {"text": "While Empaths categories are all generated and validated in the same way, we have seen though our evaluation that choice of seed words can be important.", "label": "Method", "bboxes": [{"left": 0.37949346405228757, "top": 0.0814570707070707, "width": 0.10579575163398702, "height": 0.012579545454545468, "page": 9}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3971699346405229, "height": 0.012579545454545468, "page": 9}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.11450490196078432, "height": 0.01257954545454544, "page": 9}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3899"}, {"text": "Special thanks to our reviewers and colleagues at Stanford for their helpful feedback.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.6988080808080809, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.712645202020202, "width": 0.17174019607843138, "height": 0.012579545454545538, "page": 9}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3900"}, {"text": "We show how the open-ended nature of Empaths model can replicate and extend classic work in classifying deceptive language [31], identifying the patterns of language in movie reviews [32], and analyzing mood on twitter [13].", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.21538888888888888, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.22922474747474747, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.2430618686868687, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.2568989898989899, "width": 0.3074411764705882, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "3901"}, {"text": "In our second example, we show how Empath can help us discover trends in a dataset of movie reviews collected by Pang et al.", "label": "Result", "bboxes": [{"left": 0.7693169934640522, "top": 0.7227095959595959, "width": 0.15251633986928115, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.7365467171717172, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.7503838383838384, "width": 0.27245751633986937, "height": 0.012579545454545538, "page": 3}], "section": "Example 2: Understanding language in movie reviews", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3902"}, {"text": "We see a tendency towards more mundane activities among the truth-tellers through categories like eating (1.3 odds), cleaning (1.3 odds), or hygiene (1.2 odds).", "label": "Result", "bboxes": [{"left": 0.15605228758169937, "top": 0.6769671717171717, "width": 0.32923692810457517, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6905151515151515, "width": 0.3971748366013072, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.7043522727272727, "width": 0.3268039215686274, "height": 0.012868686868686807, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3903"}, {"text": "As we see in our results, this scenario does not happen often, and when it does happen the effect size is small.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.7128661616161617, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7267032828282828, "width": 0.364171568627451, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3904"}, {"text": "We see potential for training Empath on other text beyond ction.", "label": "Result", "bboxes": [{"left": 0.30272712418300657, "top": 0.268885101010101, "width": 0.18256535947712416, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.2827222222222222, "width": 0.2382565359477124, "height": 0.012579545454545427, "page": 9}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3905"}, {"text": "CHI16, May 07-12, 2016, San Jose, CA, USA 2016 ACM.", "label": "Conclusion", "bboxes": [{"left": 0.3873153594771242, "top": 0.8609684343434343, "width": 0.09473856209150322, "height": 0.008804292929292967, "page": 0}, {"left": 0.0913611111111111, "top": 0.8710315656565656, "width": 0.18154738562091505, "height": 0.008804292929292967, "page": 0}], "section": "", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3906"}, {"text": "Finally, we demonstrate how we can lter these relationships through the crowd to efciently construct new, human validated dictionaries.", "label": "Conclusion", "bboxes": [{"left": 0.27136274509803926, "top": 0.8606957070707071, "width": 0.21393464052287575, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8745328282828283, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8883699494949495, "width": 0.28928921568627447, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3907"}, {"text": "Other categories may benet from updating with modern terms like paypal for money or sele for leisure .", "label": "Conclusion", "bboxes": [{"left": 0.16376633986928105, "top": 0.3122474747474748, "width": 0.32152287581699346, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.3257954545454545, "width": 0.3947581699346406, "height": 0.012868686868686918, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3908"}, {"text": "For example, researchers have investigated the publics response to major holidays and news events [2], how conversational partners mirror each others [18], the topical and emotional content of blogs [27, 16, 29], and whether one persons writing may inuence her friends when she posts to social media like Facebook [19] or Twitter [7].", "label": "Conclusion", "bboxes": [{"left": 0.18390359477124185, "top": 0.5390340909090909, "width": 0.3013856209150327, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5528712121212122, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5667083333333334, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5805441919191919, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.5943813131313131, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6082184343434344, "width": 0.30421732026143794, "height": 0.012579545454545538, "page": 2}], "section": "Applications for Text Analysis", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3909"}, {"text": "To motivate the opportunities that Empath creates, we rst present three example analyses that illustrate its breadth and exibility.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.6780328282828282, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.6918699494949495, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7057070707070706, "width": 0.06609150326797386, "height": 0.012579545454545538, "page": 2}], "section": "EMPATH APPLICATIONS", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3910"}, {"text": "Some aspects of these connotations may be mineable from social media, if they are of the sort that people are likely to advertise on Twitter [17].", "label": "Conclusion", "bboxes": [{"left": 0.6854967320261438, "top": 0.5885277777777778, "width": 0.2363366013071896, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.602364898989899, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.6162020202020202, "width": 0.33511437908496733, "height": 0.012579545454545427, "page": 2}], "section": "Data Mining and Modeling", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3911"}, {"text": "For instance, uglyness is 1.4 times more likely to appear in a negative review, swear words are 1.3 times more likely, and pain is 1.3 times more likely.", "label": "Conclusion", "bboxes": [{"left": 0.6837875816993464, "top": 0.850270202020202, "width": 0.23805718954248356, "height": 0.012868686868686918, "page": 3}, {"left": 0.5246633986928104, "top": 0.8641073232323233, "width": 0.39717483660130726, "height": 0.012868686868686807, "page": 3}, {"left": 0.5246633986928104, "top": 0.8779444444444444, "width": 0.3680016339869282, "height": 0.012868686868686918, "page": 3}], "section": "Exploring the movie dataset", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3912"}, {"text": "Using normalized means of the category counts for each group, we then computed odds ratios and p-values for the categories most likely to appear in deceptive and truthful reviews.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.4341919191919192, "width": 0.39716993464052286, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.4480277777777778, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.461864898989899, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.4757020202020202, "width": 0.041640522875816974, "height": 0.012579545454545482, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3913"}, {"text": "For exploratory research questions, Empath provides a highlevel view over many potential categories, some of which a researcher may not have thought to investigate.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3195522875816994, "height": 0.012579545454545427, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3914"}, {"text": "Lying hotel reviewers, for example, may not have realized they give themselves away by xating on smell (1.4 odds), the room was pungent with what smelled like human excrement, or their systematic overuse of emotional terms, producing signicantly higher odds ratios for 13 of Empaths 32 emo-", "label": "Conclusion", "bboxes": [{"left": 0.4193267973856209, "top": 0.822885101010101, "width": 0.06596241830065358, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.850270202020202, "width": 0.3971764705882353, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.8643093434343434, "width": 0.39717973856209154, "height": 0.012666666666666715, "page": 3}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3915"}, {"text": "Our results provide new evidence in support of the Ott et al. study, suggesting that deceptive reviews convey stronger sentiment across both positively and negatively charged categories, and tend towards exaggerated language (Figure 4).", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.5109242424242424, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5247613636363636, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.5385972222222223, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5524343434343435, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3916"}, {"text": "When analyzing textual data, researchers collectively engage with many possible linguistic categories.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.383875, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.39771212121212124, "width": 0.2726372549019608, "height": 0.012579545454545482, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3917"}, {"text": "The movie dataset also allows us to demonstrate convergent validity between Empath and gold standard tools like LIWC.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.6681047979797979, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.6819419191919192, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 4}], "section": "Exploring the movie dataset", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3918"}, {"text": "Empath aims to make possible all of these analyses (and more) through its 200 human validated categories, which cover topics like violence , depression , or femininity .", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.47444444444444445, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.48828156565656566, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.5018295454545455, "width": 0.34544444444444444, "height": 0.012868686868686807, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3919"}, {"text": "Similarly, positive reviews are associated with more positively charged categories: beauty is 1.8 times more likely to appear in a positive review, joy is 1.5 times more likely, and pride is 1.4 times more likely.", "label": "Conclusion", "bboxes": [{"left": 0.3389754901960784, "top": 0.32092171717171714, "width": 0.1463137254901961, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.3347588383838384, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.34830681818181813, "width": 0.3971748366013072, "height": 0.012868686868686918, "page": 4}, {"left": 0.08811928104575163, "top": 0.36214393939393935, "width": 0.3971699346405229, "height": 0.012868686868686918, "page": 4}], "section": "Exploring the movie dataset", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3920"}, {"text": "Positive reviews tend to co-occur with the deeper organizing principals of human life, like politics (1.4 odds), philosophy (1.4 odds), and law (1.3 odds)  possibly indicating the interests of lm reviewers.", "label": "Conclusion", "bboxes": [{"left": 0.19437091503267973, "top": 0.4668396464646465, "width": 0.2909183006535948, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.48038762626262627, "width": 0.3971748366013072, "height": 0.012868686868686863, "page": 4}, {"left": 0.08811928104575163, "top": 0.4942247474747475, "width": 0.3971748366013072, "height": 0.012868686868686863, "page": 4}, {"left": 0.08811928104575163, "top": 0.50835101010101, "width": 0.300359477124183, "height": 0.012579545454545427, "page": 4}], "section": "Exploring the movie dataset", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3921"}, {"text": "These scores indicate that Empath and LIWC are strongly correlated  similar to the correlation between LIWC and other published and validated tools.", "label": "Conclusion", "bboxes": [{"left": 0.6623169934640524, "top": 0.5926578282828283, "width": 0.259516339869281, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6064949494949495, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.6203320707070707, "width": 0.34773039215686274, "height": 0.012579545454545427, "page": 7}], "section": "Results", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3922"}, {"text": "If we can demonstrate that Empath provides very similar results across these categories, this would suggest that Empaths predictions are close to achieving gold standard accuracy.", "label": "Conclusion", "bboxes": [{"left": 0.47444607843137254, "top": 0.21416666666666667, "width": 0.010843137254901991, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.2280037878787879, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.24183964646464648, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.25567676767676767, "width": 0.3536241830065359, "height": 0.012579545454545427, "page": 7}], "section": "EVALUATION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3923"}, {"text": "We suggest that crowd validation offers the qualitative benet of removing false positives from analyses, while on the whole performing almost identically to (and usually slightly better than) the unltered version of Empath.", "label": "Conclusion", "bboxes": [{"left": 0.4639950980392156, "top": 0.7267032828282828, "width": 0.021294117647058908, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7405404040404041, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7543762626262627, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7682133838383838, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.7820505050505051, "width": 0.25204411764705886, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3924"}, {"text": "While ction allows Empath to learn an approximation of the gold-standard categories that dene tools like LIWC, its data-driven reasoning may succeed less well on corner cases of analysis and connotation.", "label": "Conclusion", "bboxes": [{"left": 0.3780277777777778, "top": 0.8367222222222221, "width": 0.10726143790849674, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.08093790849673203, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3925"}, {"text": "Just because ctional characters often pull guns", "label": "Conclusion", "bboxes": [{"left": 0.17444607843137255, "top": 0.892070707070707, "width": 0.310843137254902, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3926"}, {"text": "This may speak to the limited inuence of ction bias.", "label": "Conclusion", "bboxes": [{"left": 0.5728970588235294, "top": 0.6015404040404041, "width": 0.348936274509804, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3927"}, {"text": "Fiction may suffer from the overly fanciful plot events and motifs that surround death (e.g. suffocation, torture), but it captures more relevant words around most categories.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.6153775252525253, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6292146464646465, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6430517676767677, "width": 0.3500424836601309, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3928"}, {"text": "First, while Empath reports high Pearson correlations with LIWCs categories, it is possible that other more qualitative properties are important to lexical categories.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.3006029411764706, "height": 0.012579545454545538, "page": 8}], "section": "Limitations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3929"}, {"text": "Empaths design decisions suggest a set of limitations, many of which we hope to address in future work.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.6832575757575757, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6970946969696971, "width": 0.2869133986928105, "height": 0.012579545454545427, "page": 8}], "section": "Limitations", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3930"}, {"text": "Because Empath expands the number of categories available for analysis, it is important to consider the risk of a scientist analyzing so many categories that one of them, through sheer randomness, appears to be elevated in the text.", "label": "Conclusion", "bboxes": [{"left": 0.2919624183006536, "top": 0.338395202020202, "width": 0.19332679738562092, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.35223232323232323, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.36606944444444445, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.37990656565656566, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.3937436868686869, "width": 0.12586764705882358, "height": 0.012579545454545427, "page": 9}], "section": "Statistical false positives", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3931"}, {"text": "In the case of regression analysis, it is likewise important not to do so-called garbage can regressions that include every possible predictor.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.4490921717171717, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4629280303030303, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.47676515151515153, "width": 0.12132516339869283, "height": 0.012579545454545427, "page": 9}], "section": "Statistical false positives", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3932"}, {"text": "Finally, while ction provides a powerful model for generating lexical categories, we have also seen that, for certain topics (e.g. death in Google News), other corpora may have even greater potential.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.17202651515151515, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.18586363636363637, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.19941161616161615, "width": 0.397171568627451, "height": 0.01286868686868689, "page": 9}, {"left": 0.08811928104575163, "top": 0.21353661616161618, "width": 0.1446699346405229, "height": 0.012579545454545454, "page": 9}], "section": "Limitations", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "3933"}, {"text": "Could different datasets be targeted at specic categories?", "label": "Conclusion", "bboxes": [{"left": 0.23816013071895423, "top": 0.21353661616161618, "width": 0.2471290849673203, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.12734803921568633, "height": 0.012579545454545454, "page": 9}], "section": "Limitations", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3934"}, {"text": "Results that suggest Empath can generate categories extremely similar to categories that have been hand-tuned and psychometrically validated by humans (average Pearson correlation of 0.906), even without a crowd lter (0.90).", "label": "Conclusion", "bboxes": [{"left": 0.5251029411764706, "top": 0.6539785353535353, "width": 0.3967303921568628, "height": 0.015284090909090997, "page": 1}, {"left": 0.5409428104575164, "top": 0.670520202020202, "width": 0.38089052287581704, "height": 0.012579545454545427, "page": 1}, {"left": 0.5409428104575164, "top": 0.6843573232323232, "width": 0.38089052287581704, "height": 0.012579545454545538, "page": 1}, {"left": 0.5409428104575164, "top": 0.6981944444444445, "width": 0.3642369281045752, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "3935"}, {"text": "Further, many potentially useful categories like violence or social media dont exist in current lexicons, requiring the ad hoc curation and validation of new gold standard word lists.", "label": "Future Work", "bboxes": [{"left": 0.1407483660130719, "top": 0.2704469696969697, "width": 0.34454575163398693, "height": 0.012868686868686863, "page": 1}, {"left": 0.08811928104575163, "top": 0.2842840909090909, "width": 0.39717810457516345, "height": 0.012868686868686863, "page": 1}, {"left": 0.08811928104575163, "top": 0.29841035353535356, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.3122474747474748, "width": 0.06840359477124182, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "3936"}, {"text": "Further, Empath itself captures a set of relations on the topical and emotional connotations of words.", "label": "Future Work", "bboxes": [{"left": 0.815532679738562, "top": 0.5608535353535354, "width": 0.10630065359477137, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5746906565656565, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.5885277777777778, "width": 0.15147385620915033, "height": 0.012579545454545427, "page": 2}], "section": "Data Mining and Modeling", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3937"}, {"text": "The party that keeps you awake will not be your favorite band practicing for their next concert .", "label": "Future Work", "bboxes": [{"left": 0.3810588235294118, "top": 0.7460656565656566, "width": 0.10423529411764704, "height": 0.012666666666666715, "page": 3}, {"left": 0.08811928104575163, "top": 0.759989898989899, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.773739898989899, "width": 0.1274460784313725, "height": 0.012666666666666604, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3938"}, {"text": "More formally, for word w and context C in a network with negative sampling, a skip-gram network will learn weights that maximize the dot product w  w c and minimize w  w n for w c  C and w n sampled randomly from the vocabulary.", "label": "Future Work", "bboxes": [{"left": 0.08811928104575163, "top": 0.8283093434343434, "width": 0.3971813725490197, "height": 0.013145202020202085, "page": 5}, {"left": 0.08811928104575163, "top": 0.8421464646464647, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8532790404040405, "width": 0.3971748366013072, "height": 0.01675252525252524, "page": 5}, {"left": 0.08811928104575163, "top": 0.8671161616161616, "width": 0.36533660130718953, "height": 0.016751262626262697, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "3939"}, {"text": "If two of three workers believe a word is at least weakly related to the category, then Empath will keep the word, otherwise we remove it from the category.", "label": "Future Work", "bboxes": [{"left": 0.4463169934640523, "top": 0.730455808080808, "width": 0.03897222222222224, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7442929292929293, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.7581300505050504, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7719671717171718, "width": 0.1886388888888889, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3940"}, {"text": "Empaths design decisions suggest a set of limitations, many of which we hope to address in future work.", "label": "Future Work", "bboxes": [{"left": 0.5246633986928104, "top": 0.6832575757575757, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6970946969696971, "width": 0.2869133986928105, "height": 0.012579545454545427, "page": 8}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3941"}, {"text": "In future work, we hope to investigate these questions more closely.", "label": "Future Work", "bboxes": [{"left": 0.3288169934640523, "top": 0.13680429292929294, "width": 0.15647222222222223, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.2801078431372549, "height": 0.012579545454545427, "page": 9}], "section": "Limitations", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "3942"}, {"text": "Social science aims to avoid Type I errors  false claims that statistically appear to be true.", "label": "Objective", "bboxes": [{"left": 0.08811928104575163, "top": 0.3245580808080808, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.338395202020202, "width": 0.1955571895424837, "height": 0.012579545454545427, "page": 9}], "section": "Statistical false positives", "prob": 0.8424039483070374, "is_author_statement": false, "is_in_expected_section": false, "id": "3943"}, {"text": "Empath aims to make possible all of these analyses (and more) through its 200 human validated categories, which cover topics like violence , depression , or femininity .", "label": "Objective", "bboxes": [{"left": 0.5246633986928104, "top": 0.47444444444444445, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.48828156565656566, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.5018295454545455, "width": 0.34544444444444444, "height": 0.012868686868686807, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "prob": 0.761212944984436, "is_author_statement": false, "is_in_expected_section": false, "id": "3944"}, {"text": "We train our skip-gram network on the ction corpus from Wattpad, lemmatizing all words through a preprocessing step.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.23107323232323232, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.24490909090909088, "width": 0.39716993464052297, "height": 0.01257954545454551, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": 0.5019764304161072, "is_author_statement": true, "is_in_expected_section": true, "id": "3945"}, {"text": "We use the neural embeddings created by our skip-gram network to construct a vector space model (VSM).", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.41045454545454546, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.4242916666666667, "width": 0.30719607843137264, "height": 0.012579545454545482, "page": 5}], "section": "Building categories with a vector space", "prob": 0.49357497692108154, "is_author_statement": true, "is_in_expected_section": true, "id": "3946"}, {"text": "We borrow from the last of these approaches in constructing of Empaths unsupervised model.", "label": "Method", "bboxes": [{"left": 0.6961274509803921, "top": 0.2690189393939394, "width": 0.2257058823529413, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.2828560606060606, "width": 0.39651797385620924, "height": 0.012579545454545482, "page": 2}], "section": "Data Mining and Modeling", "prob": 0.4827876389026642, "is_author_statement": true, "is_in_expected_section": true, "id": "3947"}, {"text": "In Empath, we adapt these techniques to mine natural language for its relation to emotional and topical categories.", "label": "Method", "bboxes": [{"left": 0.7879395424836602, "top": 0.4426111111111111, "width": 0.13389379084967323, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.45644823232323234, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4702840909090909, "width": 0.21496078431372545, "height": 0.012579545454545482, "page": 2}], "section": "Data Mining and Modeling", "prob": 0.416332483291626, "is_author_statement": true, "is_in_expected_section": true, "id": "3948"}, {"text": "Next we selected two parameters for Empath: the minimum cosine similarity for category inclusion and the seed words for each category (we xed the size of each category at a maximum of 200 words).", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.461864898989899, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4757020202020202, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.4895391414141414, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.5033762626262626, "width": 0.13587908496732026, "height": 0.012579545454545427, "page": 7}], "section": "Method", "prob": 0.41184762120246887, "is_author_statement": true, "is_in_expected_section": true, "id": "3949"}, {"text": "In this paper, we used Bonferroni correction to handle the issue, but there are more mature methods available.", "label": "Method", "bboxes": [{"left": 0.21896732026143792, "top": 0.3937436868686869, "width": 0.26632189542483664, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.40758080808080804, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.42141792929292926, "width": 0.0621029411764706, "height": 0.012579545454545482, "page": 9}], "section": "Statistical false positives", "prob": 0.4099583923816681, "is_author_statement": true, "is_in_expected_section": true, "id": "3950"}, {"text": "Following this section, we explain the techniques behind Empaths model in more detail.", "label": "Method", "bboxes": [{"left": 0.6753071895424837, "top": 0.7472171717171717, "width": 0.2465261437908497, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7610542929292929, "width": 0.32691013071895425, "height": 0.012579545454545427, "page": 2}], "section": "EMPATH APPLICATIONS", "prob": 0.3835938274860382, "is_author_statement": true, "is_in_expected_section": true, "id": "3951"}, {"text": "To choose these parameters, we divided our mixed text dataset into a training corpus of 900 documents and a test corpus of 3500 documents.", "label": "Method", "bboxes": [{"left": 0.22888235294117645, "top": 0.5033762626262626, "width": 0.25640686274509805, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.5172133838383838, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.531050505050505, "width": 0.2797009803921569, "height": 0.012579545454545427, "page": 7}], "section": "Method", "prob": 0.3707526624202728, "is_author_statement": true, "is_in_expected_section": true, "id": "3952"}, {"text": "If two of three workers believe a word is at least weakly related to the category, then Empath will keep the word, otherwise we remove it from the category.", "label": "Method", "bboxes": [{"left": 0.4463169934640523, "top": 0.730455808080808, "width": 0.03897222222222224, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7442929292929293, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.7581300505050504, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7719671717171718, "width": 0.1886388888888889, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": 0.37024688720703125, "is_author_statement": true, "is_in_expected_section": true, "id": "3953"}, {"text": "We then manually rened this list, eliminating redundant or sparse categories.", "label": "Method", "bboxes": [{"left": 0.5834133986928104, "top": 0.7876641414141414, "width": 0.338419934640523, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8015012626262626, "width": 0.19075490196078437, "height": 0.012579545454545427, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "prob": 0.32592150568962097, "is_author_statement": true, "is_in_expected_section": true, "id": "3954"}, {"text": "If we can demonstrate that Empath provides very similar results across these categories, this would suggest that Empaths predictions are close to achieving gold standard accuracy.", "label": "Result", "bboxes": [{"left": 0.47444607843137254, "top": 0.21416666666666667, "width": 0.010843137254901991, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.2280037878787879, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.24183964646464648, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.25567676767676767, "width": 0.3536241830065359, "height": 0.012579545454545427, "page": 7}], "section": "EVALUATION", "prob": 0.8723037838935852, "is_author_statement": true, "is_in_expected_section": true, "id": "3955"}, {"text": "We found a benchmark correlation of 0.876 between GI and LIWC over positive emotion , negative emotion , religion , work , and achievement , and a correlation of 0.899 between EmoLex and LIWC over positive emotion , negative emotion , anger , and sadness .", "label": "Result", "bboxes": [{"left": 0.25631209150326795, "top": 0.7675366161616162, "width": 0.2289771241830066, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.781084595959596, "width": 0.3971732026143791, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.7949217171717171, "width": 0.3971764705882353, "height": 0.012868686868686918, "page": 7}, {"left": 0.08811928104575163, "top": 0.8087588383838384, "width": 0.3971748366013072, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.8225959595959595, "width": 0.28770261437908495, "height": 0.012868686868686918, "page": 7}], "section": "Method", "prob": 0.8306524753570557, "is_author_statement": true, "is_in_expected_section": false, "id": "3956"}, {"text": "While adding a crowd lter to Empath improves its overall correlations with LIWC, the improvement is not statistically signicant.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.45625252525252524, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.47008964646464646, "width": 0.39716993464052286, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.4839267676767677, "width": 0.07190359477124182, "height": 0.012579545454545482, "page": 8}], "section": "The role of human validation", "prob": 0.7904740571975708, "is_author_statement": false, "is_in_expected_section": true, "id": "3957"}, {"text": "Pearsons r measures the linear correlation between two variables, and returns a value between (-1,1), where 1 is total positive correlation, 0 is no correlation, and 1 is total negative correlation.", "label": "Result", "bboxes": [{"left": 0.3718578431372549, "top": 0.6490050505050504, "width": 0.11343627450980392, "height": 0.012868686868686918, "page": 7}, {"left": 0.08811928104575163, "top": 0.6631313131313132, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6769671717171717, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6908042929292929, "width": 0.3509542483660131, "height": 0.012579545454545538, "page": 7}], "section": "Method", "prob": 0.7833228707313538, "is_author_statement": false, "is_in_expected_section": false, "id": "3958"}, {"text": "Here we discuss our results and the limitations of our approach.", "label": "Result", "bboxes": [{"left": 0.8667777777777778, "top": 0.8782335858585859, "width": 0.055055555555555635, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3558366013071895, "height": 0.012579545454545427, "page": 7}], "section": "DISCUSSION", "prob": 0.771224856376648, "is_author_statement": true, "is_in_expected_section": true, "id": "3959"}, {"text": "All the results we report are signicant after a Bonferroni correction (  = 2 . 5 e  5 ).", "label": "Result", "bboxes": [{"left": 0.13472549019607843, "top": 0.4757020202020202, "width": 0.35056372549019604, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.4860391414141414, "width": 0.18570424836601307, "height": 0.016909090909090874, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": 0.7660936117172241, "is_author_statement": true, "is_in_expected_section": false, "id": "3960"}, {"text": "Our results provide new evidence in support of the Ott et al. study, suggesting that deceptive reviews convey stronger sentiment across both positively and negatively charged categories, and tend towards exaggerated language (Figure 4).", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.5109242424242424, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5247613636363636, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.5385972222222223, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5524343434343435, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": 0.75883549451828, "is_author_statement": true, "is_in_expected_section": false, "id": "3961"}, {"text": "Contrary to this critique, we have found that ction is a useful training dataset for Empath given its abundance of concrete descriptors and emotional terms.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.4770075757575758, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.490844696969697, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.5046818181818182, "width": 0.24756699346405242, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": 0.7498504519462585, "is_author_statement": true, "is_in_expected_section": false, "id": "3962"}, {"text": "While the corpus of tweets analyzed by the original paper is not publicly available, we reproduce the papers ndings on a smaller corpus of 591,520 tweets from the PST time-zone, running LIWC on our data as an additional benchmark (Figure 6).", "label": "Result", "bboxes": [{"left": 0.35401633986928105, "top": 0.8367222222222221, "width": 0.13127287581699348, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.30923202614379086, "height": 0.012579545454545427, "page": 4}], "section": "Exploring the movie dataset", "prob": 0.748261034488678, "is_author_statement": true, "is_in_expected_section": false, "id": "3963"}, {"text": "We nd no signicant difference between tools ( p = 0 . 43 ).", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.7622474747474748, "width": 0.3821666666666667, "height": 0.01612626262626249, "page": 4}], "section": "Exploring the movie dataset", "prob": 0.7082540392875671, "is_author_statement": true, "is_in_expected_section": false, "id": "3964"}, {"text": "These scores indicate that Empath and LIWC are strongly correlated  similar to the correlation between LIWC and other published and validated tools.", "label": "Result", "bboxes": [{"left": 0.6623169934640524, "top": 0.5926578282828283, "width": 0.259516339869281, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6064949494949495, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.6203320707070707, "width": 0.34773039215686274, "height": 0.012579545454545427, "page": 7}], "section": "Results", "prob": 0.7052928805351257, "is_author_statement": false, "is_in_expected_section": true, "id": "3965"}, {"text": "We nd the correlation between Empath and LIWC across a mixed-corpus dataset is high (r=0.906), and remains high even without the crowd lter (0.90), which suggests Empaths data-driven word counts are very similar to those made by a heavily validated dictionary.", "label": "Result", "bboxes": [{"left": 0.7461029411764706, "top": 0.37514267676767676, "width": 0.1757303921568628, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.388979797979798, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.4028169191919192, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.4166540404040404, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.43049116161616163, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.44432828282828285, "width": 0.04912908496732027, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.7014033198356628, "is_author_statement": true, "is_in_expected_section": true, "id": "3966"}, {"text": "When we then add the new spatial category to our analysis, we nd it favors truthful reviews by 1.2 odds ( p < 0 . 001 ).", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.5856792929292929, "width": 0.39717320261437905, "height": 0.012868686868686918, "page": 3}, {"left": 0.5246633986928104, "top": 0.5970883838383838, "width": 0.39717483660130715, "height": 0.0161262626262626, "page": 3}], "section": "Spatial language in lies", "prob": 0.6876612901687622, "is_author_statement": true, "is_in_expected_section": false, "id": "3967"}, {"text": "While the original study provided some evidence that liars use less spatially descriptive language, it wasnt able to test the theory directly.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.40482954545454547, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4186666666666667, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4325037878787879, "width": 0.12611111111111117, "height": 0.012579545454545427, "page": 3}], "section": "Spatial language in lies", "prob": 0.6866369843482971, "is_author_statement": false, "is_in_expected_section": false, "id": "3968"}, {"text": "We have not evaluated Empath over these more qualitative aspects of usability.", "label": "Result", "bboxes": [{"left": 0.7721813725490196, "top": 0.8430113636363636, "width": 0.14965196078431375, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.8568484848484849, "width": 0.36221895424836614, "height": 0.012579545454545427, "page": 8}], "section": "Limitations", "prob": 0.6801165342330933, "is_author_statement": true, "is_in_expected_section": false, "id": "3969"}, {"text": "Here we evaluate Empaths crowd ltered and unsupervised predictions against similar gold standard categories in LIWC.", "label": "Result", "bboxes": [{"left": 0.6799460784313726, "top": 0.829199494949495, "width": 0.24188725490196084, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.8430366161616161, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.8568737373737374, "width": 0.1652124183006537, "height": 0.012579545454545427, "page": 6}], "section": "EVALUATION", "prob": 0.6749953031539917, "is_author_statement": true, "is_in_expected_section": true, "id": "3970"}, {"text": "We nd a similar relationship on our data with both Empath and LIWC: a low in the morning (around 8am), peaking to a high around 11pm.", "label": "Result", "bboxes": [{"left": 0.8090539215686274, "top": 0.09529419191919192, "width": 0.11277941176470596, "height": 0.012579545454545468, "page": 4}, {"left": 0.5246633986928104, "top": 0.10913131313131313, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.12296717171717171, "width": 0.39716993464052297, "height": 0.01257954545454544, "page": 4}], "section": "Exploring the movie dataset", "prob": 0.6742184162139893, "is_author_statement": true, "is_in_expected_section": false, "id": "3971"}, {"text": "Augurs evaluation indicated that with regard to low-level behaviors such as actions, ction provides a surprisingly accurate mirror of human behavior.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.3457512626262626, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.3595883838383838, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.37342550505050504, "width": 0.19692483660130722, "height": 0.012579545454545482, "page": 2}], "section": "Data Mining and Modeling", "prob": 0.6642745137214661, "is_author_statement": false, "is_in_expected_section": false, "id": "3972"}, {"text": "We see potential for training Empath on other text beyond ction.", "label": "Result", "bboxes": [{"left": 0.30272712418300657, "top": 0.268885101010101, "width": 0.18256535947712416, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.2827222222222222, "width": 0.2382565359477124, "height": 0.012579545454545427, "page": 9}], "section": "Limitations", "prob": 0.658129096031189, "is_author_statement": true, "is_in_expected_section": false, "id": "3973"}, {"text": "As we see in our results, this scenario does not happen often, and when it does happen the effect size is small.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.7128661616161617, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7267032828282828, "width": 0.364171568627451, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "prob": 0.6550796031951904, "is_author_statement": true, "is_in_expected_section": false, "id": "3974"}, {"text": "We nd that ction offers a richer source of affective signal.", "label": "Result", "bboxes": [{"left": 0.869562091503268, "top": 0.6162020202020202, "width": 0.052271241830065374, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6300391414141414, "width": 0.3362222222222222, "height": 0.012579545454545538, "page": 2}], "section": "Data Mining and Modeling", "prob": 0.655065655708313, "is_author_statement": true, "is_in_expected_section": false, "id": "3975"}, {"text": "In the movie review dataset, we nd positive reviews are more strongly connected with intellectual categories like philosophy, politics, and law.", "label": "Result", "bboxes": [{"left": 0.7384362745098039, "top": 0.29841035353535356, "width": 0.18339705882352952, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.3122474747474748, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.326084595959596, "width": 0.3638464052287582, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.6364542245864868, "is_author_statement": true, "is_in_expected_section": true, "id": "3976"}, {"text": "While Empaths categories are all generated and validated in the same way, we have seen though our evaluation that choice of seed words can be important.", "label": "Result", "bboxes": [{"left": 0.37949346405228757, "top": 0.0814570707070707, "width": 0.10579575163398702, "height": 0.012579545454545468, "page": 9}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3971699346405229, "height": 0.012579545454545468, "page": 9}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.11450490196078432, "height": 0.01257954545454544, "page": 9}], "section": "Limitations", "prob": 0.6243283152580261, "is_author_statement": true, "is_in_expected_section": false, "id": "3977"}, {"text": "We chose this lower threshold for term inclusion as it showed the highest agreement with LIWC in our benchmarks.", "label": "Result", "bboxes": [{"left": 0.28527124183006536, "top": 0.7719671717171718, "width": 0.20001797385620917, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.7858042929292929, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7996414141414142, "width": 0.17528921568627454, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": 0.6151658892631531, "is_author_statement": true, "is_in_expected_section": false, "id": "3978"}, {"text": "We selected up to ve seed words that best approximated each LIWC category, and found that a minimum cosine similarity of 0.5 offered the best performance.", "label": "Result", "bboxes": [{"left": 0.37262254901960784, "top": 0.531050505050505, "width": 0.11266666666666669, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.5448876262626262, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.5587247474747474, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.5725618686868686, "width": 0.11598692810457516, "height": 0.012579545454545538, "page": 7}], "section": "Method", "prob": 0.5994812846183777, "is_author_statement": true, "is_in_expected_section": false, "id": "3979"}, {"text": "The movie review dataset reveals, unsurprisingly, a strong correlation between negative reviews and negatively charged categories (Figure 5).", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.14530392156862737, "height": 0.012579545454545427, "page": 3}], "section": "Exploring the movie dataset", "prob": 0.5934415459632874, "is_author_statement": false, "is_in_expected_section": false, "id": "3980"}, {"text": "Finally, we demonstrate how we can lter these relationships through the crowd to efciently construct new, human validated dictionaries.", "label": "Result", "bboxes": [{"left": 0.27136274509803926, "top": 0.8606957070707071, "width": 0.21393464052287575, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8745328282828283, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8883699494949495, "width": 0.28928921568627447, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5916317105293274, "is_author_statement": true, "is_in_expected_section": true, "id": "3981"}, {"text": "The original study found that liars tend to write more", "label": "Result", "bboxes": [{"left": 0.5646764705882353, "top": 0.892070707070707, "width": 0.35715686274509806, "height": 0.012579545454545427, "page": 2}], "section": "Example 1: Understanding deception in hotel reviews", "prob": 0.5866645574569702, "is_author_statement": false, "is_in_expected_section": false, "id": "3982"}, {"text": "Exploring the deception dataset", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.3930037878787879, "width": 0.20545098039215687, "height": 0.011321969696969691, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "prob": 0.5851123929023743, "is_author_statement": false, "is_in_expected_section": false, "id": "3983"}, {"text": "In permuting Empaths seed terms, we found it retained high unsupervised agreement with LIWC (between 0.82 and 0.88).", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.641715909090909, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6555530303030302, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}], "section": "Results", "prob": 0.5762056112289429, "is_author_statement": true, "is_in_expected_section": true, "id": "3984"}, {"text": "We provide a sample of terms accepted and rejected by the crowd in Table 2.", "label": "Result", "bboxes": [{"left": 0.7258856209150327, "top": 0.3515795454545455, "width": 0.19594771241830067, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.3654166666666667, "width": 0.30161274509803926, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": 0.573114275932312, "is_author_statement": true, "is_in_expected_section": false, "id": "3985"}, {"text": "Beyond these obviously polarized categories, we nd interesting trends in the topics associated with positive and negative reviews.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.43916540404040405, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.45300252525252527, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.4668396464646465, "width": 0.09200816993464052, "height": 0.012579545454545427, "page": 4}], "section": "Exploring the movie dataset", "prob": 0.572016179561615, "is_author_statement": true, "is_in_expected_section": false, "id": "3986"}, {"text": "Second, we have not tested how well Empaths categories generalize beyond the core set it shares with LIWC.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3360424836601307, "height": 0.012579545454545427, "page": 8}], "section": "Limitations", "prob": 0.5351861119270325, "is_author_statement": true, "is_in_expected_section": false, "id": "3987"}, {"text": "Over one trial, we dropped one seed term from each category.", "label": "Result", "bboxes": [{"left": 0.6461029411764706, "top": 0.37522853535353534, "width": 0.2757303921568628, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.38906565656565656, "width": 0.12734803921568638, "height": 0.012579545454545482, "page": 7}], "section": "Method", "prob": 0.5313065052032471, "is_author_statement": true, "is_in_expected_section": false, "id": "3988"}, {"text": "Here we compare the predictions of Empath and LIWC over 12 shared categories: sadness , anger , positive emotion , negative emotion , sexual , money , death , achievement , home , religion , work , and health .", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.27706186868686866, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.2906098484848485, "width": 0.3971732026143791, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.3044469696969697, "width": 0.3971748366013072, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.3182840909090909, "width": 0.1501225490196078, "height": 0.012868686868686918, "page": 7}], "section": "EVALUATION", "prob": 0.48609068989753723, "is_author_statement": true, "is_in_expected_section": true, "id": "3989"}, {"text": "When analyzing textual data, researchers collectively engage with many possible linguistic categories.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.383875, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.39771212121212124, "width": 0.2726372549019608, "height": 0.012579545454545482, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "prob": 0.48283514380455017, "is_author_statement": false, "is_in_expected_section": false, "id": "3990"}, {"text": "Results that suggest Empath can generate categories extremely similar to categories that have been hand-tuned and psychometrically validated by humans (average Pearson correlation of 0.906), even without a crowd lter (0.90).", "label": "Result", "bboxes": [{"left": 0.5251029411764706, "top": 0.6539785353535353, "width": 0.3967303921568628, "height": 0.015284090909090997, "page": 1}, {"left": 0.5409428104575164, "top": 0.670520202020202, "width": 0.38089052287581704, "height": 0.012579545454545427, "page": 1}, {"left": 0.5409428104575164, "top": 0.6843573232323232, "width": 0.38089052287581704, "height": 0.012579545454545538, "page": 1}, {"left": 0.5409428104575164, "top": 0.6981944444444445, "width": 0.3642369281045752, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.45080024003982544, "is_author_statement": false, "is_in_expected_section": true, "id": "3991"}, {"text": "The web service 3 allows users to analyze documents across Empaths built-in categories (Figure 1), generate new unsupervised categories, and request new categories be validated using our crowdsourcing pipeline.", "label": "Result", "bboxes": [{"left": 0.6224820261437909, "top": 0.7001073232323233, "width": 0.2993627450980392, "height": 0.014186868686868692, "page": 6}, {"left": 0.5246633986928104, "top": 0.7155517676767676, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7293876262626262, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.7432247474747475, "width": 0.3194215686274511, "height": 0.012579545454545427, "page": 6}], "section": "Empath API and web service", "prob": 0.41982078552246094, "is_author_statement": true, "is_in_expected_section": false, "id": "3992"}, {"text": "For some categories we added additional seed terms to better represent the concept, resulting in a nal set of two to ve seed terms for each category.", "label": "Result", "bboxes": [{"left": 0.7224983660130718, "top": 0.8015012626262626, "width": 0.19933496732026157, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8153383838383839, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8291742424242424, "width": 0.3627238562091505, "height": 0.012579545454545427, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "prob": 0.41926252841949463, "is_author_statement": true, "is_in_expected_section": false, "id": "3993"}, {"text": "For each word, tell us how strongly it relates to the topic.", "label": "Result", "bboxes": [{"left": 0.10439869281045752, "top": 0.4320593434343434, "width": 0.33873039215686274, "height": 0.011321969696969747, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": 0.39404988288879395, "is_author_statement": false, "is_in_expected_section": false, "id": "3994"}, {"text": "Using an embedding function v that maps a word to the vector space, we can nd the eight terms nearest to v ( depressed ) , by comparing its cosine similarity with all other terms in the space, and selecting the ones that are most similar:", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.725189393939394, "width": 0.39716830065359465, "height": 0.013145202020201863, "page": 5}, {"left": 0.5246633986928104, "top": 0.7390265151515151, "width": 0.39717483660130715, "height": 0.013409090909090926, "page": 5}, {"left": 0.5246633986928104, "top": 0.7528636363636364, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7667007575757575, "width": 0.3295964052287582, "height": 0.012579545454545538, "page": 5}], "section": "Building categories with a vector space", "prob": 0.3791716992855072, "is_author_statement": true, "is_in_expected_section": false, "id": "3995"}, {"text": "By ltering Empaths categories through the crowd, we offer the benets of both modern NLP and human validation: increasing category precision, and more carefully validating category contents.", "label": "Result", "bboxes": [{"left": 0.2706862745098039, "top": 0.3191199494949495, "width": 0.21460294117647066, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3329570707070707, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3467941919191919, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.36063131313131314, "width": 0.2864248366013072, "height": 0.012579545454545482, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "prob": 0.3589678108692169, "is_author_statement": true, "is_in_expected_section": false, "id": "3996"}, {"text": "While Empath can be trained on any text corpus, for the analyses in this paper we use a dataset of modern ction from Wattpad, 1 a community of amateur writers.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.5885340909090909, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6023712121212121, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6146022727272727, "width": 0.2821045751633987, "height": 0.014185606060606148, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "prob": 0.3571099638938904, "is_author_statement": true, "is_in_expected_section": false, "id": "3997"}, {"text": "To generate the query vector for one of Empaths categories, we add the vector corresponding to the", "label": "Result", "bboxes": [{"left": 0.658718954248366, "top": 0.8509810606060606, "width": 0.2631143790849674, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.8648181818181818, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 5}], "section": "Building categories with a vector space", "prob": 0.3534300625324249, "is_author_statement": true, "is_in_expected_section": false, "id": "3998"}, {"text": "The space is M ( n  h ) where n is the size of our vocabulary (40,000), and h the number of hidden nodes in the network (150).", "label": "Result", "bboxes": [{"left": 0.839624183006536, "top": 0.5702083333333333, "width": 0.08220915032679743, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.5813282828282827, "width": 0.39716666666666667, "height": 0.01612626262626271, "page": 5}, {"left": 0.5246633986928104, "top": 0.5978825757575758, "width": 0.3353104575163399, "height": 0.013145202020202085, "page": 5}], "section": "Building categories with a vector space", "prob": 0.3226011097431183, "is_author_statement": true, "is_in_expected_section": false, "id": "3999"}], "uist-6": [{"text": "In this paper, we share the same vision than On-skin interaction, which builds on the advantages of the skin to increase interaction bandwidth.", "label": "Author", "bboxes": [{"left": 0.6765424836601307, "top": 0.468604797979798, "width": 0.24530555555555567, "height": 0.012579545454545482, "page": 0}, {"left": 0.5246633986928104, "top": 0.4824419191919192, "width": 0.3971928104575164, "height": 0.012579545454545482, "page": 0}, {"left": 0.5246633986928104, "top": 0.4962790404040404, "width": 0.2698578431372549, "height": 0.012579545454545482, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4000"}, {"text": "However, we argue that the benets of human skin should not only be used for Onskin interfaces but also for what we call Skin-On interfaces.", "label": "Author", "bboxes": [{"left": 0.799514705882353, "top": 0.4962790404040404, "width": 0.1223333333333334, "height": 0.012579545454545482, "page": 0}, {"left": 0.5246633986928104, "top": 0.5101161616161616, "width": 0.39989705882352944, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.5239532828282828, "width": 0.40003267973856205, "height": 0.012579545454545538, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4001"}, {"text": "Our approach augments I/O capabilities of interactive systems with new gestures and rich kinesthetic feedback.", "label": "Author", "bboxes": [{"left": 0.8963137254901961, "top": 0.5516275252525252, "width": 0.025821895424836594, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.5654633838383838, "width": 0.39718464052287594, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.579300505050505, "width": 0.2825947712418301, "height": 0.012579545454545538, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4002"}, {"text": "Our work contributes towards this direction.", "label": "Author", "bboxes": [{"left": 0.8602418300653594, "top": 0.8434621212121213, "width": 0.062019607843137314, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8572992424242424, "width": 0.2206666666666668, "height": 0.012579545454545427, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4003"}, {"text": "We present an exploration of the design space of Skin-On interfaces.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.4006470588235294, "height": 0.012579545454545468, "page": 1}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.05375490196078429, "height": 0.012579545454545468, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4004"}, {"text": "In particular, we follow a bio-driven approach where we take inspiration from the human skin to design this new type of interfaces.", "label": "Author", "bboxes": [{"left": 0.14692973856209152, "top": 0.09529419191919192, "width": 0.33836764705882355, "height": 0.012579545454545468, "page": 1}, {"left": 0.08753267973856209, "top": 0.10913131313131313, "width": 0.39835294117647063, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.11796732026143793, "height": 0.01257954545454544, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4005"}, {"text": "Our approach shares similar goals, seeking to reproduce the sensing capabilities of biological skin, but it also goes beyond the typical bio-inspired approach by focusing on interactive aspects, which we believe are crucial for human-computer interfaces:", "label": "Author", "bboxes": [{"left": 0.35525653594771245, "top": 0.16447853535353535, "width": 0.13003594771241828, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.19215277777777778, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.205989898989899, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.21982702020202022, "width": 0.2782859477124183, "height": 0.012579545454545454, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4006"}, {"text": "Our work relates to on-skin interfaces in HCI, articial skin in robotics and exible input sensors.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.8219835858585858, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.8358207070707071, "width": 0.22629084967320262, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4007"}, {"text": "We are not aware of any research looking at exploiting realistic articial skin as a new input method for interactive devices.", "label": "Author", "bboxes": [{"left": 0.8525163398692811, "top": 0.6811098484848485, "width": 0.06932026143790848, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.6949469696969697, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7087840909090909, "width": 0.3290261437908497, "height": 0.012579545454545538, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4008"}, {"text": "We assemble the insights from these three steps and present the implementation of several Skin-On interfaces and applications to demonstrate the added value of our approach (see Figure 1 for examples).", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.5305037878787878, "width": 0.39793790849673205, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811764705882352, "top": 0.544340909090909, "width": 0.3998807189542483, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811764705882352, "top": 0.5581780303030303, "width": 0.39717483660130726, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811764705882352, "top": 0.5720151515151515, "width": 0.1609624183006536, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4009"}, {"text": "We believe our work extends the boundary of traditional interactive devices by opening up the user experience to anthropomorphic interfaces and to new familiar organic interaction between humans and machines.", "label": "Author", "bboxes": [{"left": 0.25972385620915034, "top": 0.5720151515151515, "width": 0.22556862745098039, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5858522727272727, "width": 0.3971748366013072, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5996893939393939, "width": 0.3998790849673202, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6135265151515151, "width": 0.3640049019607843, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4010"}, {"text": "This work also explores the intersection between man and machine (human augmentation) from a new perspective: Instead of augmenting the human with parts of machines, we demonstrate how machines can be augmented with parts of human.", "label": "Author", "bboxes": [{"left": 0.4569264705882353, "top": 0.6135265151515151, "width": 0.028366013071895457, "height": 0.012579545454545427, "page": 1}, {"left": 0.08753267973856209, "top": 0.6273623737373737, "width": 0.39776633986928106, "height": 0.012579545454545427, "page": 1}, {"left": 0.08758169934640524, "top": 0.6411994949494949, "width": 0.4004117647058823, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.6550366161616161, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6688737373737375, "width": 0.36655555555555563, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4011"}, {"text": "In this paper we considered one aspect (skin) but we hope our work will inspire researchers to investigate how interfaces can be designed to integrate elements from nature.", "label": "Author", "bboxes": [{"left": 0.3442892156862745, "top": 0.7380593434343434, "width": 0.14371078431372547, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7518964646464645, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.7657335858585859, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7795694444444444, "width": 0.20319117647058826, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4012"}, {"text": "However we do not focus on interacting directly on human skin but rather aim at mimicking its properties to augment interactive devices.", "label": "Author", "bboxes": [{"left": 0.7970604575163398, "top": 0.2965593434343434, "width": 0.12478594771241835, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.31039646464646464, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.32423358585858586, "width": 0.3647254901960786, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4013"}, {"text": "To better understand which are the desirable properties of the human skin to reproduce within articial skin, we looked through the Biology literature [20, 36, 38] and gathered information about the visual, haptic and sensing properties of the skin (described below).", "label": "Author", "bboxes": [{"left": 0.5241601307189543, "top": 0.2687032828282828, "width": 0.3976715686274508, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.28254040404040404, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.29637752525252525, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.31021464646464647, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.3240517676767677, "width": 0.18175000000000008, "height": 0.012579545454545482, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4014"}, {"text": "We excluded properties related to biological features out of scope of this work such as the semi-impermeable barrier (useful for both uid excretion and absorption), the anatomical barrier (preventing pathogens or preventing external damage), heat regulation, and storage (e.g. for vitamin D synthesis).", "label": "Author", "bboxes": [{"left": 0.7140343137254902, "top": 0.3240517676767677, "width": 0.20780228758169927, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.3378888888888889, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.3517260101010101, "width": 0.39717156862745095, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.36556313131313134, "width": 0.3974493464052288, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.37940025252525256, "width": 0.40002450980392157, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.3932373737373738, "width": 0.1584477124183007, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4015"}, {"text": "We also only focused on input rather than output (e.g. self-lubrication, actuation of hair follicles or temperature) that we discuss in the future work section.", "label": "Author", "bboxes": [{"left": 0.6881045751633987, "top": 0.3932373737373738, "width": 0.23401797385620915, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.4070732323232323, "width": 0.3974460784313727, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4209103535353535, "width": 0.36106209150326796, "height": 0.012579545454545482, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4016"}, {"text": "To design the articial skin, we propose a bio-driven approach (illustrated in Figure 2) aiming to replicate the main properties of the human skin.", "label": "Author", "bboxes": [{"left": 0.28613071895424835, "top": 0.5432689393939394, "width": 0.19916176470588237, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5571060606060606, "width": 0.3971781045751634, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5709431818181818, "width": 0.33454248366013073, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4017"}, {"text": "We then use this knowledge to dene the most suitable material for creating articial skin.", "label": "Author", "bboxes": [{"left": 0.2857990196078431, "top": 0.5986174242424243, "width": 0.19949509803921572, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6124532828282828, "width": 0.3847647058823529, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4018"}, {"text": "Our goal is to replicate the three layers of the human skin: The epidermis layer provides both visual and tactile perception (e.g. texture); The dermis layer is the sensory layer embedding nerves to detect mechanical contact; The hypodermis layer provides kinesthetic feedback due to its soft mechanical properties (viscosity, thickness, etc.).", "label": "Author", "bboxes": [{"left": 0.1476339869281046, "top": 0.7325366161616161, "width": 0.33982352941176475, "height": 0.010063131313131302, "page": 2}, {"left": 0.08768954248366012, "top": 0.7438573232323232, "width": 0.39976797385620916, "height": 0.010063131313131302, "page": 2}, {"left": 0.08811928104575163, "top": 0.7551780303030303, "width": 0.3971764705882353, "height": 0.010063131313131302, "page": 2}, {"left": 0.08811928104575163, "top": 0.7665, "width": 0.3975277777777778, "height": 0.010063131313131302, "page": 2}, {"left": 0.0881062091503268, "top": 0.777820707070707, "width": 0.3455669934640523, "height": 0.010063131313131302, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4019"}, {"text": "Our exploration into simulating human skin properties starts with the replication of its sensory properties.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.6453472222222222, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 3}, {"left": 0.08753267973856209, "top": 0.6591830808080807, "width": 0.29792973856209143, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4020"}, {"text": "Because these properties have a large range of values, we choose to look at the question under a different angle: how to reproduce the skin so it is valuable for interaction as well.", "label": "Author", "bboxes": [{"left": 0.3914575163398693, "top": 0.6591830808080807, "width": 0.09383006535947708, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6730202020202021, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6868573232323232, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7006944444444445, "width": 0.2895947712418301, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4021"}, {"text": "We particularly look at the pigmentation , texture and strain/thickness in three studies, that helped guiding the design of our articial skin.", "label": "Author", "bboxes": [{"left": 0.38330882352941176, "top": 0.7006944444444445, "width": 0.10254901960784318, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7142424242424242, "width": 0.39720424836601304, "height": 0.012868686868686807, "page": 3}, {"left": 0.08811928104575163, "top": 0.7283686868686869, "width": 0.40001307189542484, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4022"}, {"text": "After, we pour an even layer of Ecoex Gel, and let it cool at room temperature for 2 hours before removing the sample from its mold.", "label": "Author", "bboxes": [{"left": 0.3696830065359477, "top": 0.7837171717171717, "width": 0.1156127450980392, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.7975542929292929, "width": 0.3971666666666667, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.8113901515151515, "width": 0.3542254901960784, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4023"}, {"text": "All these questions sparked our interest in understanding how to adapt articial skin to our interactive context.", "label": "Author", "bboxes": [{"left": 0.08753267973856209, "top": 0.5785037878787879, "width": 0.3983513071895425, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5923409090909091, "width": 0.3167401960784314, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4024"}, {"text": "We address these points in the following section through three user studies.", "label": "Author", "bboxes": [{"left": 0.40991013071895427, "top": 0.5923409090909091, "width": 0.07537745098039211, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6061780303030303, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4025"}, {"text": "Moving on to reproducing the properties of the skin described above, we looked at common material used in other elds of research.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.14829924242424242, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.16213636363636363, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.17597348484848485, "width": 0.0794019607843137, "height": 0.012579545454545454, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4026"}, {"text": "We recruited 15 participants (10 males, mean age 21) from our university to test each sample.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.31520580808080806, "width": 0.39822549019607856, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3290429292929293, "width": 0.20200653594771234, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4027"}, {"text": "We also asked participants to rate their impressions about the samples according to the following scales: fake/natural , machinelike/humanlike , articial/lifelike , which are often used to assess anthropomorphism [9].", "label": "Author", "bboxes": [{"left": 0.7030212418300654, "top": 0.4259015151515152, "width": 0.21881699346405215, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.4397386363636364, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.45328661616161614, "width": 0.39922549019607845, "height": 0.012868686868686918, "page": 3}, {"left": 0.5240784313725491, "top": 0.46741287878787874, "width": 0.3508398692810458, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4028"}, {"text": "We use different silicone products from Smooth-On Inc to reproduce the skin properties listed above.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.2942171717171717, "width": 0.3979379084967321, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811764705882352, "top": 0.3080542929292929, "width": 0.2819901960784314, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4029"}, {"text": "In particular, we use DragonSkin Pro-FX [84] platinum cured silicone to create the epidermis and dermis layers.", "label": "Author", "bboxes": [{"left": 0.37611928104575165, "top": 0.3080542929292929, "width": 0.10917320261437907, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.32189015151515155, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.33543813131313127, "width": 0.21113562091503268, "height": 0.012868686868686918, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4030"}, {"text": "We combine it with Silc pig pigments for the pigmentation and mould strategies (using Mold Start) for generating specic textures.", "label": "Author", "bboxes": [{"left": 0.3043169934640523, "top": 0.33543813131313127, "width": 0.18097222222222226, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.349564393939394, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3634015151515152, "width": 0.2899607843137255, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4031"}, {"text": "We use Ecoex Gel [85] for the hypodermis layer where we can manipulate its thickness and strain, a highly soft and exible silicone presenting mechanical properties close to human fat [98, 25].", "label": "Author", "bboxes": [{"left": 0.3831601307189543, "top": 0.3634015151515152, "width": 0.10253104575163396, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3772386363636364, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.39107575757575763, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.40491287878787874, "width": 0.39858496732026144, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4032"}, {"text": "Our rst experiment aims at understanding the impact of pigmentation on the perception of skin human-likeness and comfort, but also at detecting possible negative anthropomorphic effects.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.850270202020202, "width": 0.39987091503267974, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.8641073232323233, "width": 0.3998660130718954, "height": 0.012868686868686807, "page": 3}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.04704575163398693, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4033"}, {"text": "We study the impact of the strain/thickness on easiness and comfort of interaction, as well as human-likeness.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.3674179292929293, "width": 0.397921568627451, "height": 0.012868686868686863, "page": 4}, {"left": 0.5246633986928104, "top": 0.3815441919191919, "width": 0.324452614379085, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4034"}, {"text": "In the next experiment, we use the texture with small pores.", "label": "Author", "bboxes": [{"left": 0.9082320261437908, "top": 0.3109671717171717, "width": 0.013601307189542555, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.3248042929292929, "width": 0.37045588235294125, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4035"}, {"text": "Figure 5 illustrates the four samples of texture we compared.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.44551641414141413, "width": 0.4000261437908497, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4036"}, {"text": "We considered two realistic human skin samples (Fig.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.45935353535353535, "width": 0.3486797385620915, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4037"}, {"text": "We also considered two less realistic samples: (a) very smooth skin without any pores and wrinkles, and (d) skin with exaggerated pores size and wrinkles.", "label": "Author", "bboxes": [{"left": 0.25432352941176467, "top": 0.5147020202020202, "width": 0.2309722222222222, "height": 0.012579545454545538, "page": 4}, {"left": 0.08812254901960784, "top": 0.5285391414141414, "width": 0.39921568627450976, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.5423762626262626, "width": 0.3537369281045752, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4038"}, {"text": "We study different surface textures to mimic wrinkles of different body locations.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.3784709595959596, "width": 0.4006388888888889, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.39230808080808083, "width": 0.1418872549019608, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4039"}, {"text": "We compare their effect on comfort as well as the perception of skin human-likeness.", "label": "Author", "bboxes": [{"left": 0.23505228758169935, "top": 0.39230808080808083, "width": 0.2502369281045752, "height": 0.012579545454545427, "page": 4}, {"left": 0.08753267973856209, "top": 0.406145202020202, "width": 0.3014019607843137, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4040"}, {"text": "We recruited 16 participants (10 male, mean age 21) from our university.", "label": "Author", "bboxes": [{"left": 0.3071797385620915, "top": 0.5817474747474748, "width": 0.17811764705882355, "height": 0.012579545454545427, "page": 4}, {"left": 0.08758169934640524, "top": 0.5955845959595959, "width": 0.28918137254901954, "height": 0.012579545454545538, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4041"}, {"text": "We expected that the black and white colors would be perceived as more interactive because of their similarity with devices, but natural skin pigmentation was associated to a higher degree of interactivity.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.26375126262626264, "width": 0.4006437908496732, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811764705882352, "top": 0.27758838383838386, "width": 0.39717483660130726, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811764705882352, "top": 0.2914255050505051, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.3052626262626263, "width": 0.19246568627450977, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4042"}, {"text": "For the following, we keep the beige pigmentation and study different textures to investigate whether it can change the opinion of users regarding comfort.", "label": "Author", "bboxes": [{"left": 0.2856192810457516, "top": 0.3052626262626263, "width": 0.1996748366013072, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.31909974747474745, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 4}, {"left": 0.0875343137254902, "top": 0.33293686868686867, "width": 0.4006078431372549, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4043"}, {"text": "We used a similar design than previous studies.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.5259633838383838, "width": 0.3092042483660131, "height": 0.012579545454545538, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4044"}, {"text": "We recruited 16 participants (10 males, mean age 22) from our university.", "label": "Author", "bboxes": [{"left": 0.8381470588235295, "top": 0.5259633838383838, "width": 0.08369934640522858, "height": 0.012579545454545538, "page": 4}, {"left": 0.5234428104575163, "top": 0.539800505050505, "width": 0.40124183006535963, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4045"}, {"text": "Figure 7 illustrates the four different skin thicknesses we compared.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.4191616161616162, "width": 0.399874183006536, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4329987373737374, "width": 0.040787581699346376, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4046"}, {"text": "For the hypodermis thickness, we considered four values corresponding to different body areas: 2mm (face [74]), 5mm , 10mm (forearm [37]), 17mm (mean body [39]).", "label": "Author", "bboxes": [{"left": 0.5883823529411765, "top": 0.46067297979797983, "width": 0.3337450980392157, "height": 0.012579545454545427, "page": 4}, {"left": 0.5242565359477125, "top": 0.4742209595959596, "width": 0.3996290849673202, "height": 0.012868686868686863, "page": 4}, {"left": 0.5242565359477125, "top": 0.4880580808080808, "width": 0.3471764705882353, "height": 0.012868686868686918, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4047"}, {"text": "We also had the opportunity to observe that users spontaneously performed these gestures during the studies.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.822885101010101, "width": 0.4006437908496732, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3463807189542484, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4048"}, {"text": "The last part of our exploration focuses on the reproduction of the human skin sensing acuity.", "label": "Author", "bboxes": [{"left": 0.5241601307189543, "top": 0.2807828282828283, "width": 0.3976830065359477, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.2946199494949495, "width": 0.2032189542483661, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4049"}, {"text": "We present different sensing techniques and discuss which ones are more adapted to mimic human skin.", "label": "Author", "bboxes": [{"left": 0.7331470588235294, "top": 0.2946199494949495, "width": 0.1886895424836602, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.30845707070707074, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.32229419191919195, "width": 0.07743790849673204, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4050"}, {"text": "We then present our fabrication method and nish by presenting our hardware/software open toolkit that enables controlling the sensing layer, and demonstrate how we can detect the previously dened gestures.", "label": "Author", "bboxes": [{"left": 0.6069183006535949, "top": 0.32229419191919195, "width": 0.3149183006535947, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3361313131313131, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.3499671717171717, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3638042929292929, "width": 0.24870751633986932, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4051"}, {"text": "Our sensory exploration let us to form a series of guidelines for mimicking human skin for an interactive setup: for the pigmentation using a skin-like color; for the texture using a realistic skin pore and wrinkle structure; for the thickness , using a fat layer of 5mm to 10mm and a dermis of 1.2mm.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.5375795454545454, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.5514166666666667, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5649646464646465, "width": 0.3971895424836602, "height": 0.012868686868686918, "page": 5}, {"left": 0.08813562091503269, "top": 0.5788017676767677, "width": 0.39919934640522875, "height": 0.012868686868686807, "page": 5}, {"left": 0.08811928104575163, "top": 0.5929267676767677, "width": 0.38054901960784315, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4052"}, {"text": "We found that the human skin affords two main types of gestures: gestures of mediated communication between individuals from the social literature [33, 32] and traditional 2D multi-touch gestures, for interface control though extracted from On-Skin literature [100].", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.7046414141414141, "width": 0.4006470588235294, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.39987418300653593, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.759989898989899, "width": 0.20301960784313722, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4053"}, {"text": "Our design space (Figure 9) summarizes the gestures relevant for Skin-On interfaces.", "label": "Author", "bboxes": [{"left": 0.2983954248366013, "top": 0.759989898989899, "width": 0.18799346405228762, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7738270202020202, "width": 0.36356699346405236, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4054"}, {"text": "Our results suggest that users tend to transpose the interactions they are doing with real skin to articial skin, and that articial skin leverages the expressive gestures and tactile expressions of pro-social emotions.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.1305151515151515, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.14435227272727272, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.15818939393939394, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.17202651515151515, "width": 0.1579232026143791, "height": 0.012579545454545454, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4055"}, {"text": "So, we decided to build on the gestures illustrated in Figure 9 to dene the sensing capabilities of Skin-On interfaces.", "label": "Author", "bboxes": [{"left": 0.5602549019607843, "top": 0.2273737373737374, "width": 0.3642924836601308, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.24121085858585856, "width": 0.39679411764705885, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4056"}, {"text": "We choose to use a matrix layout because it is easier to fabricate and requires less components and apparatus.", "label": "Author", "bboxes": [{"left": 0.8527418300653595, "top": 0.8291755050505051, "width": 0.06909477124183006, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8430113636363636, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.8568484848484849, "width": 0.2617630718954249, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4057"}, {"text": "To inform our choices we have a series of requirements:", "label": "Author", "bboxes": [{"left": 0.8330245098039215, "top": 0.47256186868686867, "width": 0.08909803921568638, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.48639772727272723, "width": 0.26975490196078433, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4058"}, {"text": "We choose to implement our sensor using a matrix layout sensing mutual capacitance.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.6832575757575757, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.6970946969696971, "width": 0.1786928104575164, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4059"}, {"text": "To understand the reasons behind this choice we need to explain the different techniques that can be used.", "label": "Author", "bboxes": [{"left": 0.7083970588235294, "top": 0.6970946969696971, "width": 0.21343954248366015, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7109318181818182, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7247676767676767, "width": 0.054627450980392234, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4060"}, {"text": "Our next step in the design of the articial skin was to identify the types of gestures which are desirable for Skin-On interfaces.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.6417462121212121, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6555833333333333, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6694204545454546, "width": 0.03808986928104573, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4061"}, {"text": "In doing this we aimed to rene our specication for the design of the sensing aspect of the skin.", "label": "Author", "bboxes": [{"left": 0.1327516339869281, "top": 0.6694204545454546, "width": 0.35282352941176476, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6832575757575757, "width": 0.28258333333333335, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4062"}, {"text": "This approach is often used for smart textiles [17] but requires large electrodes (>1cm), which does not t with our requirement of spacial acuity.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.2367121212121212, "width": 0.39768300653594774, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.2505492424242424, "width": 0.3998725490196079, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.26438636363636364, "width": 0.15418627450980393, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4063"}, {"text": "We choose this approach for all these reasons.", "label": "Author", "bboxes": [{"left": 0.38734313725490194, "top": 0.3612449494949495, "width": 0.09794771241830069, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3750820707070707, "width": 0.19754248366013072, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4064"}, {"text": "We used conductive insulated Datastretch thread [93], which allows a strain up to 30%, is 0.2mm thick, and has a conductivity of 4.2  /m.", "label": "Author", "bboxes": [{"left": 0.8643986928104574, "top": 0.3687929292929293, "width": 0.057450980392157014, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246748366013072, "top": 0.3823409090909091, "width": 0.39717483660130715, "height": 0.012868686868686807, "page": 6}, {"left": 0.5246748366013072, "top": 0.3964671717171717, "width": 0.3977385620915034, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246748366013072, "top": 0.4103030303030303, "width": 0.07438888888888895, "height": 0.012729797979798008, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4065"}, {"text": "The electrode pattern can only have a limited number of electric lines, but this technique remains the fastest to fabricate and requires few material which makes it appropriate considering our requirements.", "label": "Author", "bboxes": [{"left": 0.6936519607843137, "top": 0.47948863636363637, "width": 0.2281879084967321, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.4933257575757576, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5071628787878788, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.521, "width": 0.2941111111111111, "height": 0.012579545454545538, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4066"}, {"text": "Next, we explain how we used this material to fabricate our articial skin.", "label": "Author", "bboxes": [{"left": 0.7781274509803922, "top": 0.5486742424242425, "width": 0.1442908496732026, "height": 0.012579545454545427, "page": 6}, {"left": 0.5240784313725491, "top": 0.562510101010101, "width": 0.3348545751633988, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4067"}, {"text": "To implement the electrode pattern described above, we need a conductive material that ts our requirements.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.41280681818181814, "width": 0.3976748366013072, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.42664393939393935, "width": 0.3088137254901961, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4068"}, {"text": "We excluded solutions that rely on complex machinery or a complex fabrication process to t with our accessible requirement.", "label": "Author", "bboxes": [{"left": 0.4019918300653595, "top": 0.42664393939393935, "width": 0.08329901960784308, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.44048106060606057, "width": 0.39988071895424837, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.4543181818181818, "width": 0.3725490196078431, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4069"}, {"text": "In particular, we excluded solutions such as depositing of hard conductive particles or liquid conductive metal in a microuidic channel [57, 54].", "label": "Author", "bboxes": [{"left": 0.47146078431372546, "top": 0.4543181818181818, "width": 0.013831699346405268, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.468155303030303, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.4819924242424242, "width": 0.39988235294117647, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.49582828282828284, "width": 0.1410751633986928, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4070"}, {"text": "We also tested the solutions described below before choosing to use conductive thread.", "label": "Author", "bboxes": [{"left": 0.23426470588235296, "top": 0.49582828282828284, "width": 0.25102941176470583, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5096654040404041, "width": 0.3140490196078432, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4071"}, {"text": "Once cured, we laser cut it to the desired pattern and sealed it into another silicone layer.", "label": "Author", "bboxes": [{"left": 0.6353970588235294, "top": 0.222875, "width": 0.2864395424836601, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.2367121212121212, "width": 0.3077973856209151, "height": 0.012579545454545454, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4072"}, {"text": "We thus discarded this solution.", "label": "Author", "bboxes": [{"left": 0.6202859477124183, "top": 0.3335707070707071, "width": 0.20714705882352946, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4073"}, {"text": "We tested two conductive silicones.", "label": "Author", "bboxes": [{"left": 0.12394444444444444, "top": 0.6354570707070707, "width": 0.22861274509803917, "height": 0.012579545454545538, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4074"}, {"text": "First, we prepared a cPDMS mixing carbon black, EcoFlex 00-30 silicone and D5 solvent.", "label": "Author", "bboxes": [{"left": 0.35762908496732027, "top": 0.6354570707070707, "width": 0.1276683006535948, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.6492941919191919, "width": 0.397171568627451, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.6631313131313132, "width": 0.05187091503267975, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4075"}, {"text": "It has a theoretical conductivity of 2  / cm when mixing manually, but we could not get a conductivity under 10k  / cm .", "label": "Author", "bboxes": [{"left": 0.20300490196078433, "top": 0.7320265151515152, "width": 0.2822892156862745, "height": 0.013018939393939388, "page": 6}, {"left": 0.08753267973856209, "top": 0.7461527777777778, "width": 0.3983366013071895, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811437908496732, "top": 0.7597007575757576, "width": 0.11201307189542482, "height": 0.013018939393939388, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4076"}, {"text": "We also explored conductive fabric, which is used in the DIY wearable community [17].", "label": "Author", "bboxes": [{"left": 0.2121062091503268, "top": 0.8505593434343435, "width": 0.27319934640522875, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.29930718954248364, "height": 0.012579545454545538, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4077"}, {"text": "We used a Silver plated stretchable conductive fabric (stretch-width:65%, stretch-length:100%) to create a composite fabric + silicone", "label": "Author", "bboxes": [{"left": 0.3924934640522876, "top": 0.8643964646464646, "width": 0.09550816993464051, "height": 0.012579545454545538, "page": 6}, {"left": 0.0877124183006536, "top": 0.8782335858585859, "width": 0.39961437908496744, "height": 0.012579545454545427, "page": 6}, {"left": 0.08812745098039215, "top": 0.892070707070707, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4078"}, {"text": "Thus, we discarded this solution.", "label": "Author", "bboxes": [{"left": 0.27314379084967316, "top": 0.5863977272727273, "width": 0.21432679738562094, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4079"}, {"text": "We now present the steps needed to fabricate our articial skin (Figure 10).", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.6065252525252525, "width": 0.39793790849673216, "height": 0.012579545454545427, "page": 6}, {"left": 0.5241274509803922, "top": 0.6203623737373737, "width": 0.08014869281045756, "height": 0.012579545454545538, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4080"}, {"text": "We focus here on how embedding the sensing layer impacts the fabrication process.", "label": "Author", "bboxes": [{"left": 0.6120310457516339, "top": 0.6203623737373737, "width": 0.30980065359477127, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6341982323232324, "width": 0.24260294117647063, "height": 0.012579545454545538, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4081"}, {"text": "We developed an Open Source and Open Hardware multitouch controller 1 with a total cost of $4.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.5497083333333334, "width": 0.40064379084967316, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811764705882352, "top": 0.5615429292929294, "width": 0.2728921568627451, "height": 0.014582070707070627, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4082"}, {"text": "This contribution enables DIY fabrication of multi-touch interfaces on nonconventional surfaces such as human skin [59], walls [110] or, as in our case, exible silicone.", "label": "Author", "bboxes": [{"left": 0.3696437908496732, "top": 0.5635454545454545, "width": 0.11564869281045759, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.5773825757575757, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.5912184343434344, "width": 0.39920098039215685, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6050555555555556, "width": 0.19791666666666669, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4083"}, {"text": "We used both an Arduino Pro Micro board for sending the data via serial communication to a laptop, and a Wemos D1 mini for transmitting the information wirelessly to the mobile device.", "label": "Author", "bboxes": [{"left": 0.2973545751633987, "top": 0.6880782828282828, "width": 0.18794444444444447, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7019154040404041, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7157512626262627, "width": 0.3998758169934641, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7295883838383838, "width": 0.23384640522875821, "height": 0.012579545454545538, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4084"}, {"text": "We now explain how we detect touch contact, then more complex gestures.", "label": "Author", "bboxes": [{"left": 0.32700490196078436, "top": 0.7295883838383838, "width": 0.15828758169934642, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7434255050505051, "width": 0.32526633986928105, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4085"}, {"text": "Using the data read (in serial or wireless) from the sensing and transmitting electrodes, we build a 2D image of 12x21 pixels.", "label": "Author", "bboxes": [{"left": 0.17165522875816994, "top": 0.8247941919191919, "width": 0.3136372549019608, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.8386313131313131, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.8524671717171717, "width": 0.10744607843137256, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4086"}, {"text": "To minimize the background noise, we perform an initial calibration.", "label": "Author", "bboxes": [{"left": 0.5241601307189543, "top": 0.3222373737373737, "width": 0.3976715686274508, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.33607449494949493, "width": 0.07222058823529409, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4087"}, {"text": "After the board is detected, we create a calibration matrix, by averaging the individual value of each coordinate 10 times.", "label": "Author", "bboxes": [{"left": 0.6018447712418301, "top": 0.33607449494949493, "width": 0.31999183006535936, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.34991161616161615, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5234428104575163, "top": 0.36374873737373736, "width": 0.05850000000000011, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4088"}, {"text": "We apply a threshold to remove points under 0.1%, that we consider as background noise.", "label": "Author", "bboxes": [{"left": 0.7105653594771242, "top": 0.40525883838383836, "width": 0.2112712418300653, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4190959595959596, "width": 0.3748022875816993, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4089"}, {"text": "We present the implementation of our hardware and software toolkit and demonstrate its gesture recognition algorithm, which can detect gestures proposed in the previous section of this paper.", "label": "Author", "bboxes": [{"left": 0.3634983660130719, "top": 0.4683396464646465, "width": 0.1245016339869281, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4821767676767677, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4960138888888889, "width": 0.3998758169934641, "height": 0.012579545454545371, "page": 7}, {"left": 0.08811928104575163, "top": 0.5098510101010101, "width": 0.3346258169934641, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4090"}, {"text": "To support accurate spacial interpolation, we upscale the image 5x using the Lanczos-4 algorithm (Figure 12-c).", "label": "Author", "bboxes": [{"left": 0.5241601307189543, "top": 0.44048106060606057, "width": 0.3976830065359477, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4543181818181818, "width": 0.33054084967320263, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4091"}, {"text": "We apply contour detection to separate distinct elements on the image as blobs .", "label": "Author", "bboxes": [{"left": 0.8101797385620915, "top": 0.4819924242424242, "width": 0.11194281045751642, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4955391414141414, "width": 0.4000179738562092, "height": 0.012868686868686863, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4092"}, {"text": "We calculate the relative surface of each blob area and the nearest tting ellipsoid to get its center and orientation (Figure 12-d).", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.5096654040404041, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5235025252525253, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 7}, {"left": 0.5234428104575163, "top": 0.5373396464646465, "width": 0.040116013071895495, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4093"}, {"text": "In a pilot study, we dened the maximum radius (5mm) that a single nger press can have on this surface (Fig. 12-c).", "label": "Author", "bboxes": [{"left": 0.7420130718954249, "top": 0.7046414141414141, "width": 0.1798235294117646, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.1685947712418301, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4094"}, {"text": "To determine and track the position of multiple points over time, we use the contour points (stored in a k-d tree), and nd the closest blob position in O(log n ).", "label": "Author", "bboxes": [{"left": 0.6981225490196078, "top": 0.7323156565656566, "width": 0.22371405228758168, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7597007575757576, "width": 0.3829738562091505, "height": 0.012868686868686807, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4095"}, {"text": "Overall, our data processing algorithm provides a spacial acuity of 2mm with an electrode spacing of 4mm.", "label": "Author", "bboxes": [{"left": 0.6369101307189542, "top": 0.5863977272727273, "width": 0.2876356209150328, "height": 0.012579545454545427, "page": 7}, {"left": 0.5242663398692811, "top": 0.6002348484848485, "width": 0.3975718954248365, "height": 0.012579545454545427, "page": 7}, {"left": 0.52409477124183, "top": 0.6140719696969698, "width": 0.03701307189542491, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4096"}, {"text": "The two-point discrimination threshold of our prototype is 10mm, which is better, in average, than with the human skin [102].", "label": "Author", "bboxes": [{"left": 0.6626666666666667, "top": 0.6279090909090909, "width": 0.25916993464052285, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.6417462121212121, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.5240784313725491, "top": 0.6555833333333333, "width": 0.17589215686274506, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4097"}, {"text": "We ran a preliminary study with 8 participants on a subset of 8 gestures.", "label": "Author", "bboxes": [{"left": 0.27924346405228756, "top": 0.1997007575757576, "width": 0.20605228758169936, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.21353661616161618, "width": 0.253983660130719, "height": 0.012579545454545454, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4098"}, {"text": "The selected gestures are representative of the capabilities of our device: they leverage skin depth, allow multi-touch interaction, and are not a combination of basic gestures.", "label": "Author", "bboxes": [{"left": 0.3471421568627451, "top": 0.21353661616161618, "width": 0.1381503267973857, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.39987091503267974, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.24121085858585856, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.20541176470588235, "height": 0.012579545454545482, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4099"}, {"text": "Although preliminary, these results are promising and demonstrate the feasibility of our approach.", "label": "Author", "bboxes": [{"left": 0.1694330065359477, "top": 0.32423358585858586, "width": 0.31585947712418305, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.3380707070707071, "width": 0.31149346405228756, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4100"}, {"text": "We rst describe the implementation of three Skin-on interface prototypes with different form factors shown in Figure 1.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.3809684343434343, "width": 0.4006437908496732, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811764705882352, "top": 0.3948055555555555, "width": 0.39717483660130726, "height": 0.012579545454545482, "page": 8}, {"left": 0.08688888888888889, "top": 0.40864267676767674, "width": 0.012454248366013068, "height": 0.012579545454545482, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4101"}, {"text": "We then present the applications we developed for these prototypes.", "label": "Author", "bboxes": [{"left": 0.10686437908496732, "top": 0.40864267676767674, "width": 0.37841176470588234, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.42247853535353536, "width": 0.07134313725490195, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4102"}, {"text": "We also fabricated a Skin-On wristband to alleviate the limited input and output capabilities of smartwatches [65] (Figure 1-c).", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.7222474747474747, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 8}, {"left": 0.08812091503267974, "top": 0.736084595959596, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4103"}, {"text": "We implemented a pressure-based menu.", "label": "Author", "bboxes": [{"left": 0.6447124183006536, "top": 0.6726237373737374, "width": 0.2799640522875817, "height": 0.012579545454545538, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4104"}, {"text": "We built a Skin-On smartphone case (Figure ?? -bottom) providing advanced input and output capabilities on the back and side of a mobile device [47, 11, 80].", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.4947260101010101, "width": 0.40063725490196084, "height": 0.01266666666666677, "page": 8}, {"left": 0.0877124183006536, "top": 0.5086502525252525, "width": 0.3975816993464052, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5224873737373738, "width": 0.2440996732026144, "height": 0.012579545454545538, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4105"}, {"text": "We also built a Skin-On interface for built-in and external touchpads.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.6292866161616162, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6431224747474747, "width": 0.06958986928104575, "height": 0.012579545454545538, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4106"}, {"text": "We created two interfaces with two different sizes and thicknesses (9cm x 12cm and 10cm x 14.5cm, thickness 7mm) that can be connected to a device via USB (Figure ?? -top).", "label": "Author", "bboxes": [{"left": 0.16275326797385622, "top": 0.6431224747474747, "width": 0.3225375816993463, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.656959595959596, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 8}, {"left": 0.0877124183006536, "top": 0.6707967171717171, "width": 0.3975718954248366, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.6845467171717171, "width": 0.05199509803921569, "height": 0.012666666666666715, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4107"}, {"text": "For instance in study 3 we saw several users spontaneously pulling", "label": "Author", "bboxes": [{"left": 0.46349673202614383, "top": 0.8782335858585859, "width": 0.02207516339869281, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4108"}, {"text": "Further tests are needed to evaluate the robustness of our system.", "label": "Author", "bboxes": [{"left": 0.6820898692810458, "top": 0.0814570707070707, "width": 0.23974836601307192, "height": 0.012579545454545468, "page": 9}, {"left": 0.5246633986928104, "top": 0.09529419191919192, "width": 0.18997385620915042, "height": 0.012579545454545468, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4109"}, {"text": "While preliminary studies indicate that we can recognize 8 touch gestures and multi-touch ones, taking individual variability into account and using better recognition algorithms (typically relying on machine learning) would improve the recognition rate and allow distinguishing variations of these gestures (e.g. soft grab vs. hard grab).", "label": "Author", "bboxes": [{"left": 0.7196764705882353, "top": 0.09529419191919192, "width": 0.20486111111111116, "height": 0.012579545454545468, "page": 9}, {"left": 0.5246633986928104, "top": 0.10913131313131313, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.12296717171717171, "width": 0.39745915032679746, "height": 0.01257954545454544, "page": 9}, {"left": 0.5246633986928104, "top": 0.13680429292929294, "width": 0.39827287581699344, "height": 0.012579545454545454, "page": 9}, {"left": 0.5240784313725491, "top": 0.15064141414141416, "width": 0.3977614379084967, "height": 0.012579545454545427, "page": 9}, {"left": 0.5242565359477125, "top": 0.16447853535353535, "width": 0.3712189542483658, "height": 0.012579545454545454, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4110"}, {"text": "We also plan to study the factors (e.g.the number of repetitions, gesture strength, etc.) that alter the sensing capabilities and the mechanical properties of the articial skin.", "label": "Author", "bboxes": [{"left": 0.9005261437908496, "top": 0.16447853535353535, "width": 0.021313725490196278, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.17831565656565657, "width": 0.3991993464052288, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.19215277777777778, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.205989898989899, "width": 0.2829673202614379, "height": 0.012579545454545454, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4111"}, {"text": "This work also brings technical challenges that are worth deepening and that are not covered in this paper, including the impact of curvature on spatial sensing acuity and signal to noise ratio.", "label": "Author", "bboxes": [{"left": 0.5593137254901961, "top": 0.3166856060606061, "width": 0.3625228758169935, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.3305227272727273, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.34435984848484846, "width": 0.39717973856209154, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.3581969696969697, "width": 0.07234313725490193, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4112"}, {"text": "We see several directions to investigate other form factors.", "label": "Author", "bboxes": [{"left": 0.8226143790849673, "top": 0.3795808080808081, "width": 0.09921078431372554, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246683006535947, "top": 0.3934179292929293, "width": 0.2918856209150328, "height": 0.012579545454545482, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4113"}, {"text": "While our paper focuses on common interactive systems (PC, smartphones, smartwatches), Skin-On interfaces could also be useful in a wide range of setups, including robots and connected objects, or for extending the capabilities of everyday life objects.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.5531729797979797, "width": 0.3999722222222223, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.567010101010101, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5808472222222222, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.5946830808080807, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.608520202020202, "width": 0.0785539215686275, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4114"}, {"text": "We envision interaction scenarios where SkinOn and On-Skin interfaces co-exist in a complementary way: the continuity of interaction across existing devices (mobile, desktop and skin-worn) would be maintained through similar skin-based interaction paradigms.", "label": "Author", "bboxes": [{"left": 0.6117189542483661, "top": 0.608520202020202, "width": 0.3128251633986928, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.6223573232323232, "width": 0.3994395424836602, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.6361944444444444, "width": 0.3992124183006537, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.6500315656565657, "width": 0.3974477124183007, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.6638686868686868, "width": 0.2192745098039216, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4115"}, {"text": "We implemented a messaging application where users can express rich tactile emoticons on the articial skin.", "label": "Author", "bboxes": [{"left": 0.31266176470588236, "top": 0.5136212121212121, "width": 0.17534150326797387, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5274583333333334, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5412954545454546, "width": 0.13718464052287582, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4116"}, {"text": "We now discuss future directions regarding the implementation, the concept and the approach.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.8782335858585859, "width": 0.40064379084967316, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.22739869281045755, "height": 0.012579545454545427, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4117"}, {"text": "We aim to study Skin-On interfaces as an output modality.", "label": "Author", "bboxes": [{"left": 0.8085049019607843, "top": 0.6852525252525253, "width": 0.11390522875816989, "height": 0.012579545454545427, "page": 9}, {"left": 0.524671568627451, "top": 0.6990896464646464, "width": 0.277921568627451, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4118"}, {"text": "While, so far, we have focused on conveying different types of information with Skin-On interfaces, our future aim is to perceive affect through articial skin to reinforce engagement between interaction partners.", "label": "Author", "bboxes": [{"left": 0.5668545751633988, "top": 0.8097866161616162, "width": 0.3549771241830063, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8236237373737373, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.837459595959596, "width": 0.399872549019608, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8512967171717172, "width": 0.22749673202614384, "height": 0.012579545454545538, "page": 9}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4119"}, {"text": "More generally, our goal is to further explore various types of anthropomorphism towards human-like devices.", "label": "Author", "bboxes": [{"left": 0.14719444444444443, "top": 0.15064141414141416, "width": 0.3380980392156864, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.369609477124183, "height": 0.012579545454545454, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4120"}, {"text": "Uncanny valley has been principally a no-go zone in HCI [8], and our work challenges this.", "label": "Author", "bboxes": [{"left": 0.19773366013071897, "top": 0.18586363636363637, "width": 0.2875588235294118, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.1997007575757576, "width": 0.3041143790849673, "height": 0.012579545454545454, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4121"}, {"text": "For instance, the perception of our participants changed from Study 1 (visual condition only) to Study 2 (visual and tactile perception) although the same interfaces were used.", "label": "Author", "bboxes": [{"left": 0.4053186274509804, "top": 0.2273737373737374, "width": 0.08201470588235288, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.24121085858585856, "width": 0.3998807189542484, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.39826633986928106, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.268885101010101, "width": 0.27304084967320263, "height": 0.012579545454545427, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4122"}, {"text": "We interpret this result as subtle interaction effects between visual and haptic perception regarding skin perception, which also depends on a combination of factors (including the duration of the interaction, the degree of realism of the device, etc.).", "label": "Author", "bboxes": [{"left": 0.37158660130718957, "top": 0.268885101010101, "width": 0.11370588235294121, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.2827222222222222, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.2965593434343434, "width": 0.3971781045751634, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.31039646464646464, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.32423358585858586, "width": 0.3040571895424837, "height": 0.012579545454545427, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4123"}, {"text": "We think this would merit a qualitative study of its own.", "label": "Author", "bboxes": [{"left": 0.39725653594771243, "top": 0.32423358585858586, "width": 0.08803594771241835, "height": 0.012579545454545427, "page": 10}, {"left": 0.0875375816993464, "top": 0.3380707070707071, "width": 0.2860915032679739, "height": 0.012579545454545427, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4124"}, {"text": "More generally, our work explores the intersection between man and machine (human augmentation) from a new and radical perspective: instead of augmenting the human with parts of machines, we demonstrate how machines can be augmented with parts of human.", "label": "Author", "bboxes": [{"left": 0.38148366013071894, "top": 0.3380707070707071, "width": 0.10585130718954255, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.3519078282828283, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 10}, {"left": 0.08758169934640524, "top": 0.36574368686868686, "width": 0.3999803921568627, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.3795808080808081, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.3934179292929293, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.4072550505050505, "width": 0.04838071895424835, "height": 0.012579545454545482, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4125"}, {"text": "This work was supported by the Engineering and Physical Sciences Research Council (grant number EPSRC EP/P004342/1, EP/M021882/1, EP/R02961X/1) and the Agence Nationale de la Recherche (ANR-17-CE33-0006 SocialTouch, ANR-16CE330023 GESTURE).", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.822885101010101, "width": 0.4003790849673202, "height": 0.012579545454545427, "page": 10}, {"left": 0.08812418300653595, "top": 0.8367222222222221, "width": 0.3992156862745098, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.16343464052287582, "height": 0.012579545454545427, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4126"}, {"text": "Using articial skin on a device may create similar effects, and could change the engagement or affection that we have towards inanimate objects such as interactive devices.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.49782449494949493, "width": 0.39919934640522875, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.5116616161616161, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.5254987373737373, "width": 0.34108006535947716, "height": 0.012579545454545538, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4127"}, {"text": "We thus believe that our anthropomorphic approach can inspire other researchers and lead to a novel generation of devices with an input system closer to nature.", "label": "Author", "bboxes": [{"left": 0.4341290849673203, "top": 0.5254987373737373, "width": 0.05116339869281045, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.5393358585858585, "width": 0.39745588235294127, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.5531729797979797, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.567010101010101, "width": 0.1912598039215686, "height": 0.012579545454545427, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4128"}, {"text": "We presented a bio-driven approach which is singular in HCI.", "label": "Author", "bboxes": [{"left": 0.24072222222222223, "top": 0.5883939393939394, "width": 0.24456372549019612, "height": 0.012579545454545538, "page": 10}, {"left": 0.08753267973856209, "top": 0.6022310606060606, "width": 0.16311928104575163, "height": 0.012579545454545427, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4129"}, {"text": "One challenge we faced was to conciliate an holistic approach and an iterative design.", "label": "Author", "bboxes": [{"left": 0.25472549019607843, "top": 0.6022310606060606, "width": 0.23327614379084965, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.6160681818181818, "width": 0.33062254901960786, "height": 0.012579545454545538, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4130"}, {"text": "e.g. we observed that it was difcult to study the color of the skin independently from its texture.", "label": "Author", "bboxes": [{"left": 0.16297058823529412, "top": 0.6437424242424242, "width": 0.3223218954248367, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.6575795454545454, "width": 0.3128202614379085, "height": 0.012579545454545427, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4131"}, {"text": "Further investigations are needed to provide clearer guidelines to follow a bio-driven approach in HCI, and we believe that exploring synergies with other elds such as Material engineering or Robotics will be a powerful means to further the development of advanced interactive devices [68].", "label": "Author", "bboxes": [{"left": 0.4165343137254902, "top": 0.6852525252525253, "width": 0.07146568627450978, "height": 0.012579545454545427, "page": 10}, {"left": 0.0877124183006536, "top": 0.6990896464646464, "width": 0.39816666666666667, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.7129267676767678, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.7267638888888889, "width": 0.3974558823529412, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.7406010101010102, "width": 0.3971781045751634, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.7544381313131313, "width": 0.2385653594771242, "height": 0.012579545454545427, "page": 10}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4132"}, {"text": "1. From a sensory point of view, we study how to reproduce the visual, tactile and kinesthetic aspects of the human skin.", "label": "Author", "bboxes": [{"left": 0.08486274509803922, "top": 0.24112373737373738, "width": 0.40042647058823533, "height": 0.01266666666666666, "page": 1}, {"left": 0.10439869281045752, "top": 0.2550479797979798, "width": 0.38375000000000004, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4133"}, {"text": "We motivate our use of silicone to mimic the skin deformability with reference to relevant literature.", "label": "Author", "bboxes": [{"left": 0.10363235294117647, "top": 0.268885101010101, "width": 0.38436764705882354, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.2827222222222222, "width": 0.2793333333333333, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4134"}, {"text": "Then, through three user studies, we investigate how visual factors (color) and haptic factors (texture and thickness) impact user experience, and the perception of realism.", "label": "Author", "bboxes": [{"left": 0.39055555555555554, "top": 0.2827222222222222, "width": 0.09472875816993465, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.2965593434343434, "width": 0.38197875816993465, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.31039646464646464, "width": 0.38359313725490196, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.32423358585858586, "width": 0.24185294117647058, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4135"}, {"text": "2. From a gestural point of view, we explore how gestures naturally performed on skin can be transposed to Skin-On interfaces.", "label": "Author", "bboxes": [{"left": 0.08486437908496731, "top": 0.34740404040404044, "width": 0.4004346405228758, "height": 0.012667929292929259, "page": 1}, {"left": 0.10439869281045752, "top": 0.3613282828282829, "width": 0.3809003267973856, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.375165404040404, "width": 0.06852614379084968, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4136"}, {"text": "We use this knowledge to propose a series of gestures that are desirable for Skin-on interfaces (e.g. multitouch touch, pressure and complex gestures such as strokes, stretching or grabbing).", "label": "Author", "bboxes": [{"left": 0.1806781045751634, "top": 0.375165404040404, "width": 0.3046062091503269, "height": 0.012579545454545482, "page": 1}, {"left": 0.10439869281045752, "top": 0.3890025252525252, "width": 0.38359640522875815, "height": 0.012579545454545482, "page": 1}, {"left": 0.10439869281045752, "top": 0.40283964646464643, "width": 0.38292320261437907, "height": 0.012579545454545482, "page": 1}, {"left": 0.10439869281045752, "top": 0.41667676767676765, "width": 0.15326470588235294, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4137"}, {"text": "3. From a sensing point of view, we analyze different fabrication methods to create a silicone layer that can track the previously dened gestures with a spatial acuity comparable to human skin.", "label": "Author", "bboxes": [{"left": 0.08486437908496731, "top": 0.4398472222222222, "width": 0.40312418300653596, "height": 0.012667929292929314, "page": 1}, {"left": 0.10439869281045752, "top": 0.4537714646464647, "width": 0.3809003267973856, "height": 0.012579545454545482, "page": 1}, {"left": 0.10439869281045752, "top": 0.4676085858585859, "width": 0.3809003267973856, "height": 0.012579545454545482, "page": 1}, {"left": 0.10439869281045752, "top": 0.48144570707070705, "width": 0.09305555555555553, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4138"}, {"text": "We also contribute a DIY fabrication method and offer an open-hardware tool enabling easy reproduction by other researchers and practitioners.", "label": "Author", "bboxes": [{"left": 0.20224019607843138, "top": 0.48144570707070705, "width": 0.2830588235294117, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.49528282828282827, "width": 0.3809003267973856, "height": 0.012579545454545371, "page": 1}, {"left": 0.10439869281045752, "top": 0.5091199494949495, "width": 0.24816993464052287, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4139"}, {"text": "Strain/thickness : we want to reproduce the deformability of the skin as described earlier.", "label": "Author", "bboxes": [{"left": 0.5254771241830065, "top": 0.5074936868686869, "width": 0.3963529411764706, "height": 0.012868686868686807, "page": 5}, {"left": 0.5409428104575164, "top": 0.5216199494949495, "width": 0.1804003267973856, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4140"}, {"text": "We are particularly focused on sensing layer of thickness below 1.2mm to match human dermis thickness.", "label": "Author", "bboxes": [{"left": 0.7263676470588235, "top": 0.5216199494949495, "width": 0.19547549019607846, "height": 0.012579545454545538, "page": 5}, {"left": 0.5409428104575164, "top": 0.5354570707070707, "width": 0.38088562091503264, "height": 0.012579545454545427, "page": 5}, {"left": 0.5409428104575164, "top": 0.5492941919191919, "width": 0.11303921568627451, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4141"}, {"text": "Accuracy : we want to build accurate sensors that can reproduce the human skin sensing acuity and detect the gestures dened previously.", "label": "Author", "bboxes": [{"left": 0.5254787581699347, "top": 0.5701868686868687, "width": 0.39906209150326777, "height": 0.012868686868686807, "page": 5}, {"left": 0.5409428104575164, "top": 0.5843118686868687, "width": 0.3809019607843137, "height": 0.012579545454545427, "page": 5}, {"left": 0.5409428104575164, "top": 0.5981489898989899, "width": 0.12332843137254901, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4142"}, {"text": "Accessibility : we want to use accessible technologies, i.e. the process should be easy to reproduce by HCI practitioners with affordable material and without high-end equipment.", "label": "Author", "bboxes": [{"left": 0.5254787581699347, "top": 0.6190416666666667, "width": 0.399204248366013, "height": 0.012868686868686918, "page": 5}, {"left": 0.5409428104575164, "top": 0.6331679292929293, "width": 0.3809003267973856, "height": 0.012579545454545427, "page": 5}, {"left": 0.5403562091503268, "top": 0.6470050505050505, "width": 0.37711437908496726, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4143"}, {"text": "We prepare a rectangular mold of the size of the desired articial skin and place it on top of", "label": "Author", "bboxes": [{"left": 0.7167287581699346, "top": 0.8782335858585859, "width": 0.20511274509803923, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4144"}, {"text": "To ensure an even spacing between the electrodes, we laser cut guide holes on the edge of the acrylic plate and then sew the thread, following the holes (Figure 10-2).", "label": "Author", "bboxes": [{"left": 0.5992042483660132, "top": 0.7738270202020202, "width": 0.32466666666666655, "height": 0.012579545454545427, "page": 6}, {"left": 0.5240784313725491, "top": 0.7876641414141414, "width": 0.39775490196078434, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246601307189542, "top": 0.8015012626262626, "width": 0.3648937908496733, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4145"}, {"text": "Once the electrode grid is positioned, we pour another thin layer of silicone to seal it in place.", "label": "Author", "bboxes": [{"left": 0.7560735294117646, "top": 0.8291742424242424, "width": 0.16576960784313732, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.8430113636363636, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.8568484848484849, "width": 0.03841830065359475, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4146"}, {"text": "We ensure that the total interface is under 1.2mm.", "label": "Author", "bboxes": [{"left": 0.5681274509803922, "top": 0.8568484848484849, "width": 0.32404575163398697, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4147"}, {"text": "Our work contributes towards this direction.", "label": "Contribution", "bboxes": [{"left": 0.8602418300653594, "top": 0.8434621212121213, "width": 0.062019607843137314, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8572992424242424, "width": 0.2206666666666668, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4148"}, {"text": "This contribution enables DIY fabrication of multi-touch interfaces on nonconventional surfaces such as human skin [59], walls [110] or, as in our case, exible silicone.", "label": "Contribution", "bboxes": [{"left": 0.3696437908496732, "top": 0.5635454545454545, "width": 0.11564869281045759, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.5773825757575757, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.5912184343434344, "width": 0.39920098039215685, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6050555555555556, "width": 0.19791666666666669, "height": 0.012579545454545427, "page": 7}], "section": "Hardware Platform", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4149"}, {"text": "We also contribute a DIY fabrication method and offer an open-hardware tool enabling easy reproduction by other researchers and practitioners.", "label": "Contribution", "bboxes": [{"left": 0.20224019607843138, "top": 0.48144570707070705, "width": 0.2830588235294117, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.49528282828282827, "width": 0.3809003267973856, "height": 0.012579545454545371, "page": 1}, {"left": 0.10439869281045752, "top": 0.5091199494949495, "width": 0.24816993464052287, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4150"}, {"text": "However, we argue that the benets of human skin should not only be used for Onskin interfaces but also for what we call Skin-On interfaces.", "label": "Novelty", "bboxes": [{"left": 0.799514705882353, "top": 0.4962790404040404, "width": 0.1223333333333334, "height": 0.012579545454545482, "page": 0}, {"left": 0.5246633986928104, "top": 0.5101161616161616, "width": 0.39989705882352944, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.5239532828282828, "width": 0.40003267973856205, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4151"}, {"text": "Articial skin is however usually designed with aesthetic and safety requirements in mind, rather than for harvesting interactive properties of the skin that are specically useful for human-computer interaction.", "label": "Novelty", "bboxes": [{"left": 0.6997140522875818, "top": 0.8019507575757575, "width": 0.2227075163398693, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8157878787878788, "width": 0.3974722222222222, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.829625, "width": 0.39719281045751653, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8434621212121213, "width": 0.33053921568627465, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "4152"}, {"text": "Many researchers have explored the design of exible input although these studies did not use human skin as inspiration.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.7537007575757576, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.4000179738562092, "height": 0.012579545454545427, "page": 1}], "section": "Flexible sensors", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4153"}, {"text": "[64] demonstrate that it is possible to detect different touch pressures by using resisting fabric materials [75], but their surface texture does not look or feel like skin, which impairs visual and tactile perception.", "label": "Novelty", "bboxes": [{"left": 0.7126683006535948, "top": 0.8505593434343435, "width": 0.21186928104575165, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.34753594771241825, "height": 0.012579545454545427, "page": 1}], "section": "Flexible sensors", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4154"}, {"text": "Our approach shares similar goals, seeking to reproduce the sensing capabilities of biological skin, but it also goes beyond the typical bio-inspired approach by focusing on interactive aspects, which we believe are crucial for human-computer interfaces:", "label": "Novelty", "bboxes": [{"left": 0.35525653594771245, "top": 0.16447853535353535, "width": 0.13003594771241828, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.19215277777777778, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.205989898989899, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.21982702020202022, "width": 0.2782859477124183, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4155"}, {"text": "These overlays sense touch [41], multi-touch [59] or pressure [4] but can only be placed on top of the skin, and have not been designed for a repeated amount of stretch and strain.", "label": "Novelty", "bboxes": [{"left": 0.70575, "top": 0.15064141414141416, "width": 0.21811274509803924, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.16447853535353535, "width": 0.39717810457516345, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.17831565656565657, "width": 0.39717810457516356, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.19215277777777778, "width": 0.1313333333333334, "height": 0.012579545454545454, "page": 1}], "section": "On-Skin interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4156"}, {"text": "In summary, designing articial skin has thus been largely studied in the eld of Robotic, but with a focus in reproducing the sensing capability of the skin [14] or its visual aspects [62] for safety, sensing or cosmetic aspects.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.5980883838383838, "width": 0.39773856209150327, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.611925505050505, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6257626262626262, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.6395997474747475, "width": 0.26317810457516333, "height": 0.012579545454545427, "page": 1}], "section": "Arti\ufb01cial skin", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4157"}, {"text": "However, previous studies on articial skin did not consider the hypodermis layer (fat).", "label": "Novelty", "bboxes": [{"left": 0.7962761437908498, "top": 0.6395997474747475, "width": 0.12556045751633982, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6534368686868688, "width": 0.39745915032679746, "height": 0.012579545454545427, "page": 1}, {"left": 0.5241274509803922, "top": 0.6672739898989899, "width": 0.03150816993464045, "height": 0.012579545454545427, "page": 1}], "section": "Arti\ufb01cial skin", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4158"}, {"text": "We believe our work extends the boundary of traditional interactive devices by opening up the user experience to anthropomorphic interfaces and to new familiar organic interaction between humans and machines.", "label": "Novelty", "bboxes": [{"left": 0.25972385620915034, "top": 0.5720151515151515, "width": 0.22556862745098039, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5858522727272727, "width": 0.3971748366013072, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5996893939393939, "width": 0.3998790849673202, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6135265151515151, "width": 0.3640049019607843, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4159"}, {"text": "In this paper we considered one aspect (skin) but we hope our work will inspire researchers to investigate how interfaces can be designed to integrate elements from nature.", "label": "Novelty", "bboxes": [{"left": 0.3442892156862745, "top": 0.7380593434343434, "width": 0.14371078431372547, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7518964646464645, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.7657335858585859, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7795694444444444, "width": 0.20319117647058826, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4160"}, {"text": "The articial skin sensors generally have a high spatial resolution (1 mm  4 mm ) but cover only small surfaces ( 2 cm ).", "label": "Novelty", "bboxes": [{"left": 0.5999722222222222, "top": 0.47984469696969695, "width": 0.32185620915032676, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.4933926767676768, "width": 0.39716993464052297, "height": 0.012868686868686863, "page": 1}, {"left": 0.5241274509803922, "top": 0.507229797979798, "width": 0.046040849673202655, "height": 0.012868686868686918, "page": 1}], "section": "Arti\ufb01cial skin", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4161"}, {"text": "However we do not focus on interacting directly on human skin but rather aim at mimicking its properties to augment interactive devices.", "label": "Novelty", "bboxes": [{"left": 0.7970604575163398, "top": 0.2965593434343434, "width": 0.12478594771241835, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.31039646464646464, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.32423358585858586, "width": 0.3647254901960786, "height": 0.012579545454545427, "page": 1}], "section": "On-Skin interfaces", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4162"}, {"text": "Such sensors detect pressure but the foam is hardly stretchable and prevent more complex gestures such as stretching or twisting.", "label": "Novelty", "bboxes": [{"left": 0.39909967320261436, "top": 0.32423358585858586, "width": 0.08619281045751642, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3380707070707071, "width": 0.3971781045751634, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3519078282828283, "width": 0.3488055555555556, "height": 0.012579545454545427, "page": 2}], "section": "Flexible sensors", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4163"}, {"text": "In summary, there have been some research aiming at creating deformable sensors, but none has looked at the skin for inspiration; moreover the gestures these sensors can detect are limited to particular ones (e.g. bending but no stretching, stretching with no pressure, or pressure deformation but no stretching etc.).", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.37329166666666663, "width": 0.3998807189542484, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.38712878787878785, "width": 0.3974558823529412, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.40096590909090907, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4148030303030303, "width": 0.39988071895424837, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4286401515151515, "width": 0.3971764705882353, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4424772727272727, "width": 0.1274542483660131, "height": 0.012579545454545427, "page": 2}], "section": "Flexible sensors", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4164"}, {"text": "It is also worth mentioning that some researchers have proposed to augment everyday life objects (particularly non-interactive ones) [109, 110, 27] but these objects are rigid.", "label": "Novelty", "bboxes": [{"left": 0.22664869281045752, "top": 0.4424772727272727, "width": 0.25864379084967326, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.45631439393939394, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 2}, {"left": 0.08758823529411765, "top": 0.47015151515151515, "width": 0.39770424836601315, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.4839873737373737, "width": 0.10895424836601307, "height": 0.012579545454545482, "page": 2}], "section": "Flexible sensors", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4165"}, {"text": "Because these properties have a large range of values, we choose to look at the question under a different angle: how to reproduce the skin so it is valuable for interaction as well.", "label": "Novelty", "bboxes": [{"left": 0.3914575163398693, "top": 0.6591830808080807, "width": 0.09383006535947708, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6730202020202021, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6868573232323232, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7006944444444445, "width": 0.2895947712418301, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4166"}, {"text": "While Silicone is the best available approximation of human skin, it is unclear, in an interaction context, whether similarity to human skin is the most important factor.", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.4262967171717172, "width": 0.39793464052287586, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.4401338383838384, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.4539709595959596, "width": 0.3081258169934641, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4167"}, {"text": "Figure 3 illustrates the ve types of pigmentation compared: beige and brown colors representative of realistic human skin colors; white and black colors representative of usual device colors; green color to suggest something organic, but not necessarily human (e.g. alien or reptilian).", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.22334722222222222, "width": 0.3994411764705883, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.23718434343434341, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.25102146464646463, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.26485858585858585, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.27869570707070707, "width": 0.27584313725490206, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4168"}, {"text": "We use different silicone products from Smooth-On Inc to reproduce the skin properties listed above.", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.2942171717171717, "width": 0.3979379084967321, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811764705882352, "top": 0.3080542929292929, "width": 0.2819901960784314, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4169"}, {"text": "Our rst experiment aims at understanding the impact of pigmentation on the perception of skin human-likeness and comfort, but also at detecting possible negative anthropomorphic effects.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.850270202020202, "width": 0.39987091503267974, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.8641073232323233, "width": 0.3998660130718954, "height": 0.012868686868686807, "page": 3}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.04704575163398693, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4170"}, {"text": "We study different surface textures to mimic wrinkles of different body locations.", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.3784709595959596, "width": 0.4006388888888889, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.39230808080808083, "width": 0.1418872549019608, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 1", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4171"}, {"text": "The experiment was divided into two phases: in the haptic phase, the task consisted of touching lightly the different samples without seeing them to avoid any bias of the beige pigmentation.", "label": "Novelty", "bboxes": [{"left": 0.38183169934640526, "top": 0.5955845959595959, "width": 0.10347058823529409, "height": 0.012579545454545538, "page": 4}, {"left": 0.08753267973856209, "top": 0.6094217171717171, "width": 0.3981699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811437908496732, "top": 0.6232588383838383, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.6370959595959595, "width": 0.3588055555555555, "height": 0.012579545454545427, "page": 4}], "section": "Participants and experimental design", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4172"}, {"text": "We expected that the black and white colors would be perceived as more interactive because of their similarity with devices, but natural skin pigmentation was associated to a higher degree of interactivity.", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.26375126262626264, "width": 0.4006437908496732, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811764705882352, "top": 0.27758838383838386, "width": 0.39717483660130726, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811764705882352, "top": 0.2914255050505051, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.3052626262626263, "width": 0.19246568627450977, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 1", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4173"}, {"text": "For the following, we keep the beige pigmentation and study different textures to investigate whether it can change the opinion of users regarding comfort.", "label": "Novelty", "bboxes": [{"left": 0.2856192810457516, "top": 0.3052626262626263, "width": 0.1996748366013072, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.31909974747474745, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 4}, {"left": 0.0875343137254902, "top": 0.33293686868686867, "width": 0.4006078431372549, "height": 0.012579545454545482, "page": 4}], "section": "Results of study 1", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4174"}, {"text": "Figure 7 illustrates the four different skin thicknesses we compared.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.4191616161616162, "width": 0.399874183006536, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4329987373737374, "width": 0.040787581699346376, "height": 0.012579545454545482, "page": 4}], "section": "Samples", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4175"}, {"text": "For the hypodermis thickness, we considered four values corresponding to different body areas: 2mm (face [74]), 5mm , 10mm (forearm [37]), 17mm (mean body [39]).", "label": "Novelty", "bboxes": [{"left": 0.5883823529411765, "top": 0.46067297979797983, "width": 0.3337450980392157, "height": 0.012579545454545427, "page": 4}, {"left": 0.5242565359477125, "top": 0.4742209595959596, "width": 0.3996290849673202, "height": 0.012868686868686863, "page": 4}, {"left": 0.5242565359477125, "top": 0.4880580808080808, "width": 0.3471764705882353, "height": 0.012868686868686918, "page": 4}], "section": "Samples", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4176"}, {"text": "However, the most frequent gestures were pulling the skin (pinching), stroking and slapping, which are skin-specic gestures.", "label": "Novelty", "bboxes": [{"left": 0.4240228758169935, "top": 0.8643964646464646, "width": 0.06329575163398687, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39919934640522875, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.3539248366013072, "height": 0.012579545454545427, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4177"}, {"text": "also shows that samples with a thicker layer are perceived slightly more comfortable and easier to interact or manipulate although this was not signicantly different.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.41957449494949495, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.43341161616161616, "width": 0.39988071895424837, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4472487373737374, "width": 0.3152859477124183, "height": 0.012579545454545482, "page": 5}], "section": "Results of study 3", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4178"}, {"text": "We present different sensing techniques and discuss which ones are more adapted to mimic human skin.", "label": "Novelty", "bboxes": [{"left": 0.7331470588235294, "top": 0.2946199494949495, "width": 0.1886895424836602, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.30845707070707074, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.32229419191919195, "width": 0.07743790849673204, "height": 0.012579545454545482, "page": 5}], "section": "SENSING INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4179"}, {"text": "We found that the human skin affords two main types of gestures: gestures of mediated communication between individuals from the social literature [33, 32] and traditional 2D multi-touch gestures, for interface control though extracted from On-Skin literature [100].", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.7046414141414141, "width": 0.4006470588235294, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.39987418300653593, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.759989898989899, "width": 0.20301960784313722, "height": 0.012579545454545427, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4180"}, {"text": "In contrast, a matrix layout uses a grid of perpendicular lines intercepting at multiple points on the surface.", "label": "Novelty", "bboxes": [{"left": 0.5733529411764706, "top": 0.8153383838383839, "width": 0.3487696078431374, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8291755050505051, "width": 0.3230702614379085, "height": 0.012579545454545427, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4181"}, {"text": "To understand the reasons behind this choice we need to explain the different techniques that can be used.", "label": "Novelty", "bboxes": [{"left": 0.7083970588235294, "top": 0.6970946969696971, "width": 0.21343954248366015, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7109318181818182, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7247676767676767, "width": 0.054627450980392234, "height": 0.012579545454545538, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4182"}, {"text": "This approach is often used for smart textiles [17] but requires large electrodes (>1cm), which does not t with our requirement of spacial acuity.", "label": "Novelty", "bboxes": [{"left": 0.08761437908496732, "top": 0.2367121212121212, "width": 0.39768300653594774, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.2505492424242424, "width": 0.3998725490196079, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.26438636363636364, "width": 0.15418627450980393, "height": 0.012579545454545427, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4183"}, {"text": "It is less stretchable than conductive textile or cPDMS, but 30% is sufcient compared to the skin maximum strain, which is approximately of 40% [7, 24].", "label": "Novelty", "bboxes": [{"left": 0.6087777777777778, "top": 0.4103030303030303, "width": 0.31333823529411764, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.4241401515151515, "width": 0.39716830065359465, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.4379772727272727, "width": 0.31127941176470586, "height": 0.012579545454545482, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4184"}, {"text": "The threads can be positioned with a specic pattern, and electrical insulation allows superposing multiple electrodes while keeping the layer sufciently thin.", "label": "Novelty", "bboxes": [{"left": 0.8432483660130718, "top": 0.4379772727272727, "width": 0.07858823529411763, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.45181439393939393, "width": 0.3998758169934641, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.46565151515151515, "width": 0.39716830065359476, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.47948863636363637, "width": 0.16394281045751635, "height": 0.012579545454545482, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4185"}, {"text": "The electrode pattern can only have a limited number of electric lines, but this technique remains the fastest to fabricate and requires few material which makes it appropriate considering our requirements.", "label": "Novelty", "bboxes": [{"left": 0.6936519607843137, "top": 0.47948863636363637, "width": 0.2281879084967321, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.4933257575757576, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5071628787878788, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.521, "width": 0.2941111111111111, "height": 0.012579545454545538, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4186"}, {"text": "However, its thickness was 0.8mm (about the same as the fabric thickness), which is over the size of the dermis thickness when using multiple layers (two layers are needed, plus the dielectric, which would make the sensor more than 1.2mm thick).", "label": "Novelty", "bboxes": [{"left": 0.5859803921568627, "top": 0.27822348484848486, "width": 0.33585947712418307, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.2920606060606061, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.30589772727272724, "width": 0.39921568627450976, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.31973358585858586, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.5234428104575163, "top": 0.3335707070707071, "width": 0.09179575163398701, "height": 0.012579545454545482, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4187"}, {"text": "It has a theoretical conductivity of 2  / cm when mixing manually, but we could not get a conductivity under 10k  / cm .", "label": "Novelty", "bboxes": [{"left": 0.20300490196078433, "top": 0.7320265151515152, "width": 0.2822892156862745, "height": 0.013018939393939388, "page": 6}, {"left": 0.08753267973856209, "top": 0.7461527777777778, "width": 0.3983366013071895, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811437908496732, "top": 0.7597007575757576, "width": 0.11201307189542482, "height": 0.013018939393939388, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4188"}, {"text": "However, the electrical resistance increases drastically after every stretch [103], which makes it impossible to build an efcient deformable sensor.", "label": "Novelty", "bboxes": [{"left": 0.08811274509803921, "top": 0.5587247474747474, "width": 0.39745588235294116, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811274509803921, "top": 0.5725618686868686, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.5863977272727273, "width": 0.17997875816993464, "height": 0.012579545454545427, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4189"}, {"text": "Advanced gestures differ from multi-touch gestures by their specic dynamic and/or the number and size of the contact area (radius larger than 1cm).", "label": "Novelty", "bboxes": [{"left": 0.7180245098039215, "top": 0.7813737373737373, "width": 0.20380065359477129, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7952108585858586, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.3701094771241831, "height": 0.012579545454545538, "page": 7}], "section": "Data processing", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4190"}, {"text": "Although preliminary, these results are promising and demonstrate the feasibility of our approach.", "label": "Novelty", "bboxes": [{"left": 0.1694330065359477, "top": 0.32423358585858586, "width": 0.31585947712418305, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.3380707070707071, "width": 0.31149346405228756, "height": 0.012579545454545427, "page": 8}], "section": "Data processing", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4191"}, {"text": "We rst describe the implementation of three Skin-on interface prototypes with different form factors shown in Figure 1.", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.3809684343434343, "width": 0.4006437908496732, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811764705882352, "top": 0.3948055555555555, "width": 0.39717483660130726, "height": 0.012579545454545482, "page": 8}, {"left": 0.08688888888888889, "top": 0.40864267676767674, "width": 0.012454248366013068, "height": 0.012579545454545482, "page": 8}], "section": "USE CASES", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4192"}, {"text": "In contrast, Skin-On interfaces have a much smaller stiffness, providing a higher level of control.", "label": "Novelty", "bboxes": [{"left": 0.790843137254902, "top": 0.6449494949494949, "width": 0.1309983660130719, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6587866161616162, "width": 0.39745588235294116, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6726237373737374, "width": 0.10887418300653606, "height": 0.012579545454545538, "page": 8}], "section": "Increasing the degree of control", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4193"}, {"text": "We created two interfaces with two different sizes and thicknesses (9cm x 12cm and 10cm x 14.5cm, thickness 7mm) that can be connected to a device via USB (Figure ?? -top).", "label": "Novelty", "bboxes": [{"left": 0.16275326797385622, "top": 0.6431224747474747, "width": 0.3225375816993463, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.656959595959596, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 8}, {"left": 0.0877124183006536, "top": 0.6707967171717171, "width": 0.3975718954248366, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.6845467171717171, "width": 0.05199509803921569, "height": 0.012666666666666715, "page": 8}], "section": "Skin-On Touchpads", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4194"}, {"text": "While preliminary studies indicate that we can recognize 8 touch gestures and multi-touch ones, taking individual variability into account and using better recognition algorithms (typically relying on machine learning) would improve the recognition rate and allow distinguishing variations of these gestures (e.g. soft grab vs. hard grab).", "label": "Novelty", "bboxes": [{"left": 0.7196764705882353, "top": 0.09529419191919192, "width": 0.20486111111111116, "height": 0.012579545454545468, "page": 9}, {"left": 0.5246633986928104, "top": 0.10913131313131313, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.12296717171717171, "width": 0.39745915032679746, "height": 0.01257954545454544, "page": 9}, {"left": 0.5246633986928104, "top": 0.13680429292929294, "width": 0.39827287581699344, "height": 0.012579545454545454, "page": 9}, {"left": 0.5240784313725491, "top": 0.15064141414141416, "width": 0.3977614379084967, "height": 0.012579545454545427, "page": 9}, {"left": 0.5242565359477125, "top": 0.16447853535353535, "width": 0.3712189542483658, "height": 0.012579545454545454, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4195"}, {"text": "Indeed, the grid layout of the electrodes facilitates the stretch in diagonal directions, where the stretch is greater than 50% while it is limited to 30% on the horizontal and vertical axes.", "label": "Novelty", "bboxes": [{"left": 0.723764705882353, "top": 0.23366414141414144, "width": 0.1980718954248365, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.2475012626262626, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.2613383838383838, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.27517550505050503, "width": 0.21130555555555564, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4196"}, {"text": "However, different areas could have different acuity, as it is the case with the human body.", "label": "Novelty", "bboxes": [{"left": 0.6311895424836601, "top": 0.4626035353535354, "width": 0.29065359477124186, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.4764406565656566, "width": 0.3044558823529412, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4197"}, {"text": "While our paper focuses on common interactive systems (PC, smartphones, smartwatches), Skin-On interfaces could also be useful in a wide range of setups, including robots and connected objects, or for extending the capabilities of everyday life objects.", "label": "Novelty", "bboxes": [{"left": 0.5238986928104574, "top": 0.5531729797979797, "width": 0.3999722222222223, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.567010101010101, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5808472222222222, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.5946830808080807, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.608520202020202, "width": 0.0785539215686275, "height": 0.012579545454545538, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4198"}, {"text": "A strong grip conveys anger while tickling the skin displays a laughing emoji (Figure 14-a) and tapping creates a surprised emoji.", "label": "Novelty", "bboxes": [{"left": 0.21051960784313725, "top": 0.5551325757575758, "width": 0.2747794117647059, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.568969696969697, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5828068181818182, "width": 0.17031045751633989, "height": 0.012579545454545427, "page": 9}], "section": "Applications for emotional communication", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4199"}, {"text": "While, so far, we have focused on conveying different types of information with Skin-On interfaces, our future aim is to perceive affect through articial skin to reinforce engagement between interaction partners.", "label": "Novelty", "bboxes": [{"left": 0.5668545751633988, "top": 0.8097866161616162, "width": 0.3549771241830063, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8236237373737373, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.837459595959596, "width": 0.399872549019608, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8512967171717172, "width": 0.22749673202614384, "height": 0.012579545454545538, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4200"}, {"text": "For instance, the perception of our participants changed from Study 1 (visual condition only) to Study 2 (visual and tactile perception) although the same interfaces were used.", "label": "Novelty", "bboxes": [{"left": 0.4053186274509804, "top": 0.2273737373737374, "width": 0.08201470588235288, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.24121085858585856, "width": 0.3998807189542484, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.39826633986928106, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.268885101010101, "width": 0.27304084967320263, "height": 0.012579545454545427, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4201"}, {"text": "In theory , the different parameters of the skin should be investigated altogether.", "label": "Novelty", "bboxes": [{"left": 0.42379084967320263, "top": 0.6157790404040404, "width": 0.0635457516339869, "height": 0.012868686868686918, "page": 10}, {"left": 0.08811928104575163, "top": 0.629905303030303, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.6437424242424242, "width": 0.06965522875816994, "height": 0.012579545454545538, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4202"}, {"text": "However, in practice , there are too many dimensions to investigate, which requires making design choices at each iteration.", "label": "Novelty", "bboxes": [{"left": 0.40598856209150325, "top": 0.6575795454545454, "width": 0.07930882352941176, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.6711275252525253, "width": 0.39717647058823524, "height": 0.012868686868686807, "page": 10}, {"left": 0.08811928104575163, "top": 0.6852525252525253, "width": 0.32316830065359475, "height": 0.012579545454545427, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4203"}, {"text": "3. From a sensing point of view, we analyze different fabrication methods to create a silicone layer that can track the previously dened gestures with a spatial acuity comparable to human skin.", "label": "Novelty", "bboxes": [{"left": 0.08486437908496731, "top": 0.4398472222222222, "width": 0.40312418300653596, "height": 0.012667929292929314, "page": 1}, {"left": 0.10439869281045752, "top": 0.4537714646464647, "width": 0.3809003267973856, "height": 0.012579545454545482, "page": 1}, {"left": 0.10439869281045752, "top": 0.4676085858585859, "width": 0.3809003267973856, "height": 0.012579545454545482, "page": 1}, {"left": 0.10439869281045752, "top": 0.48144570707070705, "width": 0.09305555555555553, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4204"}, {"text": "Our approach shares similar goals, seeking to reproduce the sensing capabilities of biological skin, but it also goes beyond the typical bio-inspired approach by focusing on interactive aspects, which we believe are crucial for human-computer interfaces:", "label": "Objective", "bboxes": [{"left": 0.35525653594771245, "top": 0.16447853535353535, "width": 0.13003594771241828, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.19215277777777778, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.205989898989899, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.21982702020202022, "width": 0.2782859477124183, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4205"}, {"text": "In such cases, the goal is to replicate the sensing capability of the human ngertip [35, 108, 104].", "label": "Objective", "bboxes": [{"left": 0.7757630718954248, "top": 0.45217045454545457, "width": 0.14607026143790858, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.46600757575757573, "width": 0.3992140522875818, "height": 0.012579545454545482, "page": 1}, {"left": 0.5234428104575163, "top": 0.47984469696969695, "width": 0.06882516339869293, "height": 0.012579545454545482, "page": 1}], "section": "Arti\ufb01cial skin", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4206"}, {"text": "In summary, like On-Skin interfaces, Skin-On interfaces also aim to use the affordances of human skin.", "label": "Objective", "bboxes": [{"left": 0.5246633986928104, "top": 0.2827222222222222, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.2965593434343434, "width": 0.2673464052287582, "height": 0.012579545454545427, "page": 1}], "section": "On-Skin interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4207"}, {"text": "However we do not focus on interacting directly on human skin but rather aim at mimicking its properties to augment interactive devices.", "label": "Objective", "bboxes": [{"left": 0.7970604575163398, "top": 0.2965593434343434, "width": 0.12478594771241835, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.31039646464646464, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.32423358585858586, "width": 0.3647254901960786, "height": 0.012579545454545427, "page": 1}], "section": "On-Skin interfaces", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4208"}, {"text": "Our goal is to replicate the three layers of the human skin: The epidermis layer provides both visual and tactile perception (e.g. texture); The dermis layer is the sensory layer embedding nerves to detect mechanical contact; The hypodermis layer provides kinesthetic feedback due to its soft mechanical properties (viscosity, thickness, etc.).", "label": "Objective", "bboxes": [{"left": 0.1476339869281046, "top": 0.7325366161616161, "width": 0.33982352941176475, "height": 0.010063131313131302, "page": 2}, {"left": 0.08768954248366012, "top": 0.7438573232323232, "width": 0.39976797385620916, "height": 0.010063131313131302, "page": 2}, {"left": 0.08811928104575163, "top": 0.7551780303030303, "width": 0.3971764705882353, "height": 0.010063131313131302, "page": 2}, {"left": 0.08811928104575163, "top": 0.7665, "width": 0.3975277777777778, "height": 0.010063131313131302, "page": 2}, {"left": 0.0881062091503268, "top": 0.777820707070707, "width": 0.3455669934640523, "height": 0.010063131313131302, "page": 2}], "section": "Flexible sensors", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4209"}, {"text": "Non-parametric Friedman tests were conducted followed by post hoc comparison tests for all the questions asked.", "label": "Objective", "bboxes": [{"left": 0.8167107843137255, "top": 0.7398636363636364, "width": 0.10512091503267962, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.7537007575757576, "width": 0.399872549019608, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.2239362745098039, "height": 0.012579545454545427, "page": 3}], "section": "Results of study 1", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4210"}, {"text": "An effect was found on the following questions: interactive (Chi-square = 13.6, p<0.05) and looks like human (Chi-square = 36, p<0.05).", "label": "Objective", "bboxes": [{"left": 0.7533382352941177, "top": 0.7675366161616162, "width": 0.1684983660130719, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.781084595959596, "width": 0.39825816993464047, "height": 0.012868686868686807, "page": 3}, {"left": 0.5246470588235295, "top": 0.7949217171717171, "width": 0.3193692810457517, "height": 0.012868686868686918, "page": 3}], "section": "Results of study 1", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4211"}, {"text": "Because these properties have a large range of values, we choose to look at the question under a different angle: how to reproduce the skin so it is valuable for interaction as well.", "label": "Objective", "bboxes": [{"left": 0.3914575163398693, "top": 0.6591830808080807, "width": 0.09383006535947708, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6730202020202021, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6868573232323232, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7006944444444445, "width": 0.2895947712418301, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4212"}, {"text": "All these questions sparked our interest in understanding how to adapt articial skin to our interactive context.", "label": "Objective", "bboxes": [{"left": 0.08753267973856209, "top": 0.5785037878787879, "width": 0.3983513071895425, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5923409090909091, "width": 0.3167401960784314, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4213"}, {"text": "Non-parametric Friedman tests were conducted followed by post hoc comparison tests for the questions asked.", "label": "Objective", "bboxes": [{"left": 0.5246633986928104, "top": 0.17259722222222224, "width": 0.39773856209150327, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.18643434343434345, "width": 0.3305147058823531, "height": 0.012579545454545454, "page": 4}], "section": "Participants and experimental design", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4214"}, {"text": "Non-parametric Friedman tests were conducted followed by post hoc comparison tests for all the questions asked and found a main effect on the look alike question (chi-square = 7.4, p<0.05).", "label": "Objective", "bboxes": [{"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39773856209150327, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8364330808080809, "width": 0.3971895424836601, "height": 0.012868686868686807, "page": 4}, {"left": 0.5242565359477125, "top": 0.8505593434343435, "width": 0.08346732026143788, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 3", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4215"}, {"text": "We aim to study Skin-On interfaces as an output modality.", "label": "Objective", "bboxes": [{"left": 0.8085049019607843, "top": 0.6852525252525253, "width": 0.11390522875816989, "height": 0.012579545454545427, "page": 9}, {"left": 0.524671568627451, "top": 0.6990896464646464, "width": 0.277921568627451, "height": 0.012579545454545538, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4216"}, {"text": "Engagement in a social interaction can be dened as \"the value that a participant in an interaction attributes to the goal of being together with the other participant(s) and of continuing the interaction\" [67].", "label": "Objective", "bboxes": [{"left": 0.8093513071895424, "top": 0.6990896464646464, "width": 0.11249346405228755, "height": 0.012579545454545538, "page": 9}, {"left": 0.524671568627451, "top": 0.7129267676767678, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 9}, {"left": 0.524671568627451, "top": 0.7267638888888889, "width": 0.39745588235294116, "height": 0.012579545454545427, "page": 9}, {"left": 0.5240898692810457, "top": 0.7406010101010102, "width": 0.40046241830065366, "height": 0.012578282828282772, "page": 9}, {"left": 0.5246633986928104, "top": 0.7544381313131313, "width": 0.06825653594771253, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4217"}, {"text": "While, so far, we have focused on conveying different types of information with Skin-On interfaces, our future aim is to perceive affect through articial skin to reinforce engagement between interaction partners.", "label": "Objective", "bboxes": [{"left": 0.5668545751633988, "top": 0.8097866161616162, "width": 0.3549771241830063, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8236237373737373, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.837459595959596, "width": 0.399872549019608, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8512967171717172, "width": 0.22749673202614384, "height": 0.012579545454545538, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4218"}, {"text": "More generally, our goal is to further explore various types of anthropomorphism towards human-like devices.", "label": "Objective", "bboxes": [{"left": 0.14719444444444443, "top": 0.15064141414141416, "width": 0.3380980392156864, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.369609477124183, "height": 0.012579545454545454, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4219"}, {"text": "In this paper, we share the same vision than On-skin interaction, which builds on the advantages of the skin to increase interaction bandwidth.", "label": "Method", "bboxes": [{"left": 0.6765424836601307, "top": 0.468604797979798, "width": 0.24530555555555567, "height": 0.012579545454545482, "page": 0}, {"left": 0.5246633986928104, "top": 0.4824419191919192, "width": 0.3971928104575164, "height": 0.012579545454545482, "page": 0}, {"left": 0.5246633986928104, "top": 0.4962790404040404, "width": 0.2698578431372549, "height": 0.012579545454545482, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4220"}, {"text": "Our work contributes towards this direction.", "label": "Method", "bboxes": [{"left": 0.8602418300653594, "top": 0.8434621212121213, "width": 0.062019607843137314, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8572992424242424, "width": 0.2206666666666668, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4221"}, {"text": "We present an exploration of the design space of Skin-On interfaces.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.4006470588235294, "height": 0.012579545454545468, "page": 1}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.05375490196078429, "height": 0.012579545454545468, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4222"}, {"text": "Our work relates to on-skin interfaces in HCI, articial skin in robotics and exible input sensors.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.8219835858585858, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.8358207070707071, "width": 0.22629084967320262, "height": 0.012579545454545427, "page": 1}], "section": "RELATED WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4223"}, {"text": "We are not aware of any research looking at exploiting realistic articial skin as a new input method for interactive devices.", "label": "Method", "bboxes": [{"left": 0.8525163398692811, "top": 0.6811098484848485, "width": 0.06932026143790848, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.6949469696969697, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7087840909090909, "width": 0.3290261437908497, "height": 0.012579545454545538, "page": 1}], "section": "Arti\ufb01cial skin", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4224"}, {"text": "We assemble the insights from these three steps and present the implementation of several Skin-On interfaces and applications to demonstrate the added value of our approach (see Figure 1 for examples).", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.5305037878787878, "width": 0.39793790849673205, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811764705882352, "top": 0.544340909090909, "width": 0.3998807189542483, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811764705882352, "top": 0.5581780303030303, "width": 0.39717483660130726, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811764705882352, "top": 0.5720151515151515, "width": 0.1609624183006536, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4225"}, {"text": "However we do not focus on interacting directly on human skin but rather aim at mimicking its properties to augment interactive devices.", "label": "Method", "bboxes": [{"left": 0.7970604575163398, "top": 0.2965593434343434, "width": 0.12478594771241835, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.31039646464646464, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.32423358585858586, "width": 0.3647254901960786, "height": 0.012579545454545427, "page": 1}], "section": "On-Skin interfaces", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4226"}, {"text": "To better understand which are the desirable properties of the human skin to reproduce within articial skin, we looked through the Biology literature [20, 36, 38] and gathered information about the visual, haptic and sensing properties of the skin (described below).", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.2687032828282828, "width": 0.3976715686274508, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.28254040404040404, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.29637752525252525, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.31021464646464647, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.3240517676767677, "width": 0.18175000000000008, "height": 0.012579545454545482, "page": 2}], "section": "Human Skin properties", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4227"}, {"text": "To design the articial skin, we propose a bio-driven approach (illustrated in Figure 2) aiming to replicate the main properties of the human skin.", "label": "Method", "bboxes": [{"left": 0.28613071895424835, "top": 0.5432689393939394, "width": 0.19916176470588237, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5571060606060606, "width": 0.3971781045751634, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5709431818181818, "width": 0.33454248366013073, "height": 0.012579545454545427, "page": 2}], "section": "Flexible sensors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4228"}, {"text": "Our goal is to replicate the three layers of the human skin: The epidermis layer provides both visual and tactile perception (e.g. texture); The dermis layer is the sensory layer embedding nerves to detect mechanical contact; The hypodermis layer provides kinesthetic feedback due to its soft mechanical properties (viscosity, thickness, etc.).", "label": "Method", "bboxes": [{"left": 0.1476339869281046, "top": 0.7325366161616161, "width": 0.33982352941176475, "height": 0.010063131313131302, "page": 2}, {"left": 0.08768954248366012, "top": 0.7438573232323232, "width": 0.39976797385620916, "height": 0.010063131313131302, "page": 2}, {"left": 0.08811928104575163, "top": 0.7551780303030303, "width": 0.3971764705882353, "height": 0.010063131313131302, "page": 2}, {"left": 0.08811928104575163, "top": 0.7665, "width": 0.3975277777777778, "height": 0.010063131313131302, "page": 2}, {"left": 0.0881062091503268, "top": 0.777820707070707, "width": 0.3455669934640523, "height": 0.010063131313131302, "page": 2}], "section": "Flexible sensors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4229"}, {"text": "Our exploration into simulating human skin properties starts with the replication of its sensory properties.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.6453472222222222, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 3}, {"left": 0.08753267973856209, "top": 0.6591830808080807, "width": 0.29792973856209143, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4230"}, {"text": "All these questions sparked our interest in understanding how to adapt articial skin to our interactive context.", "label": "Method", "bboxes": [{"left": 0.08753267973856209, "top": 0.5785037878787879, "width": 0.3983513071895425, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5923409090909091, "width": 0.3167401960784314, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4231"}, {"text": "Moving on to reproducing the properties of the skin described above, we looked at common material used in other elds of research.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.14829924242424242, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.16213636363636363, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.17597348484848485, "width": 0.0794019607843137, "height": 0.012579545454545454, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4232"}, {"text": "We recruited 15 participants (10 males, mean age 21) from our university to test each sample.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.31520580808080806, "width": 0.39822549019607856, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3290429292929293, "width": 0.20200653594771234, "height": 0.012579545454545482, "page": 3}], "section": "Participants and experimental design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4233"}, {"text": "We use different silicone products from Smooth-On Inc to reproduce the skin properties listed above.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.2942171717171717, "width": 0.3979379084967321, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811764705882352, "top": 0.3080542929292929, "width": 0.2819901960784314, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4234"}, {"text": "Our rst experiment aims at understanding the impact of pigmentation on the perception of skin human-likeness and comfort, but also at detecting possible negative anthropomorphic effects.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.850270202020202, "width": 0.39987091503267974, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.8641073232323233, "width": 0.3998660130718954, "height": 0.012868686868686807, "page": 3}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.04704575163398693, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4235"}, {"text": "We study the impact of the strain/thickness on easiness and comfort of interaction, as well as human-likeness.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.3674179292929293, "width": 0.397921568627451, "height": 0.012868686868686863, "page": 4}, {"left": 0.5246633986928104, "top": 0.3815441919191919, "width": 0.324452614379085, "height": 0.012579545454545482, "page": 4}], "section": "Study 3: Replicating thickness", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4236"}, {"text": "In the next experiment, we use the texture with small pores.", "label": "Method", "bboxes": [{"left": 0.9082320261437908, "top": 0.3109671717171717, "width": 0.013601307189542555, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.3248042929292929, "width": 0.37045588235294125, "height": 0.012579545454545482, "page": 4}], "section": "Participants and experimental design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4237"}, {"text": "Figure 5 illustrates the four samples of texture we compared.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.44551641414141413, "width": 0.4000261437908497, "height": 0.012579545454545482, "page": 4}], "section": "Samples", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4238"}, {"text": "We study different surface textures to mimic wrinkles of different body locations.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.3784709595959596, "width": 0.4006388888888889, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.39230808080808083, "width": 0.1418872549019608, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 1", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4239"}, {"text": "We recruited 16 participants (10 male, mean age 21) from our university.", "label": "Method", "bboxes": [{"left": 0.3071797385620915, "top": 0.5817474747474748, "width": 0.17811764705882355, "height": 0.012579545454545427, "page": 4}, {"left": 0.08758169934640524, "top": 0.5955845959595959, "width": 0.28918137254901954, "height": 0.012579545454545538, "page": 4}], "section": "Participants and experimental design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4240"}, {"text": "We expected that the black and white colors would be perceived as more interactive because of their similarity with devices, but natural skin pigmentation was associated to a higher degree of interactivity.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.26375126262626264, "width": 0.4006437908496732, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811764705882352, "top": 0.27758838383838386, "width": 0.39717483660130726, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811764705882352, "top": 0.2914255050505051, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.3052626262626263, "width": 0.19246568627450977, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 1", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4241"}, {"text": "We used a similar design than previous studies.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.5259633838383838, "width": 0.3092042483660131, "height": 0.012579545454545538, "page": 4}], "section": "Experimental design", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4242"}, {"text": "Figure 7 illustrates the four different skin thicknesses we compared.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.4191616161616162, "width": 0.399874183006536, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4329987373737374, "width": 0.040787581699346376, "height": 0.012579545454545482, "page": 4}], "section": "Samples", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4243"}, {"text": "We also had the opportunity to observe that users spontaneously performed these gestures during the studies.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.822885101010101, "width": 0.4006437908496732, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3463807189542484, "height": 0.012579545454545538, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4244"}, {"text": "The last part of our exploration focuses on the reproduction of the human skin sensing acuity.", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.2807828282828283, "width": 0.3976830065359477, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.2946199494949495, "width": 0.2032189542483661, "height": 0.012579545454545427, "page": 5}], "section": "SENSING INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4245"}, {"text": "Our sensory exploration let us to form a series of guidelines for mimicking human skin for an interactive setup: for the pigmentation using a skin-like color; for the texture using a realistic skin pore and wrinkle structure; for the thickness , using a fat layer of 5mm to 10mm and a dermis of 1.2mm.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.5375795454545454, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.5514166666666667, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5649646464646465, "width": 0.3971895424836602, "height": 0.012868686868686918, "page": 5}, {"left": 0.08813562091503269, "top": 0.5788017676767677, "width": 0.39919934640522875, "height": 0.012868686868686807, "page": 5}, {"left": 0.08811928104575163, "top": 0.5929267676767677, "width": 0.38054901960784315, "height": 0.012579545454545427, "page": 5}], "section": "Sensory exploration results", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4246"}, {"text": "We found that the human skin affords two main types of gestures: gestures of mediated communication between individuals from the social literature [33, 32] and traditional 2D multi-touch gestures, for interface control though extracted from On-Skin literature [100].", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.7046414141414141, "width": 0.4006470588235294, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.39987418300653593, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.759989898989899, "width": 0.20301960784313722, "height": 0.012579545454545427, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4247"}, {"text": "Our results suggest that users tend to transpose the interactions they are doing with real skin to articial skin, and that articial skin leverages the expressive gestures and tactile expressions of pro-social emotions.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.1305151515151515, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.14435227272727272, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.15818939393939394, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.17202651515151515, "width": 0.1579232026143791, "height": 0.012579545454545454, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4248"}, {"text": "We choose to use a matrix layout because it is easier to fabricate and requires less components and apparatus.", "label": "Method", "bboxes": [{"left": 0.8527418300653595, "top": 0.8291755050505051, "width": 0.06909477124183006, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8430113636363636, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.8568484848484849, "width": 0.2617630718954249, "height": 0.012579545454545427, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4249"}, {"text": "To inform our choices we have a series of requirements:", "label": "Method", "bboxes": [{"left": 0.8330245098039215, "top": 0.47256186868686867, "width": 0.08909803921568638, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.48639772727272723, "width": 0.26975490196078433, "height": 0.012579545454545482, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4250"}, {"text": "We choose to implement our sensor using a matrix layout sensing mutual capacitance.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.6832575757575757, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.6970946969696971, "width": 0.1786928104575164, "height": 0.012579545454545427, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4251"}, {"text": "Our next step in the design of the articial skin was to identify the types of gestures which are desirable for Skin-On interfaces.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.6417462121212121, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6555833333333333, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6694204545454546, "width": 0.03808986928104573, "height": 0.012579545454545427, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4252"}, {"text": "This approach is often used for smart textiles [17] but requires large electrodes (>1cm), which does not t with our requirement of spacial acuity.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.2367121212121212, "width": 0.39768300653594774, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.2505492424242424, "width": 0.3998725490196079, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.26438636363636364, "width": 0.15418627450980393, "height": 0.012579545454545427, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4253"}, {"text": "We used conductive insulated Datastretch thread [93], which allows a strain up to 30%, is 0.2mm thick, and has a conductivity of 4.2  /m.", "label": "Method", "bboxes": [{"left": 0.8643986928104574, "top": 0.3687929292929293, "width": 0.057450980392157014, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246748366013072, "top": 0.3823409090909091, "width": 0.39717483660130715, "height": 0.012868686868686807, "page": 6}, {"left": 0.5246748366013072, "top": 0.3964671717171717, "width": 0.3977385620915034, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246748366013072, "top": 0.4103030303030303, "width": 0.07438888888888895, "height": 0.012729797979798008, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4254"}, {"text": "To implement the electrode pattern described above, we need a conductive material that ts our requirements.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.41280681818181814, "width": 0.3976748366013072, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.42664393939393935, "width": 0.3088137254901961, "height": 0.012579545454545482, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4255"}, {"text": "Once cured, we laser cut it to the desired pattern and sealed it into another silicone layer.", "label": "Method", "bboxes": [{"left": 0.6353970588235294, "top": 0.222875, "width": 0.2864395424836601, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.2367121212121212, "width": 0.3077973856209151, "height": 0.012579545454545454, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4256"}, {"text": "We tested two conductive silicones.", "label": "Method", "bboxes": [{"left": 0.12394444444444444, "top": 0.6354570707070707, "width": 0.22861274509803917, "height": 0.012579545454545538, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4257"}, {"text": "We also explored conductive fabric, which is used in the DIY wearable community [17].", "label": "Method", "bboxes": [{"left": 0.2121062091503268, "top": 0.8505593434343435, "width": 0.27319934640522875, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.29930718954248364, "height": 0.012579545454545538, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4258"}, {"text": "Thus, we discarded this solution.", "label": "Method", "bboxes": [{"left": 0.27314379084967316, "top": 0.5863977272727273, "width": 0.21432679738562094, "height": 0.012579545454545427, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4259"}, {"text": "We now present the steps needed to fabricate our articial skin (Figure 10).", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.6065252525252525, "width": 0.39793790849673216, "height": 0.012579545454545427, "page": 6}, {"left": 0.5241274509803922, "top": 0.6203623737373737, "width": 0.08014869281045756, "height": 0.012579545454545538, "page": 6}], "section": "Skin-On Fabrication Process", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4260"}, {"text": "We developed an Open Source and Open Hardware multitouch controller 1 with a total cost of $4.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.5497083333333334, "width": 0.40064379084967316, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811764705882352, "top": 0.5615429292929294, "width": 0.2728921568627451, "height": 0.014582070707070627, "page": 7}], "section": "Hardware Platform", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4261"}, {"text": "Using the data read (in serial or wireless) from the sensing and transmitting electrodes, we build a 2D image of 12x21 pixels.", "label": "Method", "bboxes": [{"left": 0.17165522875816994, "top": 0.8247941919191919, "width": 0.3136372549019608, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.8386313131313131, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.8524671717171717, "width": 0.10744607843137256, "height": 0.012579545454545427, "page": 7}], "section": "Data processing", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4262"}, {"text": "To minimize the background noise, we perform an initial calibration.", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.3222373737373737, "width": 0.3976715686274508, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.33607449494949493, "width": 0.07222058823529409, "height": 0.012579545454545482, "page": 7}], "section": "Data processing", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4263"}, {"text": "We present the implementation of our hardware and software toolkit and demonstrate its gesture recognition algorithm, which can detect gestures proposed in the previous section of this paper.", "label": "Method", "bboxes": [{"left": 0.3634983660130719, "top": 0.4683396464646465, "width": 0.1245016339869281, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4821767676767677, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4960138888888889, "width": 0.3998758169934641, "height": 0.012579545454545371, "page": 7}, {"left": 0.08811928104575163, "top": 0.5098510101010101, "width": 0.3346258169934641, "height": 0.012579545454545427, "page": 7}], "section": "Open-toolkit for touch and gestures detection", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4264"}, {"text": "To support accurate spacial interpolation, we upscale the image 5x using the Lanczos-4 algorithm (Figure 12-c).", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.44048106060606057, "width": 0.3976830065359477, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4543181818181818, "width": 0.33054084967320263, "height": 0.012579545454545482, "page": 7}], "section": "Data processing", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4265"}, {"text": "In a pilot study, we dened the maximum radius (5mm) that a single nger press can have on this surface (Fig. 12-c).", "label": "Method", "bboxes": [{"left": 0.7420130718954249, "top": 0.7046414141414141, "width": 0.1798235294117646, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.1685947712418301, "height": 0.012579545454545427, "page": 7}], "section": "Data processing", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4266"}, {"text": "Overall, our data processing algorithm provides a spacial acuity of 2mm with an electrode spacing of 4mm.", "label": "Method", "bboxes": [{"left": 0.6369101307189542, "top": 0.5863977272727273, "width": 0.2876356209150328, "height": 0.012579545454545427, "page": 7}, {"left": 0.5242663398692811, "top": 0.6002348484848485, "width": 0.3975718954248365, "height": 0.012579545454545427, "page": 7}, {"left": 0.52409477124183, "top": 0.6140719696969698, "width": 0.03701307189542491, "height": 0.012579545454545427, "page": 7}], "section": "Data processing", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4267"}, {"text": "We ran a preliminary study with 8 participants on a subset of 8 gestures.", "label": "Method", "bboxes": [{"left": 0.27924346405228756, "top": 0.1997007575757576, "width": 0.20605228758169936, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.21353661616161618, "width": 0.253983660130719, "height": 0.012579545454545454, "page": 8}], "section": "Data processing", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4268"}, {"text": "We rst describe the implementation of three Skin-on interface prototypes with different form factors shown in Figure 1.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.3809684343434343, "width": 0.4006437908496732, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811764705882352, "top": 0.3948055555555555, "width": 0.39717483660130726, "height": 0.012579545454545482, "page": 8}, {"left": 0.08688888888888889, "top": 0.40864267676767674, "width": 0.012454248366013068, "height": 0.012579545454545482, "page": 8}], "section": "USE CASES", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4269"}, {"text": "We also fabricated a Skin-On wristband to alleviate the limited input and output capabilities of smartwatches [65] (Figure 1-c).", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.7222474747474747, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 8}, {"left": 0.08812091503267974, "top": 0.736084595959596, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 8}], "section": "Skin-On Touchpads", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4270"}, {"text": "We implemented a pressure-based menu.", "label": "Method", "bboxes": [{"left": 0.6447124183006536, "top": 0.6726237373737374, "width": 0.2799640522875817, "height": 0.012579545454545538, "page": 8}], "section": "Increasing input bandwidth", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4271"}, {"text": "We built a Skin-On smartphone case (Figure ?? -bottom) providing advanced input and output capabilities on the back and side of a mobile device [47, 11, 80].", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.4947260101010101, "width": 0.40063725490196084, "height": 0.01266666666666677, "page": 8}, {"left": 0.0877124183006536, "top": 0.5086502525252525, "width": 0.3975816993464052, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5224873737373738, "width": 0.2440996732026144, "height": 0.012579545454545538, "page": 8}], "section": "USE CASES", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4272"}, {"text": "We also built a Skin-On interface for built-in and external touchpads.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.6292866161616162, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6431224747474747, "width": 0.06958986928104575, "height": 0.012579545454545538, "page": 8}], "section": "Skin-On Touchpads", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4273"}, {"text": "For instance in study 3 we saw several users spontaneously pulling", "label": "Method", "bboxes": [{"left": 0.46349673202614383, "top": 0.8782335858585859, "width": 0.02207516339869281, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}], "section": "Communicating interaction", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4274"}, {"text": "Further tests are needed to evaluate the robustness of our system.", "label": "Method", "bboxes": [{"left": 0.6820898692810458, "top": 0.0814570707070707, "width": 0.23974836601307192, "height": 0.012579545454545468, "page": 9}, {"left": 0.5246633986928104, "top": 0.09529419191919192, "width": 0.18997385620915042, "height": 0.012579545454545468, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4275"}, {"text": "We see several directions to investigate other form factors.", "label": "Method", "bboxes": [{"left": 0.8226143790849673, "top": 0.3795808080808081, "width": 0.09921078431372554, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246683006535947, "top": 0.3934179292929293, "width": 0.2918856209150328, "height": 0.012579545454545482, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4276"}, {"text": "While our paper focuses on common interactive systems (PC, smartphones, smartwatches), Skin-On interfaces could also be useful in a wide range of setups, including robots and connected objects, or for extending the capabilities of everyday life objects.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.5531729797979797, "width": 0.3999722222222223, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.567010101010101, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5808472222222222, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.5946830808080807, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.608520202020202, "width": 0.0785539215686275, "height": 0.012579545454545538, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4277"}, {"text": "We implemented a messaging application where users can express rich tactile emoticons on the articial skin.", "label": "Method", "bboxes": [{"left": 0.31266176470588236, "top": 0.5136212121212121, "width": 0.17534150326797387, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5274583333333334, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5412954545454546, "width": 0.13718464052287582, "height": 0.012579545454545427, "page": 9}], "section": "Applications for emotional communication", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4278"}, {"text": "We now discuss future directions regarding the implementation, the concept and the approach.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.8782335858585859, "width": 0.40064379084967316, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.22739869281045755, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4279"}, {"text": "We aim to study Skin-On interfaces as an output modality.", "label": "Method", "bboxes": [{"left": 0.8085049019607843, "top": 0.6852525252525253, "width": 0.11390522875816989, "height": 0.012579545454545427, "page": 9}, {"left": 0.524671568627451, "top": 0.6990896464646464, "width": 0.277921568627451, "height": 0.012579545454545538, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4280"}, {"text": "More generally, our goal is to further explore various types of anthropomorphism towards human-like devices.", "label": "Method", "bboxes": [{"left": 0.14719444444444443, "top": 0.15064141414141416, "width": 0.3380980392156864, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.369609477124183, "height": 0.012579545454545454, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4281"}, {"text": "Uncanny valley has been principally a no-go zone in HCI [8], and our work challenges this.", "label": "Method", "bboxes": [{"left": 0.19773366013071897, "top": 0.18586363636363637, "width": 0.2875588235294118, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.1997007575757576, "width": 0.3041143790849673, "height": 0.012579545454545454, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4282"}, {"text": "This work was supported by the Engineering and Physical Sciences Research Council (grant number EPSRC EP/P004342/1, EP/M021882/1, EP/R02961X/1) and the Agence Nationale de la Recherche (ANR-17-CE33-0006 SocialTouch, ANR-16CE330023 GESTURE).", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.822885101010101, "width": 0.4003790849673202, "height": 0.012579545454545427, "page": 10}, {"left": 0.08812418300653595, "top": 0.8367222222222221, "width": 0.3992156862745098, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.16343464052287582, "height": 0.012579545454545427, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4283"}, {"text": "Using articial skin on a device may create similar effects, and could change the engagement or affection that we have towards inanimate objects such as interactive devices.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.49782449494949493, "width": 0.39919934640522875, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.5116616161616161, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.5254987373737373, "width": 0.34108006535947716, "height": 0.012579545454545538, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4284"}, {"text": "We presented a bio-driven approach which is singular in HCI.", "label": "Method", "bboxes": [{"left": 0.24072222222222223, "top": 0.5883939393939394, "width": 0.24456372549019612, "height": 0.012579545454545538, "page": 10}, {"left": 0.08753267973856209, "top": 0.6022310606060606, "width": 0.16311928104575163, "height": 0.012579545454545427, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4285"}, {"text": "1. From a sensory point of view, we study how to reproduce the visual, tactile and kinesthetic aspects of the human skin.", "label": "Method", "bboxes": [{"left": 0.08486274509803922, "top": 0.24112373737373738, "width": 0.40042647058823533, "height": 0.01266666666666666, "page": 1}, {"left": 0.10439869281045752, "top": 0.2550479797979798, "width": 0.38375000000000004, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4286"}, {"text": "Strain/thickness : we want to reproduce the deformability of the skin as described earlier.", "label": "Method", "bboxes": [{"left": 0.5254771241830065, "top": 0.5074936868686869, "width": 0.3963529411764706, "height": 0.012868686868686807, "page": 5}, {"left": 0.5409428104575164, "top": 0.5216199494949495, "width": 0.1804003267973856, "height": 0.012579545454545538, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4287"}, {"text": "We prepare a rectangular mold of the size of the desired articial skin and place it on top of", "label": "Method", "bboxes": [{"left": 0.7167287581699346, "top": 0.8782335858585859, "width": 0.20511274509803923, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 6}], "section": "Skin-On Fabrication Process", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4288"}, {"text": "To ensure an even spacing between the electrodes, we laser cut guide holes on the edge of the acrylic plate and then sew the thread, following the holes (Figure 10-2).", "label": "Method", "bboxes": [{"left": 0.5992042483660132, "top": 0.7738270202020202, "width": 0.32466666666666655, "height": 0.012579545454545427, "page": 6}, {"left": 0.5240784313725491, "top": 0.7876641414141414, "width": 0.39775490196078434, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246601307189542, "top": 0.8015012626262626, "width": 0.3648937908496733, "height": 0.012579545454545427, "page": 6}], "section": "Skin-On Fabrication Process", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4289"}, {"text": "We also had the opportunity to observe that users spontaneously performed these gestures during the studies.", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.822885101010101, "width": 0.4006437908496732, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3463807189542484, "height": 0.012579545454545538, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4290"}, {"text": "We see several directions to investigate other form factors.", "label": "Result", "bboxes": [{"left": 0.8226143790849673, "top": 0.3795808080808081, "width": 0.09921078431372554, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246683006535947, "top": 0.3934179292929293, "width": 0.2918856209150328, "height": 0.012579545454545482, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4291"}, {"text": "For instance, the back of a mobile device could be covered with articial skin that can sense novel user gestures (e.g. grab, twist, scratch, etc.) and provide tactile and kinesthetic feedback in order to enhance user expressiveness and user experience for mediated communication or interface control.", "label": "Conclusion", "bboxes": [{"left": 0.7721584967320261, "top": 0.6698699494949495, "width": 0.150109477124183, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246601307189542, "top": 0.6837070707070707, "width": 0.3971928104575163, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246601307189542, "top": 0.6975454545454546, "width": 0.3982892156862746, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246601307189542, "top": 0.7113825757575757, "width": 0.39989869281045753, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246601307189542, "top": 0.725219696969697, "width": 0.3971928104575163, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.7390555555555556, "width": 0.23403267973856212, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4292"}, {"text": "[64] demonstrate that it is possible to detect different touch pressures by using resisting fabric materials [75], but their surface texture does not look or feel like skin, which impairs visual and tactile perception.", "label": "Conclusion", "bboxes": [{"left": 0.7126683006535948, "top": 0.8505593434343435, "width": 0.21186928104575165, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.34753594771241825, "height": 0.012579545454545427, "page": 1}], "section": "Flexible sensors", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4293"}, {"text": "In summary, designing articial skin has thus been largely studied in the eld of Robotic, but with a focus in reproducing the sensing capability of the skin [14] or its visual aspects [62] for safety, sensing or cosmetic aspects.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.5980883838383838, "width": 0.39773856209150327, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.611925505050505, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6257626262626262, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.6395997474747475, "width": 0.26317810457516333, "height": 0.012579545454545427, "page": 1}], "section": "Arti\ufb01cial skin", "prob": 1, "is_author_statement": false, "is_in_expected_section": false, "id": "4294"}, {"text": "Because of its thickness, this layer enables new gestures (e.g. squeeze) and provides kinaesthetic feedback.", "label": "Conclusion", "bboxes": [{"left": 0.5607124183006537, "top": 0.6672739898989899, "width": 0.3611258169934639, "height": 0.012579545454545427, "page": 1}, {"left": 0.5241274509803922, "top": 0.6811098484848485, "width": 0.3233562091503267, "height": 0.012579545454545538, "page": 1}], "section": "Arti\ufb01cial skin", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4295"}, {"text": "We assemble the insights from these three steps and present the implementation of several Skin-On interfaces and applications to demonstrate the added value of our approach (see Figure 1 for examples).", "label": "Conclusion", "bboxes": [{"left": 0.08735457516339869, "top": 0.5305037878787878, "width": 0.39793790849673205, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811764705882352, "top": 0.544340909090909, "width": 0.3998807189542483, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811764705882352, "top": 0.5581780303030303, "width": 0.39717483660130726, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811764705882352, "top": 0.5720151515151515, "width": 0.1609624183006536, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4296"}, {"text": "This work also explores the intersection between man and machine (human augmentation) from a new perspective: Instead of augmenting the human with parts of machines, we demonstrate how machines can be augmented with parts of human.", "label": "Conclusion", "bboxes": [{"left": 0.4569264705882353, "top": 0.6135265151515151, "width": 0.028366013071895457, "height": 0.012579545454545427, "page": 1}, {"left": 0.08753267973856209, "top": 0.6273623737373737, "width": 0.39776633986928106, "height": 0.012579545454545427, "page": 1}, {"left": 0.08758169934640524, "top": 0.6411994949494949, "width": 0.4004117647058823, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.6550366161616161, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6688737373737375, "width": 0.36655555555555563, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4297"}, {"text": "Additionally, bio-driven approaches are not mainstream in HCI research, and this study presents a new research method to create devices with novel form factors that could be suitable for areas of research such as Shape Changing Interfaces [2] [43] or Organic User Interfaces [34].", "label": "Conclusion", "bboxes": [{"left": 0.4621797385620915, "top": 0.6688737373737375, "width": 0.025820261437908554, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6827108585858586, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6965479797979799, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.710385101010101, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7242222222222221, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.7380593434343434, "width": 0.248233660130719, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4298"}, {"text": "In summary, like On-Skin interfaces, Skin-On interfaces also aim to use the affordances of human skin.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.2827222222222222, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.2965593434343434, "width": 0.2673464052287582, "height": 0.012579545454545427, "page": 1}], "section": "On-Skin interfaces", "prob": 1, "is_author_statement": false, "is_in_expected_section": false, "id": "4299"}, {"text": "The skin is about 1.7 m 2 in area and approximately 4 kg in weight, thus accounting for about 5.5% of body mass.", "label": "Conclusion", "bboxes": [{"left": 0.08761437908496732, "top": 0.8271729797979798, "width": 0.39767647058823524, "height": 0.014580808080808083, "page": 2}, {"left": 0.08753594771241831, "top": 0.8430113636363636, "width": 0.36424673202614377, "height": 0.012579545454545538, "page": 2}], "section": "Human skin overview", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4300"}, {"text": "It is thus crucial to rst understand the biology of the human skin to identify its unique properties.", "label": "Conclusion", "bboxes": [{"left": 0.42804084967320266, "top": 0.5709431818181818, "width": 0.05725163398692812, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.584780303030303, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5986174242424243, "width": 0.19262418300653594, "height": 0.012579545454545427, "page": 2}], "section": "Flexible sensors", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4301"}, {"text": "In summary, there have been some research aiming at creating deformable sensors, but none has looked at the skin for inspiration; moreover the gestures these sensors can detect are limited to particular ones (e.g. bending but no stretching, stretching with no pressure, or pressure deformation but no stretching etc.).", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.37329166666666663, "width": 0.3998807189542484, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.38712878787878785, "width": 0.3974558823529412, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.40096590909090907, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4148030303030303, "width": 0.39988071895424837, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4286401515151515, "width": 0.3971764705882353, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4424772727272727, "width": 0.1274542483660131, "height": 0.012579545454545427, "page": 2}], "section": "Flexible sensors", "prob": 1, "is_author_statement": false, "is_in_expected_section": false, "id": "4302"}, {"text": "The results suggest that the two human skin colors (beige and brown) better communicate interactivity than the others (p<0.05), in particular the usual white/black device pigmentation.", "label": "Conclusion", "bboxes": [{"left": 0.849062091503268, "top": 0.7952108585858586, "width": 0.07275490196078449, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39826960784313736, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.356078431372549, "height": 0.012579545454545538, "page": 3}], "section": "Results of study 1", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4303"}, {"text": "Because these properties have a large range of values, we choose to look at the question under a different angle: how to reproduce the skin so it is valuable for interaction as well.", "label": "Conclusion", "bboxes": [{"left": 0.3914575163398693, "top": 0.6591830808080807, "width": 0.09383006535947708, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6730202020202021, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6868573232323232, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7006944444444445, "width": 0.2895947712418301, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4304"}, {"text": "For example it is possible that replicating the exact color of human skin may not be ideal because the human likeness is tight to the Uncanny Valley effect [56] and can elicit feelings of eeriness and revulsion in observers.", "label": "Conclusion", "bboxes": [{"left": 0.4021225490196078, "top": 0.4539709595959596, "width": 0.08316993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.46780808080808084, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.48164520202020206, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.4954823232323233, "width": 0.3971797385620915, "height": 0.012579545454545371, "page": 3}, {"left": 0.08811928104575163, "top": 0.5093194444444444, "width": 0.1845228758169935, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4305"}, {"text": "It is also unclear which texture , and which thickness is the most appropriate for interaction (as users may prefer interacting with thick viscous layers).", "label": "Conclusion", "bboxes": [{"left": 0.28184477124183005, "top": 0.5367045454545455, "width": 0.20549183006535948, "height": 0.012868686868686807, "page": 3}, {"left": 0.08811928104575163, "top": 0.5508308080808081, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 3}, {"left": 0.08758823529411765, "top": 0.5646679292929293, "width": 0.40056045751633995, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4306"}, {"text": "Figure 3 illustrates the ve types of pigmentation compared: beige and brown colors representative of realistic human skin colors; white and black colors representative of usual device colors; green color to suggest something organic, but not necessarily human (e.g. alien or reptilian).", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.22334722222222222, "width": 0.3994411764705883, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.23718434343434341, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.25102146464646463, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.26485858585858585, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.27869570707070707, "width": 0.27584313725490206, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4307"}, {"text": "This material is for example used to create skin simulators for medical training [40, 86, 12] because of its mechanical properties.", "label": "Conclusion", "bboxes": [{"left": 0.08762091503267974, "top": 0.20364772727272726, "width": 0.397671568627451, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.21748484848484848, "width": 0.39717320261437916, "height": 0.012578282828282855, "page": 3}, {"left": 0.08811928104575163, "top": 0.23132070707070707, "width": 0.06778594771241829, "height": 0.012579545454545482, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4308"}, {"text": "Silicone thus appears as a promising material to reproduce skin properties within Skin-on interfaces.", "label": "Conclusion", "bboxes": [{"left": 0.1850049019607843, "top": 0.2589949494949495, "width": 0.30028758169934644, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.2728320707070707, "width": 0.3547467320261438, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4309"}, {"text": "Our rst experiment aims at understanding the impact of pigmentation on the perception of skin human-likeness and comfort, but also at detecting possible negative anthropomorphic effects.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.850270202020202, "width": 0.39987091503267974, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.8641073232323233, "width": 0.3998660130718954, "height": 0.012868686868686807, "page": 3}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.04704575163398693, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4310"}, {"text": "The results (Figure 6) suggest that the exaggerated sample is less comfortable than the three others (p<0.05).", "label": "Conclusion", "bboxes": [{"left": 0.7569117647058824, "top": 0.22794570707070705, "width": 0.1676356209150326, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.24178282828282827, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.25561868686868683, "width": 0.13985457516339872, "height": 0.012579545454545482, "page": 4}], "section": "Participants and experimental design", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4311"}, {"text": "In the visual phase the task was similar except that participants could only rely on the visual modality.", "label": "Conclusion", "bboxes": [{"left": 0.23245424836601308, "top": 0.6924431818181818, "width": 0.2531045751633986, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7062803030303031, "width": 0.40003267973856216, "height": 0.012579545454545427, "page": 4}], "section": "Participants and experimental design", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4312"}, {"text": "We expected that the black and white colors would be perceived as more interactive because of their similarity with devices, but natural skin pigmentation was associated to a higher degree of interactivity.", "label": "Conclusion", "bboxes": [{"left": 0.08735457516339869, "top": 0.26375126262626264, "width": 0.4006437908496732, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811764705882352, "top": 0.27758838383838386, "width": 0.39717483660130726, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811764705882352, "top": 0.2914255050505051, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.3052626262626263, "width": 0.19246568627450977, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 1", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4313"}, {"text": "Finally, the results did not suggest that the two human skin colors are perceived signicantly looking less comfortable than the other colors.", "label": "Conclusion", "bboxes": [{"left": 0.26920424836601303, "top": 0.21469318181818184, "width": 0.21608823529411775, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.22853030303030306, "width": 0.3977385620915033, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.24236742424242425, "width": 0.30247549019607844, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 1", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4314"}, {"text": "We then present our fabrication method and nish by presenting our hardware/software open toolkit that enables controlling the sensing layer, and demonstrate how we can detect the previously dened gestures.", "label": "Conclusion", "bboxes": [{"left": 0.6069183006535949, "top": 0.32229419191919195, "width": 0.3149183006535947, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3361313131313131, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.3499671717171717, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3638042929292929, "width": 0.24870751633986932, "height": 0.012579545454545482, "page": 5}], "section": "SENSING INSPIRATION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4315"}, {"text": "Our results suggest that users tend to transpose the interactions they are doing with real skin to articial skin, and that articial skin leverages the expressive gestures and tactile expressions of pro-social emotions.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.1305151515151515, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.14435227272727272, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.15818939393939394, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.17202651515151515, "width": 0.1579232026143791, "height": 0.012579545454545454, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4316"}, {"text": "Gestures with similar characteristics to conventional multi-touch devices and traditional input paradigms suggest that users transpose conventional multitouch gestures onto other interactive surfaces, like articial skin.", "label": "Conclusion", "bboxes": [{"left": 0.6943758169934641, "top": 0.17202651515151515, "width": 0.23016830065359484, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.18586363636363637, "width": 0.39717483660130715, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.1997007575757576, "width": 0.3998807189542485, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.2135378787878788, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.2273737373737374, "width": 0.0305816993464052, "height": 0.012579545454545454, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4317"}, {"text": "We choose to use a matrix layout because it is easier to fabricate and requires less components and apparatus.", "label": "Conclusion", "bboxes": [{"left": 0.8527418300653595, "top": 0.8291755050505051, "width": 0.06909477124183006, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8430113636363636, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.8568484848484849, "width": 0.2617630718954249, "height": 0.012579545454545427, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4318"}, {"text": "We thus discarded this solution.", "label": "Conclusion", "bboxes": [{"left": 0.6202859477124183, "top": 0.3335707070707071, "width": 0.20714705882352946, "height": 0.012579545454545482, "page": 6}], "section": "Skin-On Fabrication Process", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4319"}, {"text": "It has a theoretical conductivity of 2  / cm when mixing manually, but we could not get a conductivity under 10k  / cm .", "label": "Conclusion", "bboxes": [{"left": 0.20300490196078433, "top": 0.7320265151515152, "width": 0.2822892156862745, "height": 0.013018939393939388, "page": 6}, {"left": 0.08753267973856209, "top": 0.7461527777777778, "width": 0.3983366013071895, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811437908496732, "top": 0.7597007575757576, "width": 0.11201307189542482, "height": 0.013018939393939388, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4320"}, {"text": "Thus, we discarded this solution.", "label": "Conclusion", "bboxes": [{"left": 0.27314379084967316, "top": 0.5863977272727273, "width": 0.21432679738562094, "height": 0.012579545454545427, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4321"}, {"text": "We present the implementation of our hardware and software toolkit and demonstrate its gesture recognition algorithm, which can detect gestures proposed in the previous section of this paper.", "label": "Conclusion", "bboxes": [{"left": 0.3634983660130719, "top": 0.4683396464646465, "width": 0.1245016339869281, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4821767676767677, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4960138888888889, "width": 0.3998758169934641, "height": 0.012579545454545371, "page": 7}, {"left": 0.08811928104575163, "top": 0.5098510101010101, "width": 0.3346258169934641, "height": 0.012579545454545427, "page": 7}], "section": "Open-toolkit for touch and gestures detection", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4322"}, {"text": "To support accurate spacial interpolation, we upscale the image 5x using the Lanczos-4 algorithm (Figure 12-c).", "label": "Conclusion", "bboxes": [{"left": 0.5241601307189543, "top": 0.44048106060606057, "width": 0.3976830065359477, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4543181818181818, "width": 0.33054084967320263, "height": 0.012579545454545482, "page": 7}], "section": "Data processing", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4323"}, {"text": "Paint or makeup can be added to shade the articial skin with eshlike tonal variation, thus increasing anthropomorphism.", "label": "Conclusion", "bboxes": [{"left": 0.4527467320261438, "top": 0.2487588383838384, "width": 0.03254575163398693, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.2625959595959596, "width": 0.39986928104575165, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.2764330808080808, "width": 0.3607042483660131, "height": 0.012579545454545427, "page": 7}], "section": "Skin-On Fabrication Process", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4324"}, {"text": "Although preliminary, these results are promising and demonstrate the feasibility of our approach.", "label": "Conclusion", "bboxes": [{"left": 0.1694330065359477, "top": 0.32423358585858586, "width": 0.31585947712418305, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.3380707070707071, "width": 0.31149346405228756, "height": 0.012579545454545427, "page": 8}], "section": "Data processing", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4325"}, {"text": "Other gestures are detected because of their large surface area.", "label": "Conclusion", "bboxes": [{"left": 0.20268464052287583, "top": 0.0814570707070707, "width": 0.28287745098039213, "height": 0.012579545454545468, "page": 8}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.12039379084967321, "height": 0.012579545454545468, "page": 8}], "section": "Data processing", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4326"}, {"text": "This prototype has a dimension of 8cm x 15cm and could easily be extended to tablets.", "label": "Conclusion", "bboxes": [{"left": 0.3655996732026144, "top": 0.5639987373737374, "width": 0.11969934640522872, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5778358585858586, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5916729797979798, "width": 0.04657352941176472, "height": 0.012579545454545427, "page": 8}], "section": "USE CASES", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4327"}, {"text": "Moreover, once users discover the skin metaphor (either by themselves or after communicating with others), they may be more inclined to explore additional gestures and discover new controls.", "label": "Conclusion", "bboxes": [{"left": 0.7793055555555556, "top": 0.34442676767676766, "width": 0.142531045751634, "height": 0.012579545454545482, "page": 8}, {"left": 0.5246633986928104, "top": 0.3582638888888889, "width": 0.39988398692810456, "height": 0.012579545454545482, "page": 8}, {"left": 0.5246633986928104, "top": 0.3721010101010101, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.38593686868686866, "width": 0.3002124183006536, "height": 0.012579545454545482, "page": 8}], "section": "Communicating interaction", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4328"}, {"text": "For example, a simple touch by the user can indicate she is paying attention to the ECA speech.", "label": "Conclusion", "bboxes": [{"left": 0.14020588235294118, "top": 0.7979090909090909, "width": 0.34509150326797383, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.8117462121212121, "width": 0.2898611111111112, "height": 0.012579545454545427, "page": 9}], "section": "Applications for emotional communication", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4329"}, {"text": "While preliminary studies indicate that we can recognize 8 touch gestures and multi-touch ones, taking individual variability into account and using better recognition algorithms (typically relying on machine learning) would improve the recognition rate and allow distinguishing variations of these gestures (e.g. soft grab vs. hard grab).", "label": "Conclusion", "bboxes": [{"left": 0.7196764705882353, "top": 0.09529419191919192, "width": 0.20486111111111116, "height": 0.012579545454545468, "page": 9}, {"left": 0.5246633986928104, "top": 0.10913131313131313, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.12296717171717171, "width": 0.39745915032679746, "height": 0.01257954545454544, "page": 9}, {"left": 0.5246633986928104, "top": 0.13680429292929294, "width": 0.39827287581699344, "height": 0.012579545454545454, "page": 9}, {"left": 0.5240784313725491, "top": 0.15064141414141416, "width": 0.3977614379084967, "height": 0.012579545454545427, "page": 9}, {"left": 0.5242565359477125, "top": 0.16447853535353535, "width": 0.3712189542483658, "height": 0.012579545454545454, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "4330"}, {"text": "Thus, the orientation of the articial skin should be preferably chosen in such a way that frequent stretch gestures are performed on the diagonal of the grid.", "label": "Conclusion", "bboxes": [{"left": 0.7413480392156863, "top": 0.27517550505050503, "width": 0.1804885620915032, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.28901136363636365, "width": 0.39718137254901953, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.30284848484848487, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.3166856060606061, "width": 0.02968954248366018, "height": 0.012579545454545482, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "4331"}, {"text": "However, different areas could have different acuity, as it is the case with the human body.", "label": "Conclusion", "bboxes": [{"left": 0.6311895424836601, "top": 0.4626035353535354, "width": 0.29065359477124186, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.4764406565656566, "width": 0.3044558823529412, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "4332"}, {"text": "Similarly, the sides of an interactive table could have a higher resolution than its center, as more interactions occur in the vicinity of the user position.", "label": "Conclusion", "bboxes": [{"left": 0.5688218954248366, "top": 0.5041136363636364, "width": 0.35302124183006534, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.5179507575757576, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5317878787878788, "width": 0.20709803921568637, "height": 0.012579545454545538, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "4333"}, {"text": "While our paper focuses on common interactive systems (PC, smartphones, smartwatches), Skin-On interfaces could also be useful in a wide range of setups, including robots and connected objects, or for extending the capabilities of everyday life objects.", "label": "Conclusion", "bboxes": [{"left": 0.5238986928104574, "top": 0.5531729797979797, "width": 0.3999722222222223, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.567010101010101, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5808472222222222, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.5946830808080807, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.608520202020202, "width": 0.0785539215686275, "height": 0.012579545454545538, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "4334"}, {"text": "For instance, the color of the skin could change (using thermochromatic ink) to inform about a new message or to communicate the emotional state", "label": "Conclusion", "bboxes": [{"left": 0.7572287581699346, "top": 0.8512967171717172, "width": 0.16461111111111115, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.8651338383838384, "width": 0.39717647058823535, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8789709595959596, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "4335"}, {"text": "Similarly, the texture of the skin could change (sweat or goosebumps) to convey disgust or frustration.", "label": "Conclusion", "bboxes": [{"left": 0.17192156862745098, "top": 0.0814570707070707, "width": 0.3133709150326798, "height": 0.012579545454545468, "page": 10}, {"left": 0.08758169934640524, "top": 0.09529419191919192, "width": 0.3512271241830065, "height": 0.012579545454545468, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "4336"}, {"text": "Shapechanging mechanisms such as air cavity [22] [1] could be used to stiffen some parts of the skin (e.g. veins, muscles) to modify the relief of the skin epidermis, thus the gesture performed on the skin.", "label": "Conclusion", "bboxes": [{"left": 0.4436911764705882, "top": 0.09529419191919192, "width": 0.04430228758169935, "height": 0.012579545454545468, "page": 10}, {"left": 0.08812418300653595, "top": 0.10913131313131313, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 10}, {"left": 0.08812418300653595, "top": 0.12296843434343435, "width": 0.39774673202614375, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.13680429292929294, "width": 0.3971797385620915, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.05406535947712417, "height": 0.012579545454545427, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "4337"}, {"text": "Emotional reactions and social acceptance of new form factors may change quickly, and they also depend on various aspects.", "label": "Conclusion", "bboxes": [{"left": 0.3972761437908497, "top": 0.1997007575757576, "width": 0.09071732026143786, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.21353661616161618, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.31223692810457515, "height": 0.012579545454545454, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": true, "id": "4338"}, {"text": "More generally, our work explores the intersection between man and machine (human augmentation) from a new and radical perspective: instead of augmenting the human with parts of machines, we demonstrate how machines can be augmented with parts of human.", "label": "Conclusion", "bboxes": [{"left": 0.38148366013071894, "top": 0.3380707070707071, "width": 0.10585130718954255, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.3519078282828283, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 10}, {"left": 0.08758169934640524, "top": 0.36574368686868686, "width": 0.3999803921568627, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.3795808080808081, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.3934179292929293, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.4072550505050505, "width": 0.04838071895424835, "height": 0.012579545454545482, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "4339"}, {"text": "Using articial skin on a device may create similar effects, and could change the engagement or affection that we have towards inanimate objects such as interactive devices.", "label": "Conclusion", "bboxes": [{"left": 0.08811928104575163, "top": 0.49782449494949493, "width": 0.39919934640522875, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.5116616161616161, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.5254987373737373, "width": 0.34108006535947716, "height": 0.012579545454545538, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "4340"}, {"text": "We thus believe that our anthropomorphic approach can inspire other researchers and lead to a novel generation of devices with an input system closer to nature.", "label": "Conclusion", "bboxes": [{"left": 0.4341290849673203, "top": 0.5254987373737373, "width": 0.05116339869281045, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.5393358585858585, "width": 0.39745588235294127, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.5531729797979797, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.567010101010101, "width": 0.1912598039215686, "height": 0.012579545454545427, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "4341"}, {"text": "It provides depth and resistance when human interacts with the skin, thus providing kinesthetic feedback.", "label": "Conclusion", "bboxes": [{"left": 0.6197467320261438, "top": 0.21205555555555555, "width": 0.3020816993464053, "height": 0.012579545454545454, "page": 2}, {"left": 0.5409428104575164, "top": 0.22589267676767677, "width": 0.38364215686274505, "height": 0.012579545454545454, "page": 2}], "section": "Human skin overview", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4342"}, {"text": "Another approach to implement On-skin interfaces without the need for additional overlays is to use optical tracking to detect gestures directly on the user skin [15, 30, 101, 87, 97, 28, 50, 61].", "label": "Future Work", "bboxes": [{"left": 0.6610408496732026, "top": 0.19215277777777778, "width": 0.2608022875816993, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.205989898989899, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.21982702020202022, "width": 0.3992140522875818, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.23366414141414144, "width": 0.18028594771241824, "height": 0.012579545454545454, "page": 1}], "section": "On-Skin interfaces", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4343"}, {"text": "In this paper we considered one aspect (skin) but we hope our work will inspire researchers to investigate how interfaces can be designed to integrate elements from nature.", "label": "Future Work", "bboxes": [{"left": 0.3442892156862745, "top": 0.7380593434343434, "width": 0.14371078431372547, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7518964646464645, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.7657335858585859, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7795694444444444, "width": 0.20319117647058826, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4344"}, {"text": "To go further in the direction of deformable material, some works use silicone or PDMS layers.", "label": "Future Work", "bboxes": [{"left": 0.08761437908496732, "top": 0.0814570707070707, "width": 0.39767156862745096, "height": 0.012579545454545468, "page": 2}, {"left": 0.08753104575163399, "top": 0.09529419191919192, "width": 0.2522859477124183, "height": 0.012579545454545468, "page": 2}], "section": "Flexible sensors", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4345"}, {"text": "We also only focused on input rather than output (e.g. self-lubrication, actuation of hair follicles or temperature) that we discuss in the future work section.", "label": "Future Work", "bboxes": [{"left": 0.6881045751633987, "top": 0.3932373737373738, "width": 0.23401797385620915, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.4070732323232323, "width": 0.3974460784313727, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4209103535353535, "width": 0.36106209150326796, "height": 0.012579545454545482, "page": 2}], "section": "Human Skin properties", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4346"}, {"text": "To understand the reasons behind this choice we need to explain the different techniques that can be used.", "label": "Future Work", "bboxes": [{"left": 0.7083970588235294, "top": 0.6970946969696971, "width": 0.21343954248366015, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7109318181818182, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7247676767676767, "width": 0.054627450980392234, "height": 0.012579545454545538, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4347"}, {"text": "To implement the electrode pattern described above, we need a conductive material that ts our requirements.", "label": "Future Work", "bboxes": [{"left": 0.08761437908496732, "top": 0.41280681818181814, "width": 0.3976748366013072, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.42664393939393935, "width": 0.3088137254901961, "height": 0.012579545454545482, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4348"}, {"text": "Further tests are needed to evaluate the robustness of our system.", "label": "Future Work", "bboxes": [{"left": 0.6820898692810458, "top": 0.0814570707070707, "width": 0.23974836601307192, "height": 0.012579545454545468, "page": 9}, {"left": 0.5246633986928104, "top": 0.09529419191919192, "width": 0.18997385620915042, "height": 0.012579545454545468, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4349"}, {"text": "We now discuss future directions regarding the implementation, the concept and the approach.", "label": "Future Work", "bboxes": [{"left": 0.08735457516339869, "top": 0.8782335858585859, "width": 0.40064379084967316, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.22739869281045755, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4350"}, {"text": "While, so far, we have focused on conveying different types of information with Skin-On interfaces, our future aim is to perceive affect through articial skin to reinforce engagement between interaction partners.", "label": "Future Work", "bboxes": [{"left": 0.5668545751633988, "top": 0.8097866161616162, "width": 0.3549771241830063, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8236237373737373, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.837459595959596, "width": 0.399872549019608, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8512967171717172, "width": 0.22749673202614384, "height": 0.012579545454545538, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4351"}, {"text": "More generally, our goal is to further explore various types of anthropomorphism towards human-like devices.", "label": "Future Work", "bboxes": [{"left": 0.14719444444444443, "top": 0.15064141414141416, "width": 0.3380980392156864, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.369609477124183, "height": 0.012579545454545454, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4352"}, {"text": "Further investigations are needed to provide clearer guidelines to follow a bio-driven approach in HCI, and we believe that exploring synergies with other elds such as Material engineering or Robotics will be a powerful means to further the development of advanced interactive devices [68].", "label": "Future Work", "bboxes": [{"left": 0.4165343137254902, "top": 0.6852525252525253, "width": 0.07146568627450978, "height": 0.012579545454545427, "page": 10}, {"left": 0.0877124183006536, "top": 0.6990896464646464, "width": 0.39816666666666667, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.7129267676767678, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.7267638888888889, "width": 0.3974558823529412, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.7406010101010102, "width": 0.3971781045751634, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.7544381313131313, "width": 0.2385653594771242, "height": 0.012579545454545427, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4353"}, {"text": "More generally, our goal is to further explore various types of anthropomorphism towards human-like devices.", "label": "Objective", "bboxes": [{"left": 0.14719444444444443, "top": 0.15064141414141416, "width": 0.3380980392156864, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.369609477124183, "height": 0.012579545454545454, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.8259490132331848, "is_author_statement": true, "is_in_expected_section": false, "id": "4354"}, {"text": "While, so far, we have focused on conveying different types of information with Skin-On interfaces, our future aim is to perceive affect through articial skin to reinforce engagement between interaction partners.", "label": "Objective", "bboxes": [{"left": 0.5668545751633988, "top": 0.8097866161616162, "width": 0.3549771241830063, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8236237373737373, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.837459595959596, "width": 0.399872549019608, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8512967171717172, "width": 0.22749673202614384, "height": 0.012579545454545538, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.7903719544410706, "is_author_statement": true, "is_in_expected_section": false, "id": "4355"}, {"text": "Our sensory exploration let us to form a series of guidelines for mimicking human skin for an interactive setup: for the pigmentation using a skin-like color; for the texture using a realistic skin pore and wrinkle structure; for the thickness , using a fat layer of 5mm to 10mm and a dermis of 1.2mm.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.5375795454545454, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.5514166666666667, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5649646464646465, "width": 0.3971895424836602, "height": 0.012868686868686918, "page": 5}, {"left": 0.08813562091503269, "top": 0.5788017676767677, "width": 0.39919934640522875, "height": 0.012868686868686807, "page": 5}, {"left": 0.08811928104575163, "top": 0.5929267676767677, "width": 0.38054901960784315, "height": 0.012579545454545427, "page": 5}], "section": "Sensory exploration results", "prob": 0.6657529473304749, "is_author_statement": true, "is_in_expected_section": false, "id": "4356"}, {"text": "Once the electrode grid is positioned, we pour another thin layer of silicone to seal it in place.", "label": "Method", "bboxes": [{"left": 0.7560735294117646, "top": 0.8291742424242424, "width": 0.16576960784313732, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.8430113636363636, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.8568484848484849, "width": 0.03841830065359475, "height": 0.012579545454545427, "page": 6}], "section": "Skin-On Fabrication Process", "prob": 0.5443034768104553, "is_author_statement": true, "is_in_expected_section": true, "id": "4357"}, {"text": "The nal step is to detect users gestures.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.4683396464646465, "width": 0.2708218954248366, "height": 0.012579545454545427, "page": 7}], "section": "Open-toolkit for touch and gestures detection", "prob": 0.5319761633872986, "is_author_statement": false, "is_in_expected_section": true, "id": "4358"}, {"text": "To support accurate spacial interpolation, we upscale the image 5x using the Lanczos-4 algorithm (Figure 12-c).", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.44048106060606057, "width": 0.3976830065359477, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4543181818181818, "width": 0.33054084967320263, "height": 0.012579545454545482, "page": 7}], "section": "Data processing", "prob": 0.5276324152946472, "is_author_statement": true, "is_in_expected_section": true, "id": "4359"}, {"text": "We combine it with Silc pig pigments for the pigmentation and mould strategies (using Mold Start) for generating specic textures.", "label": "Method", "bboxes": [{"left": 0.3043169934640523, "top": 0.33543813131313127, "width": 0.18097222222222226, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.349564393939394, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3634015151515152, "width": 0.2899607843137255, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": 0.520754337310791, "is_author_statement": true, "is_in_expected_section": true, "id": "4360"}, {"text": "To better understand which are the desirable properties of the human skin to reproduce within articial skin, we looked through the Biology literature [20, 36, 38] and gathered information about the visual, haptic and sensing properties of the skin (described below).", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.2687032828282828, "width": 0.3976715686274508, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.28254040404040404, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.29637752525252525, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.31021464646464647, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.3240517676767677, "width": 0.18175000000000008, "height": 0.012579545454545482, "page": 2}], "section": "Human Skin properties", "prob": 0.514478325843811, "is_author_statement": true, "is_in_expected_section": true, "id": "4361"}, {"text": "We implemented a pressure-based menu.", "label": "Method", "bboxes": [{"left": 0.6447124183006536, "top": 0.6726237373737374, "width": 0.2799640522875817, "height": 0.012579545454545538, "page": 8}], "section": "Increasing input bandwidth", "prob": 0.5138163566589355, "is_author_statement": true, "is_in_expected_section": true, "id": "4362"}, {"text": "The Datastretch conductive threads [93] are then placed in a perpendicular grid on top of the articial epidermis to form the electrodes.", "label": "Method", "bboxes": [{"left": 0.8960228758169935, "top": 0.7323156565656566, "width": 0.025818627450980514, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246683006535947, "top": 0.7461527777777778, "width": 0.39988071895424826, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246683006535947, "top": 0.759989898989899, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.0695032679738562, "height": 0.012579545454545427, "page": 6}], "section": "Skin-On Fabrication Process", "prob": 0.5049647688865662, "is_author_statement": false, "is_in_expected_section": true, "id": "4363"}, {"text": "An API is provided to share touch points and gesture events using Unity3d.", "label": "Method", "bboxes": [{"left": 0.7767352941176471, "top": 0.5511767676767677, "width": 0.1451045751633988, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.5650138888888888, "width": 0.3429771241830065, "height": 0.012579545454545538, "page": 7}], "section": "Data processing", "prob": 0.4971051812171936, "is_author_statement": false, "is_in_expected_section": true, "id": "4364"}, {"text": "To minimize the background noise, we perform an initial calibration.", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.3222373737373737, "width": 0.3976715686274508, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.33607449494949493, "width": 0.07222058823529409, "height": 0.012579545454545482, "page": 7}], "section": "Data processing", "prob": 0.4916882812976837, "is_author_statement": true, "is_in_expected_section": true, "id": "4365"}, {"text": "The ECA then detects the touch gesture, interprets it and reacts accordingly.", "label": "Method", "bboxes": [{"left": 0.3869297385620915, "top": 0.8117462121212121, "width": 0.09836274509803927, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.8255833333333333, "width": 0.3967777777777778, "height": 0.012579545454545538, "page": 9}], "section": "Applications for emotional communication", "prob": 0.4912797808647156, "is_author_statement": false, "is_in_expected_section": true, "id": "4366"}, {"text": "We used both an Arduino Pro Micro board for sending the data via serial communication to a laptop, and a Wemos D1 mini for transmitting the information wirelessly to the mobile device.", "label": "Method", "bboxes": [{"left": 0.2973545751633987, "top": 0.6880782828282828, "width": 0.18794444444444447, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7019154040404041, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7157512626262627, "width": 0.3998758169934641, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7295883838383838, "width": 0.23384640522875821, "height": 0.012579545454545538, "page": 7}], "section": "Hardware Platform", "prob": 0.49102896451950073, "is_author_statement": true, "is_in_expected_section": true, "id": "4367"}, {"text": "Once cured, we laser cut it to the desired pattern and sealed it into another silicone layer.", "label": "Method", "bboxes": [{"left": 0.6353970588235294, "top": 0.222875, "width": 0.2864395424836601, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.2367121212121212, "width": 0.3077973856209151, "height": 0.012579545454545454, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.4855283200740814, "is_author_statement": true, "is_in_expected_section": true, "id": "4368"}, {"text": "We present different sensing techniques and discuss which ones are more adapted to mimic human skin.", "label": "Method", "bboxes": [{"left": 0.7331470588235294, "top": 0.2946199494949495, "width": 0.1886895424836602, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.30845707070707074, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.32229419191919195, "width": 0.07743790849673204, "height": 0.012579545454545482, "page": 5}], "section": "SENSING INSPIRATION", "prob": 0.4696391522884369, "is_author_statement": true, "is_in_expected_section": true, "id": "4369"}, {"text": "We choose to implement our sensor using a matrix layout sensing mutual capacitance.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.6832575757575757, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.6970946969696971, "width": 0.1786928104575164, "height": 0.012579545454545427, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.4665643274784088, "is_author_statement": true, "is_in_expected_section": true, "id": "4370"}, {"text": "After the board is detected, we create a calibration matrix, by averaging the individual value of each coordinate 10 times.", "label": "Method", "bboxes": [{"left": 0.6018447712418301, "top": 0.33607449494949493, "width": 0.31999183006535936, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.34991161616161615, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5234428104575163, "top": 0.36374873737373736, "width": 0.05850000000000011, "height": 0.012579545454545482, "page": 7}], "section": "Data processing", "prob": 0.4559709429740906, "is_author_statement": true, "is_in_expected_section": true, "id": "4371"}, {"text": "To implement the electrode pattern described above, we need a conductive material that ts our requirements.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.41280681818181814, "width": 0.3976748366013072, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.42664393939393935, "width": 0.3088137254901961, "height": 0.012579545454545482, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.44589781761169434, "is_author_statement": true, "is_in_expected_section": true, "id": "4372"}, {"text": "In particular, we follow a bio-driven approach where we take inspiration from the human skin to design this new type of interfaces.", "label": "Method", "bboxes": [{"left": 0.14692973856209152, "top": 0.09529419191919192, "width": 0.33836764705882355, "height": 0.012579545454545468, "page": 1}, {"left": 0.08753267973856209, "top": 0.10913131313131313, "width": 0.39835294117647063, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.11796732026143793, "height": 0.01257954545454544, "page": 1}], "section": "INTRODUCTION", "prob": 0.4048430025577545, "is_author_statement": true, "is_in_expected_section": true, "id": "4373"}, {"text": "First, we prepared a cPDMS mixing carbon black, EcoFlex 00-30 silicone and D5 solvent.", "label": "Method", "bboxes": [{"left": 0.35762908496732027, "top": 0.6354570707070707, "width": 0.1276683006535948, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.6492941919191919, "width": 0.397171568627451, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.6631313131313132, "width": 0.05187091503267975, "height": 0.012579545454545427, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.3969782590866089, "is_author_statement": true, "is_in_expected_section": true, "id": "4374"}, {"text": "To determine and track the position of multiple points over time, we use the contour points (stored in a k-d tree), and nd the closest blob position in O(log n ).", "label": "Method", "bboxes": [{"left": 0.6981225490196078, "top": 0.7323156565656566, "width": 0.22371405228758168, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7597007575757576, "width": 0.3829738562091505, "height": 0.012868686868686807, "page": 7}], "section": "Data processing", "prob": 0.395550012588501, "is_author_statement": true, "is_in_expected_section": true, "id": "4375"}, {"text": "material by pouring a thin layer of silicone on top of the conductive textile.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.20903787878787877, "width": 0.3998807189542485, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.222875, "width": 0.09970915032679739, "height": 0.012579545454545454, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.3933703303337097, "is_author_statement": false, "is_in_expected_section": true, "id": "4376"}, {"text": "Our goal is to replicate the three layers of the human skin: The epidermis layer provides both visual and tactile perception (e.g. texture); The dermis layer is the sensory layer embedding nerves to detect mechanical contact; The hypodermis layer provides kinesthetic feedback due to its soft mechanical properties (viscosity, thickness, etc.).", "label": "Method", "bboxes": [{"left": 0.1476339869281046, "top": 0.7325366161616161, "width": 0.33982352941176475, "height": 0.010063131313131302, "page": 2}, {"left": 0.08768954248366012, "top": 0.7438573232323232, "width": 0.39976797385620916, "height": 0.010063131313131302, "page": 2}, {"left": 0.08811928104575163, "top": 0.7551780303030303, "width": 0.3971764705882353, "height": 0.010063131313131302, "page": 2}, {"left": 0.08811928104575163, "top": 0.7665, "width": 0.3975277777777778, "height": 0.010063131313131302, "page": 2}, {"left": 0.0881062091503268, "top": 0.777820707070707, "width": 0.3455669934640523, "height": 0.010063131313131302, "page": 2}], "section": "Flexible sensors", "prob": 0.3748762011528015, "is_author_statement": true, "is_in_expected_section": true, "id": "4377"}, {"text": "We then use this knowledge to dene the most suitable material for creating articial skin.", "label": "Method", "bboxes": [{"left": 0.2857990196078431, "top": 0.5986174242424243, "width": 0.19949509803921572, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6124532828282828, "width": 0.3847647058823529, "height": 0.012579545454545538, "page": 2}], "section": "Flexible sensors", "prob": 0.3650933504104614, "is_author_statement": true, "is_in_expected_section": true, "id": "4378"}, {"text": "Using the data read (in serial or wireless) from the sensing and transmitting electrodes, we build a 2D image of 12x21 pixels.", "label": "Method", "bboxes": [{"left": 0.17165522875816994, "top": 0.8247941919191919, "width": 0.3136372549019608, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.8386313131313131, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.8524671717171717, "width": 0.10744607843137256, "height": 0.012579545454545427, "page": 7}], "section": "Data processing", "prob": 0.3640366196632385, "is_author_statement": true, "is_in_expected_section": true, "id": "4379"}, {"text": "After, we pour an even layer of Ecoex Gel, and let it cool at room temperature for 2 hours before removing the sample from its mold.", "label": "Method", "bboxes": [{"left": 0.3696830065359477, "top": 0.7837171717171717, "width": 0.1156127450980392, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.7975542929292929, "width": 0.3971666666666667, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.8113901515151515, "width": 0.3542254901960784, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "prob": 0.3622454106807709, "is_author_statement": true, "is_in_expected_section": true, "id": "4380"}, {"text": "Next, we explain how we used this material to fabricate our articial skin.", "label": "Method", "bboxes": [{"left": 0.7781274509803922, "top": 0.5486742424242425, "width": 0.1442908496732026, "height": 0.012579545454545427, "page": 6}, {"left": 0.5240784313725491, "top": 0.562510101010101, "width": 0.3348545751633988, "height": 0.012579545454545427, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.3504909574985504, "is_author_statement": true, "is_in_expected_section": true, "id": "4381"}, {"text": "Once set, the silicone is cured a 90  for 5 minutes with a heat gun.", "label": "Method", "bboxes": [{"left": 0.3286846405228758, "top": 0.7698800505050505, "width": 0.15660130718954246, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.781510101010101, "width": 0.27649673202614383, "height": 0.01478661616161625, "page": 3}], "section": "SENSORY INSPIRATION", "prob": 0.32147297263145447, "is_author_statement": false, "is_in_expected_section": true, "id": "4382"}, {"text": "We now explain how we detect touch contact, then more complex gestures.", "label": "Method", "bboxes": [{"left": 0.32700490196078436, "top": 0.7295883838383838, "width": 0.15828758169934642, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.7434255050505051, "width": 0.32526633986928105, "height": 0.012579545454545427, "page": 7}], "section": "Hardware Platform", "prob": 0.31843841075897217, "is_author_statement": true, "is_in_expected_section": true, "id": "4383"}, {"text": "We prepare a rectangular mold of the size of the desired articial skin and place it on top of", "label": "Method", "bboxes": [{"left": 0.7167287581699346, "top": 0.8782335858585859, "width": 0.20511274509803923, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 6}], "section": "Skin-On Fabrication Process", "prob": 0.3141323924064636, "is_author_statement": true, "is_in_expected_section": true, "id": "4384"}, {"text": "More generally, our work explores the intersection between man and machine (human augmentation) from a new and radical perspective: instead of augmenting the human with parts of machines, we demonstrate how machines can be augmented with parts of human.", "label": "Method", "bboxes": [{"left": 0.38148366013071894, "top": 0.3380707070707071, "width": 0.10585130718954255, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.3519078282828283, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 10}, {"left": 0.08758169934640524, "top": 0.36574368686868686, "width": 0.3999803921568627, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.3795808080808081, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.3934179292929293, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.4072550505050505, "width": 0.04838071895424835, "height": 0.012579545454545482, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.30022236704826355, "is_author_statement": true, "is_in_expected_section": false, "id": "4385"}, {"text": "c) Grab detection to display an adaptive pie menu,", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.29984469696969696, "width": 0.2793839869281046, "height": 0.010063131313131302, "page": 8}], "section": "Communicating interaction", "prob": 0.29048171639442444, "is_author_statement": false, "is_in_expected_section": true, "id": "4386"}, {"text": "This work also explores the intersection between man and machine (human augmentation) from a new perspective: Instead of augmenting the human with parts of machines, we demonstrate how machines can be augmented with parts of human.", "label": "Method", "bboxes": [{"left": 0.4569264705882353, "top": 0.6135265151515151, "width": 0.028366013071895457, "height": 0.012579545454545427, "page": 1}, {"left": 0.08753267973856209, "top": 0.6273623737373737, "width": 0.39776633986928106, "height": 0.012579545454545427, "page": 1}, {"left": 0.08758169934640524, "top": 0.6411994949494949, "width": 0.4004117647058823, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.6550366161616161, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6688737373737375, "width": 0.36655555555555563, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.2903573215007782, "is_author_statement": true, "is_in_expected_section": true, "id": "4387"}, {"text": "The results suggest that the two human skin colors (beige and brown) better communicate interactivity than the others (p<0.05), in particular the usual white/black device pigmentation.", "label": "Result", "bboxes": [{"left": 0.849062091503268, "top": 0.7952108585858586, "width": 0.07275490196078449, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39826960784313736, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.356078431372549, "height": 0.012579545454545538, "page": 3}], "section": "Results of study 1", "prob": 0.8875380158424377, "is_author_statement": false, "is_in_expected_section": true, "id": "4388"}, {"text": "After each sample, participants indicated their level of agreement about the two following afrmations using a 5-point Likert scale: Touching this surface feels comfortable ; This surface feels like human skin .", "label": "Result", "bboxes": [{"left": 0.4519183006535948, "top": 0.6370959595959595, "width": 0.033660130718954184, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.6509318181818182, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.6647689393939393, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.6783169191919192, "width": 0.39717320261437916, "height": 0.012868686868686918, "page": 4}, {"left": 0.08811928104575163, "top": 0.6921540404040405, "width": 0.13925326797385618, "height": 0.012868686868686807, "page": 4}], "section": "Participants and experimental design", "prob": 0.8862485289573669, "is_author_statement": false, "is_in_expected_section": true, "id": "4389"}, {"text": "Our results suggest that users tend to transpose the interactions they are doing with real skin to articial skin, and that articial skin leverages the expressive gestures and tactile expressions of pro-social emotions.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.1305151515151515, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.14435227272727272, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.15818939393939394, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.17202651515151515, "width": 0.1579232026143791, "height": 0.012579545454545454, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": 0.8490935564041138, "is_author_statement": true, "is_in_expected_section": false, "id": "4390"}, {"text": "Figure 8 illustrates the results.", "label": "Result", "bboxes": [{"left": 0.612766339869281, "top": 0.8505593434343435, "width": 0.19592483660130722, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 3", "prob": 0.8358173370361328, "is_author_statement": false, "is_in_expected_section": true, "id": "4391"}, {"text": "Figure 7 illustrates the four different skin thicknesses we compared.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.4191616161616162, "width": 0.399874183006536, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4329987373737374, "width": 0.040787581699346376, "height": 0.012579545454545482, "page": 4}], "section": "Samples", "prob": 0.8326056599617004, "is_author_statement": true, "is_in_expected_section": false, "id": "4392"}, {"text": "Although preliminary, these results are promising and demonstrate the feasibility of our approach.", "label": "Result", "bboxes": [{"left": 0.1694330065359477, "top": 0.32423358585858586, "width": 0.31585947712418305, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.3380707070707071, "width": 0.31149346405228756, "height": 0.012579545454545427, "page": 8}], "section": "Data processing", "prob": 0.8291643261909485, "is_author_statement": true, "is_in_expected_section": false, "id": "4393"}, {"text": "Figure 5 illustrates the four samples of texture we compared.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.44551641414141413, "width": 0.4000261437908497, "height": 0.012579545454545482, "page": 4}], "section": "Samples", "prob": 0.8110579252243042, "is_author_statement": true, "is_in_expected_section": false, "id": "4394"}, {"text": "Participants naturally compared the 5mm and 10mm with their own skin (respectively hand and forearm) suggesting that these surfaces are the most successful at replicating these skin thickness.", "label": "Result", "bboxes": [{"left": 0.4084346405228758, "top": 0.4472487373737374, "width": 0.07685457516339872, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4607967171717172, "width": 0.39717320261437916, "height": 0.012868686868686807, "page": 5}, {"left": 0.08758169934640524, "top": 0.4749229797979798, "width": 0.397718954248366, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.48876010101010103, "width": 0.37752124183006536, "height": 0.012579545454545427, "page": 5}], "section": "Results of study 3", "prob": 0.8063369393348694, "is_author_statement": false, "is_in_expected_section": true, "id": "4395"}, {"text": "For each sample, participants indicated their levels of agreement regarding the three following afrmations, using a 5-point Likert scale: This interface looks like an interactive device ; This surface looks like human skin ; It looks comfortable touching this surface .", "label": "Result", "bboxes": [{"left": 0.5628366013071896, "top": 0.3705542929292929, "width": 0.3589999999999999, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.38439141414141414, "width": 0.39716666666666667, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.39793813131313127, "width": 0.39717320261437905, "height": 0.012868686868686918, "page": 3}, {"left": 0.5246633986928104, "top": 0.4117752525252525, "width": 0.39988071895424837, "height": 0.012868686868686918, "page": 3}, {"left": 0.5246633986928104, "top": 0.4256123737373737, "width": 0.1733022875816993, "height": 0.012868686868686918, "page": 3}], "section": "Participants and experimental design", "prob": 0.7999869585037231, "is_author_statement": false, "is_in_expected_section": true, "id": "4396"}, {"text": "Finally, the results did not suggest that the two human skin colors are perceived signicantly looking less comfortable than the other colors.", "label": "Result", "bboxes": [{"left": 0.26920424836601303, "top": 0.21469318181818184, "width": 0.21608823529411775, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.22853030303030306, "width": 0.3977385620915033, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.24236742424242425, "width": 0.30247549019607844, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 1", "prob": 0.7989982962608337, "is_author_statement": false, "is_in_expected_section": true, "id": "4397"}, {"text": "After each, participants indicated their level of agreement regarding the following afrmations with a 5-point Likert Scale: It was comfortable doing gestures on this sample ; It was easy doing gestures on this sample ; This surface feels like human skin .", "label": "Result", "bboxes": [{"left": 0.8496748366013072, "top": 0.5674747474747475, "width": 0.07418790849673207, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.5813118686868687, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.5948598484848485, "width": 0.39717320261437905, "height": 0.012868686868686807, "page": 4}, {"left": 0.5246633986928104, "top": 0.6086969696969697, "width": 0.39717647058823535, "height": 0.012868686868686807, "page": 4}, {"left": 0.5246633986928104, "top": 0.622534090909091, "width": 0.38432516339869294, "height": 0.012868686868686807, "page": 4}], "section": "Experimental design", "prob": 0.7624013423919678, "is_author_statement": false, "is_in_expected_section": true, "id": "4398"}, {"text": "For instance, the perception of our participants changed from Study 1 (visual condition only) to Study 2 (visual and tactile perception) although the same interfaces were used.", "label": "Result", "bboxes": [{"left": 0.4053186274509804, "top": 0.2273737373737374, "width": 0.08201470588235288, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.24121085858585856, "width": 0.3998807189542484, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.39826633986928106, "height": 0.012579545454545482, "page": 10}, {"left": 0.08811928104575163, "top": 0.268885101010101, "width": 0.27304084967320263, "height": 0.012579545454545427, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.7603424191474915, "is_author_statement": true, "is_in_expected_section": true, "id": "4399"}, {"text": "Further tests are needed to evaluate the robustness of our system.", "label": "Result", "bboxes": [{"left": 0.6820898692810458, "top": 0.0814570707070707, "width": 0.23974836601307192, "height": 0.012579545454545468, "page": 9}, {"left": 0.5246633986928104, "top": 0.09529419191919192, "width": 0.18997385620915042, "height": 0.012579545454545468, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.7489903569221497, "is_author_statement": true, "is_in_expected_section": true, "id": "4400"}, {"text": "It has a theoretical conductivity of 2  / cm when mixing manually, but we could not get a conductivity under 10k  / cm .", "label": "Result", "bboxes": [{"left": 0.20300490196078433, "top": 0.7320265151515152, "width": 0.2822892156862745, "height": 0.013018939393939388, "page": 6}, {"left": 0.08753267973856209, "top": 0.7461527777777778, "width": 0.3983366013071895, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811437908496732, "top": 0.7597007575757576, "width": 0.11201307189542482, "height": 0.013018939393939388, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.7200828194618225, "is_author_statement": true, "is_in_expected_section": false, "id": "4401"}, {"text": "We see several directions to investigate other form factors.", "label": "Result", "bboxes": [{"left": 0.8226143790849673, "top": 0.3795808080808081, "width": 0.09921078431372554, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246683006535947, "top": 0.3934179292929293, "width": 0.2918856209150328, "height": 0.012579545454545482, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.700441837310791, "is_author_statement": true, "is_in_expected_section": true, "id": "4402"}, {"text": "An effect was found for each: comfortable (Chi-square = 21.8, p<0.05); feels like human (Chi-square = 12.3, p<0.05); and looks like human (Chi-square = 18.6, p<0.05).", "label": "Result", "bboxes": [{"left": 0.8602614379084967, "top": 0.18643434343434345, "width": 0.061586601307189626, "height": 0.012579545454545454, "page": 4}, {"left": 0.5240784313725491, "top": 0.19998232323232323, "width": 0.39911111111111097, "height": 0.01286868686868689, "page": 4}, {"left": 0.5246633986928104, "top": 0.21381944444444445, "width": 0.39717320261437905, "height": 0.01286868686868689, "page": 4}, {"left": 0.5246633986928104, "top": 0.22794570707070705, "width": 0.2274297385620916, "height": 0.012579545454545482, "page": 4}], "section": "Participants and experimental design", "prob": 0.699255645275116, "is_author_statement": false, "is_in_expected_section": true, "id": "4403"}, {"text": "Results of study 2", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.15908333333333333, "width": 0.11644607843137256, "height": 0.01132196969696972, "page": 4}], "section": "Participants and experimental design", "prob": 0.6927782297134399, "is_author_statement": false, "is_in_expected_section": true, "id": "4404"}, {"text": "In a pilot study, we dened the maximum radius (5mm) that a single nger press can have on this surface (Fig. 12-c).", "label": "Result", "bboxes": [{"left": 0.7420130718954249, "top": 0.7046414141414141, "width": 0.1798235294117646, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.1685947712418301, "height": 0.012579545454545427, "page": 7}], "section": "Data processing", "prob": 0.6504942178726196, "is_author_statement": true, "is_in_expected_section": false, "id": "4405"}, {"text": "We compare their effect on comfort as well as the perception of skin human-likeness.", "label": "Result", "bboxes": [{"left": 0.23505228758169935, "top": 0.39230808080808083, "width": 0.2502369281045752, "height": 0.012579545454545427, "page": 4}, {"left": 0.08753267973856209, "top": 0.406145202020202, "width": 0.3014019607843137, "height": 0.012579545454545482, "page": 4}], "section": "Results of study 1", "prob": 0.6499096751213074, "is_author_statement": true, "is_in_expected_section": true, "id": "4406"}, {"text": "On the opposite, a grab gesture (Fig. 12-bottom) is characterized by a large blob on a side of the surface (palm) and four ellipses with large eccentricity at the center of the surface (ngers) (Fig. 12-d).", "label": "Result", "bboxes": [{"left": 0.24088398692810456, "top": 0.13680555555555554, "width": 0.24725326797385624, "height": 0.012579545454545454, "page": 8}, {"left": 0.0868954248366013, "top": 0.15035353535353535, "width": 0.3984035947712418, "height": 0.012868686868686863, "page": 8}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.3971781045751634, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.27113888888888893, "height": 0.012579545454545454, "page": 8}], "section": "Data processing", "prob": 0.647406280040741, "is_author_statement": false, "is_in_expected_section": false, "id": "4407"}, {"text": "This accuracy is comparable to the acuity of the human skin on the forearm.", "label": "Result", "bboxes": [{"left": 0.5661960784313725, "top": 0.6140719696969698, "width": 0.35563725490196085, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6279090909090909, "width": 0.13297549019607846, "height": 0.012579545454545538, "page": 7}], "section": "Data processing", "prob": 0.6422240734100342, "is_author_statement": false, "is_in_expected_section": false, "id": "4408"}, {"text": "We then present the applications we developed for these prototypes.", "label": "Result", "bboxes": [{"left": 0.10686437908496732, "top": 0.40864267676767674, "width": 0.37841176470588234, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.42247853535353536, "width": 0.07134313725490195, "height": 0.012579545454545427, "page": 8}], "section": "USE CASES", "prob": 0.6257522106170654, "is_author_statement": true, "is_in_expected_section": false, "id": "4409"}, {"text": "We ensure that the total interface is under 1.2mm.", "label": "Result", "bboxes": [{"left": 0.5681274509803922, "top": 0.8568484848484849, "width": 0.32404575163398697, "height": 0.012579545454545427, "page": 6}], "section": "Skin-On Fabrication Process", "prob": 0.6072442531585693, "is_author_statement": true, "is_in_expected_section": false, "id": "4410"}, {"text": "We also tested the solutions described below before choosing to use conductive thread.", "label": "Result", "bboxes": [{"left": 0.23426470588235296, "top": 0.49582828282828284, "width": 0.25102941176470583, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5096654040404041, "width": 0.3140490196078432, "height": 0.012579545454545427, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.5767825841903687, "is_author_statement": true, "is_in_expected_section": false, "id": "4411"}, {"text": "We address these points in the following section through three user studies.", "label": "Result", "bboxes": [{"left": 0.40991013071895427, "top": 0.5923409090909091, "width": 0.07537745098039211, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6061780303030303, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": 0.573649525642395, "is_author_statement": true, "is_in_expected_section": false, "id": "4412"}, {"text": "This prototype has a dimension of 8cm x 15cm and could easily be extended to tablets.", "label": "Result", "bboxes": [{"left": 0.3655996732026144, "top": 0.5639987373737374, "width": 0.11969934640522872, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5778358585858586, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5916729797979798, "width": 0.04657352941176472, "height": 0.012579545454545427, "page": 8}], "section": "USE CASES", "prob": 0.5668013095855713, "is_author_statement": false, "is_in_expected_section": false, "id": "4413"}, {"text": "Similarly, a twist gesture can be used to manipulate a tangible knob: the amplitude of the twist rotation controls the volume of a music player (Figure 1-b).", "label": "Result", "bboxes": [{"left": 0.8436977124183006, "top": 0.4670138888888889, "width": 0.07813888888888887, "height": 0.012579545454545482, "page": 8}, {"left": 0.5246633986928104, "top": 0.4808510101010101, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 8}, {"left": 0.5246633986928104, "top": 0.49468813131313133, "width": 0.39717810457516356, "height": 0.012579545454545371, "page": 8}, {"left": 0.5246633986928104, "top": 0.5085252525252525, "width": 0.127248366013072, "height": 0.012579545454545427, "page": 8}], "section": "Leveraging physical interaction", "prob": 0.5637373924255371, "is_author_statement": false, "is_in_expected_section": false, "id": "4414"}, {"text": "The intensity of the touch controls the size of the emojis.", "label": "Result", "bboxes": [{"left": 0.2312483660130719, "top": 0.5412954545454546, "width": 0.2540441176470589, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5551325757575758, "width": 0.11735620915032678, "height": 0.012579545454545427, "page": 9}], "section": "Applications for emotional communication", "prob": 0.5589669942855835, "is_author_statement": false, "is_in_expected_section": false, "id": "4415"}, {"text": "We thus discarded this solution.", "label": "Result", "bboxes": [{"left": 0.6202859477124183, "top": 0.3335707070707071, "width": 0.20714705882352946, "height": 0.012579545454545482, "page": 6}], "section": "Skin-On Fabrication Process", "prob": 0.5539061427116394, "is_author_statement": true, "is_in_expected_section": false, "id": "4416"}, {"text": "For instance, Figure 13-c shows an adaptive", "label": "Result", "bboxes": [{"left": 0.6343627450980392, "top": 0.892070707070707, "width": 0.2874787581699346, "height": 0.012579545454545427, "page": 8}], "section": "Increasing input bandwidth", "prob": 0.5535476207733154, "is_author_statement": false, "is_in_expected_section": false, "id": "4417"}, {"text": "We assemble the insights from these three steps and present the implementation of several Skin-On interfaces and applications to demonstrate the added value of our approach (see Figure 1 for examples).", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.5305037878787878, "width": 0.39793790849673205, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811764705882352, "top": 0.544340909090909, "width": 0.3998807189542483, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811764705882352, "top": 0.5581780303030303, "width": 0.39717483660130726, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811764705882352, "top": 0.5720151515151515, "width": 0.1609624183006536, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5533137321472168, "is_author_statement": true, "is_in_expected_section": true, "id": "4418"}, {"text": "Our design space (Figure 9) summarizes the gestures relevant for Skin-On interfaces.", "label": "Result", "bboxes": [{"left": 0.2983954248366013, "top": 0.759989898989899, "width": 0.18799346405228762, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7738270202020202, "width": 0.36356699346405236, "height": 0.012579545454545427, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": 0.5522774457931519, "is_author_statement": true, "is_in_expected_section": false, "id": "4419"}, {"text": "3. From a sensing point of view, we analyze different fabrication methods to create a silicone layer that can track the previously dened gestures with a spatial acuity comparable to human skin.", "label": "Result", "bboxes": [{"left": 0.08486437908496731, "top": 0.4398472222222222, "width": 0.40312418300653596, "height": 0.012667929292929314, "page": 1}, {"left": 0.10439869281045752, "top": 0.4537714646464647, "width": 0.3809003267973856, "height": 0.012579545454545482, "page": 1}, {"left": 0.10439869281045752, "top": 0.4676085858585859, "width": 0.3809003267973856, "height": 0.012579545454545482, "page": 1}, {"left": 0.10439869281045752, "top": 0.48144570707070705, "width": 0.09305555555555553, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5508719682693481, "is_author_statement": true, "is_in_expected_section": true, "id": "4420"}, {"text": "Figure 14.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.3516691919191919, "width": 0.05732679738562092, "height": 0.010063131313131357, "page": 9}], "section": "Increasing input bandwidth", "prob": 0.5468920469284058, "is_author_statement": false, "is_in_expected_section": false, "id": "4421"}, {"text": "e.g. we observed that it was difcult to study the color of the skin independently from its texture.", "label": "Result", "bboxes": [{"left": 0.16297058823529412, "top": 0.6437424242424242, "width": 0.3223218954248367, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.6575795454545454, "width": 0.3128202614379085, "height": 0.012579545454545427, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "prob": 0.5354794859886169, "is_author_statement": true, "is_in_expected_section": true, "id": "4422"}, {"text": "Thus, we discarded this solution.", "label": "Result", "bboxes": [{"left": 0.27314379084967316, "top": 0.5863977272727273, "width": 0.21432679738562094, "height": 0.012579545454545427, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.5274077653884888, "is_author_statement": true, "is_in_expected_section": false, "id": "4423"}, {"text": "The participants did not propose additional new gestures.", "label": "Result", "bboxes": [{"left": 0.7160065359477125, "top": 0.09529419191919192, "width": 0.20583006535947712, "height": 0.012579545454545468, "page": 5}, {"left": 0.5246633986928104, "top": 0.10913131313131313, "width": 0.15738235294117653, "height": 0.012579545454545454, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": 0.5267654657363892, "is_author_statement": false, "is_in_expected_section": false, "id": "4424"}, {"text": "Figure 13.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.2772032828282828, "width": 0.05645424836601309, "height": 0.010063131313131302, "page": 8}], "section": "Communicating interaction", "prob": 0.5157054662704468, "is_author_statement": false, "is_in_expected_section": false, "id": "4425"}, {"text": "We are particularly focused on sensing layer of thickness below 1.2mm to match human dermis thickness.", "label": "Result", "bboxes": [{"left": 0.7263676470588235, "top": 0.5216199494949495, "width": 0.19547549019607846, "height": 0.012579545454545538, "page": 5}, {"left": 0.5409428104575164, "top": 0.5354570707070707, "width": 0.38088562091503264, "height": 0.012579545454545427, "page": 5}, {"left": 0.5409428104575164, "top": 0.5492941919191919, "width": 0.11303921568627451, "height": 0.012579545454545538, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.5080541968345642, "is_author_statement": true, "is_in_expected_section": false, "id": "4426"}, {"text": "The hypodermis viscous silicone layer of Ecoex Gel is poured inside the mold to reach the desired fat thickness, i.e. 10mm in this example (Figure 10-3).", "label": "Result", "bboxes": [{"left": 0.21007679738562093, "top": 0.0814570707070707, "width": 0.27521568627450976, "height": 0.012579545454545468, "page": 7}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.39717156862745095, "height": 0.012579545454545468, "page": 7}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.3339754901960784, "height": 0.012579545454545454, "page": 7}], "section": "Skin-On Fabrication Process", "prob": 0.49742352962493896, "is_author_statement": false, "is_in_expected_section": false, "id": "4427"}, {"text": "Our exploration into simulating human skin properties starts with the replication of its sensory properties.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.6453472222222222, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 3}, {"left": 0.08753267973856209, "top": 0.6591830808080807, "width": 0.29792973856209143, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "prob": 0.489788293838501, "is_author_statement": true, "is_in_expected_section": false, "id": "4428"}, {"text": "We expected that the black and white colors would be perceived as more interactive because of their similarity with devices, but natural skin pigmentation was associated to a higher degree of interactivity.", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.26375126262626264, "width": 0.4006437908496732, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811764705882352, "top": 0.27758838383838386, "width": 0.39717483660130726, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811764705882352, "top": 0.2914255050505051, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.3052626262626263, "width": 0.19246568627450977, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 1", "prob": 0.475580096244812, "is_author_statement": true, "is_in_expected_section": true, "id": "4429"}, {"text": "We also had the opportunity to observe that users spontaneously performed these gestures during the studies.", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.822885101010101, "width": 0.4006437908496732, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3463807189542484, "height": 0.012579545454545538, "page": 5}], "section": "INTERACTION INSPIRATION", "prob": 0.4753994047641754, "is_author_statement": true, "is_in_expected_section": false, "id": "4430"}, {"text": "We used conductive insulated Datastretch thread [93], which allows a strain up to 30%, is 0.2mm thick, and has a conductivity of 4.2  /m.", "label": "Result", "bboxes": [{"left": 0.8643986928104574, "top": 0.3687929292929293, "width": 0.057450980392157014, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246748366013072, "top": 0.3823409090909091, "width": 0.39717483660130715, "height": 0.012868686868686807, "page": 6}, {"left": 0.5246748366013072, "top": 0.3964671717171717, "width": 0.3977385620915034, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246748366013072, "top": 0.4103030303030303, "width": 0.07438888888888895, "height": 0.012729797979798008, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.47349151968955994, "is_author_statement": true, "is_in_expected_section": false, "id": "4431"}, {"text": "We present an exploration of the design space of Skin-On interfaces.", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.4006470588235294, "height": 0.012579545454545468, "page": 1}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.05375490196078429, "height": 0.012579545454545468, "page": 1}], "section": "INTRODUCTION", "prob": 0.4717942774295807, "is_author_statement": true, "is_in_expected_section": true, "id": "4432"}, {"text": "For instance in study 3 we saw several users spontaneously pulling", "label": "Result", "bboxes": [{"left": 0.46349673202614383, "top": 0.8782335858585859, "width": 0.02207516339869281, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}], "section": "Communicating interaction", "prob": 0.46526986360549927, "is_author_statement": true, "is_in_expected_section": false, "id": "4433"}, {"text": "We apply a threshold to remove points under 0.1%, that we consider as background noise.", "label": "Result", "bboxes": [{"left": 0.7105653594771242, "top": 0.40525883838383836, "width": 0.2112712418300653, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4190959595959596, "width": 0.3748022875816993, "height": 0.012579545454545482, "page": 7}], "section": "Data processing", "prob": 0.45926785469055176, "is_author_statement": true, "is_in_expected_section": false, "id": "4434"}, {"text": "We choose this approach for all these reasons.", "label": "Result", "bboxes": [{"left": 0.38734313725490194, "top": 0.3612449494949495, "width": 0.09794771241830069, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3750820707070707, "width": 0.19754248366013072, "height": 0.012579545454545482, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.4562433660030365, "is_author_statement": true, "is_in_expected_section": false, "id": "4435"}, {"text": "We then present our fabrication method and nish by presenting our hardware/software open toolkit that enables controlling the sensing layer, and demonstrate how we can detect the previously dened gestures.", "label": "Result", "bboxes": [{"left": 0.6069183006535949, "top": 0.32229419191919195, "width": 0.3149183006535947, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3361313131313131, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.3499671717171717, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3638042929292929, "width": 0.24870751633986932, "height": 0.012579545454545482, "page": 5}], "section": "SENSING INSPIRATION", "prob": 0.4449327886104584, "is_author_statement": true, "is_in_expected_section": false, "id": "4436"}, {"text": "The dermis is the middle layer.", "label": "Result", "bboxes": [{"left": 0.5254771241830065, "top": 0.1187929292929293, "width": 0.21506862745098043, "height": 0.012868686868686849, "page": 2}], "section": "Human skin overview", "prob": 0.4386870563030243, "is_author_statement": false, "is_in_expected_section": false, "id": "4437"}, {"text": "This is the case of Stretchis [103], which provides a fabrication process of a highly stretchable interface with stretch sensing.", "label": "Result", "bboxes": [{"left": 0.35379738562091506, "top": 0.09529419191919192, "width": 0.13148856209150322, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811274509803921, "top": 0.10913131313131313, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811274509803921, "top": 0.12296843434343435, "width": 0.32429738562091504, "height": 0.012579545454545454, "page": 2}], "section": "Flexible sensors", "prob": 0.4230710566043854, "is_author_statement": false, "is_in_expected_section": false, "id": "4438"}, {"text": "The electrodes are read 16 times per second and the data processing takes 4ms in average.", "label": "Result", "bboxes": [{"left": 0.5712124183006536, "top": 0.5373396464646465, "width": 0.3506160130718954, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5511767676767677, "width": 0.24701470588235297, "height": 0.012579545454545538, "page": 7}], "section": "Data processing", "prob": 0.4225578308105469, "is_author_statement": false, "is_in_expected_section": false, "id": "4439"}, {"text": "We use different silicone products from Smooth-On Inc to reproduce the skin properties listed above.", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.2942171717171717, "width": 0.3979379084967321, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811764705882352, "top": 0.3080542929292929, "width": 0.2819901960784314, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "prob": 0.4038446545600891, "is_author_statement": true, "is_in_expected_section": false, "id": "4440"}, {"text": "We created two interfaces with two different sizes and thicknesses (9cm x 12cm and 10cm x 14.5cm, thickness 7mm) that can be connected to a device via USB (Figure ?? -top).", "label": "Result", "bboxes": [{"left": 0.16275326797385622, "top": 0.6431224747474747, "width": 0.3225375816993463, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.656959595959596, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 8}, {"left": 0.0877124183006536, "top": 0.6707967171717171, "width": 0.3975718954248366, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.6845467171717171, "width": 0.05199509803921569, "height": 0.012666666666666715, "page": 8}], "section": "Skin-On Touchpads", "prob": 0.3980295956134796, "is_author_statement": true, "is_in_expected_section": false, "id": "4441"}, {"text": "1. Creating the top textured layer .", "label": "Result", "bboxes": [{"left": 0.5234428104575163, "top": 0.6554949494949496, "width": 0.24150000000000005, "height": 0.012667929292929148, "page": 6}], "section": "Skin-On Fabrication Process", "prob": 0.38447892665863037, "is_author_statement": false, "is_in_expected_section": false, "id": "4442"}, {"text": "We focus here on how embedding the sensing layer impacts the fabrication process.", "label": "Result", "bboxes": [{"left": 0.6120310457516339, "top": 0.6203623737373737, "width": 0.30980065359477127, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6341982323232324, "width": 0.24260294117647063, "height": 0.012579545454545538, "page": 6}], "section": "Skin-On Fabrication Process", "prob": 0.37983238697052, "is_author_statement": true, "is_in_expected_section": false, "id": "4443"}, {"text": "To inform our choices we have a series of requirements:", "label": "Result", "bboxes": [{"left": 0.8330245098039215, "top": 0.47256186868686867, "width": 0.08909803921568638, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.48639772727272723, "width": 0.26975490196078433, "height": 0.012579545454545482, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.373226135969162, "is_author_statement": true, "is_in_expected_section": false, "id": "4444"}, {"text": "It is divided into three primary layers [20]:", "label": "Result", "bboxes": [{"left": 0.4592042483660131, "top": 0.8430113636363636, "width": 0.026086601307189483, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.8568484848484849, "width": 0.24870751633986932, "height": 0.012579545454545427, "page": 2}], "section": "Human skin overview", "prob": 0.3709779381752014, "is_author_statement": false, "is_in_expected_section": false, "id": "4445"}, {"text": "To improve the visual appearance of the interface, the excess of silicone can be trimmed before being folded around the side of the hypodermis layer and glued with silicone glue (Figure 10-5).", "label": "Result", "bboxes": [{"left": 0.2616437908496732, "top": 0.17957323232323233, "width": 0.22364215686274508, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.19341035353535355, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.20724747474747474, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.08753267973856209, "top": 0.22108459595959595, "width": 0.22082026143790845, "height": 0.012579545454545482, "page": 7}], "section": "Skin-On Fabrication Process", "prob": 0.34471365809440613, "is_author_statement": false, "is_in_expected_section": false, "id": "4446"}, {"text": "We also fabricated a Skin-On wristband to alleviate the limited input and output capabilities of smartwatches [65] (Figure 1-c).", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.7222474747474747, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 8}, {"left": 0.08812091503267974, "top": 0.736084595959596, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 8}], "section": "Skin-On Touchpads", "prob": 0.3396967053413391, "is_author_statement": true, "is_in_expected_section": false, "id": "4447"}, {"text": "We used a Silver plated stretchable conductive fabric (stretch-width:65%, stretch-length:100%) to create a composite fabric + silicone", "label": "Result", "bboxes": [{"left": 0.3924934640522876, "top": 0.8643964646464646, "width": 0.09550816993464051, "height": 0.012579545454545538, "page": 6}, {"left": 0.0877124183006536, "top": 0.8782335858585859, "width": 0.39961437908496744, "height": 0.012579545454545427, "page": 6}, {"left": 0.08812745098039215, "top": 0.892070707070707, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "prob": 0.3361970782279968, "is_author_statement": true, "is_in_expected_section": false, "id": "4448"}, {"text": "Our work contributes towards this direction.", "label": "Result", "bboxes": [{"left": 0.8602418300653594, "top": 0.8434621212121213, "width": 0.062019607843137314, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8572992424242424, "width": 0.2206666666666668, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": 0.32529982924461365, "is_author_statement": true, "is_in_expected_section": true, "id": "4449"}, {"text": "Its rotation informs on the rotation and", "label": "Result", "bboxes": [{"left": 0.6615490196078432, "top": 0.892070707070707, "width": 0.26030392156862736, "height": 0.012578282828282772, "page": 7}], "section": "Data processing", "prob": 0.29956236481666565, "is_author_statement": false, "is_in_expected_section": false, "id": "4450"}, {"text": "Strain is a measure of deformation and is dependent to material thickness which, in skin, varies between individuals (age, gender) and body locations ( epidermis from 0.3mm to 1mm [37] dermis 0.9mm to 2.5mm [46, 73]; hypodermis from 1.9mm to 12mm [37]).", "label": "Result", "bboxes": [{"left": 0.7802581699346405, "top": 0.6282424242424243, "width": 0.1415816993464054, "height": 0.012579545454545427, "page": 2}, {"left": 0.5409428104575164, "top": 0.6420795454545455, "width": 0.38292320261437907, "height": 0.012579545454545538, "page": 2}, {"left": 0.5409428104575164, "top": 0.6559166666666667, "width": 0.38145751633986924, "height": 0.012579545454545427, "page": 2}, {"left": 0.5409428104575164, "top": 0.6694646464646464, "width": 0.37854411764705886, "height": 0.012868686868686918, "page": 2}, {"left": 0.5409428104575164, "top": 0.6833017676767676, "width": 0.3809019607843137, "height": 0.012868686868686918, "page": 2}, {"left": 0.5397303921568627, "top": 0.6974280303030304, "width": 0.08390196078431378, "height": 0.012579545454545427, "page": 2}], "section": "Human Skin properties", "prob": 0.29652008414268494, "is_author_statement": false, "is_in_expected_section": false, "id": "4451"}, {"text": "We present the implementation of our hardware and software toolkit and demonstrate its gesture recognition algorithm, which can detect gestures proposed in the previous section of this paper.", "label": "Result", "bboxes": [{"left": 0.3634983660130719, "top": 0.4683396464646465, "width": 0.1245016339869281, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4821767676767677, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4960138888888889, "width": 0.3998758169934641, "height": 0.012579545454545371, "page": 7}, {"left": 0.08811928104575163, "top": 0.5098510101010101, "width": 0.3346258169934641, "height": 0.012579545454545427, "page": 7}], "section": "Open-toolkit for touch and gestures detection", "prob": 0.2845837473869324, "is_author_statement": true, "is_in_expected_section": false, "id": "4452"}], "uist-7": [{"text": "In this paper, we investigate the use of hand-tracking to enable typing on any at surface at speeds comparable to typing on a physical keyboard.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.1359052287581699, "height": 0.012579545454545538, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4453"}, {"text": "Copyrights for third-party components of this work must be honored.", "label": "Author", "bboxes": [{"left": 0.17019281045751633, "top": 0.8590568181818182, "width": 0.31385457516339876, "height": 0.008805555555555511, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4454"}, {"text": "This work is licensed under a Creative Commons Attribution International 4.0 License.", "label": "Author", "bboxes": [{"left": 0.09126633986928104, "top": 0.8636830808080809, "width": 0.39849836601307187, "height": 0.008805555555555511, "page": 0}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4455"}, {"text": "Touch typing, by which we mean eyes-free input, on a at surface has been studied in the context of gesture keyboards.", "label": "Author", "bboxes": [{"left": 0.8823676470588235, "top": 0.15291161616161617, "width": 0.03946895424836594, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.16674873737373738, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.1805858585858586, "width": 0.3617565359477125, "height": 0.012579545454545454, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4456"}, {"text": "Our work is most similar to that of Zhu et al.", "label": "Author", "bboxes": [{"left": 0.8064869281045751, "top": 0.23593308080808081, "width": 0.11534967320261436, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.24977020202020203, "width": 0.1773496732026144, "height": 0.012579545454545454, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4457"}, {"text": "We show that by using hand pose rather than contact points and a richer motion model, we can achieve more accurate decoding.", "label": "Author", "bboxes": [{"left": 0.595109477124183, "top": 0.27744444444444444, "width": 0.32942973856209146, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.29128156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.3051186868686869, "width": 0.12204248366013071, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4458"}, {"text": "Similar to our proposed method, Velocitap [25]", "label": "Author", "bboxes": [{"left": 0.6088153594771242, "top": 0.892070707070707, "width": 0.3130228758169934, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4459"}, {"text": "Many other text entry systems have been proposed for AR/VR applications, and for brevity, we concentrate on those that use a QWERTY keyboard layout.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.34260984848484843, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.35644696969696965, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.37028409090909087, "width": 0.17770424836601306, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4460"}, {"text": "Our method is designed for typing on a at surface rather than in the air or against the opposite hand.", "label": "Author", "bboxes": [{"left": 0.690140522875817, "top": 0.6470239898989899, "width": 0.23169607843137263, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6608611111111111, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6746982323232323, "width": 0.03642973856209153, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4461"}, {"text": "We believe the on-surface domain, while slightly less accessible, better leverages existing typing skills, allowing our method to achieve speeds comparable to typing on a keyboard at a low error rate.", "label": "Author", "bboxes": [{"left": 0.5668382352941176, "top": 0.6746982323232323, "width": 0.354998366013072, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6885353535353536, "width": 0.39745915032679746, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7023724747474748, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7162083333333333, "width": 0.11938888888888899, "height": 0.012579545454545538, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4462"}, {"text": "We also use complete hand pose of all the fingers on both hands rather than the pose of a single finger.", "label": "Author", "bboxes": [{"left": 0.6491307189542483, "top": 0.7162083333333333, "width": 0.27270261437908505, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.7300454545454546, "width": 0.38883496732026157, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4463"}, {"text": "In our study we compromise between accessibility and speed by requiring users to type against a surface but eschewing the use of a physical keyboard or a touchpad.", "label": "Author", "bboxes": [{"left": 0.44496078431372543, "top": 0.21353661616161618, "width": 0.0406143790849674, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.3998709150326797, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.24121085858585856, "width": 0.39716830065359476, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.2512728758169934, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4464"}, {"text": "Our investigation caters particularly to the fastest typists touch typists , which we mean as those who type without the sense of sight to nd the keys.", "label": "Author", "bboxes": [{"left": 0.20481372549019608, "top": 0.2965593434343434, "width": 0.2804787581699347, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.3101073232323232, "width": 0.3971732026143791, "height": 0.012868686868686863, "page": 1}, {"left": 0.08753267973856209, "top": 0.32423358585858586, "width": 0.27122875816993464, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4465"}, {"text": "Instead of requiring users to make precise contacts on a xed keyboard layout, we investigate using a motion model to recognize nger trajectories, and we further explore how statistical decoding techniques affect performance.", "label": "Author", "bboxes": [{"left": 0.25477124183006533, "top": 0.3519078282828283, "width": 0.23051633986928105, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811437908496732, "top": 0.3657449494949495, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811437908496732, "top": 0.37958207070707073, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.08753267973856209, "top": 0.3934179292929293, "width": 0.39776797385620916, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4072550505050505, "width": 0.08632679738562092, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4466"}, {"text": "We show that these users can transfer their existing skills typing on a keyboard to typing on a at surface.", "label": "Author", "bboxes": [{"left": 0.3197581699346405, "top": 0.15064141414141416, "width": 0.16553594771241836, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.10742320261437908, "height": 0.012579545454545454, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4467"}, {"text": "We are inspired by the work of Dudley and colleagues [4] that shows the high potential of human typing efciency and error rate given a text decoding oracle with knowledge of the text being typed.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.4286401515151515, "width": 0.3979428104575164, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4424772727272727, "width": 0.3974493464052288, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.4563131313131313, "width": 0.3998839869281046, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4701502525252525, "width": 0.06425163398692811, "height": 0.012579545454545482, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4468"}, {"text": "In this work, we take the fi steps to reducing this oracle to practice by building a neural model of text decoding that combines motion modeling of ngers and a state-of-theart language model.", "label": "Author", "bboxes": [{"left": 0.15742647058823528, "top": 0.4701502525252525, "width": 0.3278709150326798, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4839873737373737, "width": 0.3971797385620915, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.49782449494949493, "width": 0.39987418300653593, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.5116616161616161, "width": 0.12940849673202615, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4469"}, {"text": "Notably, our model is the fi technique to our knowledge that converts skeletal hand motion directly into text.", "label": "Author", "bboxes": [{"left": 0.22256699346405232, "top": 0.5116616161616161, "width": 0.2627271241830066, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5254987373737373, "width": 0.39773692810457517, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5393358585858585, "width": 0.05965849673202615, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4470"}, {"text": "Instead of relying on surface contact information (e.g., from capacitive touch), we investigate using the output of a marker-based hand-tracking system [12].", "label": "Author", "bboxes": [{"left": 0.1556486928104575, "top": 0.5393358585858585, "width": 0.32964379084967327, "height": 0.012579545454545427, "page": 1}, {"left": 0.08758169934640524, "top": 0.5531729797979797, "width": 0.3977156862745098, "height": 0.012579545454545538, "page": 1}, {"left": 0.08812418300653595, "top": 0.567010101010101, "width": 0.2723218954248366, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4471"}, {"text": "While marker-based hand-tracking is still an optimistic approximation of the delity achievable from an AR/VR headset, our experiments shed light on the potential of hand-tracking-based text decoding.", "label": "Author", "bboxes": [{"left": 0.44471241830065356, "top": 0.5946830808080807, "width": 0.04058006535947717, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.608520202020202, "width": 0.39988235294117647, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.6223573232323232, "width": 0.3974558823529412, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6361944444444444, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6500315656565657, "width": 0.09354738562091507, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4472"}, {"text": "Specically, we make the following contributions:", "label": "Author", "bboxes": [{"left": 0.18670424836601307, "top": 0.6500315656565657, "width": 0.30129084967320263, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6638686868686868, "width": 0.03618790849673202, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4473"}, {"text": "To evaluate the feasibility of using hand-tracking to enable touch typing on virtual keyboards, we fi collect a dataset of skeletal hand-tracking data from touch-typists transcribing short phrases while typing on a at surface.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.30997474747474746, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811274509803921, "top": 0.3238118686868687, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3376489898989899, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3514861111111111, "width": 0.27779411764705886, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4474"}, {"text": "Next, we design a system for decoding the skeletal hand-tracking data into the text the typists intended to type.", "label": "Author", "bboxes": [{"left": 0.3709705882352941, "top": 0.3514861111111111, "width": 0.11432189542483662, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3653219696969697, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3791590909090909, "width": 0.21343137254901967, "height": 0.012579545454545482, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4475"}, {"text": "Finally we measure typing speed and error rates of our system against typing on a physical keyboard and contact-based statistical decoding methods.", "label": "Author", "bboxes": [{"left": 0.3077434640522876, "top": 0.3791590909090909, "width": 0.1775490196078432, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3929962121212121, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4068333333333333, "width": 0.37393954248366007, "height": 0.012579545454545482, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4476"}, {"text": "We apply this same general framework, substituting recurrent neural networks with a TCN-based motion model, to decode text entry via typing.", "label": "Author", "bboxes": [{"left": 0.20009313725490194, "top": 0.24236363636363636, "width": 0.28789705882352945, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.25620075757575755, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.27003787878787877, "width": 0.25114869281045754, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4477"}, {"text": "While we are targeting a different sensing modality (hand-tracking instead of capacitive touchpads), we can use touchpads as a baseline for our work.", "label": "Author", "bboxes": [{"left": 0.16333823529411765, "top": 0.6508674242424242, "width": 0.32252777777777786, "height": 0.012579545454545538, "page": 2}, {"left": 0.08758169934640524, "top": 0.6647045454545455, "width": 0.39770424836601304, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811274509803921, "top": 0.6785416666666666, "width": 0.25703431372549024, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4478"}, {"text": "We fi investigate touch typists ability to transfer their physical keyboard typing skills to typing on virtual keyboards imposed on at surfaces, and we then compare the performance of decoding text from skeletal hand-tracking data to decoding text from 2D surface contact points from a touchpad.", "label": "Author", "bboxes": [{"left": 0.3552418300653595, "top": 0.6785416666666666, "width": 0.13004411764705887, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.6923787878787879, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.706215909090909, "width": 0.39920261437908494, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7200530303030304, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7338901515151515, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7477272727272728, "width": 0.20571405228758172, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4479"}, {"text": "We use a temporal neural network as a motion model.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.30038383838383836, "width": 0.3616732026143793, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4480"}, {"text": "uses beam search with a language model to decode whole sentences at a time, although Velocitap uses contact as the input modality while our method uses hand-tracking.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.0814570707070707, "width": 0.3971732026143791, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.39717320261437916, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.34499509803921563, "height": 0.012579545454545454, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4481"}, {"text": "To collect a dataset of skeletal hand-tracking data, we make use of a high quality marker-based hand-tracking system [12] which is not subject to the current tracking limitations of consumer head mounted hand-trackers.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.4282184343434343, "width": 0.39767156862745096, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.44205555555555553, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08753267973856209, "top": 0.4558914141414141, "width": 0.4004640522875817, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4697285353535353, "width": 0.23133496732026143, "height": 0.012579545454545482, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4482"}, {"text": "While using a marker-based tracking system may introduce a gap in tracking quality compared to what is achievable on an AR/VR headset, we can study the potential of hand-tracking applied to this problem.", "label": "Author", "bboxes": [{"left": 0.39330718954248367, "top": 0.49740277777777775, "width": 0.09198039215686271, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.511239898989899, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5250770202020202, "width": 0.3992156862745098, "height": 0.012579545454545538, "page": 2}, {"left": 0.08753267973856209, "top": 0.5389141414141414, "width": 0.3977549019607843, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5527512626262626, "width": 0.058326797385620896, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4483"}, {"text": "We chose these features because they are somewhat invariant to differences in hand scale across people.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.702915404040404, "width": 0.39794281045751645, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.7167525252525253, "width": 0.27398856209150324, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4484"}, {"text": "For temporal modeling, we opt to use a temporal convolutional network (TCN) rather than a recurrent model because a xed window of hand motion data is typically sufcient to make predictions about key presses.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.404790404040404, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4186275252525252, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 2}, {"left": 0.5240784313725491, "top": 0.43246464646464644, "width": 0.39775490196078434, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.44630176767676766, "width": 0.1978415032679739, "height": 0.012579545454545482, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4485"}, {"text": "Longer term context is useful for prediction in the context of language, but since our system design separates motion modeling from language modeling, a TCN works well for the former.", "label": "Author", "bboxes": [{"left": 0.7275669934640523, "top": 0.44630176767676766, "width": 0.19426960784313718, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4601388888888889, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.4739760101010101, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 2}, {"left": 0.5241601307189543, "top": 0.4878131313131313, "width": 0.20243137254901955, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4486"}, {"text": "We follow the TCN architecture proposed by Bai et al.", "label": "Author", "bboxes": [{"left": 0.6418725490196079, "top": 0.5293232323232323, "width": 0.2799640522875817, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.5431603535353535, "width": 0.07833169934640538, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4487"}, {"text": "We use three layers of residual blocks with 64, 64, and 32 hidden units respectively, a kernel size of 2 and a dilation factor of 3.", "label": "Author", "bboxes": [{"left": 0.875421568627451, "top": 0.5569974747474747, "width": 0.046419934640522964, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.570834595959596, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5846717171717172, "width": 0.36329738562091507, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4488"}, {"text": "The problem of generating text from hand motion has strong analogs to automatic speech recognition (ASR), and we use ASR as motivation to design a multi-component system with the following three pieces.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.7876641414141414, "width": 0.39767320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8015012626262626, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08753267973856209, "top": 0.8153383838383839, "width": 0.39776307189542476, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8291742424242424, "width": 0.17229411764705882, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4489"}, {"text": "We apply a beam search decoder to resolve these ambiguities using the language model as a prior.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.24098863636363635, "width": 0.39793464052287597, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.2548257575757576, "width": 0.2346584967320262, "height": 0.012579545454545427, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4490"}, {"text": "For decoding hand motion into text, we make no such concessions and investigate the accuracy we can achieve using practical language models and statistical decoding strategies presented in this work.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.574135101010101, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5879722222222222, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.6018093434343434, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6156464646464647, "width": 0.14813725490196078, "height": 0.012579545454545538, "page": 2}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4491"}, {"text": "In our study we collected data by having participants complete a text transcription task for short phrases.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.31209974747474745, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.32593686868686866, "width": 0.27559803921568626, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4492"}, {"text": "To evaluate handtracking as an input modality compared to contact data from touchpads or to physical keyboards, we asked participants to type each phrase on either a physical keyboard or on a Sensel pressure sensitive touchpad with a printed 2D keyboard layout afxed (See Figure 3).", "label": "Author", "bboxes": [{"left": 0.8062875816993463, "top": 0.32593686868686866, "width": 0.11825653594771246, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.3397739898989899, "width": 0.39717647058823535, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.3536111111111111, "width": 0.39718137254901953, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.3674482323232323, "width": 0.39717156862745107, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.38128535353535353, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.39512247474747475, "width": 0.1492892156862745, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4493"}, {"text": "In both cases, we also captured their hand motion using a marker-based hand-tracking system [12].", "label": "Author", "bboxes": [{"left": 0.6797973856209151, "top": 0.39512247474747475, "width": 0.2423218954248365, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.40895959595959597, "width": 0.39993464052287586, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4494"}, {"text": "We use a beam search implementation with beam compaction which maximizes the objective W = argmax W p total .", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.7520138888888889, "width": 0.3979362745098039, "height": 0.012579545454545427, "page": 3}, {"left": 0.08753267973856209, "top": 0.7655618686868687, "width": 0.3366797385620915, "height": 0.015136363636363614, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4495"}, {"text": "After decoding, we compute the uncorrected error rate UER ( W , W  ) as the Levenstein edit distance between the decoded and prompted strings divided by the number of characters of the longer of the two strings.", "label": "Author", "bboxes": [{"left": 0.4292777777777778, "top": 0.7658510101010101, "width": 0.05872222222222223, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.7788712121212121, "width": 0.3984738562091504, "height": 0.015082070707070683, "page": 3}, {"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.15745915032679741, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4496"}, {"text": "For all of our results, we use B = 100 beams.", "label": "Author", "bboxes": [{"left": 0.25039542483660127, "top": 0.8225959595959595, "width": 0.23488888888888898, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.04657516339869282, "height": 0.012579545454545538, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4497"}, {"text": "For our language model, we use a Transformer [24] model similar to the small-two model described in [15] with 4.76M parameters.", "label": "Author", "bboxes": [{"left": 0.14067156862745098, "top": 0.8367222222222221, "width": 0.3446209150326798, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08754901960784313, "top": 0.8643964646464646, "width": 0.12107516339869283, "height": 0.012579545454545538, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4498"}, {"text": "Since our goal is to produce the text people intended to type, we filtered samples where the user believed they made a mistake since we dont know what text the user expects to be produced in those cases.", "label": "Author", "bboxes": [{"left": 0.8854166666666666, "top": 0.5762613636363636, "width": 0.03642483660130724, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5900984848484848, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.6039343434343434, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6177714646464646, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.6316085858585858, "width": 0.09358660130718965, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4499"}, {"text": "We recruited 20 participants who passed a pre-screening questionnaire designed to select for experienced typists.", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.43034343434343436, "width": 0.40065032679738566, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4441805555555556, "width": 0.34616503267973864, "height": 0.012579545454545482, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4500"}, {"text": "We can do better than greedy decoding by using a prex beam search decoder [13].", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.5312941919191919, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5451313131313131, "width": 0.136171568627451, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4501"}, {"text": "Furthermore, we can incorporate a joint probability from both the likelihood of the beam according to the motion model p ( W ; V ) as well as the likelihood of the compacted text according to a language model p lm ( W ) , i.e.,", "label": "Author", "bboxes": [{"left": 0.17227287581699346, "top": 0.5866426767676768, "width": 0.3157287581699346, "height": 0.012579545454545538, "page": 3}, {"left": 0.08812254901960784, "top": 0.600479797979798, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 3}, {"left": 0.08812254901960784, "top": 0.6140277777777777, "width": 0.39717647058823524, "height": 0.01286868686868703, "page": 3}, {"left": 0.08812418300653595, "top": 0.627864898989899, "width": 0.3992042483660131, "height": 0.014029040404040516, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4502"}, {"text": "As has been shown in the speech recognition community [9], beam search decoding is also well-suited to the CTC loss with which we train the network.", "label": "Author", "bboxes": [{"left": 0.33371241830065357, "top": 0.6891186868686869, "width": 0.1515800653594772, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.702955808080808, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7167929292929294, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.7306300505050505, "width": 0.057251633986928105, "height": 0.012579545454545427, "page": 3}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4503"}, {"text": "Because contact-based text decoders are sensitive to accidental or missed touches, we took care to clean the contact-based training data from the touchpad captures.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.3179431818181818, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.331780303030303, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.3456174242424242, "width": 0.28328431372549023, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4504"}, {"text": "We ltered out captures where the number of touchpad contact events did not agree with the number of characters in the prompted phrase.", "label": "Author", "bboxes": [{"left": 0.3828611111111111, "top": 0.3456174242424242, "width": 0.10243137254901968, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.3594545454545454, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.37329166666666663, "width": 0.3790343137254903, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4505"}, {"text": "In each of the remaining samples we expect that the N th contact event corresponds to the N th character in the prompted phrase.", "label": "Author", "bboxes": [{"left": 0.47200326797385617, "top": 0.37329166666666663, "width": 0.013289215686274558, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.38512752525252525, "width": 0.3971748366013072, "height": 0.014580808080808083, "page": 4}, {"left": 0.08811928104575163, "top": 0.3989633838383838, "width": 0.4000228758169935, "height": 0.014582070707070738, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4506"}, {"text": "To lter samples where both an extra and a missing contact event occurred (leading to an agreement on counts but an error in contact-key sequence alignment), we build up a distribution for each key of how many times each nger hit that key.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.4148030303030303, "width": 0.39767156862745096, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4286401515151515, "width": 0.39745915032679735, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4424772727272727, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.4563131313131313, "width": 0.37041176470588233, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4507"}, {"text": "We then lter samples where multiple keys in the sample were struck by a nger which hits that key in less than 1% of cases overall.", "label": "Author", "bboxes": [{"left": 0.46358496732026144, "top": 0.4563131313131313, "width": 0.021709150326797377, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4701502525252525, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4839873737373737, "width": 0.39717156862745095, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.49782449494949493, "width": 0.048787581699346425, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4508"}, {"text": "Our fi evaluation is to determine if participants are able to transfer their existing skills typing on a physical keyboard to typing on a at surface (with the help of our proposed decoder).", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.531050505050505, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.5448876262626262, "width": 0.39718137254901953, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.5587247474747474, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4509"}, {"text": "Participants of our user study type at different speeds, so we compare the relative speed of typing on a at surface versus a physical keyboard.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.5725618686868686, "width": 0.39716830065359465, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.5863977272727273, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6002348484848485, "width": 0.13854575163398697, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4510"}, {"text": "Because our participants were tasked with typing test-set phrases on both a physical keyboard and on a touchpad, we can make this comparison directly.", "label": "Author", "bboxes": [{"left": 0.6716111111111112, "top": 0.6002348484848485, "width": 0.2502254901960784, "height": 0.012579545454545427, "page": 4}, {"left": 0.5240784313725491, "top": 0.6140719696969698, "width": 0.39775490196078434, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6279090909090909, "width": 0.3662892156862746, "height": 0.012579545454545538, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4511"}, {"text": "We found that our expert typists generally typed efciently both on physical keyboards (median: 75 WPM, mean: 74 WPM) as well as on surfaces (median: 69 WPM, mean: 73 WPM).", "label": "Result", "bboxes": [{"left": 0.9000849673202614, "top": 0.6279090909090909, "width": 0.021751633986928143, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.6417462121212121, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6555833333333333, "width": 0.39826960784313725, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6694204545454546, "width": 0.4000130718954249, "height": 0.012579545454545427, "page": 4}], "section": "Physical keyboard versus virtual keyboard", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4512"}, {"text": "We found that our expert typists generally typed efciently both on physical keyboards (median: 75 WPM, mean: 74 WPM) as well as on surfaces (median: 69 WPM, mean: 73 WPM).", "label": "Author", "bboxes": [{"left": 0.9000849673202614, "top": 0.6279090909090909, "width": 0.021751633986928143, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.6417462121212121, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6555833333333333, "width": 0.39826960784313725, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6694204545454546, "width": 0.4000130718954249, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4513"}, {"text": "The testing dataset used for all decoders is the same, except that we only lter samples where the user felt they made a mistake, which means that the number of characters typed might not equal the number of contact events.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.5959419191919192, "width": 0.39767156862745096, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811274509803921, "top": 0.6097790404040404, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811274509803921, "top": 0.6236161616161616, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811274509803921, "top": 0.6374532828282828, "width": 0.3117941176470588, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4514"}, {"text": "We wish to quantify our ability to produce the text people intended to type, which should include all of the samples where participants felt they typed the phrase correctly.", "label": "Author", "bboxes": [{"left": 0.40865686274509805, "top": 0.6374532828282828, "width": 0.07662908496732024, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.6512891414141414, "width": 0.3992140522875817, "height": 0.012579545454545427, "page": 4}, {"left": 0.08752941176470588, "top": 0.6651262626262626, "width": 0.3977630718954249, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.6789633838383838, "width": 0.20348529411764704, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4515"}, {"text": "We present two evaluations of our proposed text entry method using the data collected from our user study.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.8505593434343435, "width": 0.39794281045751634, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.2939950980392157, "height": 0.012579545454545538, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4516"}, {"text": "Our result reduces this oracle to practice by substituting it with our proposed neural text decoder.", "label": "Author", "bboxes": [{"left": 0.5739068627450981, "top": 0.8782335858585859, "width": 0.35062745098039216, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.2882156862745099, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4517"}, {"text": "Participants typed on physical keyboards with a mean UER of 1.72% (median: 1.19%) compared to a mean UER of 2.38% (median: 1.77%) when they typed on at surfaces with our decoder.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.3971830065359476, "height": 0.012579545454545427, "page": 4}, {"left": 0.5234428104575163, "top": 0.7461527777777778, "width": 0.39974673202614375, "height": 0.012579545454545538, "page": 4}, {"left": 0.524124183006536, "top": 0.759989898989899, "width": 0.39798692810457514, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.05453267973856213, "height": 0.012579545454545427, "page": 4}], "section": "Physical keyboard versus virtual keyboard", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4518"}, {"text": "Participants typed on physical keyboards with a mean UER of 1.72% (median: 1.19%) compared to a mean UER of 2.38% (median: 1.77%) when they typed on at surfaces with our decoder.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.3971830065359476, "height": 0.012579545454545427, "page": 4}, {"left": 0.5234428104575163, "top": 0.7461527777777778, "width": 0.39974673202614375, "height": 0.012579545454545538, "page": 4}, {"left": 0.524124183006536, "top": 0.759989898989899, "width": 0.39798692810457514, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.05453267973856213, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4519"}, {"text": "While the contact-based baselines can only be trained with known key/contact-point correspondences, our motion model is trained using CTC loss, which allows for sequence level labels and can deduce the frame-level alignment of the labels.", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.7003484848484849, "width": 0.3979379084967321, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7141856060606061, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7280214646464646, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7418585858585859, "width": 0.4000261437908497, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4520"}, {"text": "Thus our motion model was able to be trained on the original training dataset, only ltering samples where users felt they made a mistake.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.755695707070707, "width": 0.39768137254901953, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7695328282828283, "width": 0.3977385620915033, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7833699494949494, "width": 0.1037843137254902, "height": 0.012579545454545538, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4521"}, {"text": "We note that this less ltered training dataset more closely mirrors the testing dataset.", "label": "Author", "bboxes": [{"left": 0.1969591503267974, "top": 0.7833699494949494, "width": 0.2883382352941176, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7972070707070708, "width": 0.26042810457516347, "height": 0.012579545454545427, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4522"}, {"text": "For our motion model we trained person-specic models that captured each users typing style from the training set portion of the data.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.39182449494949495, "width": 0.39717647058823535, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.40566161616161617, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4194987373737374, "width": 0.0763627450980392, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4523"}, {"text": "All of our results were computed over the test set portion of the data.", "label": "Author", "bboxes": [{"left": 0.6094950980392158, "top": 0.4194987373737374, "width": 0.3123415032679737, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4333358585858586, "width": 0.1479133986928105, "height": 0.012579545454545482, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4524"}, {"text": "The resulting touchpad training dataset consists of samples where we have", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.5192095959595959, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 4}, {"left": 0.08753104575163399, "top": 0.5330467171717171, "width": 0.10017320261437908, "height": 0.012579545454545538, "page": 4}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4525"}, {"text": "Our system differs from prior work in several ways.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.4223914141414141, "width": 0.3342467320261438, "height": 0.012579545454545482, "page": 5}], "section": "Comparison of Input Features and Motion Models", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4526"}, {"text": "Our system differs from prior work in several ways.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.4223914141414141, "width": 0.3342467320261438, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4527"}, {"text": "We use a higher capacity neural motion model to decode a richer input signal (i.e., with nger identity and trajectory information) into text.", "label": "Novelty", "bboxes": [{"left": 0.42742647058823524, "top": 0.4223914141414141, "width": 0.057872549019607866, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.43622853535353534, "width": 0.3971764705882353, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.45006565656565656, "width": 0.3982696078431373, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4639027777777778, "width": 0.05964215686274511, "height": 0.012579545454545482, "page": 5}], "section": "Comparison of Input Features and Motion Models", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4528"}, {"text": "We use a higher capacity neural motion model to decode a richer input signal (i.e., with nger identity and trajectory information) into text.", "label": "Author", "bboxes": [{"left": 0.42742647058823524, "top": 0.4223914141414141, "width": 0.057872549019607866, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.43622853535353534, "width": 0.3971764705882353, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.45006565656565656, "width": 0.3982696078431373, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4639027777777778, "width": 0.05964215686274511, "height": 0.012579545454545482, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4529"}, {"text": "Our decoding is also continuous (at 60Hz) rather than constrained to producing characters at discrete contact events.", "label": "Novelty", "bboxes": [{"left": 0.15554901960784315, "top": 0.4639027777777778, "width": 0.3300261437908497, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.477739898989899, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4915770202020202, "width": 0.0459281045751634, "height": 0.012579545454545427, "page": 5}], "section": "Comparison of Input Features and Motion Models", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4530"}, {"text": "Our decoding is also continuous (at 60Hz) rather than constrained to producing characters at discrete contact events.", "label": "Author", "bboxes": [{"left": 0.15554901960784315, "top": 0.4639027777777778, "width": 0.3300261437908497, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.477739898989899, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.4915770202020202, "width": 0.0459281045751634, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4531"}, {"text": "We attempt to tease apart the contributions of these differences with two experiments.", "label": "Author", "bboxes": [{"left": 0.1410702614379085, "top": 0.4915770202020202, "width": 0.3442222222222223, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5054128787878788, "width": 0.2207075163398693, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4532"}, {"text": "In our second experiment we investigate the contribution of continuous motion decoding versus discrete contact-based decoding.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.6312045454545454, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6450416666666666, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6588787878787878, "width": 0.0640669934640523, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4533"}, {"text": "We observed that many mistakes made by contactbased decoding were due to its discrete nature: Characters are generated if and only if there is a corresponding contact event, which means that spurious or omitted contacts necessarily result in decoding errors.", "label": "Result", "bboxes": [{"left": 0.15722385620915033, "top": 0.6588787878787878, "width": 0.33076960784313725, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6727146464646464, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6865517676767677, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7003888888888888, "width": 0.3998807189542484, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.7142260101010102, "width": 0.2084166666666667, "height": 0.012579545454545427, "page": 5}], "section": "Comparison of Input Features and Motion Models", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4534"}, {"text": "We observed that many mistakes made by contactbased decoding were due to its discrete nature: Characters are generated if and only if there is a corresponding contact event, which means that spurious or omitted contacts necessarily result in decoding errors.", "label": "Author", "bboxes": [{"left": 0.15722385620915033, "top": 0.6588787878787878, "width": 0.33076960784313725, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6727146464646464, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6865517676767677, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7003888888888888, "width": 0.3998807189542484, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.7142260101010102, "width": 0.2084166666666667, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4535"}, {"text": "We attempt to ablate away the continuous nature of hand motion decoding by creating a ltered test dataset containing only samples with matched contacts and keys.", "label": "Author", "bboxes": [{"left": 0.3091388888888889, "top": 0.7142260101010102, "width": 0.17671895424836603, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7280631313131313, "width": 0.3971748366013072, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.7419002525252526, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7557373737373737, "width": 0.09535130718954248, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4536"}, {"text": "We repeat our evaluation of the two baselines on this corresponded contacts dataset, isolating the value of trajectories from the value of continuous decoding and nger identity information.", "label": "Author", "bboxes": [{"left": 0.1885359477124183, "top": 0.7557373737373737, "width": 0.29675816993464055, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.769574494949495, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7834116161616161, "width": 0.39744607843137253, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7972487373737375, "width": 0.13475490196078432, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4537"}, {"text": "In our fi experiment we study the contribution of nger identity information.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.5267979797979798, "width": 0.3974558823529412, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.540635101010101, "width": 0.138047385620915, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4538"}, {"text": "To do so we create two contact-based baselines for comparison, one that uses only anonymous, discrete contact events, and another that adds nger-identity information.", "label": "Author", "bboxes": [{"left": 0.2332565359477124, "top": 0.540635101010101, "width": 0.2520359477124184, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.5544722222222223, "width": 0.39988235294117647, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5683093434343435, "width": 0.39988235294117647, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.582145202020202, "width": 0.0696388888888889, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4539"}, {"text": "We compare these baselines to each other and to our hand motion based decoding to isolate the impact of nger identity information.", "label": "Author", "bboxes": [{"left": 0.1632532679738562, "top": 0.582145202020202, "width": 0.3220392156862746, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5959823232323233, "width": 0.39745915032679735, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6098194444444444, "width": 0.13475490196078432, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4540"}, {"text": "Using the cleaned training dataset, we obtain a sequence of keys pressed { w  i } and a sequence of 2D contact points from the touchpad { x  i } (See Figure 5).", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.781084595959596, "width": 0.39717320261437916, "height": 0.013835858585858563, "page": 5}, {"left": 0.5246633986928104, "top": 0.7949217171717171, "width": 0.23283986928104572, "height": 0.013835858585858563, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4541"}, {"text": "Since these sequences are in correspondence, we can bucket 2D contact points for each key on the keyboard.", "label": "Author", "bboxes": [{"left": 0.7702401960784314, "top": 0.7952108585858586, "width": 0.15159640522875806, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39745588235294127, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.1799232026143791, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4542"}, {"text": "For each key k we t a bivariate Gaussian g k ( x ) = N ( x ;  k ,  k ) from the collection of 2D contacts.", "label": "Author", "bboxes": [{"left": 0.715047385620915, "top": 0.8225959595959595, "width": 0.20949673202614394, "height": 0.012868686868686918, "page": 5}, {"left": 0.5246633986928104, "top": 0.8364330808080809, "width": 0.39718137254901964, "height": 0.014030303030302949, "page": 5}, {"left": 0.5246699346405229, "top": 0.8505593434343435, "width": 0.05856372549019606, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4543"}, {"text": "For our fi baseline, Gaussian contact decoding , we evaluate performance using the spatial modeling approach described by Zhu et al.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.8364330808080809, "width": 0.3971732026143791, "height": 0.012868686868686807, "page": 5}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.06075000000000001, "height": 0.012579545454545538, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4544"}, {"text": "This model lacks the continuous text decoding capabilities of our motion model, but rather has to decode text only at discrete contact events.", "label": "Author", "bboxes": [{"left": 0.18848202614379084, "top": 0.8643964646464646, "width": 0.29681045751633994, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.22715359477124186, "height": 0.012579545454545427, "page": 5}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4545"}, {"text": "The previous experiments show that our continuous hand motion decoding method is more accurate than decoding of discrete contacts with either a simple Gaussian spatial model or a richer model with nger identity information.", "label": "Author", "bboxes": [{"left": 0.5241601307189543, "top": 0.13676767676767676, "width": 0.4003839869281046, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.15060479797979798, "width": 0.399874183006536, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.1644419191919192, "width": 0.39745915032679746, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.17827777777777779, "width": 0.3082058823529412, "height": 0.012579545454545454, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4546"}, {"text": "When we examined the results qualitatively, we noted several differences between the contact-based decoding approach and our handtracking-based approach, which we illustrate in Table 2.", "label": "Author", "bboxes": [{"left": 0.837919934640523, "top": 0.17827777777777779, "width": 0.08662581699346406, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.192114898989899, "width": 0.39717810457516345, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.20595202020202022, "width": 0.3998823529411766, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.21978914141414144, "width": 0.3558856209150326, "height": 0.012579545454545454, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4547"}, {"text": "Figure 1(b) shows an example where our motion model is able to learn to detect the signature trajectory of reaching for a particular key.", "label": "Author", "bboxes": [{"left": 0.8470980392156863, "top": 0.26130050505050506, "width": 0.07583169934640521, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.2751376262626263, "width": 0.39717156862745095, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.2889747474747475, "width": 0.40002614379084966, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4548"}, {"text": "In order to disentangle the benet of continuous motion decoding versus discrete contact decoding, we lter our test dataset similarly to our training set to contain only samples where we know the correspondences between contact events and ground truth character labels.In this ltered corresponded contacts dataset, there are no samples with missing or spurious contacts.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.5808093434343434, "width": 0.39988398692810456, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5946464646464646, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6084835858585859, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6223207070707071, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6361578282828283, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6499949494949495, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6638308080808081, "width": 0.03445098039215688, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4549"}, {"text": "At fi, we assumed that the discrete nature of contacts was an advantage for contact decoding.", "label": "Author", "bboxes": [{"left": 0.5240784313725491, "top": 0.33803282828282827, "width": 0.3977630718954249, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.3518699494949495, "width": 0.20806209150326793, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4550"}, {"text": "Explicit contact information is a strong signal for a key-press, whereas our motion model has to learn to detect the signature trajectory of a nger as it presses a key.", "label": "Author", "bboxes": [{"left": 0.7377712418300654, "top": 0.3518699494949495, "width": 0.18406209150326802, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.3657070707070707, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.3795441919191919, "width": 0.39716830065359476, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.39338131313131314, "width": 0.09042647058823527, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4551"}, {"text": "However, when using a touchpad, we assume no ambiguity in whether a contact event occurred, only 2D spatial ambiguity about which key was pressed.", "label": "Author", "bboxes": [{"left": 0.6205702614379085, "top": 0.39338131313131314, "width": 0.301266339869281, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.40721843434343435, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.42105555555555557, "width": 0.31675980392156866, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4552"}, {"text": "On the other hand, our motion model generates a probability distribution of which key was pressed (including no-key) at every frame of the tracking, which provides the decoder exibility to insert or remove key-presses to accommodate the language model prior (Figure 7).", "label": "Author", "bboxes": [{"left": 0.6378709150326798, "top": 0.5040770202020202, "width": 0.286673202614379, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5179141414141414, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 6}, {"left": 0.5241274509803922, "top": 0.5317512626262626, "width": 0.40041666666666675, "height": 0.012579545454545427, "page": 6}, {"left": 0.5242565359477125, "top": 0.5455883838383838, "width": 0.3975751633986927, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5594242424242425, "width": 0.33138725490196086, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4553"}, {"text": "Repeating the evaluation of the previous two baselines on this ltered corresponded contacts dataset, we found a substantial drop in the mean UER for all methods (Table 1), suggesting that removing spurious or missing key-presses makes for an easier dataset.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.6990530303030302, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.7128901515151516, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7267272727272727, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.740564393939394, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7544002525252526, "width": 0.0892271241830066, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4554"}, {"text": "For our second baseline, Per Finger Gaussian contact decoding , we specically investigate if augmenting the spatial model input with nger identity information improves performance of the contact-based model.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.1163888888888889, "width": 0.39988725490196086, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.1302260101010101, "width": 0.3971732026143791, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.14435227272727272, "width": 0.39718137254901964, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.15818939393939394, "width": 0.18443954248366018, "height": 0.012579545454545454, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4555"}, {"text": "As shown in Figure 6, we found our expert typist participants tended to have consistent key-nger correspondences.", "label": "Author", "bboxes": [{"left": 0.2581339869281046, "top": 0.18586363636363637, "width": 0.2274444444444444, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.1997007575757576, "width": 0.3974428104575164, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.21353661616161618, "width": 0.11198856209150325, "height": 0.012579545454545454, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4556"}, {"text": "Motivated by this nding, we hypothesized that providing a richer input feature that includes which nger pressed a key can help a model disambiguate which key was pressed.", "label": "Author", "bboxes": [{"left": 0.20522712418300654, "top": 0.21353661616161618, "width": 0.2800686274509803, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.3974477124183006, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.24121085858585856, "width": 0.3971764705882353, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.05288888888888889, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4557"}, {"text": "When we added the beam search language model decoder, however, we found that both Gaussian and Per Finger Gaussian contact-based methods performed comparably with a mean UERs of 5.66% (median 2.70%) and 4.76% (median 2.64%) respectively, while our motion model approach outperformed both with a mean UER of 2.38% (median 1.77%).", "label": "Author", "bboxes": [{"left": 0.31051633986928107, "top": 0.512919191919192, "width": 0.17477614379084966, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5267563131313131, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5405934343434343, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.5544305555555555, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5682676767676768, "width": 0.3974558823529412, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.582104797979798, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5959419191919192, "width": 0.16862581699346407, "height": 0.012579545454545538, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4558"}, {"text": "To use nger identity information, we make two changes to our Gaussian contact decoding baseline.", "label": "Author", "bboxes": [{"left": 0.08761437908496732, "top": 0.2764330808080808, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811274509803921, "top": 0.29027020202020204, "width": 0.2936797385620915, "height": 0.012579545454545427, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4559"}, {"text": "Fi, we nd the closest ngertip f for each contact using hand-tracking data and then bucket by both key k and nger f to t 2D Gaussians h k , f ( x ) to these subsets of contact points.", "label": "Author", "bboxes": [{"left": 0.3928333333333333, "top": 0.29027020202020204, "width": 0.09245261437908497, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811274509803921, "top": 0.30381691919191917, "width": 0.3971862745098039, "height": 0.012869949494949517, "page": 6}, {"left": 0.08812581699346406, "top": 0.3176540404040404, "width": 0.39716830065359476, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.3314911616161616, "width": 0.3583349673202615, "height": 0.014039141414141476, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4560"}, {"text": "Second, at inference time, we use the known ngertip f to compute the distribution over the set of keys for a specic nger p ( k ; x , f ) = h k , f ( x ) /  K = 1 h k 0 , f ( x , f ) k 0 .", "label": "Author", "bboxes": [{"left": 0.45849183006535943, "top": 0.331780303030303, "width": 0.02950653594771241, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3453282828282828, "width": 0.3998774509803922, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.3594545454545454, "width": 0.3974509803921568, "height": 0.012579545454545482, "page": 6}, {"left": 0.08933986928104576, "top": 0.37070454545454545, "width": 0.23860784313725492, "height": 0.018356060606060598, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4561"}, {"text": "Using our touchpad evaluation dataset we compared the two baselines with our motion model approach (Figure 8).", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.3946755050505051, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.40851262626262624, "width": 0.3664558823529412, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4562"}, {"text": "We decoded all test set samples using the Gaussian, Per Finger Gaussian, and our hand motion model with a greedy decoder with no language modeling.", "label": "Author", "bboxes": [{"left": 0.46354084967320264, "top": 0.40851262626262624, "width": 0.021751633986928143, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.42234974747474746, "width": 0.3974558823529412, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.4361868686868687, "width": 0.397452614379085, "height": 0.012579545454545482, "page": 6}, {"left": 0.08753267973856209, "top": 0.4500239898989899, "width": 0.18854084967320262, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4563"}, {"text": "We then tested a beam search decoder with a language model on the same three conditions.", "label": "Author", "bboxes": [{"left": 0.2844918300653595, "top": 0.4500239898989899, "width": 0.20079575163398689, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.4638611111111111, "width": 0.39785294117647063, "height": 0.012579545454545482, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4564"}, {"text": "of keys can be processed using the same beam search decoder and language model we described for hand motion decoding.", "label": "Author", "bboxes": [{"left": 0.08811928104575163, "top": 0.0814570707070707, "width": 0.39745098039215687, "height": 0.012579545454545468, "page": 6}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3973970588235295, "height": 0.012579545454545468, "page": 6}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4565"}, {"text": "In this interactive setting we constrained our beam search decoder to force convergence for any predictions older than 6 frames (0.1s) causing all beams to have a common prex.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.44942424242424245, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.46326136363636367, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.4770984848484849, "width": 0.4000130718954249, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4566"}, {"text": "We only rendered text in the common prex of all beams, effectively imposing a xed 0.1s delay.", "label": "Author", "bboxes": [{"left": 0.5239003267973856, "top": 0.4909356060606061, "width": 0.39997875816993456, "height": 0.012579545454545371, "page": 7}, {"left": 0.5246633986928104, "top": 0.5047727272727273, "width": 0.26259640522875816, "height": 0.012578282828282883, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4567"}, {"text": "While this serves as a proof of concept for interactive utility, our system still lacks many fundamental features required of any practical text input system, such as backspace or punctuation.", "label": "Author", "bboxes": [{"left": 0.6388300653594772, "top": 0.5462840909090909, "width": 0.28328594771241833, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246601307189542, "top": 0.5601212121212121, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246601307189542, "top": 0.5739583333333333, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5877941919191919, "width": 0.16952777777777783, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4568"}, {"text": "In this paper we take a major step towards realizing the viability of such a system by demonstrating decoding of text from hand motion captured with a high quality hand-tracking system.", "label": "Author", "bboxes": [{"left": 0.559437908496732, "top": 0.6631313131313132, "width": 0.36240359477124184, "height": 0.012579545454545427, "page": 7}, {"left": 0.5242565359477125, "top": 0.6769671717171717, "width": 0.3975882352941176, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6908042929292929, "width": 0.39716666666666667, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7046414141414141, "width": 0.050277777777777755, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4569"}, {"text": "We have shown on a 20 person dataset that touchtypists can transfer their skills typing on a physical keyboard to typing on a at surface, reaching comparable typing speeds (73WPM) while retaining an uncorrected error rate of less than 2.4%.", "label": "Author", "bboxes": [{"left": 0.5835261437908497, "top": 0.7046414141414141, "width": 0.34101797385620913, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5241274509803922, "top": 0.7461527777777778, "width": 0.3977156862745098, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.759989898989899, "width": 0.038736928104575186, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4570"}, {"text": "We achieved this result through the introduction of a novel motion model mapping hand motion to text, represented as a temporal neural network, and the application of beam search decoding combined with a modern neural language model.", "label": "Method", "bboxes": [{"left": 0.5699428104575164, "top": 0.759989898989899, "width": 0.3518937908496731, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7876641414141414, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8015012626262626, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8153383838383839, "width": 0.045661764705882346, "height": 0.012579545454545427, "page": 7}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4571"}, {"text": "We achieved this result through the introduction of a novel motion model mapping hand motion to text, represented as a temporal neural network, and the application of beam search decoding combined with a modern neural language model.", "label": "Author", "bboxes": [{"left": 0.5699428104575164, "top": 0.759989898989899, "width": 0.3518937908496731, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7876641414141414, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8015012626262626, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8153383838383839, "width": 0.045661764705882346, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4572"}, {"text": "We also show that our hand-tracking-based decoder can produce signicantly lower error than two baselines using contact-based text decoding (with and without nger identity information).", "label": "Author", "bboxes": [{"left": 0.576717320261438, "top": 0.8153383838383839, "width": 0.3454019607843136, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8291742424242424, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8430113636363636, "width": 0.39775000000000005, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.8568484848484849, "width": 0.08635947712418302, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4573"}, {"text": "We will need to handle a lower quality", "label": "Author", "bboxes": [{"left": 0.6568496732026144, "top": 0.892070707070707, "width": 0.26555228758169924, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4574"}, {"text": "Our motion model network is compact, containing just 180KB of weights, enabling efcient evaluation on a GPU.", "label": "Author", "bboxes": [{"left": 0.5246633986928104, "top": 0.28212247474747476, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.295959595959596, "width": 0.3398562091503269, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4575"}, {"text": "Our language model consists of 19MB of weights.", "label": "Author", "bboxes": [{"left": 0.8687026143790849, "top": 0.295959595959596, "width": 0.05584150326797388, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.3097967171717172, "width": 0.274874183006536, "height": 0.012579545454545427, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4576"}, {"text": "We are able to run both models, in addition to decoding, at interactive rates (i.e. 120 Hz) on a PC with an Nvidia RTX 2080Ti graphics card.", "label": "Author", "bboxes": [{"left": 0.8045833333333333, "top": 0.3097967171717172, "width": 0.11725326797385616, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.32363383838383836, "width": 0.4000228758169935, "height": 0.012579545454545482, "page": 7}, {"left": 0.5234428104575163, "top": 0.3374709595959596, "width": 0.4012418300653595, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4577"}, {"text": "We created a toy text entry application with an Oculus Rift and the Unity game engine to demonstrate real-time touch typing on a at surface in VR using hand-tracking (Figure 9).", "label": "Author", "bboxes": [{"left": 0.5238986928104574, "top": 0.40036616161616156, "width": 0.39793790849673205, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4142032828282828, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.428040404040404, "width": 0.3542908496732027, "height": 0.012579545454545482, "page": 7}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4578"}, {"text": "Our system currently does not support typing of nondictionary words effectively.", "label": "Author", "bboxes": [{"left": 0.13376470588235295, "top": 0.10913131313131313, "width": 0.3542352941176471, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.18003267973856213, "height": 0.01257954545454544, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4579"}, {"text": "For this work, we trained per-user neural motion models over an hours worth of typing samples and will need a more user-friendly version of user calibration.", "label": "Author", "bboxes": [{"left": 0.272921568627451, "top": 0.12296717171717171, "width": 0.21265686274509799, "height": 0.01257954545454544, "page": 8}, {"left": 0.08811928104575163, "top": 0.13680429292929294, "width": 0.39717483660130726, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.4000326797385621, "height": 0.012579545454545427, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4580"}, {"text": "When we attempted to train a single between-users model on the combined training sets of all users, the performance did not match the user specic models (UER rose from 2.4% to 3.9%, which a Wilcoxon signed rank test found to be signicant, p = 0 . 001).", "label": "Author", "bboxes": [{"left": 0.08735457516339869, "top": 0.16447853535353535, "width": 0.3979460784313726, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.19215277777777778, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.205989898989899, "width": 0.39987418300653593, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.21953787878787878, "width": 0.10915686274509805, "height": 0.01286868686868689, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4581"}, {"text": "We will need to explore a more efcient user calibration process in future work.", "label": "Author", "bboxes": [{"left": 0.2021895424836601, "top": 0.21982702020202022, "width": 0.28337745098039213, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.23366414141414144, "width": 0.22885620915032684, "height": 0.012579545454545454, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4582"}, {"text": "Our current experiments provide no visual feedback during typing, and more exploration of what visual feedback to offer, e.g., along the lines of Velocitap [25] is needed.", "label": "Author", "bboxes": [{"left": 0.3228529411764706, "top": 0.23366414141414144, "width": 0.16243954248366016, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.2475012626262626, "width": 0.39988071895424837, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.2613383838383838, "width": 0.3971797385620915, "height": 0.012579545454545482, "page": 8}, {"left": 0.08753267973856209, "top": 0.27517424242424243, "width": 0.16674019607843138, "height": 0.012579545454545482, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4583"}, {"text": "Finally, we are only decoding the lower case letters of the alphabet on our keyboard.", "label": "Author", "bboxes": [{"left": 0.26119607843137255, "top": 0.27517424242424243, "width": 0.22409150326797378, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811437908496732, "top": 0.28901136363636365, "width": 0.3382287581699347, "height": 0.012579545454545482, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4584"}, {"text": "We will need to handle other keys, including backspace, numbers and symbols to support general interactive text editing.", "label": "Author", "bboxes": [{"left": 0.4330833333333334, "top": 0.28901136363636365, "width": 0.052204248366013006, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.30284848484848487, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.3166856060606061, "width": 0.3296290849673203, "height": 0.012579545454545482, "page": 8}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4585"}, {"text": "We propose a motion model, represented as a temporal convolutional network (TCN), that can translate hand motion directly into (a probability distribution over the) typed text.", "label": "Author", "bboxes": [{"left": 0.08893300653594771, "top": 0.6849772727272727, "width": 0.3990669934640523, "height": 0.01285479797979805, "page": 1}, {"left": 0.10399183006535948, "top": 0.6990896464646464, "width": 0.3812941176470588, "height": 0.012579545454545538, "page": 1}, {"left": 0.10439869281045752, "top": 0.7129267676767678, "width": 0.3835767973856209, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4586"}, {"text": "We show that we can combine our motion model with a language model, also represented as a neural network, using an efcient beam search decoding technique.", "label": "Author", "bboxes": [{"left": 0.08893300653594771, "top": 0.7385934343434344, "width": 0.39635130718954253, "height": 0.012856060606060593, "page": 1}, {"left": 0.10439869281045752, "top": 0.7527070707070708, "width": 0.3809003267973856, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.7665441919191919, "width": 0.2925294117647058, "height": 0.012579545454545538, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4587"}, {"text": "We show that tracking hands typing on a at surface combined with our statistical decoding method has the potential of achieving speeds comparable to typing on a physical keyboard while maintaining low-error rates.", "label": "Author", "bboxes": [{"left": 0.08893464052287581, "top": 0.7922058080808081, "width": 0.39905555555555555, "height": 0.012861111111111101, "page": 1}, {"left": 0.10439869281045752, "top": 0.806324494949495, "width": 0.3809003267973856, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8201616161616161, "width": 0.38359640522875815, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8339987373737374, "width": 0.26878921568627445, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4588"}, {"text": "We also explore why decoding continuous hand motion data could be advantageous to decoding a discrete set of contacts on a touch surface by isolating the value nger trajectory information, nger identity information and continuous decoding.", "label": "Author", "bboxes": [{"left": 0.3787679738562092, "top": 0.8339987373737374, "width": 0.10651633986928105, "height": 0.012579545454545427, "page": 1}, {"left": 0.10381209150326798, "top": 0.8478358585858585, "width": 0.3841862745098039, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8616729797979799, "width": 0.38088562091503275, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.875510101010101, "width": 0.38292320261437907, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8893472222222223, "width": 0.3436437908496732, "height": 0.012579545454545427, "page": 1}], "section": null, "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4589"}, {"text": "Specically, we make the following contributions:", "label": "Contribution", "bboxes": [{"left": 0.18670424836601307, "top": 0.6500315656565657, "width": 0.30129084967320263, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.6638686868686868, "width": 0.03618790849673202, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4590"}, {"text": "We attempt to tease apart the contributions of these differences with two experiments.", "label": "Contribution", "bboxes": [{"left": 0.1410702614379085, "top": 0.4915770202020202, "width": 0.3442222222222223, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5054128787878788, "width": 0.2207075163398693, "height": 0.012579545454545538, "page": 5}], "section": "Comparison of Input Features and Motion Models", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4591"}, {"text": "In our second experiment we investigate the contribution of continuous motion decoding versus discrete contact-based decoding.", "label": "Contribution", "bboxes": [{"left": 0.08811928104575163, "top": 0.6312045454545454, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6450416666666666, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6588787878787878, "width": 0.0640669934640523, "height": 0.012579545454545427, "page": 5}], "section": "Comparison of Input Features and Motion Models", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4592"}, {"text": "In our fi experiment we study the contribution of finger identity information.", "label": "Contribution", "bboxes": [{"left": 0.08811928104575163, "top": 0.5267979797979798, "width": 0.3974558823529412, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.540635101010101, "width": 0.138047385620915, "height": 0.012579545454545538, "page": 5}], "section": "Comparison of Input Features and Motion Models", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4593"}, {"text": "The ATK system [28] uses skeletal hand-tracking (via a Leap Motion device) to drive text entry, although through midair typing.", "label": "Novelty", "bboxes": [{"left": 0.685326797385621, "top": 0.42563131313131314, "width": 0.2365098039215685, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.43946843434343436, "width": 0.3992075163398693, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.4533055555555555, "width": 0.20978431372549033, "height": 0.012579545454545482, "page": 1}], "section": "AR/VR text entry", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4594"}, {"text": "The VISAR system [5] also uses hand-tracking (via a Hololens device) to enable typing, although with just the index fingers.", "label": "Novelty", "bboxes": [{"left": 0.7394901960784314, "top": 0.4533055555555555, "width": 0.1823545751633987, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.46714267676767673, "width": 0.3991993464052288, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.48097979797979795, "width": 0.23434477124183006, "height": 0.012579545454545482, "page": 1}], "section": "AR/VR text entry", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4595"}, {"text": "PalmType leverages passive haptics by using the opposite hand as a tapping / typing surface, although this approach comes at the cost of limiting typing to a single hand.", "label": "Novelty", "bboxes": [{"left": 0.7596470588235295, "top": 0.6055126262626263, "width": 0.16489705882352934, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6193497474747475, "width": 0.39716830065359476, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.6331868686868687, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6470239898989899, "width": 0.15815522875817, "height": 0.012579545454545427, "page": 1}], "section": "AR/VR text entry", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4596"}, {"text": "We believe the on-surface domain, while slightly less accessible, better leverages existing typing skills, allowing our method to achieve speeds comparable to typing on a keyboard at a low error rate.", "label": "Novelty", "bboxes": [{"left": 0.5668382352941176, "top": 0.6746982323232323, "width": 0.354998366013072, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6885353535353536, "width": 0.39745915032679746, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7023724747474748, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7162083333333333, "width": 0.11938888888888899, "height": 0.012579545454545538, "page": 1}], "section": "AR/VR text entry", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4597"}, {"text": "To evaluate the feasibility of using hand-tracking to enable touch typing on virtual keyboards, we fi collect a dataset of skeletal hand-tracking data from touch-typists transcribing short phrases while typing on a at surface.", "label": "Novelty", "bboxes": [{"left": 0.08761437908496732, "top": 0.30997474747474746, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811274509803921, "top": 0.3238118686868687, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3376489898989899, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3514861111111111, "width": 0.27779411764705886, "height": 0.012579545454545427, "page": 2}], "section": "APPROACH", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4598"}, {"text": "While we are targeting a different sensing modality (hand-tracking instead of capacitive touchpads), we can use touchpads as a baseline for our work.", "label": "Novelty", "bboxes": [{"left": 0.16333823529411765, "top": 0.6508674242424242, "width": 0.32252777777777786, "height": 0.012579545454545538, "page": 2}, {"left": 0.08758169934640524, "top": 0.6647045454545455, "width": 0.39770424836601304, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811274509803921, "top": 0.6785416666666666, "width": 0.25703431372549024, "height": 0.012579545454545538, "page": 2}], "section": "APPROACH", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4599"}, {"text": "uses beam search with a language model to decode whole sentences at a time, although Velocitap uses contact as the input modality while our method uses hand-tracking.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.0814570707070707, "width": 0.3971732026143791, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.39717320261437916, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.34499509803921563, "height": 0.012579545454545454, "page": 2}], "section": "Statistical text decoding", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4600"}, {"text": "While using a marker-based tracking system may introduce a gap in tracking quality compared to what is achievable on an AR/VR headset, we can study the potential of hand-tracking applied to this problem.", "label": "Novelty", "bboxes": [{"left": 0.39330718954248367, "top": 0.49740277777777775, "width": 0.09198039215686271, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.511239898989899, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5250770202020202, "width": 0.3992156862745098, "height": 0.012579545454545538, "page": 2}, {"left": 0.08753267973856209, "top": 0.5389141414141414, "width": 0.3977549019607843, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5527512626262626, "width": 0.058326797385620896, "height": 0.012579545454545538, "page": 2}], "section": "APPROACH", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4601"}, {"text": "Longer term context is useful for prediction in the context of language, but since our system design separates motion modeling from language modeling, a TCN works well for the former.", "label": "Novelty", "bboxes": [{"left": 0.7275669934640523, "top": 0.44630176767676766, "width": 0.19426960784313718, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4601388888888889, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.4739760101010101, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 2}, {"left": 0.5241601307189543, "top": 0.4878131313131313, "width": 0.20243137254901955, "height": 0.012579545454545427, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4602"}, {"text": "Participants typed phrases up to 40 characters long drawn from two corpora; samples used to t or train models were randomly sampled from Daily Dialog [19], while samples for testing and evaluation were randomly sampled from Mackenzie and Soukeroff [23].", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.652993686868687, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6668308080808081, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6806679292929294, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6945037878787879, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.7083409090909091, "width": 0.10281372549019618, "height": 0.012579545454545538, "page": 3}], "section": "DATA COLLECTION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4603"}, {"text": "After typing each phrase, participants would use one of two foot pedals to mark that they felt they correctly typed the phrase, or that they typed the phrase but felt they made mistakes.", "label": "Novelty", "bboxes": [{"left": 0.9022794117647059, "top": 0.53475, "width": 0.022256535947712375, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.5485871212121212, "width": 0.39717810457516356, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5624242424242424, "width": 0.3974591503267976, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.5762613636363636, "width": 0.355704248366013, "height": 0.012579545454545427, "page": 3}], "section": "DATA COLLECTION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4604"}, {"text": "To filter samples where both an extra and a missing contact event occurred (leading to an agreement on counts but an error in contact-key sequence alignment), we build up a distribution for each key of how many times each finger hit that key.", "label": "Novelty", "bboxes": [{"left": 0.08761437908496732, "top": 0.4148030303030303, "width": 0.39767156862745096, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4286401515151515, "width": 0.39745915032679735, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4424772727272727, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.4563131313131313, "width": 0.37041176470588233, "height": 0.012579545454545482, "page": 4}], "section": "DATA COLLECTION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4605"}, {"text": "Participants of our user study type at different speeds, so we compare the relative speed of typing on a at surface versus a physical keyboard.", "label": "Novelty", "bboxes": [{"left": 0.5246633986928104, "top": 0.5725618686868686, "width": 0.39716830065359465, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.5863977272727273, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6002348484848485, "width": 0.13854575163398697, "height": 0.012579545454545427, "page": 4}], "section": "Physical keyboard versus virtual keyboard", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4606"}, {"text": "Hand-tracking information was recorded while participants typed during all three blocks.", "label": "Novelty", "bboxes": [{"left": 0.08811928104575163, "top": 0.14435227272727272, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.15818939393939394, "width": 0.1925751633986928, "height": 0.012579545454545454, "page": 4}], "section": "DATA COLLECTION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4607"}, {"text": "While the contact-based baselines can only be trained with known key/contact-point correspondences, our motion model is trained using CTC loss, which allows for sequence level labels and can deduce the frame-level alignment of the labels.", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.7003484848484849, "width": 0.3979379084967321, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7141856060606061, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7280214646464646, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7418585858585859, "width": 0.4000261437908497, "height": 0.012579545454545427, "page": 4}], "section": "DATA COLLECTION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4608"}, {"text": "3) a sequence of length T of skeletal hand poses while those keys were typed, recorded at 60Hz.", "label": "Novelty", "bboxes": [{"left": 0.2089640522875817, "top": 0.5604305555555555, "width": 0.2763366013071896, "height": 0.012868686868686918, "page": 4}, {"left": 0.08811928104575163, "top": 0.5745568181818181, "width": 0.34859313725490193, "height": 0.012579545454545427, "page": 4}], "section": "DATA COLLECTION", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4609"}, {"text": "This model lacks the continuous text decoding capabilities of our motion model, but rather has to decode text only at discrete contact events.", "label": "Novelty", "bboxes": [{"left": 0.18848202614379084, "top": 0.8643964646464646, "width": 0.29681045751633994, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.22715359477124186, "height": 0.012579545454545427, "page": 5}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4610"}, {"text": "However, when using a touchpad, we assume no ambiguity in whether a contact event occurred, only 2D spatial ambiguity about which key was pressed.", "label": "Novelty", "bboxes": [{"left": 0.6205702614379085, "top": 0.39338131313131314, "width": 0.301266339869281, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.40721843434343435, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.42105555555555557, "width": 0.31675980392156866, "height": 0.012579545454545482, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4611"}, {"text": "There is no mechanism to detect an accidental touch or to guess when a touch event should have happened but was missed.", "label": "Novelty", "bboxes": [{"left": 0.846968954248366, "top": 0.42105555555555557, "width": 0.07486764705882343, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.4348926767676768, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.448729797979798, "width": 0.3366176470588237, "height": 0.012578282828282827, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4612"}, {"text": "On the other hand, our motion model generates a probability distribution of which key was pressed (including no-key) at every frame of the tracking, which provides the decoder exibility to insert or remove key-presses to accommodate the language model prior (Figure 7).", "label": "Novelty", "bboxes": [{"left": 0.6378709150326798, "top": 0.5040770202020202, "width": 0.286673202614379, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5179141414141414, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 6}, {"left": 0.5241274509803922, "top": 0.5317512626262626, "width": 0.40041666666666675, "height": 0.012579545454545427, "page": 6}, {"left": 0.5242565359477125, "top": 0.5455883838383838, "width": 0.3975751633986927, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5594242424242425, "width": 0.33138725490196086, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4613"}, {"text": "However, the decrease is greater for the contact-", "label": "Novelty", "bboxes": [{"left": 0.6188202614379086, "top": 0.7544002525252526, "width": 0.3057271241830064, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4614"}, {"text": "With greedy decoding, the 2D contact-based methods appeared to have higher variance, though the median UER was comparable across the three strategies.", "label": "Novelty", "bboxes": [{"left": 0.08735457516339869, "top": 0.4852462121212121, "width": 0.3979379084967321, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.4990820707070707, "width": 0.39988235294117647, "height": 0.012579545454545371, "page": 6}, {"left": 0.08811928104575163, "top": 0.512919191919192, "width": 0.21466013071895423, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4615"}, {"text": "With greedy decoding, the 2D contact-based methods appeared to have higher variance, though the median UER was comparable across the three strategies.", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.4852462121212121, "width": 0.3979379084967321, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.4990820707070707, "width": 0.39988235294117647, "height": 0.012579545454545371, "page": 6}, {"left": 0.08811928104575163, "top": 0.512919191919192, "width": 0.21466013071895423, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "4616"}, {"text": "When we added the beam search language model decoder, however, we found that both Gaussian and Per Finger Gaussian contact-based methods performed comparably with a mean UERs of 5.66% (median 2.70%) and 4.76% (median 2.64%) respectively, while our motion model approach outperformed both with a mean UER of 2.38% (median 1.77%).", "label": "Novelty", "bboxes": [{"left": 0.31051633986928107, "top": 0.512919191919192, "width": 0.17477614379084966, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5267563131313131, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5405934343434343, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.5544305555555555, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5682676767676768, "width": 0.3974558823529412, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.582104797979798, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5959419191919192, "width": 0.16862581699346407, "height": 0.012579545454545538, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4617"}, {"text": "Note that while more sophisticated contact-based decoders such as the one described in Velocitap [25] can add or remove characters during decoding, they would not have access to nger trajectory information.", "label": "Novelty", "bboxes": [{"left": 0.16219771241830067, "top": 0.3921893939393939, "width": 0.3231029411764706, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.4060265151515151, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.41986363636363633, "width": 0.3971732026143791, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.43370075757575755, "width": 0.2518153594771242, "height": 0.012579545454545482, "page": 7}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4618"}, {"text": "This prevented any visible retroactive changes and maintained a controlled latency, while allowing corrections of the most recent 1-2 characters for most typists.", "label": "Novelty", "bboxes": [{"left": 0.7946323529411765, "top": 0.5047727272727273, "width": 0.12776960784313718, "height": 0.012578282828282883, "page": 7}, {"left": 0.5242565359477125, "top": 0.5186098484848485, "width": 0.3996127450980391, "height": 0.012579545454545427, "page": 7}, {"left": 0.5240784313725491, "top": 0.5324469696969697, "width": 0.3977549019607842, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246601307189542, "top": 0.5462840909090909, "width": 0.1080424836601308, "height": 0.012579545454545427, "page": 7}], "section": "Performance and interactive run-time", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4619"}, {"text": "While this serves as a proof of concept for interactive utility, our system still lacks many fundamental features required of any practical text input system, such as backspace or punctuation.", "label": "Novelty", "bboxes": [{"left": 0.6388300653594772, "top": 0.5462840909090909, "width": 0.28328594771241833, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246601307189542, "top": 0.5601212121212121, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246601307189542, "top": 0.5739583333333333, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5877941919191919, "width": 0.16952777777777783, "height": 0.012579545454545427, "page": 7}], "section": "Performance and interactive run-time", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4620"}, {"text": "We have shown on a 20 person dataset that touchtypists can transfer their skills typing on a physical keyboard to typing on a at surface, reaching comparable typing speeds (73WPM) while retaining an uncorrected error rate of less than 2.4%.", "label": "Novelty", "bboxes": [{"left": 0.5835261437908497, "top": 0.7046414141414141, "width": 0.34101797385620913, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5241274509803922, "top": 0.7461527777777778, "width": 0.3977156862745098, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.759989898989899, "width": 0.038736928104575186, "height": 0.012579545454545427, "page": 7}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4621"}, {"text": "We show that tracking hands typing on a at surface combined with our statistical decoding method has the potential of achieving speeds comparable to typing on a physical keyboard while maintaining low-error rates.", "label": "Novelty", "bboxes": [{"left": 0.08893464052287581, "top": 0.7922058080808081, "width": 0.39905555555555555, "height": 0.012861111111111101, "page": 1}, {"left": 0.10439869281045752, "top": 0.806324494949495, "width": 0.3809003267973856, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8201616161616161, "width": 0.38359640522875815, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8339987373737374, "width": 0.26878921568627445, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4622"}, {"text": "We use a beam search implementation with beam compaction which maximizes the objective W = argmax W p total .", "label": "Objective", "bboxes": [{"left": 0.08735457516339869, "top": 0.7520138888888889, "width": 0.3979362745098039, "height": 0.012579545454545427, "page": 3}, {"left": 0.08753267973856209, "top": 0.7655618686868687, "width": 0.3366797385620915, "height": 0.015136363636363614, "page": 3}], "section": "TEXT DECODING WITH A LANGUAGE MODEL", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4623"}, {"text": "3. A decoder which can optimize an objective function combining the likelihoods from both the motion and the language models.", "label": "Objective", "bboxes": [{"left": 0.521408496732026, "top": 0.12274242424242425, "width": 0.40313071895424846, "height": 0.012583333333333321, "page": 2}, {"left": 0.5409428104575164, "top": 0.1365820707070707, "width": 0.3809019607843137, "height": 0.012579545454545454, "page": 2}, {"left": 0.5409428104575164, "top": 0.15041919191919192, "width": 0.051099673202614326, "height": 0.012579545454545454, "page": 2}], "section": "SYSTEM DESIGN", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4624"}, {"text": "In this paper, we investigate the use of hand-tracking to enable typing on any at surface at speeds comparable to typing on a physical keyboard.", "label": "Objective", "bboxes": [{"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.1359052287581699, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4625"}, {"text": "Many other text entry systems have been proposed for AR/VR applications, and for brevity, we concentrate on those that use a QWERTY keyboard layout.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.34260984848484843, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.35644696969696965, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.37028409090909087, "width": 0.17770424836601306, "height": 0.012579545454545482, "page": 1}], "section": "AR/VR text entry", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4626"}, {"text": "In our study we compromise between accessibility and speed by requiring users to type against a surface but eschewing the use of a physical keyboard or a touchpad.", "label": "Method", "bboxes": [{"left": 0.44496078431372543, "top": 0.21353661616161618, "width": 0.0406143790849674, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.3998709150326797, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.24121085858585856, "width": 0.39716830065359476, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.2512728758169934, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4627"}, {"text": "We show that these users can transfer their existing skills typing on a keyboard to typing on a at surface.", "label": "Method", "bboxes": [{"left": 0.3197581699346405, "top": 0.15064141414141416, "width": 0.16553594771241836, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.10742320261437908, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4628"}, {"text": "To evaluate the feasibility of using hand-tracking to enable touch typing on virtual keyboards, we fi collect a dataset of skeletal hand-tracking data from touch-typists transcribing short phrases while typing on a at surface.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.30997474747474746, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811274509803921, "top": 0.3238118686868687, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3376489898989899, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3514861111111111, "width": 0.27779411764705886, "height": 0.012579545454545427, "page": 2}], "section": "APPROACH", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4629"}, {"text": "Next, we design a system for decoding the skeletal hand-tracking data into the text the typists intended to type.", "label": "Method", "bboxes": [{"left": 0.3709705882352941, "top": 0.3514861111111111, "width": 0.11432189542483662, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3653219696969697, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3791590909090909, "width": 0.21343137254901967, "height": 0.012579545454545482, "page": 2}], "section": "APPROACH", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4630"}, {"text": "We apply this same general framework, substituting recurrent neural networks with a TCN-based motion model, to decode text entry via typing.", "label": "Method", "bboxes": [{"left": 0.20009313725490194, "top": 0.24236363636363636, "width": 0.28789705882352945, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.25620075757575755, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.27003787878787877, "width": 0.25114869281045754, "height": 0.012579545454545427, "page": 2}], "section": "Automatic speech recognition", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4631"}, {"text": "While we are targeting a different sensing modality (hand-tracking instead of capacitive touchpads), we can use touchpads as a baseline for our work.", "label": "Method", "bboxes": [{"left": 0.16333823529411765, "top": 0.6508674242424242, "width": 0.32252777777777786, "height": 0.012579545454545538, "page": 2}, {"left": 0.08758169934640524, "top": 0.6647045454545455, "width": 0.39770424836601304, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811274509803921, "top": 0.6785416666666666, "width": 0.25703431372549024, "height": 0.012579545454545538, "page": 2}], "section": "APPROACH", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4632"}, {"text": "We use a temporal neural network as a motion model.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.30038383838383836, "width": 0.3616732026143793, "height": 0.012579545454545427, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4633"}, {"text": "uses beam search with a language model to decode whole sentences at a time, although Velocitap uses contact as the input modality while our method uses hand-tracking.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.0814570707070707, "width": 0.3971732026143791, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.39717320261437916, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.34499509803921563, "height": 0.012579545454545454, "page": 2}], "section": "Statistical text decoding", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4634"}, {"text": "We chose these features because they are somewhat invariant to differences in hand scale across people.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.702915404040404, "width": 0.39794281045751645, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.7167525252525253, "width": 0.27398856209150324, "height": 0.012579545454545427, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4635"}, {"text": "For temporal modeling, we opt to use a temporal convolutional network (TCN) rather than a recurrent model because a xed window of hand motion data is typically sufcient to make predictions about key presses.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.404790404040404, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4186275252525252, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 2}, {"left": 0.5240784313725491, "top": 0.43246464646464644, "width": 0.39775490196078434, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.44630176767676766, "width": 0.1978415032679739, "height": 0.012579545454545482, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4636"}, {"text": "The problem of generating text from hand motion has strong analogs to automatic speech recognition (ASR), and we use ASR as motivation to design a multi-component system with the following three pieces.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.7876641414141414, "width": 0.39767320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8015012626262626, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08753267973856209, "top": 0.8153383838383839, "width": 0.39776307189542476, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8291742424242424, "width": 0.17229411764705882, "height": 0.012579545454545427, "page": 2}], "section": "SYSTEM DESIGN", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4637"}, {"text": "We apply a beam search decoder to resolve these ambiguities using the language model as a prior.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.24098863636363635, "width": 0.39793464052287597, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.2548257575757576, "width": 0.2346584967320262, "height": 0.012579545454545427, "page": 2}], "section": "SYSTEM DESIGN", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4638"}, {"text": "In our study we collected data by having participants complete a text transcription task for short phrases.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.31209974747474745, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.32593686868686866, "width": 0.27559803921568626, "height": 0.012579545454545482, "page": 3}], "section": "DATA COLLECTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4639"}, {"text": "We use a beam search implementation with beam compaction which maximizes the objective W = argmax W p total .", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.7520138888888889, "width": 0.3979362745098039, "height": 0.012579545454545427, "page": 3}, {"left": 0.08753267973856209, "top": 0.7655618686868687, "width": 0.3366797385620915, "height": 0.015136363636363614, "page": 3}], "section": "TEXT DECODING WITH A LANGUAGE MODEL", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4640"}, {"text": "We can do better than greedy decoding by using a prex beam search decoder [13].", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.5312941919191919, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5451313131313131, "width": 0.136171568627451, "height": 0.012579545454545427, "page": 3}], "section": "TEXT DECODING WITH A LANGUAGE MODEL", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4641"}, {"text": "Our first evaluation is to determine if participants are able to transfer their existing skills typing on a physical keyboard to typing on a at surface (with the help of our proposed decoder).", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.531050505050505, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.5448876262626262, "width": 0.39718137254901953, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.5587247474747474, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 4}], "section": "Physical keyboard versus virtual keyboard", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4642"}, {"text": "We present two evaluations of our proposed text entry method using the data collected from our user study.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.8505593434343435, "width": 0.39794281045751634, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.2939950980392157, "height": 0.012579545454545538, "page": 4}], "section": "EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4643"}, {"text": "Our method is designed for typing on a at surface rather than in the air or against the opposite hand.", "label": "Method", "bboxes": [{"left": 0.690140522875817, "top": 0.6470239898989899, "width": 0.23169607843137263, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6608611111111111, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6746982323232323, "width": 0.03642973856209153, "height": 0.012579545454545427, "page": 1}], "section": "AR/VR text entry", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4644"}, {"text": "Our result reduces this oracle to practice by substituting it with our proposed neural text decoder.", "label": "Method", "bboxes": [{"left": 0.5739068627450981, "top": 0.8782335858585859, "width": 0.35062745098039216, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.2882156862745099, "height": 0.012579545454545427, "page": 4}], "section": "Physical keyboard versus virtual keyboard", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4645"}, {"text": "Participants typed on physical keyboards with a mean UER of 1.72% (median: 1.19%) compared to a mean UER of 2.38% (median: 1.77%) when they typed on at surfaces with our decoder.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.3971830065359476, "height": 0.012579545454545427, "page": 4}, {"left": 0.5234428104575163, "top": 0.7461527777777778, "width": 0.39974673202614375, "height": 0.012579545454545538, "page": 4}, {"left": 0.524124183006536, "top": 0.759989898989899, "width": 0.39798692810457514, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.05453267973856213, "height": 0.012579545454545427, "page": 4}], "section": "Physical keyboard versus virtual keyboard", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4646"}, {"text": "While the contact-based baselines can only be trained with known key/contact-point correspondences, our motion model is trained using CTC loss, which allows for sequence level labels and can deduce the frame-level alignment of the labels.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.7003484848484849, "width": 0.3979379084967321, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7141856060606061, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7280214646464646, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7418585858585859, "width": 0.4000261437908497, "height": 0.012579545454545427, "page": 4}], "section": "DATA COLLECTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4647"}, {"text": "For our motion model we trained person-specic models that captured each users typing style from the training set portion of the data.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.39182449494949495, "width": 0.39717647058823535, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.40566161616161617, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4194987373737374, "width": 0.0763627450980392, "height": 0.012579545454545482, "page": 4}], "section": "EVALUATION", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4648"}, {"text": "In our second experiment we investigate the contribution of continuous motion decoding versus discrete contact-based decoding.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.6312045454545454, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6450416666666666, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6588787878787878, "width": 0.0640669934640523, "height": 0.012579545454545427, "page": 5}], "section": "Comparison of Input Features and Motion Models", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4649"}, {"text": "In our first experiment we study the contribution of nger identity information.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.5267979797979798, "width": 0.3974558823529412, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.540635101010101, "width": 0.138047385620915, "height": 0.012579545454545538, "page": 5}], "section": "Comparison of Input Features and Motion Models", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4650"}, {"text": "Using the cleaned training dataset, we obtain a sequence of keys pressed { w  i } and a sequence of 2D contact points from the touchpad { x  i } (See Figure 5).", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.781084595959596, "width": 0.39717320261437916, "height": 0.013835858585858563, "page": 5}, {"left": 0.5246633986928104, "top": 0.7949217171717171, "width": 0.23283986928104572, "height": 0.013835858585858563, "page": 5}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4651"}, {"text": "For our first baseline, Gaussian contact decoding , we evaluate performance using the spatial modeling approach described by Zhu et al.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.8364330808080809, "width": 0.3971732026143791, "height": 0.012868686868686807, "page": 5}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.06075000000000001, "height": 0.012579545454545538, "page": 5}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4652"}, {"text": "The previous experiments show that our continuous hand motion decoding method is more accurate than decoding of discrete contacts with either a simple Gaussian spatial model or a richer model with nger identity information.", "label": "Method", "bboxes": [{"left": 0.5241601307189543, "top": 0.13676767676767676, "width": 0.4003839869281046, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.15060479797979798, "width": 0.399874183006536, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.1644419191919192, "width": 0.39745915032679746, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.17827777777777779, "width": 0.3082058823529412, "height": 0.012579545454545454, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4653"}, {"text": "In order to disentangle the benefit of continuous motion decoding versus discrete contact decoding, we filter our test dataset similarly to our training set to contain only samples where we know the correspondences between contact events and ground truth character labels. In this filtered corresponded contacts dataset, there are no samples with missing or spurious contacts.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.5808093434343434, "width": 0.39988398692810456, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5946464646464646, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6084835858585859, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6223207070707071, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6361578282828283, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6499949494949495, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6638308080808081, "width": 0.03445098039215688, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4654"}, {"text": "At first, we assumed that the discrete nature of contacts was an advantage for contact decoding.", "label": "Method", "bboxes": [{"left": 0.5240784313725491, "top": 0.33803282828282827, "width": 0.3977630718954249, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.3518699494949495, "width": 0.20806209150326793, "height": 0.012579545454545482, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4655"}, {"text": "For our second baseline, Per Finger Gaussian contact decoding , we specically investigate if augmenting the spatial model input with nger identity information improves performance of the contact-based model.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.1163888888888889, "width": 0.39988725490196086, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.1302260101010101, "width": 0.3971732026143791, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.14435227272727272, "width": 0.39718137254901964, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.15818939393939394, "width": 0.18443954248366018, "height": 0.012579545454545454, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4656"}, {"text": "When we added the beam search language model decoder, however, we found that both Gaussian and Per Finger Gaussian contact-based methods performed comparably with a mean UERs of 5.66% (median 2.70%) and 4.76% (median 2.64%) respectively, while our motion model approach outperformed both with a mean UER of 2.38% (median 1.77%).", "label": "Method", "bboxes": [{"left": 0.31051633986928107, "top": 0.512919191919192, "width": 0.17477614379084966, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5267563131313131, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5405934343434343, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.5544305555555555, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5682676767676768, "width": 0.3974558823529412, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.582104797979798, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5959419191919192, "width": 0.16862581699346407, "height": 0.012579545454545538, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4657"}, {"text": "To use nger identity information, we make two changes to our Gaussian contact decoding baseline.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.2764330808080808, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811274509803921, "top": 0.29027020202020204, "width": 0.2936797385620915, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4658"}, {"text": "Using our touchpad evaluation dataset we compared the two baselines with our motion model approach (Figure 8).", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.3946755050505051, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.40851262626262624, "width": 0.3664558823529412, "height": 0.012579545454545482, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4659"}, {"text": "of keys can be processed using the same beam search decoder and language model we described for hand motion decoding.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.0814570707070707, "width": 0.39745098039215687, "height": 0.012579545454545468, "page": 6}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3973970588235295, "height": 0.012579545454545468, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4660"}, {"text": "In this paper we take a major step towards realizing the viability of such a system by demonstrating decoding of text from hand motion captured with a high quality hand-tracking system.", "label": "Method", "bboxes": [{"left": 0.559437908496732, "top": 0.6631313131313132, "width": 0.36240359477124184, "height": 0.012579545454545427, "page": 7}, {"left": 0.5242565359477125, "top": 0.6769671717171717, "width": 0.3975882352941176, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6908042929292929, "width": 0.39716666666666667, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7046414141414141, "width": 0.050277777777777755, "height": 0.012579545454545427, "page": 7}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4661"}, {"text": "Our motion model network is compact, containing just 180KB of weights, enabling efcient evaluation on a GPU.", "label": "Method", "bboxes": [{"left": 0.5246633986928104, "top": 0.28212247474747476, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.295959595959596, "width": 0.3398562091503269, "height": 0.012579545454545427, "page": 7}], "section": "Performance and interactive run-time", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4662"}, {"text": "We created a toy text entry application with an Oculus Rift and the Unity game engine to demonstrate real-time touch typing on a at surface in VR using hand-tracking (Figure 9).", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.40036616161616156, "width": 0.39793790849673205, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4142032828282828, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.428040404040404, "width": 0.3542908496732027, "height": 0.012579545454545482, "page": 7}], "section": "Performance and interactive run-time", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4663"}, {"text": "Our system currently does not support typing of nondictionary words effectively.", "label": "Method", "bboxes": [{"left": 0.13376470588235295, "top": 0.10913131313131313, "width": 0.3542352941176471, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.18003267973856213, "height": 0.01257954545454544, "page": 8}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4664"}, {"text": "We propose a motion model, represented as a temporal convolutional network (TCN), that can translate hand motion directly into (a probability distribution over the) typed text.", "label": "Method", "bboxes": [{"left": 0.08893300653594771, "top": 0.6849772727272727, "width": 0.3990669934640523, "height": 0.01285479797979805, "page": 1}, {"left": 0.10399183006535948, "top": 0.6990896464646464, "width": 0.3812941176470588, "height": 0.012579545454545538, "page": 1}, {"left": 0.10439869281045752, "top": 0.7129267676767678, "width": 0.3835767973856209, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4665"}, {"text": "We show that by using hand pose rather than contact points and a richer motion model, we can achieve more accurate decoding.", "label": "Result", "bboxes": [{"left": 0.595109477124183, "top": 0.27744444444444444, "width": 0.32942973856209146, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.29128156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.3051186868686869, "width": 0.12204248366013071, "height": 0.012579545454545427, "page": 1}], "section": "Surface and eyes-free keyboards", "prob": null, "is_author_statement": true, "is_in_expected_section": false, "id": "4666"}, {"text": "Surveys of internet users show that even average typists can achieve speeds greater than 50 words per minute (WPM) with the fastest 90 th percentile achieving more than 78 WPM [3].", "label": "Result", "bboxes": [{"left": 0.32515196078431374, "top": 0.10913131313131313, "width": 0.16014379084967317, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.3971732026143791, "height": 0.01257954545454544, "page": 1}, {"left": 0.08811928104575163, "top": 0.13480303030303029, "width": 0.39717483660130715, "height": 0.01458080808080811, "page": 1}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.22660130718954252, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "4667"}, {"text": "We show that these users can transfer their existing skills typing on a keyboard to typing on a at surface.", "label": "Result", "bboxes": [{"left": 0.3197581699346405, "top": 0.15064141414141416, "width": 0.16553594771241836, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.10742320261437908, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4668"}, {"text": "The previous experiments show that our continuous hand motion decoding method is more accurate than decoding of discrete contacts with either a simple Gaussian spatial model or a richer model with finger identity information.", "label": "Result", "bboxes": [{"left": 0.5241601307189543, "top": 0.13676767676767676, "width": 0.4003839869281046, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.15060479797979798, "width": 0.399874183006536, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.1644419191919192, "width": 0.39745915032679746, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.17827777777777779, "width": 0.3082058823529412, "height": 0.012579545454545454, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4669"}, {"text": "Recent studies show that touch typists and faster typists tend to have lower entropy in their finger to key mapping [6].", "label": "Result", "bboxes": [{"left": 0.27878594771241827, "top": 0.15818939393939394, "width": 0.20650653594771246, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.17202651515151515, "width": 0.39745588235294127, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.18586363636363637, "width": 0.16511601307189544, "height": 0.012579545454545454, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "4670"}, {"text": "We also show that our hand-tracking-based decoder can produce signicantly lower error than two baselines using contact-based text decoding (with and without finger identity information).", "label": "Result", "bboxes": [{"left": 0.576717320261438, "top": 0.8153383838383839, "width": 0.3454019607843136, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8291742424242424, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8430113636363636, "width": 0.39775000000000005, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.8568484848484849, "width": 0.08635947712418302, "height": 0.012579545454545427, "page": 7}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4671"}, {"text": "We show that we can combine our motion model with a language model, also represented as a neural network, using an efcient beam search decoding technique.", "label": "Result", "bboxes": [{"left": 0.08893300653594771, "top": 0.7385934343434344, "width": 0.39635130718954253, "height": 0.012856060606060593, "page": 1}, {"left": 0.10439869281045752, "top": 0.7527070707070708, "width": 0.3809003267973856, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.7665441919191919, "width": 0.2925294117647058, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4672"}, {"text": "We show that tracking hands typing on a at surface combined with our statistical decoding method has the potential of achieving speeds comparable to typing on a physical keyboard while maintaining low-error rates.", "label": "Result", "bboxes": [{"left": 0.08893464052287581, "top": 0.7922058080808081, "width": 0.39905555555555555, "height": 0.012861111111111101, "page": 1}, {"left": 0.10439869281045752, "top": 0.806324494949495, "width": 0.3809003267973856, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8201616161616161, "width": 0.38359640522875815, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8339987373737374, "width": 0.26878921568627445, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4673"}, {"text": "Automatic speech recognition is more accessible, but may not be socially acceptable for certain environments or for private communication.", "label": "Conclusion", "bboxes": [{"left": 0.8517336601307189, "top": 0.6908042929292929, "width": 0.07010294117647065, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.7046414141414141, "width": 0.39775163398692825, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.04442483660130714, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4674"}, {"text": "This model takes as input a sequence of frames of hand-tracking features and for each frame outputs a probability distribution over the set of possible keys, plus one additional label for blank / no-key.", "label": "Conclusion", "bboxes": [{"left": 0.892313725490196, "top": 0.30038383838383836, "width": 0.02952287581699342, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246617647058823, "top": 0.3142209595959596, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.3280580808080808, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.341895202020202, "width": 0.39745588235294127, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.35573232323232323, "width": 0.0987957516339869, "height": 0.012579545454545427, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4675"}, {"text": "While using a marker-based tracking system may introduce a gap in tracking quality compared to what is achievable on an AR/VR headset, we can study the potential of hand-tracking applied to this problem.", "label": "Conclusion", "bboxes": [{"left": 0.39330718954248367, "top": 0.49740277777777775, "width": 0.09198039215686271, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.511239898989899, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5250770202020202, "width": 0.3992156862745098, "height": 0.012579545454545538, "page": 2}, {"left": 0.08753267973856209, "top": 0.5389141414141414, "width": 0.3977549019607843, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5527512626262626, "width": 0.058326797385620896, "height": 0.012579545454545538, "page": 2}], "section": "APPROACH", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4676"}, {"text": "We chose these features because they are somewhat invariant to differences in hand scale across people.", "label": "Conclusion", "bboxes": [{"left": 0.5238986928104574, "top": 0.702915404040404, "width": 0.39794281045751645, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.7167525252525253, "width": 0.27398856209150324, "height": 0.012579545454545427, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4677"}, {"text": "For temporal modeling, we opt to use a temporal convolutional network (TCN) rather than a recurrent model because a xed window of hand motion data is typically sufcient to make predictions about key presses.", "label": "Conclusion", "bboxes": [{"left": 0.5246633986928104, "top": 0.404790404040404, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4186275252525252, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 2}, {"left": 0.5240784313725491, "top": 0.43246464646464644, "width": 0.39775490196078434, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.44630176767676766, "width": 0.1978415032679739, "height": 0.012579545454545482, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4678"}, {"text": "Because our participants were tasked with typing test-set phrases on both a physical keyboard and on a touchpad, we can make this comparison directly.", "label": "Conclusion", "bboxes": [{"left": 0.6716111111111112, "top": 0.6002348484848485, "width": 0.2502254901960784, "height": 0.012579545454545427, "page": 4}, {"left": 0.5240784313725491, "top": 0.6140719696969698, "width": 0.39775490196078434, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6279090909090909, "width": 0.3662892156862746, "height": 0.012579545454545538, "page": 4}], "section": "Physical keyboard versus virtual keyboard", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4679"}, {"text": "Thus our motion model was able to be trained on the original training dataset, only filtering samples where users felt they made a mistake.", "label": "Conclusion", "bboxes": [{"left": 0.08761437908496732, "top": 0.755695707070707, "width": 0.39768137254901953, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7695328282828283, "width": 0.3977385620915033, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7833699494949494, "width": 0.1037843137254902, "height": 0.012579545454545538, "page": 4}], "section": "DATA COLLECTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4680"}, {"text": "When we examined the results qualitatively, we noted several differences between the contact-based decoding approach and our handtracking-based approach, which we illustrate in Table 2.", "label": "Conclusion", "bboxes": [{"left": 0.837919934640523, "top": 0.17827777777777779, "width": 0.08662581699346406, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.192114898989899, "width": 0.39717810457516345, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.20595202020202022, "width": 0.3998823529411766, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.21978914141414144, "width": 0.3558856209150326, "height": 0.012579545454545454, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4681"}, {"text": "Some of the error discrepancies could be explained by augmenting with finger identity information.", "label": "Conclusion", "bboxes": [{"left": 0.8854950980392157, "top": 0.21978914141414144, "width": 0.03634150326797381, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.23362626262626263, "width": 0.39717647058823546, "height": 0.012579545454545454, "page": 6}, {"left": 0.5240784313725491, "top": 0.24746338383838384, "width": 0.21595424836601307, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4682"}, {"text": "Others could be attributed to the addition of nger trajectory information.", "label": "Conclusion", "bboxes": [{"left": 0.7472728758169935, "top": 0.24746338383838384, "width": 0.17456045751633986, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246601307189542, "top": 0.26130050505050506, "width": 0.31519771241830075, "height": 0.012579545454545482, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": 0.5, "is_author_statement": false, "is_in_expected_section": false, "id": "4683"}, {"text": "We created a toy text entry application with an Oculus Rift and the Unity game engine to demonstrate real-time touch typing on a at surface in VR using hand-tracking (Figure 9).", "label": "Conclusion", "bboxes": [{"left": 0.5238986928104574, "top": 0.40036616161616156, "width": 0.39793790849673205, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4142032828282828, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.428040404040404, "width": 0.3542908496732027, "height": 0.012579545454545482, "page": 7}], "section": "Performance and interactive run-time", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4684"}, {"text": "Our system currently does not support typing of nondictionary words effectively.", "label": "Conclusion", "bboxes": [{"left": 0.13376470588235295, "top": 0.10913131313131313, "width": 0.3542352941176471, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.18003267973856213, "height": 0.01257954545454544, "page": 8}], "section": "CONCLUSION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "4685"}, {"text": "We will need to handle other keys, including backspace, numbers and symbols to support general interactive text editing.", "label": "Conclusion", "bboxes": [{"left": 0.4330833333333334, "top": 0.28901136363636365, "width": 0.052204248366013006, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.30284848484848487, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.3166856060606061, "width": 0.3296290849673203, "height": 0.012579545454545482, "page": 8}], "section": "CONCLUSION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": true, "id": "4686"}, {"text": "We also explore why decoding continuous hand motion data could be advantageous to decoding a discrete set of contacts on a touch surface by isolating the value finger trajectory information, finger identity information and continuous decoding.", "label": "Conclusion", "bboxes": [{"left": 0.3787679738562092, "top": 0.8339987373737374, "width": 0.10651633986928105, "height": 0.012579545454545427, "page": 1}, {"left": 0.10381209150326798, "top": 0.8478358585858585, "width": 0.3841862745098039, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8616729797979799, "width": 0.38088562091503275, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.875510101010101, "width": 0.38292320261437907, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8893472222222223, "width": 0.3436437908496732, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.5, "is_author_statement": true, "is_in_expected_section": false, "id": "4687"}, {"text": "Without the feel of physical keys and without using sight to nd the keys, fingers will drift during typing.", "label": "Future Work", "bboxes": [{"left": 0.36382843137254905, "top": 0.32423358585858586, "width": 0.12146078431372548, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.3380707070707071, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 1}, {"left": 0.08753267973856209, "top": 0.3519078282828283, "width": 0.15938398692810457, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "4688"}, {"text": "Instead of requiring users to make precise contacts on a fixed keyboard layout, we investigate using a motion model to recognize finger trajectories, and we further explore how statistical decoding techniques affect performance.", "label": "Future Work", "bboxes": [{"left": 0.25477124183006533, "top": 0.3519078282828283, "width": 0.23051633986928105, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811437908496732, "top": 0.3657449494949495, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811437908496732, "top": 0.37958207070707073, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.08753267973856209, "top": 0.3934179292929293, "width": 0.39776797385620916, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4072550505050505, "width": 0.08632679738562092, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4689"}, {"text": "Still, the remaining discrepancy in descriptive statistics between hand motion decoding UER and both baselines suggests that finger trajectory information is helpful for further disambiguating typed text.", "label": "Future Work", "bboxes": [{"left": 0.4286552287581699, "top": 0.33684217171717173, "width": 0.056637254901960865, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.35067803030303035, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.36451515151515157, "width": 0.39744934640522883, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.3783522727272727, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.3921893939393939, "width": 0.06903594771241832, "height": 0.012579545454545482, "page": 7}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4690"}, {"text": "Typing on any at surface without the need to bring a physical keyboard would provide a valuable addition to AR/VR interaction.", "label": "Future Work", "bboxes": [{"left": 0.5241601307189543, "top": 0.6354570707070707, "width": 0.3976830065359477, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246699346405229, "top": 0.6492941919191919, "width": 0.39988562091503266, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6631313131313132, "width": 0.029692810457516372, "height": 0.012579545454545427, "page": 7}], "section": "CONCLUSION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "4691"}, {"text": "Many challenges remain to making this system practical for general text input.", "label": "Future Work", "bboxes": [{"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39745588235294127, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.12283823529411775, "height": 0.012579545454545427, "page": 7}], "section": "CONCLUSION", "prob": null, "is_author_statement": false, "is_in_expected_section": true, "id": "4692"}, {"text": "These models were not optimized for compute and can be further accelerated using modern distillation and quantization techniques for neural networks [22].", "label": "Future Work", "bboxes": [{"left": 0.524156862745098, "top": 0.3513080808080808, "width": 0.397671568627451, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.3651439393939394, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.3789810606060606, "width": 0.23626960784313733, "height": 0.012579545454545427, "page": 7}], "section": "Performance and interactive run-time", "prob": null, "is_author_statement": false, "is_in_expected_section": false, "id": "4693"}, {"text": "For this work, we trained per-user neural motion models over an hours worth of typing samples and will need a more user-friendly version of user calibration.", "label": "Future Work", "bboxes": [{"left": 0.272921568627451, "top": 0.12296717171717171, "width": 0.21265686274509799, "height": 0.01257954545454544, "page": 8}, {"left": 0.08811928104575163, "top": 0.13680429292929294, "width": 0.39717483660130726, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.15064141414141416, "width": 0.4000326797385621, "height": 0.012579545454545427, "page": 8}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4694"}, {"text": "We will need to explore a more efcient user calibration process in future work.", "label": "Future Work", "bboxes": [{"left": 0.2021895424836601, "top": 0.21982702020202022, "width": 0.28337745098039213, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.23366414141414144, "width": 0.22885620915032684, "height": 0.012579545454545454, "page": 8}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4695"}, {"text": "We will need to handle other keys, including backspace, numbers and symbols to support general interactive text editing.", "label": "Future Work", "bboxes": [{"left": 0.4330833333333334, "top": 0.28901136363636365, "width": 0.052204248366013006, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.30284848484848487, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.3166856060606061, "width": 0.3296290849673203, "height": 0.012579545454545482, "page": 8}], "section": "CONCLUSION", "prob": null, "is_author_statement": true, "is_in_expected_section": true, "id": "4696"}, {"text": "Participants wore elastic mesh gloves with 19 retro-reflective motion capture markers afxed, and the method from Han and colleagues [12] was used to generate skeletal hand tracking information-specifically, joint angles and wrist transforms necessary to drive a fitted hand mesh.", "label": "Method", "bboxes": [{"left": 0.2857401960784314, "top": 0.15818939393939394, "width": 0.19954901960784316, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.17202651515151515, "width": 0.39921405228758167, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.18586363636363637, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.1997007575757576, "width": 0.3971797385620915, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.21353661616161618, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.03920261437908497, "height": 0.012579545454545454, "page": 4}], "section": "DATA COLLECTION", "prob": 0.8342400193214417, "is_author_statement": false, "is_in_expected_section": true, "id": "4697"}, {"text": "uses beam search with a language model to decode whole sentences at a time, although Velocitap uses contact as the input modality while our method uses hand-tracking.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.0814570707070707, "width": 0.3971732026143791, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.39717320261437916, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.34499509803921563, "height": 0.012579545454545454, "page": 2}], "section": "Statistical text decoding", "prob": 0.8088526725769043, "is_author_statement": true, "is_in_expected_section": true, "id": "4698"}, {"text": "We apply a beam search decoder to resolve these ambiguities using the language model as a prior.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.24098863636363635, "width": 0.39793464052287597, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.2548257575757576, "width": 0.2346584967320262, "height": 0.012579545454545427, "page": 2}], "section": "SYSTEM DESIGN", "prob": 0.5718343257904053, "is_author_statement": true, "is_in_expected_section": true, "id": "4699"}, {"text": "We show that we can combine our motion model with a language model, also represented as a neural network, using an efcient beam search decoding technique.", "label": "Method", "bboxes": [{"left": 0.08893300653594771, "top": 0.7385934343434344, "width": 0.39635130718954253, "height": 0.012856060606060593, "page": 1}, {"left": 0.10439869281045752, "top": 0.7527070707070708, "width": 0.3809003267973856, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.7665441919191919, "width": 0.2925294117647058, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "prob": 0.51229327917099, "is_author_statement": true, "is_in_expected_section": true, "id": "4700"}, {"text": "We believe the on-surface domain, while slightly less accessible, better leverages existing typing skills, allowing our method to achieve speeds comparable to typing on a keyboard at a low error rate.", "label": "Method", "bboxes": [{"left": 0.5668382352941176, "top": 0.6746982323232323, "width": 0.354998366013072, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6885353535353536, "width": 0.39745915032679746, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7023724747474748, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7162083333333333, "width": 0.11938888888888899, "height": 0.012579545454545538, "page": 1}], "section": "AR/VR text entry", "prob": 0.5083824396133423, "is_author_statement": true, "is_in_expected_section": true, "id": "4701"}, {"text": "We use a temporal neural network as a motion model.", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.30038383838383836, "width": 0.3616732026143793, "height": 0.012579545454545427, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "prob": 0.49971312284469604, "is_author_statement": true, "is_in_expected_section": true, "id": "4702"}, {"text": "To use finger identity information, we make two changes to our Gaussian contact decoding baseline.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.2764330808080808, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811274509803921, "top": 0.29027020202020204, "width": 0.2936797385620915, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": 0.4913274943828583, "is_author_statement": true, "is_in_expected_section": false, "id": "4703"}, {"text": "Finally we measure typing speed and error rates of our system against typing on a physical keyboard and contact-based statistical decoding methods.", "label": "Method", "bboxes": [{"left": 0.3077434640522876, "top": 0.3791590909090909, "width": 0.1775490196078432, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3929962121212121, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4068333333333333, "width": 0.37393954248366007, "height": 0.012579545454545482, "page": 2}], "section": "APPROACH", "prob": 0.4903002679347992, "is_author_statement": true, "is_in_expected_section": true, "id": "4704"}, {"text": "While the contact-based baselines can only be trained with known key/contact-point correspondences, our motion model is trained using CTC loss, which allows for sequence level labels and can deduce the frame-level alignment of the labels.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.7003484848484849, "width": 0.3979379084967321, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7141856060606061, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7280214646464646, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7418585858585859, "width": 0.4000261437908497, "height": 0.012579545454545427, "page": 4}], "section": "DATA COLLECTION", "prob": 0.46997445821762085, "is_author_statement": true, "is_in_expected_section": true, "id": "4705"}, {"text": "We first investigate touch typists ability to transfer their physical keyboard typing skills to typing on virtual keyboards imposed on at surfaces, and we then compare the performance of decoding text from skeletal hand-tracking data to decoding text from 2D surface contact points from a touchpad.", "label": "Method", "bboxes": [{"left": 0.3552418300653595, "top": 0.6785416666666666, "width": 0.13004411764705887, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.6923787878787879, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.706215909090909, "width": 0.39920261437908494, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7200530303030304, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7338901515151515, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7477272727272728, "width": 0.20571405228758172, "height": 0.012579545454545427, "page": 2}], "section": "APPROACH", "prob": 0.46114349365234375, "is_author_statement": true, "is_in_expected_section": true, "id": "4706"}, {"text": "In this work, we take the first steps to reducing this oracle to practice by building a neural model of text decoding that combines motion modeling of fingers and a state-of-the art language model.", "label": "Method", "bboxes": [{"left": 0.15742647058823528, "top": 0.4701502525252525, "width": 0.3278709150326798, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4839873737373737, "width": 0.3971797385620915, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.49782449494949493, "width": 0.39987418300653593, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.5116616161616161, "width": 0.12940849673202615, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.4504130482673645, "is_author_statement": true, "is_in_expected_section": true, "id": "4707"}, {"text": "First, we nd the closest fingertip f for each contact using hand-tracking data and then bucket by both key k and finger f to t 2D Gaussians h k , f ( x ) to these subsets of contact points.", "label": "Method", "bboxes": [{"left": 0.3928333333333333, "top": 0.29027020202020204, "width": 0.09245261437908497, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811274509803921, "top": 0.30381691919191917, "width": 0.3971862745098039, "height": 0.012869949494949517, "page": 6}, {"left": 0.08812581699346406, "top": 0.3176540404040404, "width": 0.39716830065359476, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.3314911616161616, "width": 0.3583349673202615, "height": 0.014039141414141476, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": 0.4462301731109619, "is_author_statement": true, "is_in_expected_section": false, "id": "4708"}, {"text": "We apply this same general framework, substituting recurrent neural networks with a TCN-based motion model, to decode text entry via typing.", "label": "Method", "bboxes": [{"left": 0.20009313725490194, "top": 0.24236363636363636, "width": 0.28789705882352945, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.25620075757575755, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.27003787878787877, "width": 0.25114869281045754, "height": 0.012579545454545427, "page": 2}], "section": "Automatic speech recognition", "prob": 0.4456369876861572, "is_author_statement": true, "is_in_expected_section": true, "id": "4709"}, {"text": "of keys can be processed using the same beam search decoder and language model we described for hand motion decoding.", "label": "Method", "bboxes": [{"left": 0.08811928104575163, "top": 0.0814570707070707, "width": 0.39745098039215687, "height": 0.012579545454545468, "page": 6}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3973970588235295, "height": 0.012579545454545468, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": 0.4187394380569458, "is_author_statement": true, "is_in_expected_section": false, "id": "4710"}, {"text": "We created a toy text entry application with an Oculus Rift and the Unity game engine to demonstrate real-time touch typing on a at surface in VR using hand-tracking (Figure 9).", "label": "Method", "bboxes": [{"left": 0.5238986928104574, "top": 0.40036616161616156, "width": 0.39793790849673205, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4142032828282828, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.428040404040404, "width": 0.3542908496732027, "height": 0.012579545454545482, "page": 7}], "section": "Performance and interactive run-time", "prob": 0.4040766954421997, "is_author_statement": true, "is_in_expected_section": true, "id": "4711"}, {"text": "We use three layers of residual blocks with 64, 64, and 32 hidden units respectively, a kernel size of 2 and a dilation factor of 3.", "label": "Method", "bboxes": [{"left": 0.875421568627451, "top": 0.5569974747474747, "width": 0.046419934640522964, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.570834595959596, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5846717171717172, "width": 0.36329738562091507, "height": 0.012579545454545538, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "prob": 0.38961076736450195, "is_author_statement": true, "is_in_expected_section": true, "id": "4712"}, {"text": "To filter samples where both an extra and a missing contact event occurred (leading to an agreement on counts but an error in contact-key sequence alignment), we build up a distribution for each key of how many times each finger hit that key.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.4148030303030303, "width": 0.39767156862745096, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4286401515151515, "width": 0.39745915032679735, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.4424772727272727, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.4563131313131313, "width": 0.37041176470588233, "height": 0.012579545454545482, "page": 4}], "section": "DATA COLLECTION", "prob": 0.36983561515808105, "is_author_statement": true, "is_in_expected_section": true, "id": "4713"}, {"text": "Instead of requiring users to make precise contacts on a xed keyboard layout, we investigate using a motion model to recognize finger trajectories, and we further explore how statistical decoding techniques affect performance.", "label": "Objective", "bboxes": [{"left": 0.25477124183006533, "top": 0.3519078282828283, "width": 0.23051633986928105, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811437908496732, "top": 0.3657449494949495, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811437908496732, "top": 0.37958207070707073, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.08753267973856209, "top": 0.3934179292929293, "width": 0.39776797385620916, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4072550505050505, "width": 0.08632679738562092, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "prob": 0.3620185852050781, "is_author_statement": true, "is_in_expected_section": true, "id": "4714"}, {"text": "To evaluate the feasibility of using hand-tracking to enable touch typing on virtual keyboards, we first collect a dataset of skeletal hand-tracking data from touch-typists transcribing short phrases while typing on a at surface.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.30997474747474746, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811274509803921, "top": 0.3238118686868687, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3376489898989899, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3514861111111111, "width": 0.27779411764705886, "height": 0.012579545454545427, "page": 2}], "section": "APPROACH", "prob": 0.3252009153366089, "is_author_statement": true, "is_in_expected_section": true, "id": "4715"}, {"text": "The problem of generating text from hand motion has strong analogs to automatic speech recognition (ASR), and we use ASR as motivation to design a multi-component system with the following three pieces.", "label": "Method", "bboxes": [{"left": 0.08761437908496732, "top": 0.7876641414141414, "width": 0.39767320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8015012626262626, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08753267973856209, "top": 0.8153383838383839, "width": 0.39776307189542476, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8291742424242424, "width": 0.17229411764705882, "height": 0.012579545454545427, "page": 2}], "section": "SYSTEM DESIGN", "prob": 0.3195696771144867, "is_author_statement": true, "is_in_expected_section": true, "id": "4716"}, {"text": "We propose a motion model, represented as a temporal convolutional network (TCN), that can translate hand motion directly into (a probability distribution over the) typed text.", "label": "Method", "bboxes": [{"left": 0.08893300653594771, "top": 0.6849772727272727, "width": 0.3990669934640523, "height": 0.01285479797979805, "page": 1}, {"left": 0.10399183006535948, "top": 0.6990896464646464, "width": 0.3812941176470588, "height": 0.012579545454545538, "page": 1}, {"left": 0.10439869281045752, "top": 0.7129267676767678, "width": 0.3835767973856209, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.29518747329711914, "is_author_statement": true, "is_in_expected_section": true, "id": "4717"}, {"text": "We have shown on a 20 person dataset that touchtypists can transfer their skills typing on a physical keyboard to typing on a at surface, reaching comparable typing speeds (73WPM) while retaining an uncorrected error rate of less than 2.4%.", "label": "Result", "bboxes": [{"left": 0.5835261437908497, "top": 0.7046414141414141, "width": 0.34101797385620913, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5241274509803922, "top": 0.7461527777777778, "width": 0.3977156862745098, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.759989898989899, "width": 0.038736928104575186, "height": 0.012579545454545427, "page": 7}], "section": "CONCLUSION", "prob": 0.860016942024231, "is_author_statement": true, "is_in_expected_section": true, "id": "4718"}, {"text": "Repeating the evaluation of the previous two baselines on this filtered corresponded contacts dataset, we found a substantial drop in the mean UER for all methods (Table 1), suggesting that removing spurious or missing key-presses makes for an easier dataset.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.6990530303030302, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.7128901515151516, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7267272727272727, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.740564393939394, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7544002525252526, "width": 0.0892271241830066, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": 0.8542827367782593, "is_author_statement": true, "is_in_expected_section": true, "id": "4719"}, {"text": "When we added the beam search language model decoder, however, we found that both Gaussian and Per Finger Gaussian contact-based methods performed comparably with a mean UERs of 5.66% (median 2.70%) and 4.76% (median 2.64%) respectively, while our motion model approach outperformed both with a mean UER of 2.38% (median 1.77%).", "label": "Result", "bboxes": [{"left": 0.31051633986928107, "top": 0.512919191919192, "width": 0.17477614379084966, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5267563131313131, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5405934343434343, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.5544305555555555, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5682676767676768, "width": 0.3974558823529412, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.582104797979798, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5959419191919192, "width": 0.16862581699346407, "height": 0.012579545454545538, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": 0.8525882959365845, "is_author_statement": true, "is_in_expected_section": true, "id": "4720"}, {"text": "When we attempted to train a single between-users model on the combined training sets of all users, the performance did not match the user specic models (UER rose from 2.4% to 3.9%, which a Wilcoxon signed rank test found to be signicant, p = 0 . 001).", "label": "Result", "bboxes": [{"left": 0.08735457516339869, "top": 0.16447853535353535, "width": 0.3979460784313726, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.19215277777777778, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.205989898989899, "width": 0.39987418300653593, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.21953787878787878, "width": 0.10915686274509805, "height": 0.01286868686868689, "page": 8}], "section": "CONCLUSION", "prob": 0.8370351195335388, "is_author_statement": true, "is_in_expected_section": true, "id": "4721"}, {"text": "When we examined the results qualitatively, we noted several differences between the contact-based decoding approach and our handtracking-based approach, which we illustrate in Table 2.", "label": "Result", "bboxes": [{"left": 0.837919934640523, "top": 0.17827777777777779, "width": 0.08662581699346406, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.192114898989899, "width": 0.39717810457516345, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.20595202020202022, "width": 0.3998823529411766, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.21978914141414144, "width": 0.3558856209150326, "height": 0.012579545454545454, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": 0.8095165491104126, "is_author_statement": true, "is_in_expected_section": true, "id": "4722"}, {"text": "(p = 0 . 011), indicating that factors beyond finger identity had a strong impact on performance.", "label": "Result", "bboxes": [{"left": 0.5241274509803922, "top": 0.08118055555555556, "width": 0.39771241830065374, "height": 0.012856060606060607, "page": 6}, {"left": 0.5246633986928104, "top": 0.09529419191919192, "width": 0.21111928104575162, "height": 0.012579545454545468, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "prob": 0.8051679134368896, "is_author_statement": false, "is_in_expected_section": true, "id": "4723"}, {"text": "based methods, which makes the differences between hand motion decoding and the two baselines no longer statistically signicant ( p = 0 . 300 for Gaussian and p = 0 . 109 for Per Finger Gaussian) according to a Wilcoxon signed rank test.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.26765656565656565, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.28149368686868687, "width": 0.3977385620915033, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.29504166666666665, "width": 0.39988071895424837, "height": 0.012868686868686918, "page": 7}, {"left": 0.08811928104575163, "top": 0.3091679292929293, "width": 0.3632418300653595, "height": 0.012579545454545482, "page": 7}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": 0.7984917759895325, "is_author_statement": false, "is_in_expected_section": true, "id": "4724"}, {"text": "While we are targeting a different sensing modality (hand-tracking instead of capacitive touchpads), we can use touchpads as a baseline for our work.", "label": "Result", "bboxes": [{"left": 0.16333823529411765, "top": 0.6508674242424242, "width": 0.32252777777777786, "height": 0.012579545454545538, "page": 2}, {"left": 0.08758169934640524, "top": 0.6647045454545455, "width": 0.39770424836601304, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811274509803921, "top": 0.6785416666666666, "width": 0.25703431372549024, "height": 0.012579545454545538, "page": 2}], "section": "APPROACH", "prob": 0.7451671957969666, "is_author_statement": true, "is_in_expected_section": false, "id": "4725"}, {"text": "This result is consistent with previous ndings on the performance envelopes of human typing [4] which described the high potential of typing speeds given a limited text decoding oracle.", "label": "Result", "bboxes": [{"left": 0.5241601307189543, "top": 0.8367222222222221, "width": 0.4003856209150327, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39718137254901964, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.04418954248366014, "height": 0.012579545454545427, "page": 4}], "section": "Physical keyboard versus virtual keyboard", "prob": 0.7385640144348145, "is_author_statement": false, "is_in_expected_section": false, "id": "4726"}, {"text": "Using the foot pedals participants discarded a median 15.89% of surface phrases compared to 12.5% of physical keyboard phrases; a Wilcoxon signed rank test found no signicance ( p = 0 . 328).", "label": "Result", "bboxes": [{"left": 0.34344281045751635, "top": 0.0814570707070707, "width": 0.14184967320261443, "height": 0.012579545454545468, "page": 4}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.39717320261437916, "height": 0.012579545454545468, "page": 4}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.3971683006535947, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.1226780303030303, "width": 0.3316960784313726, "height": 0.012868686868686849, "page": 4}], "section": "DATA COLLECTION", "prob": 0.7142394781112671, "is_author_statement": false, "is_in_expected_section": false, "id": "4727"}, {"text": "We present two evaluations of our proposed text entry method using the data collected from our user study.", "label": "Method", "bboxes": [{"left": 0.08735457516339869, "top": 0.8505593434343435, "width": 0.39794281045751634, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.2939950980392157, "height": 0.012579545454545538, "page": 4}], "section": "EVALUATION", "prob": 0.6739934682846069, "is_author_statement": true, "is_in_expected_section": true, "id": "4728"}, {"text": "better understanding which aspects of the motion model make it perform so well.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.35660353535353534, "width": 0.39718300653594774, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.37044065656565656, "width": 0.12026797385620924, "height": 0.012579545454545427, "page": 4}], "section": "EVALUATION", "prob": 0.6713749170303345, "is_author_statement": false, "is_in_expected_section": true, "id": "4729"}, {"text": "[26] use Gaussian Process regression to model key targets.", "label": "Result", "bboxes": [{"left": 0.7283970588235295, "top": 0.8367222222222221, "width": 0.19614705882352945, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.19323529411764706, "height": 0.012579545454545427, "page": 1}], "section": "Statistical text decoding", "prob": 0.6025448441505432, "is_author_statement": false, "is_in_expected_section": false, "id": "4730"}, {"text": "The ARKB system [18] showed a very similar concept of typing on a QWERTY virtual keyboard on a surface using hand-tracking through color-based segmentation and markers.", "label": "Result", "bboxes": [{"left": 0.707328431372549, "top": 0.37028409090909087, "width": 0.2145081699346406, "height": 0.012579545454545482, "page": 1}, {"left": 0.5242483660130719, "top": 0.3841212121212121, "width": 0.3975882352941176, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.3979583333333333, "width": 0.3998725490196079, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.4117954545454545, "width": 0.12444444444444458, "height": 0.012579545454545482, "page": 1}], "section": "AR/VR text entry", "prob": 0.5572448372840881, "is_author_statement": false, "is_in_expected_section": false, "id": "4731"}, {"text": "Graves and Jaitly [9] first applied CTC loss with a prex beam search decoder to speech recognition, and Hannun and colleagues [13] showed how to directly incorporate a language model.", "label": "Result", "bboxes": [{"left": 0.1524281045751634, "top": 0.2008522727272727, "width": 0.33286437908496735, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.21468939393939393, "width": 0.39988071895424837, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.22852651515151515, "width": 0.3971797385620915, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.24236363636363636, "width": 0.10688235294117647, "height": 0.012579545454545454, "page": 2}], "section": "Automatic speech recognition", "prob": 0.5217179656028748, "is_author_statement": false, "is_in_expected_section": false, "id": "4732"}, {"text": "Notably, our model is the first technique to our knowledge that converts skeletal hand motion directly into text.", "label": "Novelty", "bboxes": [{"left": 0.22256699346405232, "top": 0.5116616161616161, "width": 0.2627271241830066, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5254987373737373, "width": 0.39773692810457517, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5393358585858585, "width": 0.05965849673202615, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.504271924495697, "is_author_statement": true, "is_in_expected_section": true, "id": "4733"}, {"text": "We show that by using hand pose rather than contact points and a richer motion model, we can achieve more accurate decoding.", "label": "Result", "bboxes": [{"left": 0.595109477124183, "top": 0.27744444444444444, "width": 0.32942973856209146, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.29128156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.3051186868686869, "width": 0.12204248366013071, "height": 0.012579545454545427, "page": 1}], "section": "Surface and eyes-free keyboards", "prob": 0.4879584014415741, "is_author_statement": true, "is_in_expected_section": false, "id": "4734"}, {"text": "We show that these users can transfer their existing skills typing on a keyboard to typing on a at surface.", "label": "Result", "bboxes": [{"left": 0.3197581699346405, "top": 0.15064141414141416, "width": 0.16553594771241836, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.16447853535353535, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.17831565656565657, "width": 0.10742320261437908, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "prob": 0.448879212141037, "is_author_statement": true, "is_in_expected_section": true, "id": "4735"}, {"text": "3. 5 minutes of test set phrases on a touchpad", "label": "Result", "bboxes": [{"left": 0.521408496732026, "top": 0.8015025252525252, "width": 0.29804738562091515, "height": 0.012578282828282772, "page": 3}], "section": "DATA COLLECTION", "prob": 0.41693708300590515, "is_author_statement": false, "is_in_expected_section": false, "id": "4736"}, {"text": "We show that tracking hands typing on a at surface combined with our statistical decoding method has the potential of achieving speeds comparable to typing on a physical keyboard while maintaining low-error rates.", "label": "Result", "bboxes": [{"left": 0.08893464052287581, "top": 0.7922058080808081, "width": 0.39905555555555555, "height": 0.012861111111111101, "page": 1}, {"left": 0.10439869281045752, "top": 0.806324494949495, "width": 0.3809003267973856, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8201616161616161, "width": 0.38359640522875815, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.8339987373737374, "width": 0.26878921568627445, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "prob": 0.40057066082954407, "is_author_statement": true, "is_in_expected_section": true, "id": "4737"}, {"text": "Hand-tracking information was recorded while participants typed during all three blocks.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.14435227272727272, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.15818939393939394, "width": 0.1925751633986928, "height": 0.012579545454545454, "page": 4}], "section": "DATA COLLECTION", "prob": 0.39907699823379517, "is_author_statement": false, "is_in_expected_section": false, "id": "4738"}, {"text": "On the other hand, our motion model generates a probability distribution of which key was pressed (including no-key) at every frame of the tracking, which provides the decoder exibility to insert or remove key-presses to accommodate the language model prior (Figure 7).", "label": "Result", "bboxes": [{"left": 0.6378709150326798, "top": 0.5040770202020202, "width": 0.286673202614379, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5179141414141414, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 6}, {"left": 0.5241274509803922, "top": 0.5317512626262626, "width": 0.40041666666666675, "height": 0.012579545454545427, "page": 6}, {"left": 0.5242565359477125, "top": 0.5455883838383838, "width": 0.3975751633986927, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5594242424242425, "width": 0.33138725490196086, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "prob": 0.38019001483917236, "is_author_statement": true, "is_in_expected_section": true, "id": "4739"}, {"text": "In our first experiment we study the contribution of finger identity information.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.5267979797979798, "width": 0.3974558823529412, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.540635101010101, "width": 0.138047385620915, "height": 0.012579545454545538, "page": 5}], "section": "Comparison of Input Features and Motion Models", "prob": 0.35905739665031433, "is_author_statement": true, "is_in_expected_section": false, "id": "4740"}, {"text": "containing the prediction of which key was hit at which time (Figure 2).", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.3640681818181818, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 3}, {"left": 0.08758169934640524, "top": 0.377905303030303, "width": 0.07153104575163398, "height": 0.012579545454545482, "page": 3}], "section": "MAPPING SKELETAL MOTION TO TEXT", "prob": 0.3551461398601532, "is_author_statement": false, "is_in_expected_section": false, "id": "4741"}, {"text": "For decoding hand motion into text, we make no such concessions and investigate the accuracy we can achieve using practical language models and statistical decoding strategies presented in this work.", "label": "Result", "bboxes": [{"left": 0.08811928104575163, "top": 0.574135101010101, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5879722222222222, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.6018093434343434, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6156464646464647, "width": 0.14813725490196078, "height": 0.012579545454545538, "page": 2}], "section": "APPROACH", "prob": 0.3506585359573364, "is_author_statement": true, "is_in_expected_section": false, "id": "4742"}, {"text": "In our study we collected data by having participants complete a text transcription task for short phrases.", "label": "Result", "bboxes": [{"left": 0.5246633986928104, "top": 0.31209974747474745, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.32593686868686866, "width": 0.27559803921568626, "height": 0.012579545454545482, "page": 3}], "section": "DATA COLLECTION", "prob": 0.3475267291069031, "is_author_statement": true, "is_in_expected_section": false, "id": "4743"}]}