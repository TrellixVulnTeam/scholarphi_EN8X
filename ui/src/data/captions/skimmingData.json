{"uist-8": [{"type": "figure", "text": "Figure 2. (a) Eye cursor movement, dotted lines indicate effective bound- ary of sub-keyboards (b) Magni\ufb01ed view, activated sub-keyboard with labeling of GUI components; The user input view on an activated sub- keyboard only show a last 6 letters of user inputted.", "bbox": {"left": 0.5257137024324704, "top": 0.36186665236347854, "width": 0.39946048711639603, "height": 0.04555299546983507, "page": 2}, "id": "324"}, {"type": "figure", "text": "the target size for eye pointing on HWD, i.e. , 7\u00b0 of width. The actual viewing angle varies with distance from the monitor; however, we attempted to set the keyboard size as: keyboard width: 21.6\u00b0 and sub-keyboard width: 6.85\u00b0, at a distance of 65 cm. A participant was seated at a distance of approximately 65 cm from the 27 inch monitor (1920 \u00d7 1080 px, viewing angle: 54\u00b0 \u00d7 30\u00b0 at the distance, as shown in Figures 3).", "bbox": {"left": 0.5258782081354677, "top": 0.2532619129527699, "width": 0.39819595237183414, "height": 0.09566617252850773, "page": 3}, "id": "325"}, {"type": "figure", "text": "Figure 3. Simulated environment for preliminary study and Experiment 1. (a) instructor view, (b) built-in touchpad, (c) participant-side view.", "bbox": {"left": 0.5244972877253115, "top": 0.20530359672777582, "width": 0.3953847947463491, "height": 0.02303520356765901, "page": 3}, "id": "326"}, {"type": "figure", "text": "Figure 6. The sub-keyboards layouts of the GAT variations in Experi- ment 1. Keys are placed in an alphabetical order. (a), (c), and (e) are the layouts of GAT3, GAT6, and GAT9, respectively. (b), (d), and (f) show the sets of touch gestures used for GAT3, GAT6, and GAT9, respectively, with the corresponding keys of the activated sub-keyboards.", "bbox": {"left": 0.5265658509497549, "top": 0.8462903571851326, "width": 0.39913711049198325, "height": 0.056562943892045456, "page": 4}, "id": "327"}, {"type": "figure", "text": "Figure 4. Text entry speed of each technique, per block. Error bars mean standard errors. Asterisks mean signi\ufb01cant difference.", "bbox": {"left": 0.08851060680314607, "top": 0.1760584128023398, "width": 0.39904280893163746, "height": 0.022810752945716935, "page": 4}, "id": "328"}, {"type": "figure", "text": "Figure 5. Corrected error rate (CER) and uncorrected error rate (UER) of each technique/block. Error bars indicate standard errors.", "bbox": {"left": 0.09059631123262293, "top": 0.3300745029642124, "width": 0.40141725851819404, "height": 0.02327762950550426, "page": 4}, "id": "329"}, {"type": "figure", "text": "Figure 8. Text entry speed of each technique, per block. Error bars indicate standard errors.", "bbox": {"left": 0.5250794155145783, "top": 0.2002146364462496, "width": 0.3966982473734937, "height": 0.023219927392824732, "page": 5}, "id": "330"}, {"type": "figure", "text": "Figure 7. (a) Visual guidance condition example with GAT6, when a target key is \"o\". (b) Guidance is removed during the touch input mode.", "bbox": {"left": 0.08868302862628613, "top": 0.18273128162730823, "width": 0.3982766406988007, "height": 0.022995785029247554, "page": 5}, "id": "331"}, {"type": "figure", "text": "Figure 9. Corrected error rate (CER) and uncorrected error rate (UER) of each technique/block. Error bars indicate standard errors.", "bbox": {"left": 0.5245152392418556, "top": 0.3582599138972735, "width": 0.3967997731726154, "height": 0.024208993622750946, "page": 5}, "id": "332"}, {"type": "figure", "text": "Figure 10. Cumulated rating counts on the questionnaire for each layout condition in Experiment 1. A: Easy to learn, B: Easy to use, C: Prefer to use, and D: Eye feels natural. Annotated values on the bars indicate the mean values on each option.", "bbox": {"left": 0.526764663995481, "top": 0.18381240151145242, "width": 0.39539825838375714, "height": 0.04364056057400174, "page": 6}, "id": "333"}, {"type": "figure", "text": "Figure 11. Four decomposed parts from the key input time \u2212 (a) T1, (b) T2, (c) T3, and (d) T4 \u2212 and (e) SWITCH.", "bbox": {"left": 0.5245712878657323, "top": 0.4618486731943458, "width": 0.39788783453648385, "height": 0.02263706862324416, "page": 6}, "id": "334"}, {"type": "figure", "text": "Figure 12. Experimental set-up for Experiment 2 (a) Eye tracking head- worn display (HWD) environment, FOVE VR. Google Glass is attached on the side of FOVE VR to use the touchpad. (b) Participant view in GAT session (c) Captured eye image while conducting the experiment.", "bbox": {"left": 0.09026546104281556, "top": 0.2528468237982856, "width": 0.39611781500523385, "height": 0.04565005832248264, "page": 7}, "id": "335"}, {"type": "figure", "text": "Figure 13. GUIs for the techniques (a) GAT in the phrase memorize phase. (b) Red box means required minimum \ufb01eld-of-view for GAT. (c) SwipeZone, touch-only, in the breaking phase inter tasks. (d) A-Dwell, eye-only, in the typing phase, and (e) in the dwell time adjustment phase.", "bbox": {"left": 0.08843692455416412, "top": 0.4862742183184383, "width": 0.4017520231359145, "height": 0.044879874797782514, "page": 7}, "id": "336"}, {"type": "figure", "text": "Figure 16. Cumulated rating counts on the questionnaire for each tech- nique in Experiment 2. A: Easy to learn, B: Easy to use, C: Prefer to use, D: Eye feels natural, E: Eye Fatigue (Red means increasing fatigue). Annotated values on the bars indicate the mean values on each option.", "bbox": {"left": 0.5272340462877859, "top": 0.8589788880011048, "width": 0.3985955730762357, "height": 0.043779623628866796, "page": 8}, "id": "337"}, {"type": "figure", "text": "Figure 15. CER and UER of each technique/block. Error bars indicate standard errors. The annotated values indicate average CERs and UERs at the last block of each techniques.", "bbox": {"left": 0.5270766214607587, "top": 0.38229100391118215, "width": 0.3942631241542841, "height": 0.0342827613907631, "page": 8}, "id": "338"}, {"type": "figure", "text": "Figure 14. Text entry speed of each technique/block. Error bars indicate standard errors. The annotated values indicate average text entry speeds at the last block of each techniques.", "bbox": {"left": 0.5258273455052594, "top": 0.2114382464476306, "width": 0.3948168785743464, "height": 0.0330738876805161, "page": 8}, "id": "339"}], "uist-4": [{"type": "figure", "text": "Figure 1. (A) An overview of Lip-Interact. (B) A user uses Lip-Interact to take a screenshot while holding a cup of coffee. Lip-Interact can also work closely with touch to make interaction more \ufb02uent, e.g. (C) adjust- ing the cursor and bolding text while typing.", "bbox": {"left": 0.5311177072961346, "top": 0.5729261841436829, "width": 0.38962584850834864, "height": 0.04704519714971985, "page": 0}, "id": "340"}, {"type": "figure", "text": "Figure 2. Lip-Interact is context-aware. In different using states, Lip- Interact uses different recognition models and supports different sets of functionalities (commands).", "bbox": {"left": 0.5270158854964512, "top": 0.21998372704091698, "width": 0.3937275705773846, "height": 0.03399201595421993, "page": 2}, "id": "341"}, {"type": "figure", "text": "Figure 3. (a) For novice to Lip-Interact, red prompt windows are dis- played to guide users to use the technique. (b) When a user is issuing command with Lip-Interact, a blue circle appears at the top of the screen as real-time visual feedback. This \ufb01gure also shows part of the GUIs of the two selected applications in our evaluation: (a) WeChat , (b) Notepad .", "bbox": {"left": 0.08953149334277982, "top": 0.3467581777861624, "width": 0.3979949951171875, "height": 0.05630419952700837, "page": 3}, "id": "342"}, {"type": "figure", "text": "Figure 4. The command set covers 44 frequently-used functionalities.", "bbox": {"left": 0.5319598827486723, "top": 0.32972513063989506, "width": 0.38344130173228147, "height": 0.011619760532571811, "page": 3}, "id": "343"}, {"type": "table", "text": "Table 1. Model hyperparameters. N is the size of the command set.", "bbox": {"left": 0.5391092736736621, "top": 0.5762232770823469, "width": 0.3669690898820466, "height": 0.011765104351621685, "page": 4}, "id": "344"}, {"type": "figure", "text": "Figure 6. Architecture of the Lip-Interact recognition model.", "bbox": {"left": 0.5532112869561887, "top": 0.3778792101927478, "width": 0.337211559021395, "height": 0.012223368943339646, "page": 4}, "id": "345"}, {"type": "figure", "text": "Figure 5. (A) Our device and the experimental setup for Study 1 and Study 2. (B) Lip landmarks, mouth Opening Degree, and mouth cropping. (C) An online sliding window algorithm is used to segment mouth image sequence of issuing Lip-Interact command.", "bbox": {"left": 0.091747165505403, "top": 0.1999764105286261, "width": 0.823823062423008, "height": 0.023703392105873185, "page": 4}, "id": "346"}, {"type": "figure", "text": "Figure 7. A counter example of Lip-Interact: audio signal sampled from the device microphone when a user vocalizes \" Open WeChat \" with four signi\ufb01cant syllables.", "bbox": {"left": 0.08879894057130502, "top": 0.19456229547057488, "width": 0.39856688181559247, "height": 0.03403476753620186, "page": 5}, "id": "347"}, {"type": "", "text": "STUDY 1: EVALUATE LIP-INTERACT RECOGNITION", "bbox": {"left": 0.087483786289988, "top": 0.4731397532453441, "width": 0.3585085775337967, "height": 0.013652917110558712, "page": 5}, "id": "348"}, {"type": "table", "text": "Table 2. Classi\ufb01cation accuracies of four command groups with leave- one-subject cross-validation", "bbox": {"left": 0.5278926674836601, "top": 0.8616722954644097, "width": 0.3910917394301471, "height": 0.02879125421697443, "page": 5}, "id": "349"}, {"type": "figure", "text": "Figure 8. In our simulation, the classi\ufb01cation accuracy of Lip-Interact improved signi\ufb01cantly as the number of uses increased.", "bbox": {"left": 0.09027580186432484, "top": 0.25974907537903447, "width": 0.39402125863467946, "height": 0.022450919103140784, "page": 6}, "id": "350"}, {"type": "table", "text": "Table 4. Interaction Easiness in three input conditions in Study 2.", "bbox": {"left": 0.5445188135882608, "top": 0.2935631106598209, "width": 0.3606664620193781, "height": 0.012132663919468118, "page": 7}, "id": "351"}, {"type": "table", "text": "Table 3. Mean input time (SD) in seconds of different types of tasks in three input conditions in Study 2.", "bbox": {"left": 0.5272980234981363, "top": 0.19795739530312895, "width": 0.39250975964116114, "height": 0.023060056898328993, "page": 7}, "id": "352"}, {"type": "figure", "text": "Figure 10. Study 3 was conducted on a subway", "bbox": {"left": 0.1554140203139361, "top": 0.2512132085935034, "width": 0.26051705802967345, "height": 0.012242943349510732, "page": 8}, "id": "353"}, {"type": "table", "text": "Table 5. Participant ratings (SD) of subjective experience when using Lip-Interact and general voice control.", "bbox": {"left": 0.5281104792177288, "top": 0.1410519185692373, "width": 0.3885389340469261, "height": 0.024518138230449023, "page": 8}, "id": "354"}], "uist-5": [{"type": "figure", "text": "Figure 2: End highlights for first and last stairs. (a) Initial thick highlight with bright yellow; (b) Flashing Edge: the highlight switches between thick (b1) and thin (b2); (c) Moving Edge; (d) Moving Horizontal Zebra; (e) Moving Vertical Zebra.", "bbox": {"left": 0.10729243708591835, "top": 0.15448207084578697, "width": 0.784289578207178, "height": 0.025349462875212083, "page": 3}, "id": "355"}, {"type": "figure", "text": "Figure 3. Middle highlights: (a) Initial thin highlights with bright yellow; (b) Dull Yellow Highlights; (c) Blue Highlights.", "bbox": {"left": 0.5280907325495302, "top": 0.259490966796875, "width": 0.37268874224494486, "height": 0.026829266788983584, "page": 3}, "id": "356"}, {"type": "table", "text": "Table 1. Participant demographic information. Participants labeled with superscript \u20181\u2019 were in the study for projection-based AR, while those labeled with superscript \u20182\u2019 were in the study for smartglasses.", "bbox": {"left": 0.10273311340730953, "top": 0.38823330041133997, "width": 0.8019201989267387, "height": 0.025300921815814392, "page": 4}, "id": "357"}, {"type": "figure", "text": "Figure 4: Diverging bars that demonstrate the distribution of participant scores (strongly negative 1 to strongly positive 7) for usefulness, comfort level, and psychological security when using visualizations on projection-based AR. We label the mean and SD under each category.", "bbox": {"left": 0.11437888550602533, "top": 0.19619275102711686, "width": 0.7740451588350183, "height": 0.03833264052265822, "page": 5}, "id": "358"}, {"type": "figure", "text": "Figure 5. (a) The visual effect of adding highlights to stairs with HoloLens. (b) A user stares down to see the highlights.", "bbox": {"left": 0.5293974034926471, "top": 0.8613231158015704, "width": 0.37446703941993464, "height": 0.025313945731731378, "page": 6}, "id": "359"}, {"type": "figure", "text": "Figure 7: Glow (a\u2013d) and Path (e\u2013g). Glow: (a) thin red glow on the landing; (b) thick cyan glow in the preparation area; (c) thick yellow glow in the alert area; (d) thin blue glow on the middle of the stairs. Path: (e) view of the Path on the landing; (f) view of the Path when getting close to the first stair; (g) view of the Path on the middle of the stairs.", "bbox": {"left": 0.09453304141175513, "top": 0.8658866497001263, "width": 0.8231001149595173, "height": 0.03833107996468592, "page": 7}, "id": "360"}, {"type": "figure", "text": "Figure 6: The seven stages of the stairs.", "bbox": {"left": 0.15775198094985066, "top": 0.18333673958826546, "width": 0.2470042035470601, "height": 0.014237567631885259, "page": 7}, "id": "361"}, {"type": "figure", "text": "Figure 8: Distribution of participants\u2019 preferences for visualizations and sonification on HoloLens.", "bbox": {"left": 0.1976219875360626, "top": 0.8896102134627525, "width": 0.6211542615703508, "height": 0.012197012853140782, "page": 9}, "id": "362"}, {"type": "figure", "text": "Figure 9: Diverging bars that demonstrate the distribution of participant scores (strongly negative 1 to strongly positive 7) for the usefulness and comfort level of the visualizations, and their psychological security in three conditions: without HoloLens, with Ho- loLens but no visualizations, and with visualizations. We label the mean and SD under each category.", "bbox": {"left": 0.09720824746524587, "top": 0.22936570523965238, "width": 0.815103181826523, "height": 0.037258726177793564, "page": 10}, "id": "363"}, {"type": "", "text": "ferred visualizations on HoloLens. The x -axis represents each stair, while the y -axis represents the angle between the partici- pant\u2019s gaze direction and the horizontal surface. When the participant looks up (down), the angle is positive (negative).", "bbox": {"left": 0.09939941082125396, "top": 0.22510486178927952, "width": 0.8104065851448408, "height": 0.0388052776606396, "page": 11}, "id": "364"}], "uist-2": [{"type": "figure", "text": "Figure 1. Our Wearable Subtitles proof-of-concept shows how eyewear could benefit people who are deaf or hard of hearing. We explore hands-free access to spoken communication, situational and speaker awareness, and improved understanding while engaged in a primary task. Our lightweight (54 g) 3D-printed eyewear prototype augments the user\u2019s perception of speech and sounds in a socially acceptable form factor with an architecture that could enable up to 15 hours of continuous transcription.", "bbox": {"left": 0.08799346599703521, "top": 0.38033259998668323, "width": 0.8269595289542004, "height": 0.051259744046914454, "page": 0}, "id": "365"}, {"type": "figure", "text": "Figure 2. The 501 survey respondents who used hearing- related assistive technology (AT) indicated that activities involving eating in public and multiple speakers were likely to result in situations where it would be difficult to hear.", "bbox": {"left": 0.520478790881587, "top": 0.3828644800667811, "width": 0.3910061705346201, "height": 0.05071559096827651, "page": 2}, "id": "366"}, {"type": "figure", "text": "Figure 3. Left: Eyewear/phone/cloud thin-client system diagram. Right: System architecture.", "bbox": {"left": 0.5144624897077972, "top": 0.22803071532586608, "width": 0.3972529492347069, "height": 0.0243433750036991, "page": 3}, "id": "367"}, {"type": "figure", "text": "Figure 4. The 8-layer FR4 PCB assembly of our eyewear electronics with wireless communication and display.", "bbox": {"left": 0.5240079593035131, "top": 0.872103912661774, "width": 0.3909328685087316, "height": 0.025580974540325128, "page": 3}, "id": "368"}, {"type": "figure", "text": "Figure 6. Participants reported a range of challenges with the first version of the system. The most common reported challenges were line of sight, legibility of text, and connectivity.", "bbox": {"left": 0.09030957938798892, "top": 0.263833806972311, "width": 0.3889071021983826, "height": 0.037942693691061, "page": 5}, "id": "369"}, {"type": "", "text": "STUDY 1: IN-THE-WILD USE (THREE DAYS)", "bbox": {"left": 0.08599661534128625, "top": 0.4346425605542732, "width": 0.3088254990920522, "height": 0.014016623448843907, "page": 5}, "id": "370"}, {"type": "figure", "text": "Figure 7. Our modular nose bridge provides independent IPD and wrap adjustments. A1/A2: Reducing nose bridge width addresses display visibility. B1/B2: Adjusting nose bridge with and without wrap rotates the eyebox into a central position.", "bbox": {"left": 0.09169698852339601, "top": 0.6812143807459359, "width": 0.39036590601104537, "height": 0.05253130980212279, "page": 6}, "id": "371"}, {"type": "figure", "text": "Figure 8. 3D-printed modular nose bridges facilitate personalization for display visibility for small eyebox displays, and maximize prototype reusability.", "bbox": {"left": 0.08929715748705895, "top": 0.8671387951783459, "width": 0.4002977944667043, "height": 0.03945853493430398, "page": 6}, "id": "372"}, {"type": "figure", "text": "Figure 9. Left: The prototype eyewear was rated more discreet for on-the-go use than phone-based transcription. Center: The prototype eyewear was rated higher for environmental and speaker awareness. Right: The updated v2 design was rated favorable along all dimensions, suggesting that important challenges may have been addressed.", "bbox": {"left": 0.09449946958255144, "top": 0.869661812830453, "width": 0.8051555421617296, "height": 0.03946169458254419, "page": 7}, "id": "373"}, {"type": "figure", "text": "Figure 11. Energy efficiency of data transmission in various Bluetooth usage modes.", "bbox": {"left": 0.09062022788851869, "top": 0.8754415801077178, "width": 0.390005099228005, "height": 0.02513384578203914, "page": 8}, "id": "374"}, {"type": "figure", "text": "Figure 10. Left: Measured power numbers vs. predicted worst case values from the datasheets. Right: Battery life vs display brightness for display contrast under continuous usage.", "bbox": {"left": 0.09437915702271306, "top": 0.2603716416792436, "width": 0.8109274907828936, "height": 0.025802535240096274, "page": 8}, "id": "375"}], "uist-3": [{"type": "figure", "text": "Figure 1. With LightAnchors, a security camera\u2019s LED can be used to share its privacy policy, while a hot glue gun trans- mits its live temperature (close-up of screen in B).", "bbox": {"left": 0.5238262001985039, "top": 0.42132865058051217, "width": 0.38162166620391647, "height": 0.03946693497474747, "page": 0}, "id": "376"}, {"type": "figure", "text": "Figure 2. Light intensity over time for an example LightAn- chor with 6-bit preamble (101100), 10-bit payload (0010101100) and 6-bit \u201cpostamble\u201d (i.e., preamble of next transmission). Dashed line is interpolated binary threshold.", "bbox": {"left": 0.5275743771222682, "top": 0.268777654628561, "width": 0.375887453166488, "height": 0.05274474018751973, "page": 2}, "id": "377"}, {"type": "figure", "text": "Figure 4. Bit error rate (BER) across different study conditions and distances.", "bbox": {"left": 0.2563215766856873, "top": 0.8948750351414536, "width": 0.48705855226205064, "height": 0.012913019970209912, "page": 3}, "id": "378"}, {"type": "", "text": "Bit error rate (BER) results are computed from data collected in our main evaluation.", "bbox": {"left": 0.2245166254978554, "top": 0.2422856417569247, "width": 0.5561007331399357, "height": 0.02643232634573272, "page": 3}, "id": "379"}, {"type": "figure", "text": "Figure 6. Example uses of LightAnchors with dynamic data payloads. Left: Smoke alarm that displays its real-time battery and alarm status. Center: Power strip that transmits its power usage. Right: WiFi Router displaying its SSID and guest password.", "bbox": {"left": 0.10392948225432751, "top": 0.8807081742720171, "width": 0.798695009518293, "height": 0.026755670104363954, "page": 4}, "id": "380"}, {"type": "figure", "text": "Figure 5. Example LightAnchor applications with fixed data payloads. Left: Parking meter displaying current rate. Center: Exterior light fixture denoting building operating hours. Right: Conference speaker phone displaying call-in number.", "bbox": {"left": 0.1104078604504953, "top": 0.2827289273040463, "width": 0.7906794828527114, "height": 0.0262460612287425, "page": 4}, "id": "381"}, {"type": "figure", "text": "Figure 7. Example LightAnchors that use data payloads for connection information. Left: Light switch that offers remote control. Center: Thermostat that allows users to configure settings. Right: Terminal that permits payment via smartphone.", "bbox": {"left": 0.09626209034639246, "top": 0.2721924059318774, "width": 0.8149018381156173, "height": 0.02525556930387863, "page": 5}, "id": "382"}], "2104.03820": [{"type": "figure", "text": "Fig. 1. UX Variants. Input Java source code was shown in the left pane (A). Clicking \u201cTranslate Now\u201d translated the code into Python in the right pane, in one of three variants: code translation only (not shown), code translation with low-confidence tokens highlighted in red (B), or code translation with alternate code translations displayed in pop-up menus (C).", "bbox": {"left": 0.1819938958859911, "top": 0.30708567301432294, "width": 0.7013991362129162, "height": 0.0357018287735756, "page": 3}, "id": "383"}], "uist-0": [{"type": "figure", "text": "Figure 1: We present a photography guidance tool implemented as a capture-time app. Our tool allows the user to analyze the current clutter in the scene, and adjust until they achieve an image with less clutter. Here we show a sequence of overlays that the user sees as they make intermediate adjustments from their initial framing to achieve their fnal image. For the initial and fnal images, we show the overlay on the left and the corresponding photo on the right. The user notices the corner of the painting peeking into the photo in the initial frame, shifts the camera to the left to remove it from frame, but still notices the additional clutter caused by the strong line on the shelf in the foreground of the image in the intermediate frame. The user repositions the plant to the corner of the shelf to achieve the fnal image with a clean background.", "bbox": {"left": 0.09060028175902522, "top": 0.48188824123806423, "width": 0.824569427889157, "height": 0.09301461113823785, "page": 0}, "id": "384"}, {"type": "figure", "text": "Figure 2: Two pairs of photos (top) from two participants from our WoZ prototype: the initial photo and the fnal photo upon seeing the photo with the transparency overlay (below). Left to right: (a) The participant notices the clutter on the bookshelves, and rotates the camera and changes per- spective to focus attention more on the action of the subject entering the ofce. (b) The participant notices the clutter on the tables and wall in the background, and tilts the camera angle down to focus attention on the action of the subject writing on the paper.", "bbox": {"left": 0.08784549688202103, "top": 0.26407430629537565, "width": 0.3955085106145323, "height": 0.13735819344568734, "page": 3}, "id": "385"}, {"type": "figure", "text": "Figure 3: A range of transparencies overlaid on phone as shown to participants saw during the review step of the WoZ prototype study. Transparencies illustrate a basic abstrac- tion of the image through rough outlining.", "bbox": {"left": 0.52141172122332, "top": 0.1846547560258345, "width": 0.39126372493170447, "height": 0.054974777529938054, "page": 3}, "id": "386"}, {"type": "figure", "text": "Figure 5: Two sets of example images addressing the two decluttering principles. For each set, we show the original photo that has poor SBS (top) or distracting IBF (bottom), the outlined original with the subject outlined where there is good contrast with the background, and an updated photo where the decluttering principle is addressed. On the left, the texture and color of the origami swan blends in with the blinds in the background, creating unclear SBS in the origi- nal photo. However, against the darker backdrop in the up- dated photo, the outline of the subject becomes much clearer. On the right, the bottle and dark corner of the chair against the light wall along the left border of the original photo cause distracting IBF. Adjusting the camera angle removes the distractors from frame in the updated photo, and even improves the SBS.", "bbox": {"left": 0.5205341912562551, "top": 0.39428159925672746, "width": 0.39423978718277675, "height": 0.20457061613448943, "page": 4}, "id": "387"}, {"type": "figure", "text": "Figure 4: A few methods employed by photographers for highlighting contrast as applied on this painting by Emily Friant (left). Left to right: Original painting; A blurred and higher contrast version of the image, a representation of what it might look like to squint at the image; A grayscale version to focus on contrast without aspects of color; A recre- ation of Glover\u2019s outline annotation such that areas where the contrast is possibly too low between the subject and background are shown as gaps [20]. Painting by Emily Friant. Public Domain.", "bbox": {"left": 0.08783193351396548, "top": 0.23480492408829506, "width": 0.3941323274101307, "height": 0.11297936873002486, "page": 4}, "id": "388"}, {"type": "figure", "text": "Figure 6: Overlay options as presented to design survey participants for videos. Alongside the video with the set of overlays, we showed still frames of the beginning, middle, and end of the video with the same overlays. This video clip aims to show an example of someone initially noticing clutter (outlet) in the background (top) and moving the object to a cleaner background for the fnal photo (bottom).", "bbox": {"left": 0.08825332666534225, "top": 0.6165744126445115, "width": 0.8255501728431851, "height": 0.05299435239849669, "page": 5}, "id": "389"}, {"type": "table", "text": "Table 1: Breakdown of design survey results: count for SBS, IBF, and total per overlay and overall. Since participants were allowed to pick up to 3 overlay choices per question, the weighted totals are computed by dividing a vote for an overlay by the total number of choices selected for that par- ticular question.", "bbox": {"left": 0.08840383267870136, "top": 0.10661984453297625, "width": 0.3965180901920094, "height": 0.08086809485849708, "page": 6}, "id": "390"}, {"type": "figure", "text": "Figure 7: Here we show a few states of the decluttering vi- sualization on the phone. Left to right: The default visual- ization shows the edges around the detected subject border as well as around the image border over an opaque black background. The user can choose to reduce the opacity of the background (here the user adjusts the opacity from 1.0 to 0.4) to show some of the camera view. The user can also toggle on all edges to see edges within the subject and back- ground as well. Finally, the user can either use the bottom toggle or touch anywhere on the screen to hide the visual- ization completely.", "bbox": {"left": 0.08915948556139579, "top": 0.26062936493844696, "width": 0.39445117875641467, "height": 0.1530945517800071, "page": 7}, "id": "391"}, {"type": "figure", "text": "Figure 8: The breakdown of components of our abstraction overlay algorithm. On the left we have the original image. Next, we show results of computing the edges for the overall line drawing, and the object-based saliency map for location context. The next set of images are the intermediates for generating the overlay. On the top, we have the masks based on location context, and on the bottom, the edges masked and color-coded appropriately based on the context: yellow for subject and subject border, cyan for image border, and white for background. At the far right we have the two versions of the fnal abstraction overlay: showing the subset of edges relevant to SBS and IBF, and then showing the full color-coded line drawing.", "bbox": {"left": 0.09051189547270731, "top": 0.36054599646366003, "width": 0.8257236356049581, "height": 0.08384222936148596, "page": 8}, "id": "392"}, {"type": "figure", "text": "Figure 9: Participants were asked to choose their own sub- jects for each task. Here are a few example participant pho- tos at each task scale: (1) small, (2) medium, and (3) large.", "bbox": {"left": 0.0880535474789688, "top": 0.21118711221097697, "width": 0.3985525767008464, "height": 0.04202781060729364, "page": 9}, "id": "393"}, {"type": "figure", "text": "Figure 11: Frames from a participant using the tool\u2014on the left are a set of intermediate steps, on the right is the fnal overlay that the participant saw as they took a photo and the resulting fnal photo. Left to right: (a) The participant frames a photo of this plant on the counter, but noticed that the strong edge at the bottom of the pot provides extra un- wanted contrast and separation. (b) Attempting to move this edge out of frame, the participant tries to reframe the image with a diferent background, but doesn\u2019t like the dark wall in the background either. (c) The participant turns the cam- era towards the other direction to continue exploring the background, but notices the thermostat highlighted as it en- ters the frame at the top left. She keeps the initial edge, but frames a tighter shot of the plant to remove the high con- trast thermostat screen from the background.", "bbox": {"left": 0.5210856518714256, "top": 0.24087769094139638, "width": 0.3942468181934232, "height": 0.2046826487839824, "page": 10}, "id": "394"}, {"type": "figure", "text": "Figure 10: Frames from a participant using the tool\u2014on the left are a set of intermediate steps, on the right is the fnal overlay that the participant saw as they took a photo and the resulting fnal photo. Left to right: (a) This participant frames a large scale landscape photo of this area of the room, but notices a lot of extra clutter, (b) upon switching to por- trait to better focus on the region of interest, the hook be- comes distracting clutter right along the top left image bor- der. The participant makes small adjustments to refne the framing to remove this clutter for the fnal image.", "bbox": {"left": 0.08623470356261809, "top": 0.21227674773245148, "width": 0.39568088880551405, "height": 0.14202898198908026, "page": 10}, "id": "395"}], "2102.09039": [{"type": "figure", "text": "Figure 1: Spacewalker provides integrated support to allow a designer to rapidly explore a large design space of a webpage. De- signer actions are #1 and #8 denoted in green, and worker actions are #5 in red. The rest actions are performed by Spacewalker automatically. Spacewalker iterates through the loop of #4-7 repeatedly to search the design space and evolve a design.", "bbox": {"left": 0.08970939411836512, "top": 0.3972917807222617, "width": 0.8272532731099845, "height": 0.04229848071782276, "page": 1}, "id": "396"}, {"type": "figure", "text": "Figure 2: The Author interface allows a designer to create and launch a task.", "bbox": {"left": 0.08798806807574104, "top": 0.41385273018268626, "width": 0.39264094907473895, "height": 0.026165393867877997, "page": 3}, "id": "397"}, {"type": "figure", "text": "Figure 4: The Progress viewer allows a designer to examine the progress of the task by viewing the designs generated from the current generation, and export the HTML specifi- cation of these designs if satisfied.", "bbox": {"left": 0.5198330349392362, "top": 0.2525325736614189, "width": 0.39496717266007964, "height": 0.05525095775873974, "page": 3}, "id": "398"}, {"type": "figure", "text": "Figure 3: The Eval interface allows a crowd worker to com- pare two design alternatives, and select the one they prefer.", "bbox": {"left": 0.08682884266173918, "top": 0.6379299356479837, "width": 0.3983588125191483, "height": 0.028711299703578757, "page": 3}, "id": "399"}, {"type": "figure", "text": "Figure 5: The crossover operation for traditional genetic al- gorithms (top) and for Spacewalker (bottom).", "bbox": {"left": 0.5211698245378881, "top": 0.5962120980927439, "width": 0.39492378982843135, "height": 0.026930568194148515, "page": 4}, "id": "400"}, {"type": "table", "text": "Table 1: The summary of User Study Results. The time in- cludes participants both learning our markup extension, and creating their own specifications as well as inspecting the effects as adjusting them.", "bbox": {"left": 0.08907619177126418, "top": 0.10569995340674815, "width": 0.3960290858948153, "height": 0.05481292262221828, "page": 6}, "id": "401"}, {"type": "figure", "text": "Figure 6: Voter preference for Spacewalker with varying search space sizes. The horizontal axis uses a log scale.", "bbox": {"left": 0.5212079715105443, "top": 0.750351799858941, "width": 0.38988105923521754, "height": 0.02746936528369634, "page": 6}, "id": "402"}, {"type": "table", "text": "Table 2: Rater Preference for Spacewalker in Experiment 1", "bbox": {"left": 0.30765122058344824, "top": 0.10524935674185705, "width": 0.3857554267434513, "height": 0.013523930250996291, "page": 7}, "id": "403"}, {"type": "table", "text": "Table 3: Rater Preference for Spacewalker in Experiment 2", "bbox": {"left": 0.30643358417585786, "top": 0.21680242365056818, "width": 0.38717252444597633, "height": 0.014171561809501262, "page": 7}, "id": "404"}, {"type": "figure", "text": "Figure 7: The top designs generated by Spacewalker (top) versus those from uniform sampling (bottom) for each web page template. These pages were adapted from the Bootstrap examples (see Section 5.2.1). Figure 7: The top designs generated by Spacewalker (top) versus those from uniform sampling (bottom) for each web page template. These pages were adapted from the Bootstrap examples (see Section 5.2.1).", "bbox": {"left": 0.09313319711124196, "top": 0.8646771209408538, "width": 0.823481011234857, "height": 0.027601222799281882, "page": 8}, "id": "405"}], "uist-1": [{"type": "figure", "text": "Figure 1. StateLens is a system that enables blind users to interact with touchscreen devices in the real world by (i) reverse engineering a struc\u00ad tured model of the underlying interface, and (ii) using the model to pro\u00ad vide interactive conversational and audio guidance to the user about how to use it. A set of 3D-printed accessories enable capacitive touchscreens to be used non-visually by preventing accidental touches on the interface.", "bbox": {"left": 0.5274900049945108, "top": 0.3988199330339528, "width": 0.3972475637797437, "height": 0.06747952856198706, "page": 0}, "id": "406"}, {"type": "figure", "text": "Figure 2. StateLens uses a hybrid crowd-computer vision pipeline to dynamically generate state diagrams about interface structures from point-of-view usage videos, and using the diagrams to provide interactive guidance and feedback to help blind users access the interfaces.", "bbox": {"left": 0.09407749051362081, "top": 0.3993711760549834, "width": 0.8325426936928743, "height": 0.02392335371537642, "page": 1}, "id": "407"}, {"type": "figure", "text": "Figure 3. A set of 3D-printed accessories that prevent the wearer from accidentally triggering touches while exploring the interface. When de\u00ad sired, the wearer can activate a touch using either a tilt motion (B-C and E-F) or by touching a conductive trace on the accessory with a \ufb01n\u00ad ger (H-I). These accessories elegantly add \u201crisk-free exploration\u201d to ex\u00ad isting capacitive touchscreen devices without modifying the underlying hardware or software, which has been a major hurdle for past efforts. 3D models of these accessories are available at: https://github.com/ mriveralee/statelens-3dprints", "bbox": {"left": 0.08810815624162263, "top": 0.8013582325944997, "width": 0.40085126216115513, "height": 0.09919576933889677, "page": 4}, "id": "408"}, {"type": "table", "text": "Table 1. Categorization of our Thingiverse survey results related to assistive technologies, touchscreens, and \ufb01nger-based interactions. The number of items is shown next to each category name.", "bbox": {"left": 0.09025680317598231, "top": 0.28261013223667336, "width": 0.8353032442479352, "height": 0.022604759293373185, "page": 4}, "id": "409"}, {"type": "figure", "text": "Figure 4. Visualization of how StateLens represents the coffee machine interface structure as a state diagram. Note only some edges are shown.", "bbox": {"left": 0.09701816085117315, "top": 0.2673692992239287, "width": 0.8055123846515332, "height": 0.011908039902195786, "page": 5}, "id": "410"}, {"type": "figure", "text": "Figure 5. Sample interactions between a user and the coffee machine nat\u00ad ural language conversational agent StateLens automatically generated.", "bbox": {"left": 0.5276306750727635, "top": 0.8875513558435921, "width": 0.4017031800513174, "height": 0.022919086494831122, "page": 6}, "id": "411"}, {"type": "figure", "text": "Figure 6. We evaluated how well StateLens reconstructs state diagrams from point-of-view usage videos across a wide range of interfaces, including an ATM, coffee machine (both graphical and text only), printer, projector control, room reservation panel, treadmill, ticket kiosk, Coca-Cola machine, subway ticket machine, washer, and car infotainment system.", "bbox": {"left": 0.09000175451141557, "top": 0.2420632911450935, "width": 0.8368506026423834, "height": 0.03337364967423256, "page": 7}, "id": "412"}, {"type": "table", "text": "Table 2. Aggregated precision, recall, and F1 scores of the state diagram generation process of StateLens using a combination of features \u2013 Screen Detection (SD), SURF, and OCR \u2013 and with stationary, hand-held, and web (with links) usage videos. Each cell shows the precision (top left), recall (top right), and F1 scores (bottom). Bold values identify the feature combination with the best performance.", "bbox": {"left": 0.0907737694534601, "top": 0.4112502204047309, "width": 0.8340773925282596, "height": 0.0332999566588739, "page": 8}, "id": "413"}, {"type": "figure", "text": "Figure 8. StateLens maintains a relatively stable error rate for state detection as the number of states increases, compared to the increasing trend in the baseline approach.", "bbox": {"left": 0.08940317116531671, "top": 0.26907329366664695, "width": 0.3982744092255636, "height": 0.034145894676747945, "page": 9}, "id": "414"}, {"type": "table", "text": "Table 4. Results from the 3D-printed accessory study, showing mean and standard deviation (in parentheses).", "bbox": {"left": 0.09270925459518932, "top": 0.8803129099836253, "width": 0.39135558309118734, "height": 0.022542317708333332, "page": 10}, "id": "415"}, {"type": "table", "text": "Table 3. Participant demographics for our user evaluation with 14 visually impaired users. Thirteen were blind, and one (P12) had low vision.", "bbox": {"left": 0.1047093883838529, "top": 0.31628404482446537, "width": 0.7929111181520948, "height": 0.011288440588748816, "page": 10}, "id": "416"}], "1602.06979": [{"type": "figure", "text": "Figure 1. Empath analyzes text across 200 gold standard topics and emotions (e.g., childishness or violence), and can generate and validate new lexical categories on demand from a user-generated set of seed terms. The Empath web interface highlights category counts for the current document (right).", "bbox": {"left": 0.09633144054537505, "top": 0.488697360260318, "width": 0.8236668281305849, "height": 0.022859977953361744, "page": 0}, "id": "417"}, {"type": "figure", "text": "Figure 2. Empath learns word embeddings from 1.8 billion words of \ufb01ction, makes a vector space from these embeddings that measures the similarity between words, uses seed terms to de\ufb01ne and discover new words for each of its categories, and \ufb01nally \ufb01lters its categories using crowds.", "bbox": {"left": 0.09194759294098499, "top": 0.17828284369574654, "width": 0.8311311559739456, "height": 0.02204238043891059, "page": 1}, "id": "418"}, {"type": "figure", "text": "Figure 3. Example counts over four example categories on sample tweets from our validation dataset. Words in each category are discovered by unsupervised language modeling, and then validated by crowds.", "bbox": {"left": 0.09018531500124465, "top": 0.20411751487038352, "width": 0.8328298057606017, "height": 0.02187646037400371, "page": 2}, "id": "419"}, {"type": "figure", "text": "Figure 5. Positive reviews are associated with the deeper organizing principals of human life, like politics, philosophy or law. Negative re- views show metaphorical connections to animals, cleaning, and smells.", "bbox": {"left": 0.5260322919858047, "top": 0.2978935627022175, "width": 0.397015340967116, "height": 0.033178734056877365, "page": 3}, "id": "420"}, {"type": "figure", "text": "Figure 4. Deceptive reviews convey stronger sentiment across both pos- itively and negatively charged categories. In contrast, truthful reviews show a tendency towards more mundane activities and physical objects.", "bbox": {"left": 0.08899721133163552, "top": 0.2979609364210957, "width": 0.39749457166085833, "height": 0.03357559743553701, "page": 3}, "id": "421"}, {"type": "figure", "text": "Figure 6. We use Empath to replicate the work of Golder and Macy, investigating how mood on Twitter relates to time of day. The signals reported by Empath and LIWC by hour are strongly correlated for pos- itive (r=0.87) and negative (r=0.90) sentiment.", "bbox": {"left": 0.08993007310854843, "top": 0.2465051785864011, "width": 0.396878953073539, "height": 0.04573974224052044, "page": 4}, "id": "422"}, {"type": "table", "text": "Table 1. Empath can analyze text across hundreds of data-driven categories. Here we provide a sample of representative terms in 8 sample categories.", "bbox": {"left": 0.08211547876495162, "top": 0.16964365255953084, "width": 0.8399411307440864, "height": 0.01179171090174203, "page": 5}, "id": "423"}, {"type": "table", "text": "Table 2. Crowd workers found 95% of the words generated by Em- path\u2019s unsupervised model to be related to its categories. However, ma- chine learning is not perfect, and some unrelated terms slipped through (\u201cDid not pass\u201d above), which the crowd then removed.", "bbox": {"left": 0.5255499945746528, "top": 0.23608300180146188, "width": 0.396849089977788, "height": 0.044867082075639206, "page": 6}, "id": "424"}, {"type": "", "text": "third between EmoLex and LIWC, and the fourth between the General Inquirer and LIWC.", "bbox": {"left": 0.5277886982836755, "top": 0.25722274394950484, "width": 0.39603593614366317, "height": 0.07523170625320588, "page": 7}, "id": "425"}, {"type": "figure", "text": "Figure 7. Empath categories strongly agreed with LIWC, at an average Pearson correlation of 0.90. Here we plot Empath\u2019s best and worst correlations with LIWC. Each dot in the plot corresponds to one document. Empath\u2019s counts are graphed on the x-axis, LIWC\u2019s on the y-axis.", "bbox": {"left": 0.09199611190097784, "top": 0.4037210483743687, "width": 0.8294796663172105, "height": 0.023237247659702493, "page": 8}, "id": "426"}], "uist-6": [{"type": "figure", "text": "Figure 1. Skin-On Interface (a) is a new paradigm in which we augment interactive devices such as (b) smartphone, (c) interactive watches or (d) touchpad with arti\ufb01cial skin. It enables new forms of input gestures for interface control and emotional communication.", "bbox": {"left": 0.09360583934908599, "top": 0.3578704294532236, "width": 0.8303936229032629, "height": 0.02210709542939157, "page": 0}, "id": "427"}, {"type": "figure", "text": "Figure 4. Results of study 1 investigating the impact of pigmentation on human-likeness, comfort perception and anthropomorphism.", "bbox": {"left": 0.5274394415562449, "top": 0.6902197635535038, "width": 0.3950834087297028, "height": 0.02232830933850221, "page": 3}, "id": "428"}, {"type": "figure", "text": "Figure 3. Left : Realistic human skin colors: Beige and Brown. Center : Organic, but not human pigmentation (e.g. Alien or reptile). Right : Representative usual device colors: White and Black;", "bbox": {"left": 0.5267228768541922, "top": 0.15368194772739602, "width": 0.3752979453093086, "height": 0.033181633612122195, "page": 3}, "id": "429"}, {"type": "figure", "text": "Figure 6. Results of the study 2 investigating the impact of textures on comfort and skin human-likeness", "bbox": {"left": 0.0890960506364411, "top": 0.8727749911221591, "width": 0.39557799170998964, "height": 0.022637723672269572, "page": 4}, "id": "430"}, {"type": "figure", "text": "Figure 8. Results of the study 3 investigating the impact of thickness on comfort and skin human-likeness", "bbox": {"left": 0.5292250190684998, "top": 0.7562104042130288, "width": 0.3898611131057241, "height": 0.022840095288825756, "page": 4}, "id": "431"}, {"type": "figure", "text": "Figure 5. Textures samples considered in Study 2", "bbox": {"left": 0.15087908077863307, "top": 0.1691455648402975, "width": 0.27274155460931115, "height": 0.012154781457149622, "page": 4}, "id": "432"}, {"type": "figure", "text": "Figure 7. Different skin thickness considered in Study 3", "bbox": {"left": 0.5672922570720996, "top": 0.1344650923603713, "width": 0.3137887194265727, "height": 0.012063199823552912, "page": 4}, "id": "433"}, {"type": "figure", "text": "Figure 9. Design space for Skin-On interactions, for interface control and emotional communication", "bbox": {"left": 0.09059733970492494, "top": 0.3811395818536932, "width": 0.39625561782737184, "height": 0.023270115707859848, "page": 5}, "id": "434"}, {"type": "figure", "text": "Figure 10. Fabrication process of Skin-On arti\ufb01cial skin. 1. Epidermis layer, 2. Electrodes, 3. Hypodermis layer, 4. Electronics, 5. Aesthetics.", "bbox": {"left": 0.09787414276521969, "top": 0.1734787526756826, "width": 0.797981031579909, "height": 0.012458126954358033, "page": 6}, "id": "435"}, {"type": "figure", "text": "Figure 11. Left . Open Hardware Mutual Capacitance breakout Right. Smartphone case prototype hardware.", "bbox": {"left": 0.09100919611313764, "top": 0.40415634771790165, "width": 0.39583129508822573, "height": 0.023385173142558398, "page": 7}, "id": "436"}, {"type": "figure", "text": "Figure 12. Data processing to detect multi-touch ( top ) or grab ( bottom ) gestures: a - Gesture, b -Raw sensor data, c -5x upscale image, d - Con- tours and Blobs detection.", "bbox": {"left": 0.5254292207605699, "top": 0.25582402161877565, "width": 0.3983926212086397, "height": 0.033983943438289144, "page": 7}, "id": "437"}], "uist-7": [{"type": "figure", "text": "Figure 1. (a) We demonstrate touch typing on a \ufb02at surface using a decoder that translates hand motion from a skeletal hand-tracking system into text. Compared to contact-based sensing, e.g., capacitive touch, hand motion additionally encodes the trajectory of a \ufb01nger as it reaches for a key, which we can leverage. For example (b), we show two clusters of trajectories of the right middle \ufb01ngertip from the training dataset; one for \u2018O\u2019 keystrokes (magenta) and one for \u2018I\u2019 keystrokes (blue). The sample trajectory at test time (green) lands on the \u2018I\u2019 key but has a more similar path to the \u2018O\u2019 key. Our approach uses this trajectory information to correctly classify this case while a contact-based approach cannot.", "bbox": {"left": 0.0951337004019544, "top": 0.410914912368312, "width": 0.8284221599304599, "height": 0.05635251902570628, "page": 0}, "id": "438"}, {"type": "figure", "text": "Figure 3. Our data collection setup consists of a marker-based hand- tracking system using OptiTrack cameras and a touchpad for recording contact. A printed 2D keyboard guide is attached to the touchpad for reference.", "bbox": {"left": 0.5259838228911357, "top": 0.23097591207485008, "width": 0.3984423868017259, "height": 0.045704986109878075, "page": 3}, "id": "439"}, {"type": "figure", "text": "Figure 2. The output of our motion model is a heat map of predictions of which key was hit at which time. A greedy decoding of this heat map can result in errors, while incorporating a language model and beam search signi\ufb01cantly reduce the error rate.", "bbox": {"left": 0.0899455812242296, "top": 0.28065396318532, "width": 0.397132268918106, "height": 0.04550569707697088, "page": 3}, "id": "440"}, {"type": "figure", "text": "Figure 4. Boxplots depicting median, quartiles, and means comparing typing speed and accuracy between physical keyboard and \ufb02at surface typing.", "bbox": {"left": 0.5267607246349061, "top": 0.2892735991815124, "width": 0.3973585141250511, "height": 0.03275830817945076, "page": 4}, "id": "441"}, {"type": "figure", "text": "Figure 6. Variation in typing consistency and key-\ufb01nger correspondences shown in three participants from our dataset. Each ellipse is a bivariate Gaussian \ufb01tted to contact points for a key colored by which \ufb01nger ac- counts for 90% of contacts of that key.", "bbox": {"left": 0.5202920951095282, "top": 0.3376901799982244, "width": 0.4054070765676062, "height": 0.04598305442116477, "page": 5}, "id": "442"}, {"type": "figure", "text": "Figure 5. As a baseline, we compare our method to a contact-based spa- tial model that \ufb01ts bivariate Gaussians (bottom) to all the contact events of each key (top).", "bbox": {"left": 0.08919572518541923, "top": 0.3394776931916825, "width": 0.3990977418188955, "height": 0.03481073090524384, "page": 5}, "id": "443"}, {"type": "figure", "text": "Figure 7. A typist types the word \u201cWITH\u201d but fails to make contact for the letter \u2018H\u2019. Continuous decoding makes one prediction every 60 th of a second and can predict the possibility of the missing contact (indicated by the blue arrow) based on hand motion near the surface. Discrete contact-based decoding only predicts when contacts occur and thus can- not predict the missing letter.", "bbox": {"left": 0.5289294174294067, "top": 0.6283824034411498, "width": 0.3955726872861775, "height": 0.06781949900617504, "page": 5}, "id": "444"}, {"type": "figure", "text": "Figure 8. Uncorrected error rate for various decoding strategies. G=Gaussian contact model, PFG=Per Finger Gaussian contact model, Motion=Hand motion model.", "bbox": {"left": 0.09049972210055084, "top": 0.8704793140141651, "width": 0.3925684074950374, "height": 0.03393431384154041, "page": 6}, "id": "445"}, {"type": "table", "text": "Table 1. Comparison of mean (median) accuracy across decoding strate- gies, and the deltas when only samples with corresponded contacts are evaluated.", "bbox": {"left": 0.5256930581884447, "top": 0.8711073904326467, "width": 0.39765596078112236, "height": 0.033288435502485794, "page": 6}, "id": "446"}, {"type": "figure", "text": "Figure 9. Interactive text input in virtual reality using using hand- tracking and a calibrated surface, with a virtual keyboard composed of standard sized 19mm keys.", "bbox": {"left": 0.09023927551468992, "top": 0.868646178582702, "width": 0.39373595418493734, "height": 0.03405337863498264, "page": 7}, "id": "447"}, {"type": "table", "text": "Table 2. Sample phrases decoded with contact decoding baselines and our method (Hand Motion Decoding). In some phrases (a), we see the bene\ufb01t of \ufb01nger identity information. In others (c), using the trajectory of the \ufb01nger allows for more accurate character classi\ufb01cation. Operating on a continuous stream of hand motion allows our method to vary the number of characters (b,d,e) while contact-based decoding can only generate the same number of characters as the number of contacts.", "bbox": {"left": 0.0932820139367596, "top": 0.19865151607629025, "width": 0.8280329984777114, "height": 0.04548230797353417, "page": 7}, "id": "448"}]}