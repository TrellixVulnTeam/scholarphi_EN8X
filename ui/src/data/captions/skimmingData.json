{
  "1908.07816": [
    {
      "type": "figure",
      "text": "Figure 1: The overall architecture of our model.",
      "bbox": {
        "left": 0.3421257368100235,
        "top": 0.4905756940745344,
        "width": 0.31350284152560765,
        "height": 0.014651481551353377,
        "page": 3
      }
    },
    {
      "type": "table",
      "text": "Table 1: Statistics of the two datasets.",
      "bbox": {
        "left": 0.5930777655707465,
        "top": 0.11539915836218631,
        "width": 0.2463423286388123,
        "height": 0.014581140845712989,
        "page": 4
      }
    },
    {
      "type": "table",
      "text": "Table 2: Perplexity and average BLEU scores achieved by the models. Avg. BLEU: average of BLEU-1, -2, -3, and -4. Validation set 1 comes from the Cornell dataset, and validation set 2 comes from the DailyDialog dataset.",
      "bbox": {
        "left": 0.0912519031100803,
        "top": 0.116518897239608,
        "width": 0.821988616893494,
        "height": 0.027288465788870148,
        "page": 7
      }
    },
    {
      "type": "table",
      "text": "Table 3: Human evaluation results on grammatical correct- ness.",
      "bbox": {
        "left": 0.08814948836183237,
        "top": 0.27373473812835386,
        "width": 0.39601483064539295,
        "height": 0.026155471801757812,
        "page": 7
      }
    },
    {
      "type": "table",
      "text": "Table 5: Human evaluation results on emotional appropri- ateness.",
      "bbox": {
        "left": 0.08863794414046543,
        "top": 0.528708987765842,
        "width": 0.3935992795657488,
        "height": 0.027947820798315183,
        "page": 7
      }
    },
    {
      "type": "table",
      "text": "Table 4: Human evaluation results on contextual coherence.",
      "bbox": {
        "left": 0.08672704260333691,
        "top": 0.4082996869328046,
        "width": 0.3946585811041539,
        "height": 0.014234754774305556,
        "page": 7
      }
    },
    {
      "type": "figure",
      "text": "Figure 2: t-SNE visualization of the output layer weights in HRAN and MEED. 100 most frequent positive words and 100 most frequent negative words are shown. The weight vectors in MEED are separated into two parts and visualized individually.",
      "bbox": {
        "left": 0.09055744121277254,
        "top": 0.2856546267114504,
        "width": 0.818277078516343,
        "height": 0.026206488561148596,
        "page": 8
      }
    },
    {
      "type": "table",
      "text": "Table 6: Sample model responses. For each dialog, the ground truth is included in a pair of parentheses.",
      "bbox": {
        "left": 0.15970656451056986,
        "top": 0.11618848280473189,
        "width": 0.6889216603796466,
        "height": 0.014303486756604127,
        "page": 9
      }
    }
  ],
  "1810.11143": [
    {
      "type": "figure",
      "text": "Figure 1: The user interface of Smell Pittsburgh. The left image shows the submission console for describing smell character- istics, explaining symptoms, and providing notes for the local health department. The right image shows the visualization of smell reports, sensors, and wind directions.",
      "bbox": {
        "left": 0.08968353271484375,
        "top": 0.4435025224782,
        "width": 0.8275113074608098,
        "height": 0.040439567180595014,
        "page": 1
      }
    },
    {
      "type": "table",
      "text": "Table 1: Percentage (rounded to the first decimal place) and the total size of user groups. Abbreviation \u201cGA\u201d means Google Analytics. Characters mean the number of charac- ters that user entered in the text fields of reports.",
      "bbox": {
        "left": 0.520139357622932,
        "top": 0.10534619803380485,
        "width": 0.39421994078393074,
        "height": 0.05583195734505701,
        "page": 4
      }
    },
    {
      "type": "table",
      "text": "Table 2: Statistics of user groups (median \u00b1 semi- interquartile range), rounded to the nearest integer. Symbol \u2200 means \u201cfor each.\u201d Abbreviation \u201cGA\u201d means Google Ana- lytics. Characters mean the number of characters that user entered in the text fields of reports. Hours difference means the number of hours between the hit and data timestamps.",
      "bbox": {
        "left": 0.5199051900626788,
        "top": 0.311132970482412,
        "width": 0.3947693569208282,
        "height": 0.08123519203879616,
        "page": 4
      }
    },
    {
      "type": "figure",
      "text": "Figure 3: The distribution of smell reports on selected zip code regions. The integers on each zip code region indicate the number of reports.",
      "bbox": {
        "left": 0.5213564205792994,
        "top": 0.29397908605710427,
        "width": 0.3946570103464563,
        "height": 0.042004806826813054,
        "page": 5
      }
    },
    {
      "type": "figure",
      "text": "Figure 4: The average smell values aggregated by hour of day and day of week. Our users rarely submit smell reports at nighttime.",
      "bbox": {
        "left": 0.5191834892322814,
        "top": 0.49001400880139284,
        "width": 0.3946472367430045,
        "height": 0.041477742821279195,
        "page": 5
      }
    },
    {
      "type": "table",
      "text": "Table 3: Cross-validation of model performances (mean \u00b1 standard deviation). We run this experiment for 100 times with random seeds. Abbreviations \u201cET\u201d and \u201cRF\u201d indicate Extremely Randomized Trees and Random Forest respec- tively, which are used for predicting upcoming smell events. The Decision Tree, different from the others, is for interpret- ing air pollution patterns on a subset of the entire dataset.",
      "bbox": {
        "left": 0.5195345411113664,
        "top": 0.10594063575821694,
        "width": 0.39542802798202614,
        "height": 0.09760413507018427,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 5: This figure shows true positives (TP), false posi- tives (FP), and false negatives (FN). The x-axis represents time. The blue and red boxes indicate ground truth and pre- dicted smell events respectively.",
      "bbox": {
        "left": 0.08796138389437806,
        "top": 0.25684822930230033,
        "width": 0.3971206066655178,
        "height": 0.05517707208190301,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 6: The right map shows smell reports and sensor readings with important predictors at 10:30 am on December 3, 2017. The left graph shows the first five depth levels of the Decision Tree with F-score 0.81, which explains the pattern of about 50% smell events. The first line of a tree node indicates the ratio of the number of positive (with smell event) and negative samples (no smell event). The second and third lines of the node show the feature and its threshold for splitting. The most important predictor is the interaction between the east-west wind directions at Parkway and the previous 1-hour hydrogen sulfide readings at Liberty ( r =.58, n =16,766, p <.001), where r means the point-biserial correlation of the predictor and smell events.",
      "bbox": {
        "left": 0.08796142129337087,
        "top": 0.4650060094968237,
        "width": 0.825845169865228,
        "height": 0.09776676062381628,
        "page": 7
      }
    },
    {
      "type": "figure",
      "text": "Figure 7: The distributions of self-efficacy changes, motiva- tions, and participation level for our survey responses. The red lines in the middle of the box indicate the median. The red-filled diamonds represent the mean. The top and bot- tom edges of a box indicate 75% ( Q 3 ) and 25% ( Q 1 ) quantiles respectively. The boxes show inter-quantile ranges IQR = Q 3 \u2212 Q 1 . The top and bottom whiskers show Q 3 + 1 . 5 \u2217 IQR and Q 1 + 1 . 5 \u2217 IQR respectively. Black hollow circles show outliers.",
      "bbox": {
        "left": 0.5194375530567045,
        "top": 0.21739453017109572,
        "width": 0.3957437253465839,
        "height": 0.10964012145996094,
        "page": 8
      }
    },
    {
      "type": "",
      "text": "selected quotes. Bold emphases in the quotes were added by re- searchers to highlight key user sentiments.",
      "bbox": {
        "left": 0.5200614181219363,
        "top": 0.3539632161458333,
        "width": 0.3924011031007455,
        "height": 0.026928063594933712,
        "page": 8
      }
    },
    {
      "type": "table",
      "text": "Table 4: Demographics of participants. Columns and rows represent ages and education levels.",
      "bbox": {
        "left": 0.09033313452028761,
        "top": 0.10552673147182272,
        "width": 0.3906271379757551,
        "height": 0.027560792788110598,
        "page": 8
      }
    },
    {
      "type": "table",
      "text": "Table 5: Frequency of system usage (sorted by percentage).",
      "bbox": {
        "left": 0.09042175143372778,
        "top": 0.261830782649493,
        "width": 0.3852442697761885,
        "height": 0.014015082157019413,
        "page": 8
      }
    },
    {
      "type": "table",
      "text": "Table 6: The multiple-choice question for measuring participation level (sorted by percentage).",
      "bbox": {
        "left": 0.1845672582489213,
        "top": 0.10575699083732837,
        "width": 0.6299335380005681,
        "height": 0.0130948442401308,
        "page": 9
      }
    },
    {
      "type": "table",
      "text": "Table 7: Cross-validation of model performances (mean \u00b1 standard deviation) on the testing set for daytime. We run this experiment for 100 times with random seeds. Abbrevia- tions \u201cET\u201d and \u201cRF\u201d indicate Extremely Randomized Trees and Random Forest respectively, which are used for predict- ing upcoming smell events.",
      "bbox": {
        "left": 0.5198593139648438,
        "top": 0.10557483904289477,
        "width": 0.3949894624597886,
        "height": 0.08389335208468968,
        "page": 14
      }
    },
    {
      "type": "table",
      "text": "Table 8: Cross-validation of model performances (mean \u00b1 standard deviation) on the training and testing set for day- time. We run this experiment for 100 times, including both the cluster analysis and recursive feature elimination. The Decision Tree is used for interpreting air pollution patterns on a subset of the entire dataset.",
      "bbox": {
        "left": 0.5207711512746375,
        "top": 0.31407891861116044,
        "width": 0.3927469191208384,
        "height": 0.08186211248840948,
        "page": 14
      }
    },
    {
      "type": "figure",
      "text": "Figure 8: The right map shows smell reports and sensor readings at 10:30 am on December 3, 2017. The left graph shows one example of the Decision Tree, which explains the pattern of about 30% smell events. Each tree node indicates the ratio of the number of positive (with smell event) and negative samples (no smell event). The most important feature is the interaction between the current north-south wind directions at Lawrenceville and the previous 1-hour hydrogen sulfide readings at Liberty ( r =.58, p <.001, n =16,694), where p means the p-value, n means the number of samples, and r means the point-biserial correlation of the predictor and smell events. The second-most important feature is the interaction between the current east-west wind directions at Parkway and the current hydrogen sulfide readings at Liberty ( r =.54, p <.001, n =16,694). The corresponding Gini importance for the most and the second-most important features are 0.42 \u00b1 0.02 and 0.10 \u00b1 0.00, respectively, in the format of \u201cmean \u00b1 standard deviation\u201d for the 100-times experiment.",
      "bbox": {
        "left": 0.0886231590719784,
        "top": 0.4644489480991556,
        "width": 0.8218892290701274,
        "height": 0.12861348161793718,
        "page": 15
      }
    }
  ],
  "2010.07358": [
    {
      "type": "figure",
      "text": "Fig. 2. (a) AR simulator interface with various elements of assistance in the designed virtual HUD. (b) Three assistance conditions from the study are shown. Optimal Assistance condition uses both object highlighting via flagpoles and path highlighting via breadcrumbs.",
      "bbox": {
        "left": 0.1783885955810547,
        "top": 0.3094913598262902,
        "width": 0.7024757659513187,
        "height": 0.02512679437194208,
        "page": 7
      }
    },
    {
      "type": "figure",
      "text": "Fig.3. Thefourtask-difficultylevels(increasingindifficultygoingfromlefttoright),withtheoptimalpath(correspondingtothesolution of the associated CVRP) shown in the leftmost panel. Colored polygons represent approximate bounding boxes of the apartment\u2019s floor plan, including the boundaries for each rooms; circular markers denote objects; square markers denote bins; marker color denotes semantic object category (e.g., books); the star represents the initial position.",
      "bbox": {
        "left": 0.18389244328916463,
        "top": 0.289097988244259,
        "width": 0.7000845117506638,
        "height": 0.0484700058445786,
        "page": 9
      }
    },
    {
      "type": "figure",
      "text": "Fig. 6. Participants provided with opti- mal assistance ( \ud835\udc5d < 0 . 001 ) and object- highlighting assistance ( \ud835\udc5d < 0 . 01 ) were more likely to find shorter paths than par- ticipants in the no assistance condition.",
      "bbox": {
        "left": 0.6599874060138379,
        "top": 0.33200705171835543,
        "width": 0.2185522341260723,
        "height": 0.06232880101059422,
        "page": 11
      }
    },
    {
      "type": "figure",
      "text": "Fig. 4. On average participants in the op- timal condition deviated from the optimal ordering less frequently than participants in object-highlighting ( \ud835\udc5d < 0 . 001 ) and no assistance conditions ( \ud835\udc5d < 0 . 001 ).",
      "bbox": {
        "left": 0.18005526922886667,
        "top": 0.3325849590879498,
        "width": 0.2184260623906952,
        "height": 0.062462199818004265,
        "page": 11
      }
    },
    {
      "type": "figure",
      "text": "Fig. 5. Participants provided with opti- mal assistance followed paths that more closely resembled the optimal paths than in object-highlighting ( \ud835\udc5d < 0 . 01 ) and no assistance conditions ( \ud835\udc5d < 0 . 001 ).",
      "bbox": {
        "left": 0.4206247766033497,
        "top": 0.3326668402161261,
        "width": 0.2182039248397927,
        "height": 0.06240975736367582,
        "page": 11
      }
    },
    {
      "type": "figure",
      "text": "Fig. 7. Left , When compared against no assistance and object-highlighting assistance, participants in the optimal assistance condition were less likely to feel in control (both \ud835\udc5d < 0 . 001 ), more likely to feel the need to follow the assistance (both \ud835\udc5d < 0 . 01 ), and more likely to prefer being told what to do (both \ud835\udc5d < 0 . 001 ). Right , Participants in the optimal assistance were more likely to rate assistance as usable ( \ud835\udc5d < 0 . 001 ) and useful than those provided with no-assistance ( \ud835\udc5d < 0 . 001 ). Those in object-highlighting more strongly agreed that the assistance was useful when compared to no-assistance ( \ud835\udc5d < 0 . 05 ). Those in optimal assistance more strongly agreed that the assistance was useful as compared to the object-highlighting assistance ( \ud835\udc5d < 0 . 05 ).",
      "bbox": {
        "left": 0.12074233659731796,
        "top": 0.28086614127110954,
        "width": 0.7023623722051483,
        "height": 0.07390902740786774,
        "page": 12
      }
    },
    {
      "type": "figure",
      "text": "Fig. 8. Left , participants provided with optimal assistance were less likely to deviate from the optimal ordering than participants provided with either no assistance or object-highlighting assistance ( \ud835\udc5d < 0 . 001 , both). As the difficulty increased, participants were more likely to deviate in each group. Right , participants provided with optimal assistance were more likely to follow shortest paths (higher IPL) than those provided with both object-highlighting ( \ud835\udc5d < 0 . 01 ) and no ( \ud835\udc5d < 0 . 001 ) assistance. Participants provided with object-highlighting assistance took shorter paths than those in none ( \ud835\udc5d < 0 . 001 ).",
      "bbox": {
        "left": 0.18191715315276502,
        "top": 0.310147063900726,
        "width": 0.6970784555073657,
        "height": 0.061640421549479164,
        "page": 13
      }
    },
    {
      "type": "table",
      "text": "Table 1. The number of participants across conditions in our study.",
      "bbox": {
        "left": 0.295062295751634,
        "top": 0.2614389284692629,
        "width": 0.3461352080301522,
        "height": 0.01065331276017006,
        "page": 16
      }
    }
  ],
  "2103.06807": [
    {
      "type": "figure",
      "text": "Figure 1: We present a model-based reinforcement learning approach for adaptive UIs that can improve usability while avoid- ing unexpected changes that surprise the user or require relearning. An interface is adapted by simulating several possible sequences of adaptations and evaluating them using predictive models in HCI. This approach avoids greedy, disadvantageous adaptations, and may anticipate possible user responses even with limited observation data.",
      "bbox": {
        "left": 0.08411212995940563,
        "top": 0.5420996155401673,
        "width": 0.8290466109132455,
        "height": 0.058193091190222535,
        "page": 0
      }
    },
    {
      "type": "figure",
      "text": "Figure 3: Neural network architecture for obtaining value estimates. Training data is generated using HCI models.",
      "bbox": {
        "left": 0.5207144045362285,
        "top": 0.5371977680861347,
        "width": 0.39290249893088747,
        "height": 0.027956914420079703,
        "page": 4
      }
    },
    {
      "type": "figure",
      "text": "Figure 2: Model-based planning of adaptations using Monte-Carlo Tree Search (MCTS). Adaptations are selected using upper- confidence trees (UCT). After expanding a new node (adaptation), reward estimates are obtained through roll-outs, and back- propogated to the root node. The child with the highest value is picked as the next adaptation.",
      "bbox": {
        "left": 0.08916896620607065,
        "top": 0.320539146962792,
        "width": 0.8259832905788048,
        "height": 0.042163193827927714,
        "page": 4
      }
    },
    {
      "type": "figure",
      "text": "Figure 4: Menu adaptation with deep model-based RL. Given a current design and user observations, MCTS-based planning is used to select the next adaptation. Reward estimates are obtained either using predictive models or from a neural network.",
      "bbox": {
        "left": 0.09371844771640753,
        "top": 0.2983382446597321,
        "width": 0.814679850160686,
        "height": 0.027467650596541587,
        "page": 5
      }
    },
    {
      "type": "table",
      "text": "Table 1: Key notations used in this section.",
      "bbox": {
        "left": 0.14357905917697483,
        "top": 0.6547029283311632,
        "width": 0.28240758609148414,
        "height": 0.013081097843671086,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 5: Model-based simulation of search for a target item ( \u2019Save As...\u2019 ). (a) when the target is at an expected location, search proceeds as expected; (b) when the target is at an unexpected location, a penalty is imposed upon not finding the target at its expected location (indicated by \u00d7 ), and search reverts to slower strategies.",
      "bbox": {
        "left": 0.09133537142884497,
        "top": 0.3120045517430161,
        "width": 0.8236566182055505,
        "height": 0.04235389015891335,
        "page": 7
      }
    },
    {
      "type": "figure",
      "text": "Figure 6: Our value network architecture takes menu design and user features as input to provide individual reward pre- dictions.",
      "bbox": {
        "left": 0.5202691570606107,
        "top": 0.8505299115421796,
        "width": 0.3945051205703636,
        "height": 0.042379205877130684,
        "page": 7
      }
    },
    {
      "type": "",
      "text": "used a GNU/Linux server with an Intel Xeon Gold CPU @ 2.10GHz (64 bit processor) for simulations. The execution of the study was automated such that trials were conducted sequentially, to avoid variations in computational resource usage.",
      "bbox": {
        "left": 0.5195041232638888,
        "top": 0.5355319398822207,
        "width": 0.3942284677542892,
        "height": 0.05529395980064315,
        "page": 8
      }
    },
    {
      "type": "figure",
      "text": "Figure 7: A sample result from the simulation study, for a 15- item menu design. In 3 steps, the menu was adapted to better suit the given user\u2019s interests (\u2018Gloves\u2019 group moved to the top), and improved some grouping (\u2018Carrot\u2019 with \u2018Potato\u2019) as well.",
      "bbox": {
        "left": 0.5211252948037939,
        "top": 0.43955762458570075,
        "width": 0.393717198590048,
        "height": 0.07054631396977588,
        "page": 8
      }
    },
    {
      "type": "figure",
      "text": "Figure 8: Computation time for planning adaptations via model-based simulations vs. value network for varying search depths. Our value network exhibited consistent per- formance for longer horizons.",
      "bbox": {
        "left": 0.08812654407974942,
        "top": 0.35335621689305163,
        "width": 0.39538927639231963,
        "height": 0.05439870044438526,
        "page": 9
      }
    },
    {
      "type": "figure",
      "text": "Figure 10: Sequential adaptation of a 5-item menu. The sys- tem avoids greedily moving \u2018Coffee\u2019 (lowest frequency) at the first step. In light of new observations, this change is jus- tified by the reduced user interest.",
      "bbox": {
        "left": 0.5196101868074704,
        "top": 0.8373104056926689,
        "width": 0.396932215472452,
        "height": 0.05572494352706755,
        "page": 10
      }
    },
    {
      "type": "figure",
      "text": "Figure 9: (a) The MCTS condition was associated with significantly lower selection time as compared to the two baselines. Ver- tical bars indicate 95% confidence intervals. (b) For items positioned lower in the menu, selection time in Freqency increased more drastically than other conditions.",
      "bbox": {
        "left": 0.09251087163788041,
        "top": 0.33275095621744794,
        "width": 0.8250039044548484,
        "height": 0.04135894775390625,
        "page": 10
      }
    }
  ],
  "2104.03820": [
    {
      "type": "figure",
      "text": "Fig. 1. UX Variants. Input Java source code was shown in the left pane (A). Clicking \u201cTranslate Now\u201d translated the code into Python in the right pane, in one of three variants: code translation only (not shown), code translation with low-confidence tokens highlighted in red (B), or code translation with alternate code translations displayed in pop-up menus (C).",
      "bbox": {
        "left": 0.1819938958859911,
        "top": 0.30708567301432294,
        "width": 0.7013991362129162,
        "height": 0.0357018287735756,
        "page": 3
      }
    }
  ],
  "2102.08235": [
    {
      "type": "figure",
      "text": "Figure 1: A hypothetical coupleusing Significant Otter: Alice (left) sends her state otter to Bob (right) on her Apple Watch.",
      "bbox": {
        "left": 0.5206669327480341,
        "top": 0.8359186962397411,
        "width": 0.39369625515407985,
        "height": 0.026750275582978218,
        "page": 0
      }
    },
    {
      "type": "figure",
      "text": "Figure 2: Heart rate sensing based on people\u2019s historical data from Apple HealthKit, with thresholds determined through empirical testing. The thresholds are relative to being sta- tionary, according to prior work in emotion detection using physiological data [11].",
      "bbox": {
        "left": 0.5199746524586397,
        "top": 0.8147072840218592,
        "width": 0.3966081407335069,
        "height": 0.06802321925307765,
        "page": 3
      }
    },
    {
      "type": "table",
      "text": "Table 1: List of state otters.",
      "bbox": {
        "left": 0.6266814088509753,
        "top": 0.10431679812344638,
        "width": 0.17723377701503779,
        "height": 0.015075596896084871,
        "page": 3
      }
    },
    {
      "type": "figure",
      "text": "Figure 3: Examples of state otters.",
      "bbox": {
        "left": 0.3878554799198325,
        "top": 0.2886812422010634,
        "width": 0.22436792710248163,
        "height": 0.014233329079367897,
        "page": 4
      }
    },
    {
      "type": "table",
      "text": "Table 2: List of react otters.",
      "bbox": {
        "left": 0.625654532239328,
        "top": 0.31878082198326035,
        "width": 0.17931410845588236,
        "height": 0.015837293682676373,
        "page": 4
      }
    },
    {
      "type": "figure",
      "text": "Figure 4: Examples of react otters.",
      "bbox": {
        "left": 0.3867095997130949,
        "top": 0.30719947814941406,
        "width": 0.22584900201535693,
        "height": 0.014515924935389047,
        "page": 5
      }
    },
    {
      "type": "figure",
      "text": "Figure 6: Significant Otter notifications.",
      "bbox": {
        "left": 0.3691776500028722,
        "top": 0.5983707158252446,
        "width": 0.263748642665888,
        "height": 0.013796180185645518,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 5: Bob reacting to Alice\u2019s otter on the Apple Watch.",
      "bbox": {
        "left": 0.30982255624010674,
        "top": 0.2842146170259726,
        "width": 0.38207317177766287,
        "height": 0.013769246111012469,
        "page": 6
      }
    }
  ],
  "2102.09039": [
    {
      "type": "figure",
      "text": "Figure 1: Spacewalker provides integrated support to allow a designer to rapidly explore a large design space of a webpage. De- signer actions are #1 and #8 denoted in green, and worker actions are #5 in red. The rest actions are performed by Spacewalker automatically. Spacewalker iterates through the loop of #4-7 repeatedly to search the design space and evolve a design.",
      "bbox": {
        "left": 0.08970939411836512,
        "top": 0.3972917807222617,
        "width": 0.8272532731099845,
        "height": 0.04229848071782276,
        "page": 1
      }
    },
    {
      "type": "figure",
      "text": "Figure 2: The Author interface allows a designer to create and launch a task.",
      "bbox": {
        "left": 0.08798806807574104,
        "top": 0.41385273018268626,
        "width": 0.39264094907473895,
        "height": 0.026165393867877997,
        "page": 3
      }
    },
    {
      "type": "figure",
      "text": "Figure 4: The Progress viewer allows a designer to examine the progress of the task by viewing the designs generated from the current generation, and export the HTML specifi- cation of these designs if satisfied.",
      "bbox": {
        "left": 0.5198330349392362,
        "top": 0.2525325736614189,
        "width": 0.39496717266007964,
        "height": 0.05525095775873974,
        "page": 3
      }
    },
    {
      "type": "figure",
      "text": "Figure 3: The Eval interface allows a crowd worker to com- pare two design alternatives, and select the one they prefer.",
      "bbox": {
        "left": 0.08682884266173918,
        "top": 0.6379299356479837,
        "width": 0.3983588125191483,
        "height": 0.028711299703578757,
        "page": 3
      }
    },
    {
      "type": "figure",
      "text": "Figure 5: The crossover operation for traditional genetic al- gorithms (top) and for Spacewalker (bottom).",
      "bbox": {
        "left": 0.5211698245378881,
        "top": 0.5962120980927439,
        "width": 0.39492378982843135,
        "height": 0.026930568194148515,
        "page": 4
      }
    },
    {
      "type": "table",
      "text": "Table 1: The summary of User Study Results. The time in- cludes participants both learning our markup extension, and creating their own specifications as well as inspecting the effects as adjusting them.",
      "bbox": {
        "left": 0.08907619177126418,
        "top": 0.10569995340674815,
        "width": 0.3960290858948153,
        "height": 0.05481292262221828,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 6: Voter preference for Spacewalker with varying search space sizes. The horizontal axis uses a log scale.",
      "bbox": {
        "left": 0.5212079715105443,
        "top": 0.750351799858941,
        "width": 0.38988105923521754,
        "height": 0.02746936528369634,
        "page": 6
      }
    },
    {
      "type": "table",
      "text": "Table 2: Rater Preference for Spacewalker in Experiment 1",
      "bbox": {
        "left": 0.30765122058344824,
        "top": 0.10524935674185705,
        "width": 0.3857554267434513,
        "height": 0.013523930250996291,
        "page": 7
      }
    },
    {
      "type": "table",
      "text": "Table 3: Rater Preference for Spacewalker in Experiment 2",
      "bbox": {
        "left": 0.30643358417585786,
        "top": 0.21680242365056818,
        "width": 0.38717252444597633,
        "height": 0.014171561809501262,
        "page": 7
      }
    },
    {
      "type": "figure",
      "text": "Figure 7: The top designs generated by Spacewalker (top) versus those from uniform sampling (bottom) for each web page template. These pages were adapted from the Bootstrap examples (see Section 5.2.1). Figure 7: The top designs generated by Spacewalker (top) versus those from uniform sampling (bottom) for each web page template. These pages were adapted from the Bootstrap examples (see Section 5.2.1).",
      "bbox": {
        "left": 0.09313319711124196,
        "top": 0.8646771209408538,
        "width": 0.823481011234857,
        "height": 0.027601222799281882,
        "page": 8
      }
    }
  ],
  "2101.11778": [
    {
      "type": "figure",
      "text": "Figure 1: Mechanisms that influence how people spend their time in apps can be placed along a spectrum, as in these exam- ples. External mechanisms monitor or police apps, while internal mechanisms redesign or rebuild the experience within a problematic app. Internal mechanisms offer designers a more targeted way of supporting user agency.",
      "bbox": {
        "left": 0.08654582578372332,
        "top": 0.2973451325387666,
        "width": 0.8340120315551758,
        "height": 0.0448156799932923,
        "page": 3
      }
    },
    {
      "type": "",
      "text": "(4) Of the time they spend on YouTube, 20% or more is on their smartphone (self-estimated).",
      "bbox": {
        "left": 0.10785049239015268,
        "top": 0.5165148648348722,
        "width": 0.3736440870496962,
        "height": 0.025887383355034724,
        "page": 3
      }
    },
    {
      "type": "table",
      "text": "Table 1: Demographics of the 120 survey participants",
      "bbox": {
        "left": 0.108286477381887,
        "top": 0.4655782911512587,
        "width": 0.3569159040264055,
        "height": 0.012896874938348327,
        "page": 3
      }
    },
    {
      "type": "table",
      "text": "Table 2: The wording and format of the \u201cmore in control\u201d question in the survey. The example responses here come from a single study participant. All participants also completed a second version of this question table, with the text modified from \u201cmost\u201d to \u201cleast\u201d in the Thing Question and from \u201cmore\u201d to \u201cless\u201d in the Explain Question.",
      "bbox": {
        "left": 0.09230873008179509,
        "top": 0.28690632906827057,
        "width": 0.8251103045893651,
        "height": 0.0404345387160176,
        "page": 4
      }
    },
    {
      "type": "table",
      "text": "Table 3: This table shows nine design mechanisms that were mentioned 15 or more times in response to the survey question: \u201c What are 3 things about the mobile app that lead you to feel [most | least] in control over how you spend your time on YouTube? \u201d Design mechanisms are shown in the order of frequency of mention. The most frequently mentioned mechanism, recommen- dations, is shown with 3 subcodes. The representative quote(s) column shows one typical response for each design mechanism; both a \u201c more in control \u201d and a \u201c less in control \u201d quote are shown if the minority opinion on the direction of control was more than 20 % of total responses.",
      "bbox": {
        "left": 0.09044227724760966,
        "top": 0.6013866771351207,
        "width": 0.830930080289155,
        "height": 0.08088541512537484,
        "page": 5
      }
    },
    {
      "type": "figure",
      "text": "Figure 2: This diverging bar chart shows how many times these nine design mechanisms led participants to feel more control or less control. Recommendations, ads, and autoplay primarily made respondents feel less in control. Playlists, search, subscriptions, play controls, and watch history & stats primarily made respondents feel more in control. Noti- fications were sometimes mentioned as leading to more con- trol and sometimes to less.",
      "bbox": {
        "left": 0.08821674421721813,
        "top": 0.404854745575876,
        "width": 0.3953278485466452,
        "height": 0.10975935964873343,
        "page": 6
      }
    },
    {
      "type": "table",
      "text": "Table 4: Mockups of the redesign of the recommendations mechanism. We created three versions of the mockup that we expected to offer different levels of control. These 3 versions of each redesign were evaluated by participants in the co-design evaluation activity.",
      "bbox": {
        "left": 0.09131118674683415,
        "top": 0.5827425947093,
        "width": 0.8222985361136642,
        "height": 0.04108332624339094,
        "page": 8
      }
    },
    {
      "type": "table",
      "text": "Table 5: This table describes our redesigns of 4 existing mechanisms in the YouTube app. We created three versions of each mockup that we expected to provide different levels of control to the user: low, medium, and high. Appendix II describes more details about the search redesign and the three additional mockups we created, which we do not report on here.",
      "bbox": {
        "left": 0.09625557045531428,
        "top": 0.8132714165581597,
        "width": 0.8209493425157335,
        "height": 0.03972332886975221,
        "page": 8
      }
    },
    {
      "type": "figure",
      "text": "Figure 3: Sketches of the home screen of the YouTube mo- bile app. The participant (P11) explained that in the \u201c more in control \u201d version, recommendations are based on topics chosen by the user. In the \u201c less in control \u201d version, the user needs to scroll through recommendations to see the search bar at the bottom of the screen.",
      "bbox": {
        "left": 0.5217769348543454,
        "top": 0.5009414595786972,
        "width": 0.39235806932636336,
        "height": 0.08287961555249763,
        "page": 9
      }
    }
  ],
  "2102.00625": [
    {
      "type": "figure",
      "text": "Figure 1: Survey Instrument. In Study 1, where AI advisors or human advisors assist human judges, survey respondents were asked to assign responsibility notions to AI and human advisors. In Study 2, where AI systems are decision-makers alongside human judges, survey respondents were asked to assign responsibility notions to AI and human decision-makers. Both studies employed a factorial within-subjects design that presented eight different vignettes to each respondent. Survey instruments are shown in Figure 4 of the Appendix.",
      "bbox": {
        "left": 0.08972057018404693,
        "top": 0.2846683155406605,
        "width": 0.8199048135794845,
        "height": 0.06883297544537169,
        "page": 4
      }
    },
    {
      "type": "table",
      "text": "Table 1: Respondents\u2019 demographics compared to the 2016 U.S. Census [100] and Pew data (marked with \u2020 ) [75].",
      "bbox": {
        "left": 0.08927226845735038,
        "top": 0.39503055148654515,
        "width": 0.3930577546163322,
        "height": 0.02693465261748343,
        "page": 5
      }
    },
    {
      "type": "figure",
      "text": "Figure 2: The overall attribution of responsibility to AI or human agents in bail decisions (left) and the correlation matrix across different responsibility notions (right). The \ud835\udc66 -axis indicates the degree to which participants attributed each notion of responsibility, based on a 7-pt Likert Scale (-3 = Strongly Disagree, 3 = Strongly Agree).",
      "bbox": {
        "left": 0.08864158430909799,
        "top": 0.6437184882886482,
        "width": 0.830123614641576,
        "height": 0.039913716942372945,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 3: Differences in responsibility attribution to AI programs and humans for bail decisions. \u2217 \ud835\udc5d < . 05 , \u2217\u2217 \ud835\udc5d < . 01 , \u2217\u2217\u2217 \ud835\udc5d <",
      "bbox": {
        "left": 0.08903773937350005,
        "top": 0.42533620198567706,
        "width": 0.7915843764161752,
        "height": 0.015067129424124054,
        "page": 7
      }
    },
    {
      "type": "figure",
      "text": "Figure 4: Example screenshots of the survey instrument used for Study 1. The study is available at https://thegcamilo.github. io/responsibility-compas/.",
      "bbox": {
        "left": 0.09742629605960222,
        "top": 0.9039060033933081,
        "width": 0.8210285411161535,
        "height": 0.025327354970604482,
        "page": 14
      }
    },
    {
      "type": "figure",
      "text": "Figure 5: Attribution of responsibility for bail decisions depending on how the statements were phrased, recidivism, and advice/decision.",
      "bbox": {
        "left": 0.0916747672885072,
        "top": 0.8182425450797033,
        "width": 0.8232399310941011,
        "height": 0.028192154084793246,
        "page": 15
      }
    },
    {
      "type": "table",
      "text": "Table 3: Coefficients from the multivariate mixed effects model presented in Section 4.2. \u2217 \ud835\udc5d < . 05 , \u2217\u2217 \ud835\udc5d < . 01 , \u2217\u2217\u2217 \ud835\udc5d < . 001",
      "bbox": {
        "left": 0.11169535817663653,
        "top": 0.7724122326783459,
        "width": 0.7625837887034697,
        "height": 0.013352365204782196,
        "page": 16
      }
    }
  ],
  "2103.00912": [
    {
      "type": "figure",
      "text": "Figure 1: Our visual analytics tool, GestureMap : A configuration view to initialize and run the interactive clustering algorithm (a), an overview of the entire application window (b), a density plot projected onto the gesture map (c). The numbers mark different components further explained in Section 5.",
      "bbox": {
        "left": 0.09307575225830078,
        "top": 0.5307587517632378,
        "width": 0.8207256529066298,
        "height": 0.04089663727114899,
        "page": 0
      }
    },
    {
      "type": "table",
      "text": "Table 1: Main analysis components in GestureMap with the challenge and related work that motivated this feature and a reference to the supported action within the Knowledge Gen- eration Model for Visual Analytics [45].",
      "bbox": {
        "left": 0.5200817631740197,
        "top": 0.3921327687273122,
        "width": 0.3942996754365809,
        "height": 0.05420149215544113,
        "page": 2
      }
    },
    {
      "type": "figure",
      "text": "Figure 4: Gesture proposals for throw ball from four peo- ple (different colors). Trials per person are not discernible (same color), yet the colored paths distinctly cover different regions, revealing high consistency per person.",
      "bbox": {
        "left": 0.5194586959539675,
        "top": 0.629976445978338,
        "width": 0.3959922541200725,
        "height": 0.05481712264244003,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 3: Left: Scatter plots show gesture poses for four refer- ents elicitet by Vatavu [59] ( crouch, draw circle, draw flower, draw square) . Right: Variances of the gestures\u2019 DTW dis- tances to their average gesture sequence.",
      "bbox": {
        "left": 0.5203401154162837,
        "top": 0.32265410760436397,
        "width": 0.3947184942906199,
        "height": 0.054783869271326543,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 2: Gesture map for the dataset by Vatavu [59]. Pose landmarks represent poses in that part of the learned ges- ture space. Marked areas are referenced in Section 6.3.",
      "bbox": {
        "left": 0.08890413147172117,
        "top": 0.47543450557824335,
        "width": 0.3933533874212527,
        "height": 0.041473735462535515,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 6: Combined gesture space from [3, 8, 17, 59]. The den- sity plots projected on this gesture map refer to [3] ( blue ) and [59] ( orange ).",
      "bbox": {
        "left": 0.520047804888557,
        "top": 0.5239216197620739,
        "width": 0.39371151394314235,
        "height": 0.04204208682281802,
        "page": 7
      }
    },
    {
      "type": "figure",
      "text": "Figure 5: Gesture paths for adults and children for \u201cjumping\u201d referents from two studies [3, 23].",
      "bbox": {
        "left": 0.5223302404864941,
        "top": 0.25006117001928463,
        "width": 0.39460968815423303,
        "height": 0.027811339407256157,
        "page": 7
      }
    }
  ],
  "2102.09087": [
    {
      "type": "figure",
      "text": "Figure 1: TapNet is a one-model-for-all solution that jointly learns from cross-device data and performs multiple tap recognition tasks at a time.",
      "bbox": {
        "left": 0.5211587544360192,
        "top": 0.43938049162277065,
        "width": 0.3866172242008783,
        "height": 0.0418324326023911,
        "page": 0
      }
    },
    {
      "type": "figure",
      "text": "Figure 2: Accelerometer and gyroscope signals are processed in the Gating component and passed to TapNet if met set criteria. Using phone form factor (embedded in the device vector) as auxiliary information, TapNet jointly recognizes multiple tap properties, including tap event, direction, finger part, and location.",
      "bbox": {
        "left": 0.09097049438875485,
        "top": 0.3215764556268249,
        "width": 0.8261634103612963,
        "height": 0.0422793109007556,
        "page": 2
      }
    },
    {
      "type": "table",
      "text": "Table 1: The five tasks of TapNet: four classification tasks to detect different tap properties, such as tap direction and location, and one regression task to estimate tap location.",
      "bbox": {
        "left": 0.0892488629210229,
        "top": 0.6688697121360085,
        "width": 0.39287372040592766,
        "height": 0.040081139766808715,
        "page": 2
      }
    },
    {
      "type": "figure",
      "text": "Figure 3: Four common phone grip gestures investigated in the multi-person dataset. Only natural tapping actions with these grip gestures were collected for testing.",
      "bbox": {
        "left": 0.5218247058344823,
        "top": 0.4679154290093316,
        "width": 0.389525619207644,
        "height": 0.041885144782788826,
        "page": 4
      }
    },
    {
      "type": "table",
      "text": "Table 2: The one-person dataset contains samples in differ- ent conditions of phone, grip gestures, and tapping actions.",
      "bbox": {
        "left": 0.08917528040268842,
        "top": 0.4489067925347222,
        "width": 0.3940754560084125,
        "height": 0.026869455973307293,
        "page": 4
      }
    },
    {
      "type": "table",
      "text": "Table 3: The multi-person dataset contains samples in differ- ent conditions of phone, grip gestures, and tapping actions. The number in the parenthesis shows the data percentage.",
      "bbox": {
        "left": 0.5218610077901603,
        "top": 0.2967184432829269,
        "width": 0.39093052483851615,
        "height": 0.04091686672634549,
        "page": 4
      }
    },
    {
      "type": "figure",
      "text": "Figure 4: (a) Normalized confusion matrix of the 35-class lo- cation classification while training on one person and test- ing on multiple. (b) An enlarged part of the confusion ma- trix. (c) The region IDs in a five-by-seven grid. Each region refers to a cell in the grid. Neighboring areas above or be- low the target has an index offset of five. Predicting to the nearby areas therefore leads to three parallel lines along the diagonal in the confusion matrix.",
      "bbox": {
        "left": 0.5224988351460376,
        "top": 0.772092645818537,
        "width": 0.3910452649484273,
        "height": 0.11146113848445391,
        "page": 5
      }
    },
    {
      "type": "figure",
      "text": "Figure 5: Weighted average F1 score of tap direction classifi- cation using multi-task learning (SIMO TapNet) and single- task learning (SISO TapNet). TapNet (either SIMO or its trun- cated variant) can considerably outperform a SVM [19, 39] and a tiny CNN model [17]. TapNet with multi-task learning shows advantage given small training data (e.g. 3K samples).",
      "bbox": {
        "left": 0.5207212859509038,
        "top": 0.2870200571387705,
        "width": 0.39379149792241114,
        "height": 0.08189850624161538,
        "page": 6
      }
    },
    {
      "type": "table",
      "text": "Table 5: Performance comparison between the one-to-n- participant and leave-one-participant-out evaluation. The columns with no color shading are classification tasks, and the one with purple shading is a regression task.",
      "bbox": {
        "left": 0.08863994498658025,
        "top": 0.6786720198814316,
        "width": 0.3945367912840999,
        "height": 0.0534356627801452,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 7: Performance comparison of one-channel CNN (TapNet) against its six-channel counterpart with two levels of model capacity (large models: 163K, 144K trainable pa- rameters; small models: 11K and 9K). These are the average results over three runs to cancel out training randomness. The one-channel CNN with 11K parameters can achieves comparable performance as that of the six-channel CNN with a larger number of parameter (144K). We tuned the number of filters and maintained its ratios across layers to roughly match the total number of trainable parameters be- tween the one-channel and six-channel models.",
      "bbox": {
        "left": 0.5214211956348295,
        "top": 0.2855186269740866,
        "width": 0.3930169897141799,
        "height": 0.14993630996858232,
        "page": 7
      }
    },
    {
      "type": "figure",
      "text": "Figure 6: Performance comparison of tap direction classifica- tion with auxiliary information (A+B) against fine tuning (B- >A, A->B), and training on device-specific data (A, B) alone. X-axis shows the number of training samples per device and y-axis indicates the weighted average F1 score. The jointly trained TapNet yields comparable performance to that of the fine tuned models on specific devices and outperforms the models trained on device-specific data alone.",
      "bbox": {
        "left": 0.08789827309402765,
        "top": 0.28828410909633445,
        "width": 0.39728858424168007,
        "height": 0.1106579713147096,
        "page": 7
      }
    },
    {
      "type": "figure",
      "text": "Figure 9: Four example use cases enabled by TapNet. Assis- tiveTap uses back tap and tilting for one-handed interaction. ExplorativeTap introduces a new two-sided interaction par- adigm for users with visual impairments. Interactive wall- papers enables interaction with UI background objects. In- ertial Touch senses tapping by force and angle changes and provides auxiliary information for capacitive sensing.",
      "bbox": {
        "left": 0.5192694570503983,
        "top": 0.22146330939398873,
        "width": 0.39731572967728757,
        "height": 0.09711672079683555,
        "page": 8
      }
    },
    {
      "type": "figure",
      "text": "Figure 8: Conceptual relation between performance and training strategy with one-person and multi-person data.",
      "bbox": {
        "left": 0.08928151847490298,
        "top": 0.2982311826763731,
        "width": 0.39501823475158293,
        "height": 0.026697486337989268,
        "page": 8
      }
    }
  ],
  "2102.04174": [
    {
      "type": "figure",
      "text": "Fig. 2. Artificial learners\u2019 performance in the non-item-specific condition. One dot represents one learner. Pane A: Parameters used to create the learners (the color gradient refers to the number of items learned in the Leitner system). B: Items learned with an omniscient psychologist. C: With an omniscient psychologist, the ratio between the items learned and all items seen. D: Average prediction error (shading denotes the area within one SD). E: Items learned with a non-omniscient psychologist. F: With a non-omniscient psychologist, the learned/seen-item ratio.",
      "bbox": {
        "left": 0.11941456015593087,
        "top": 0.49041512999871767,
        "width": 0.7034960229412403,
        "height": 0.060352903423887314,
        "page": 8
      }
    },
    {
      "type": "figure",
      "text": "Fig. 3. Artificial agents\u2019 performance in the item-specific condition. One dot represents one learner. A: Items learned when the psychologist is omniscient. B: The ratio between items learned and all items seen when the psychologist is omniscient . C: Average prediction error (shading denotes the one-SD band). D: Items learned when the psychologist is not omniscient . E: The ratio of items learned to items seen when the psychologist is not omniscient .",
      "bbox": {
        "left": 0.18114982555115144,
        "top": 0.5799547445894492,
        "width": 0.7015432968638302,
        "height": 0.05005695843937421,
        "page": 9
      }
    },
    {
      "type": "figure",
      "text": "Fig. 4. Human learners\u2019 performance. Each dot represents one learner (M vs. L: \ud835\udc41 = 24 , CS vs. L: \ud835\udc41 = 29 ). Panes A and C present the items learned, and panes B and D show the ratio of items learned to items seen.",
      "bbox": {
        "left": 0.1843723596311083,
        "top": 0.6120618184407552,
        "width": 0.6930052190045126,
        "height": 0.02421781751844618,
        "page": 11
      }
    },
    {
      "type": "figure",
      "text": "Fig. 5. Parameter estimates for human learners. A: Estimates for each user (with averaging over all items), where each dot represents one learner ( \ud835\udc41 = 53 ). B: Estimates for each item (averaged over all users), where each dot represents one item ( \ud835\udc41 = 2047 ).",
      "bbox": {
        "left": 0.12242987888311249,
        "top": 0.385995074956104,
        "width": 0.6959858439327066,
        "height": 0.023486397483132103,
        "page": 12
      }
    }
  ],
  "2108.11259": [
    {
      "type": "figure",
      "text": "Figure 1: Sigma\u2019s Notebook, showing list of rocks taught so far (top) and page of notes on Shale rock, with an explana- tion provided by a participant (bottom).",
      "bbox": {
        "left": 0.520519231659135,
        "top": 0.4886810996315696,
        "width": 0.3952833686778748,
        "height": 0.04131629250266335,
        "page": 2
      }
    },
    {
      "type": "figure",
      "text": "Figure 2: Teaching interface, with articles on the left-hand side, chat window on the right, and example interaction (affiliative condition).",
      "bbox": {
        "left": 0.08591335271698197,
        "top": 0.4858719026199495,
        "width": 0.8291183297151055,
        "height": 0.025773481889204544,
        "page": 4
      }
    },
    {
      "type": "figure",
      "text": "Figure 3: Results of the Pick-a-Mood pictorial self-report scale for (a) agent\u2019s mood and personality, and (b) self, across conditions, post-interaction. The number indicates the num- ber of participants that selected each emotion pictogram.",
      "bbox": {
        "left": 0.08936719956740834,
        "top": 0.837404732752328,
        "width": 0.3943607978571474,
        "height": 0.05516329678622159,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 4: Results of answers to questions on the agent and experience teaching.",
      "bbox": {
        "left": 0.23791703367544934,
        "top": 0.46578879308218907,
        "width": 0.5266377567465789,
        "height": 0.013548879912405304,
        "page": 7
      }
    }
  ],
  "2102.00593": [
    {
      "type": "table",
      "text": "Table 1: Participant details, including years of experience prescribing antidepressants, and in which of the two user studies they participated",
      "bbox": {
        "left": 0.5197013904845792,
        "top": 0.10579662130336569,
        "width": 0.395428426904616,
        "height": 0.04137190423830591,
        "page": 3
      }
    },
    {
      "type": "figure",
      "text": "Figure 1: Features included in the initial prototype (from top to bottom): (A) a patient scenario with stability and dropout scores, (B) stability score feature importance explanation, (C) personalized treatment recommendations.",
      "bbox": {
        "left": 0.09529487136142706,
        "top": 0.8572328355577257,
        "width": 0.8157762078677907,
        "height": 0.027488939689867424,
        "page": 4
      }
    },
    {
      "type": "figure",
      "text": "Figure 2: Features included in the prototype redesign (from top to bottom): (A) patient information, (B) dropout score with links to further information about how dropout is defined and validation studies conducted on the tool, (C) interactive personalized treatment recommendations.",
      "bbox": {
        "left": 0.09298324584960938,
        "top": 0.8505287555733112,
        "width": 0.8239313512066611,
        "height": 0.0412815749043166,
        "page": 7
      }
    },
    {
      "type": "table",
      "text": "Table 2: Summary of changes made to the design of the MDD decision support prototype based on study 1 findings",
      "bbox": {
        "left": 0.1020091998031716,
        "top": 0.10557209361683238,
        "width": 0.7814973257725535,
        "height": 0.013381283692639283,
        "page": 8
      }
    }
  ],
  "2104.04842": [
    {
      "type": "figure",
      "text": "Figure 1: iChatProfile dashboard. (a) an example chatbot profile. (b) examples of auto-generated design suggestions for im- proving a chatbot.",
      "bbox": {
        "left": 0.09355545043945312,
        "top": 0.6378285571782276,
        "width": 0.8236622280544705,
        "height": 0.02762927431048769,
        "page": 0
      }
    },
    {
      "type": "figure",
      "text": "Figure 2: Examples of poorly handled conversations by an interview chatbot. (a) vague question and (b) unhandled user input. (c) chat transcripts set in practice with poorly handled conversations",
      "bbox": {
        "left": 0.08776012121462355,
        "top": 0.2973458453862354,
        "width": 0.8248105080299128,
        "height": 0.02773148122459951,
        "page": 2
      }
    },
    {
      "type": "figure",
      "text": "Figure 3: Example chatbot customizations supported by Juji. (a) Customizing a chat flow and a chatbot question. (b) Customiz- ing a chatbot response based on user sentiment. (c) The report dashboard that displays interviewee responses visually by Juji.",
      "bbox": {
        "left": 0.08937118256014157,
        "top": 0.26351003935842804,
        "width": 0.8267422782050239,
        "height": 0.027127872813831676,
        "page": 3
      }
    },
    {
      "type": "table",
      "text": "Table 1: Interview Questions Used to Build a Chatbot",
      "bbox": {
        "left": 0.11032699136173024,
        "top": 0.10905282184331104,
        "width": 0.3505629869847516,
        "height": 0.013525259615194917,
        "page": 4
      }
    },
    {
      "type": "table",
      "text": "Table 2: Metrics for evaluating the performance of interview chatbots.",
      "bbox": {
        "left": 0.2676580092486213,
        "top": 0.10854568866768269,
        "width": 0.46414563546772875,
        "height": 0.013982599431818182,
        "page": 6
      }
    },
    {
      "type": "figure",
      "text": "Figure 4: An overview of iChatProfile . (a) key components. (b) an example chat transcript and its conversation seg- ments.",
      "bbox": {
        "left": 0.08738809473374311,
        "top": 0.5055378615254104,
        "width": 0.3978477802151948,
        "height": 0.04071237583353062,
        "page": 7
      }
    },
    {
      "type": "table",
      "text": "Table 3: Metric-based chatbot design guidelines.",
      "bbox": {
        "left": 0.3427794462715099,
        "top": 0.10848951821375374,
        "width": 0.3152364344378702,
        "height": 0.014117924854008838,
        "page": 9
      }
    },
    {
      "type": "figure",
      "text": "Figure 5: Comparison of completion rate and user sentiment .",
      "bbox": {
        "left": 0.08585741005691827,
        "top": 0.30921857043950246,
        "width": 0.39950277914408766,
        "height": 0.014004293114247949,
        "page": 10
      }
    },
    {
      "type": "table",
      "text": "Table 6: Comparison of chatbot performance metrics between Group A (w/o iChatProfile) and Group B (w/ iChatProfile).",
      "bbox": {
        "left": 0.11441810458314185,
        "top": 0.48724503950639203,
        "width": 0.345352447110843,
        "height": 0.04231616704150884,
        "page": 10
      }
    },
    {
      "type": "table",
      "text": "Table 7: ANCOVA analysis results for chatbot performance metrics between baseline, Group A (w/o iChatProfile ), and Group B (w/ iChatProfile ).",
      "bbox": {
        "left": 0.2205862406811683,
        "top": 0.10932760527639677,
        "width": 0.5658700980392157,
        "height": 0.027274478565562855,
        "page": 11
      }
    },
    {
      "type": "table",
      "text": "Table 8: A summary of chatbot customizations made by designers in Group A (w/o iChatProfile) and Group B (w/ iChatProfile)",
      "bbox": {
        "left": 0.28836281471003117,
        "top": 0.109318935509884,
        "width": 0.4173692940107358,
        "height": 0.02682631906836924,
        "page": 12
      }
    }
  ]
}