{"uist-8": [{"text": "Text entry on smart glasses is an important subject considered by several recent studies [1, 11, 14, 39, 41, 46].", "bboxes": [{"left": 0.08761437908496732, "top": 0.6505618686868687, "width": 0.3976748366013072, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.6643989898989898, "width": 0.29781535947712423, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "id": "4839"}, {"text": "Gaze typing can overcome the ambiguous mapping problem of the touchpad-based typing.", "bboxes": [{"left": 0.5246633986928104, "top": 0.35404545454545455, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 0}, {"left": 0.5246633986928104, "top": 0.36788257575757577, "width": 0.19754084967320262, "height": 0.012579545454545482, "page": 0}], "section": "INTRODUCTION", "id": "4840"}, {"text": "GAT is two-step input approach for entering a character.", "bboxes": [{"left": 0.5246633986928104, "top": 0.7917979797979798, "width": 0.36678594771241846, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "id": "4841"}, {"text": "However, gaze typing also has limitations.", "bboxes": [{"left": 0.5246633986928104, "top": 0.49996338383838385, "width": 0.2844967320261438, "height": 0.012579545454545482, "page": 0}], "section": "INTRODUCTION", "id": "4842"}, {"text": "To overcome the problems of the touchpadand eye-gazebased methods, we propose the gaze-assisted typing (GAT).", "bboxes": [{"left": 0.5241601307189543, "top": 0.6873914141414141, "width": 0.4003774509803921, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246584967320261, "top": 0.7012285353535355, "width": 0.40002941176470586, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "id": "4843"}, {"text": "Copyright  2019 Association of Computing Machinery.", "bboxes": [{"left": 0.0913611111111111, "top": 0.8892525252525253, "width": 0.2607091503267974, "height": 0.008805555555555511, "page": 0}], "section": "INTRODUCTION", "id": "4844"}, {"text": "Considering the recent developments in mobile eye tracking technology (Pupil-labs 1 and Tobii Pro Glasses 2 2 ), eye input is a promising option to overcome the limitations of the touchpad-based typing methods.", "bboxes": [{"left": 0.5246633986928104, "top": 0.29115025252525256, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.3029861111111111, "width": 0.399874183006536, "height": 0.014580808080808083, "page": 0}, {"left": 0.524656862745098, "top": 0.31882449494949494, "width": 0.39717483660130704, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.33266161616161616, "width": 0.21115196078431375, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "id": "4845"}, {"text": "Previous researchers designed text entry techniques to overcome the limited input space of the touchpad on the frame of the glasses.", "bboxes": [{"left": 0.08811928104575163, "top": 0.46965909090909097, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.4834962121212121, "width": 0.3971797385620915, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4973333333333333, "width": 0.07394281045751634, "height": 0.012579545454545538, "page": 1}], "section": "On glasses frame input", "id": "4847"}, {"text": "Remote text entry techniques can be used for smart glasses.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6462714646464646, "width": 0.37890686274509805, "height": 0.012579545454545427, "page": 1}], "section": "On glasses frame input", "id": "4848"}, {"text": "Mid-air text entry was widely explored by early researchers using techniques such as mimicking typing on a standard keyboard [8, 14, 29, 45], key-stroke gesture input [35], and word-stroke gesture input [12, 25].", "bboxes": [{"left": 0.5246633986928104, "top": 0.14872727272727274, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.16256439393939395, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.17640151515151517, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 1}, {"left": 0.5240784313725491, "top": 0.1902386363636364, "width": 0.22187745098039213, "height": 0.012579545454545454, "page": 1}], "section": "Mid-air input", "id": "4849"}, {"text": "To enter text, voice input is a natural and fast technique [3]; however, its use is limited because", "bboxes": [{"left": 0.08761437908496732, "top": 0.33455681818181815, "width": 0.39903267973856205, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.34839393939393937, "width": 0.22026633986928107, "height": 0.012579545454545482, "page": 1}], "section": "Possible Text Entry Methods for Smart Glasses", "id": "4850"}, {"text": "In this study, we attempted to achieve a more usable on-glass text entry, primarily in terms of speed.", "bboxes": [{"left": 0.5246633986928104, "top": 0.34680808080808084, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.3606439393939394, "width": 0.25590522875817, "height": 0.012579545454545482, "page": 1}], "section": "Gaze-based Interaction for Head-worn Display (HWD)", "id": "4851"}, {"text": "The on-body input is usually achieved by using additional wearable devices, such as a nger-attached touch sensor for the on-nger input (DigiTouch [41]; 16.0 wpm), a wrist-worn sensor for on-palm input (PalmType [39]; 4.6 wpm), or a smartwatch for on-wrist input (SwipeBoard and HoldBoard [1]; 9.1 wpm and 10.2 wpm, respectively).", "bboxes": [{"left": 0.08761437908496732, "top": 0.7813737373737373, "width": 0.39767156862745096, "height": 0.012579545454545538, "page": 1}, {"left": 0.08753104575163399, "top": 0.7952108585858586, "width": 0.3980375816993464, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.3971797385620915, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.2709656862745098, "height": 0.012579545454545427, "page": 1}], "section": "On Body input", "id": "4852"}, {"text": "For gaze-based text entry, several studies focused on solving the Midas touch problem [15].", "bboxes": [{"left": 0.5246633986928104, "top": 0.5587247474747474, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.5725618686868686, "width": 0.19652941176470595, "height": 0.012579545454545538, "page": 1}], "section": "Gaze-based Text entry methods", "id": "4853"}, {"text": "We rst conducted a preliminary study to validate the effect of introducing eye input modality to a two-step touch-only text entry with a minimal load on eye input.", "bboxes": [{"left": 0.08735457516339869, "top": 0.11667803030303031, "width": 0.39793790849673205, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.1305151515151515, "width": 0.39717647058823524, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.14435227272727272, "width": 0.24770588235294122, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "id": "4854"}, {"text": "Another approach to avoid dwell time is to use the clickalternative rather than the dwell-click.", "bboxes": [{"left": 0.5240784313725491, "top": 0.7813737373737373, "width": 0.40046078431372545, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246601307189542, "top": 0.7952108585858586, "width": 0.2535800653594771, "height": 0.012579545454545427, "page": 1}], "section": "Gaze-based Text entry methods", "id": "4855"}, {"text": "For dwell-free eye typing, gesture-based eye typing approaches were thoroughly explored: Examples include a keystroke gesture approach (EyeWrite [43]: 4.9 wpm), continuous gesture approach (Dasher: 17.26 wpm [40], and wordstroke gesture approach (EyeSwipe [16]: 11.7 wpm).", "bboxes": [{"left": 0.5246633986928104, "top": 0.7046414141414141, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.759989898989899, "width": 0.34530555555555575, "height": 0.012579545454545427, "page": 1}], "section": "Gaze-based Text entry methods", "id": "4856"}, {"text": "On hand-held device input", "bboxes": [{"left": 0.08811928104575163, "top": 0.6327575757575757, "width": 0.1705816993464052, "height": 0.011321969696969636, "page": 1}], "section": "On glasses frame input", "id": "4857"}, {"text": "To select a sub-keyboard, a user merely looks at it as shown in Figure 1(b).", "bboxes": [{"left": 0.08761437908496732, "top": 0.4830770202020202, "width": 0.397671568627451, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4969128787878788, "width": 0.09324183006535948, "height": 0.012579545454545427, "page": 2}], "section": "Sub-keyboard Selection", "id": "4858"}, {"text": "Our proposed approach is a two-step method using a combination of gaze and touch gesture for text entry.", "bboxes": [{"left": 0.08811928104575163, "top": 0.3722070707070707, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3860441919191919, "width": 0.3324003267973856, "height": 0.012579545454545427, "page": 2}], "section": "Gaze-based Text entry methods", "id": "4859"}, {"text": "When compared to the touch-only text entry, GAT can achieve better input speed than SwipeBoard [11] owing to the higher eye movement speed.", "bboxes": [{"left": 0.5238986928104574, "top": 0.7952108585858586, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39746078431372556, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.1454869281045752, "height": 0.012579545454545427, "page": 2}], "section": "Expected advantage of GAT", "id": "4860"}, {"text": "To select a key, a user can perform one of the nine touch gestures, similar to SwipeBoard [11], to enter a key as shown in Figure 1(c), i.e. , a tap gesture to select a center-positioned key on the sub-keyboard, and one of the eight directional swipe gestures to select a key corresponding to the swipe direction from the center-positioned key as shown in Figure 1(c).", "bboxes": [{"left": 0.08761437908496732, "top": 0.6354570707070707, "width": 0.4003790849673202, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.6492941919191919, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6628421717171717, "width": 0.39774509803921565, "height": 0.012868686868686918, "page": 2}, {"left": 0.08811928104575163, "top": 0.6769671717171717, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6908042929292929, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.3753725490196078, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "id": "4861"}, {"text": "Further, it is easy to confuse the forward and backward directions because the display and the touchpad are placed perpendicularly.", "bboxes": [{"left": 0.5246633986928104, "top": 0.5231919191919192, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.5370290404040404, "width": 0.399874183006536, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.5508661616161616, "width": 0.08609313725490197, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "id": "4862"}, {"text": "Thus, when a nger touches the touchpad, the touch cursor is rst placed at the center of the selected sub-keyboard, regardless of the position of placement.", "bboxes": [{"left": 0.5241601307189543, "top": 0.6691098484848484, "width": 0.3979542483660129, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246584967320261, "top": 0.6829469696969697, "width": 0.3998790849673203, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6967840909090909, "width": 0.2389477124183006, "height": 0.012579545454545538, "page": 2}], "section": "Key Selection", "id": "4863"}, {"text": "In fact, Gaze + Gesture interaction have been used not only for text entry but also for many other interactions [30, 31, 47], e.g., pointing, object manipulation, menu selection, etc .", "bboxes": [{"left": 0.08811928104575163, "top": 0.0814570707070707, "width": 0.3977385620915033, "height": 0.012579545454545468, "page": 2}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.39919934640522875, "height": 0.012579545454545468, "page": 2}, {"left": 0.08755392156862744, "top": 0.10884217171717173, "width": 0.40057843137254906, "height": 0.012868686868686863, "page": 2}], "section": "Gaze-based Text entry methods", "id": "4864"}, {"text": "While selecting a key with a touch input, owing to eye jitter and target key searching behavior, the eyes of the user are not stable during a keystroke.", "bboxes": [{"left": 0.08735457516339869, "top": 0.822885101010101, "width": 0.398218954248366, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811764705882352, "top": 0.8367222222222221, "width": 0.39717483660130726, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.191359477124183, "height": 0.012579545454545427, "page": 2}], "section": "Key Selection", "id": "4865"}, {"text": "GAZE-ASSISTED TYPING, \"GAT\"", "bboxes": [{"left": 0.08811928104575163, "top": 0.35873863636363634, "width": 0.2291127450980392, "height": 0.011321969696969747, "page": 2}], "section": "Gaze-based Text entry methods", "id": "4866"}, {"text": "When compared to the eye-only text entry, the required delity of the eye tracker is expected to be lower than that of eye-only methods.", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.3985130718954249, "height": 0.012579545454545468, "page": 3}, {"left": 0.08812091503267974, "top": 0.09529419191919192, "width": 0.39774673202614386, "height": 0.012579545454545468, "page": 3}, {"left": 0.08812091503267974, "top": 0.10913131313131313, "width": 0.05805392156862746, "height": 0.012579545454545454, "page": 3}], "section": "Expected advantage of GAT", "id": "4867"}, {"text": "We recruited 12 participants (4 females and 8 males, mean age: 22.58, from 19 to 30 years) from our university.", "bboxes": [{"left": 0.5238986928104574, "top": 0.8090479797979797, "width": 0.39793790849673216, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246617647058823, "top": 0.822885101010101, "width": 0.35187745098039225, "height": 0.012579545454545427, "page": 3}], "section": "Participants", "id": "4868"}, {"text": "To implement a complete mobile gaze tracking system on smart glasses, we attempted to use an eye tracker from Pupil Labs and Tobii Pro Glasses", "bboxes": [{"left": 0.08761437908496732, "top": 0.614534090909091, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6283712121212122, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6422083333333334, "width": 0.18021568627450985, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "id": "4869"}, {"text": "We used a within-subject design with one factor, i.e. , technique: M-SwipeBoard and GAT.", "bboxes": [{"left": 0.5238986928104574, "top": 0.5172588383838383, "width": 0.4002042483660132, "height": 0.012868686868686918, "page": 3}, {"left": 0.5246633986928104, "top": 0.5313851010101011, "width": 0.1715882352941177, "height": 0.012579545454545427, "page": 3}], "section": "Design and Procedure", "id": "4870"}, {"text": "To verify GAT, initially, we demonstrate the net effect of introducing gaze modality with a minimum expression of the eye input.", "bboxes": [{"left": 0.08761437908496732, "top": 0.2108838383838384, "width": 0.4003774509803922, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811274509803921, "top": 0.2247209595959596, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811274509803921, "top": 0.2385580808080808, "width": 0.06527124183006536, "height": 0.012579545454545454, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "id": "4871"}, {"text": "The task provided was a memorizing and transcription task with the Mackenzie and Soukoreff phrase set [21].", "bboxes": [{"left": 0.5241601307189543, "top": 0.3782550505050505, "width": 0.3980866013071894, "height": 0.012579545454545482, "page": 3}, {"left": 0.524076797385621, "top": 0.3920921717171717, "width": 0.3374150326797384, "height": 0.012579545454545482, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "id": "4872"}, {"text": "The only difference between GAT and M-SwipeBoard is the sub-keyboard selection method.", "bboxes": [{"left": 0.08761437908496732, "top": 0.48888131313131317, "width": 0.39768137254901953, "height": 0.012579545454545371, "page": 3}, {"left": 0.08811928104575163, "top": 0.5027184343434343, "width": 0.2072892156862745, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "id": "4873"}, {"text": "For accurate and less exhausting eye input, the keyboard size was considered.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8019621212121212, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 3}, {"left": 0.08753267973856209, "top": 0.8157992424242425, "width": 0.10669934640522878, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "id": "4874"}, {"text": "To demonstrate the net effect, we modied the SwipeBoard design to be similar to our GAT design; we called this MSwipeBoard.", "bboxes": [{"left": 0.08761437908496732, "top": 0.3429633838383839, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811274509803921, "top": 0.3568005050505051, "width": 0.3998790849673203, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.3706376262626263, "width": 0.0851437908496732, "height": 0.012579545454545427, "page": 3}], "section": "PRELIMINARY STUDY: CAN ON-FRAME TOUCH TYPING BE IMPROVED WITH EYE INPUT?", "id": "4875"}, {"text": "Five of twelve experienced more eye fatigue while using GAT because they had to pay more attention to the eye movement; however, another one said that M-SwipeBoard was more tiring owing to the long usage time.", "bboxes": [{"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.39768137254901964, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3985163398692811, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.19015359477124183, "height": 0.012579545454545538, "page": 4}], "section": "Eye fatigue", "id": "4876"}, {"text": "We thought that GAT with a lesser number of touch gestures may require less effort of touch input by eliminating difcult gestures, i.e. , diagonal swipe gestures [11] and vertical swipe gestures.", "bboxes": [{"left": 0.5238986928104574, "top": 0.11353282828282829, "width": 0.39793464052287597, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.12736994949494948, "width": 0.39717483660130715, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.1409179292929293, "width": 0.39717810457516345, "height": 0.012868686868686863, "page": 4}, {"left": 0.5246633986928104, "top": 0.15504419191919191, "width": 0.05856372549019606, "height": 0.012579545454545454, "page": 4}], "section": "EXPERIMENT 1: CAN MORE EXPRESSIVE GAZE INPUT MAKE \"GAT\" BETTER?", "id": "4877"}, {"text": "We designed three variations of GAT: GAT3, GAT6, and GAT9.", "bboxes": [{"left": 0.5238986928104574, "top": 0.27064015151515153, "width": 0.40079411764705897, "height": 0.012579545454545427, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "id": "4878"}, {"text": "Figure 5 illustrates the CER and UER.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6748068181818182, "width": 0.26543627450980395, "height": 0.012579545454545427, "page": 4}], "section": "Text entry speed", "id": "4879"}, {"text": "We analyzed the text entry speed, corrected error rate (CER), and uncorrected error rate (UER) statistically using a two-way (2 techniques  3 blocks) repeated measure ANOVA (RMANOVA) on each.", "bboxes": [{"left": 0.08735457516339869, "top": 0.39248737373737375, "width": 0.39996405228758164, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.40632449494949496, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 4}, {"left": 0.08758169934640524, "top": 0.41988510101010096, "width": 0.4004101307189542, "height": 0.012856060606060649, "page": 4}, {"left": 0.08753267973856209, "top": 0.43399873737373734, "width": 0.12134803921568628, "height": 0.012579545454545482, "page": 4}], "section": "Results", "id": "4880"}, {"text": "Because we varied the number of touch gestures, the keyboard layouts were correspondingly varied.", "bboxes": [{"left": 0.5246633986928104, "top": 0.40272095959595955, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.41655808080808077, "width": 0.25012581699346403, "height": 0.012579545454545482, "page": 4}], "section": "GAT Variations: Different Number of Gestures", "id": "4881"}, {"text": "Figure 4 shows the text entry speed of GAT and MSwipeBoard.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5405656565656566, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.5544027777777778, "width": 0.08624183006535947, "height": 0.012579545454545427, "page": 4}], "section": "Text entry speed", "id": "4882"}, {"text": "We recruited 18 participants (8 females, 10 males, mean age: 20.72, from 17 to 24 years) from our university.", "bboxes": [{"left": 0.08735457516339869, "top": 0.6326792929292929, "width": 0.400202614379085, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6465151515151515, "width": 0.31986437908496734, "height": 0.012579545454545538, "page": 5}], "section": "Design and Procedure", "id": "4883"}, {"text": "The sub-keyboard size for all variations was 6.85  6.85; however, if the eye cursor out of boundary of the keyboard, a closest sub-keyboard will be activated.", "bboxes": [{"left": 0.08761437908496732, "top": 0.22955681818181817, "width": 0.39903104575163395, "height": 0.012856060606060649, "page": 5}, {"left": 0.08812745098039215, "top": 0.24367045454545455, "width": 0.39919934640522886, "height": 0.012579545454545427, "page": 5}, {"left": 0.08812745098039215, "top": 0.25750757575757577, "width": 0.2793496732026144, "height": 0.012579545454545427, "page": 5}], "section": "GAT Variations: Different Number of Gestures", "id": "4884"}, {"text": "We analyzed the results statistically for both NC and VGC, separately.", "bboxes": [{"left": 0.5238986928104574, "top": 0.5751502525252525, "width": 0.3999803921568629, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.5889873737373738, "width": 0.06900490196078435, "height": 0.012579545454545427, "page": 5}], "section": "Results", "id": "4885"}, {"text": "We used the within-subject design, with the layout as a factor, i.e. , GAT3, GAT6, and GAT9.", "bboxes": [{"left": 0.08735457516339869, "top": 0.3274785353535353, "width": 0.3999673202614379, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.34102651515151516, "width": 0.2010767973856209, "height": 0.012868686868686863, "page": 5}], "section": "Design and Procedure", "id": "4886"}, {"text": "Because the keyboard layouts were not typical, a novice user may spend a lot of time to discover a key.", "bboxes": [{"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3974460784313726, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.29313071895424836, "height": 0.012579545454545538, "page": 5}], "section": "Apparatus and Task", "id": "4887"}, {"text": "The apparatus was the same as that of the preliminary study.", "bboxes": [{"left": 0.08761437908496732, "top": 0.7441616161616161, "width": 0.40052777777777776, "height": 0.012579545454545538, "page": 5}], "section": "Apparatus and Task", "id": "4888"}, {"text": "For NC, two-way (the technique and the block) RM-ANOVA analysis on text entry speed and on CER showed signicant effects of the block (Speed: F ( 2 , 34 ) = 156.903, p < 0.01, CER: F ( 2 , 34 ) = 20.775, p < 0.01); however, the effects of layout and interaction were not signicant.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6709911616161616, "width": 0.39775653594771243, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.6848282828282829, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.698388888888889, "width": 0.3994395424836602, "height": 0.014803030303030207, "page": 5}, {"left": 0.5246633986928104, "top": 0.71222601010101, "width": 0.39716993464052297, "height": 0.014803030303030318, "page": 5}, {"left": 0.5246633986928104, "top": 0.7263396464646464, "width": 0.20920424836601315, "height": 0.012579545454545538, "page": 5}], "section": "Text entry speed and corrected error rate", "id": "4889"}, {"text": "For NC, a two-way RM-ANOVA with ART analysis indicated that all the effects and interaction were not signicant.", "bboxes": [{"left": 0.5246633986928104, "top": 0.7806691919191918, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.7945063131313131, "width": 0.36697058823529405, "height": 0.012579545454545427, "page": 5}], "section": "Uncorected Error rate", "id": "4890"}, {"text": "Figure 7 shows a visual guidance.", "bboxes": [{"left": 0.5246633986928104, "top": 0.47258459595959595, "width": 0.2243725490196079, "height": 0.012579545454545482, "page": 5}], "section": "Apparatus and Task", "id": "4891"}, {"text": "Friedman tests showed signicant effects of the layouts on all of the four questions: Easy to learn , Easy to use , Prefer to use , and Eye feels natural : p < 0.05, 0.01, 0.01, and 0.01; and,  ( 22 ) = 8.561, 15.524, 9.660, and 13.661, respectively.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8488358585858586, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8625858585858586, "width": 0.3975343137254903, "height": 0.012666666666666715, "page": 5}, {"left": 0.5246699346405229, "top": 0.8762335858585858, "width": 0.39851470588235294, "height": 0.012856060606060704, "page": 5}, {"left": 0.5246633986928104, "top": 0.8883472222222223, "width": 0.38145098039215686, "height": 0.018300505050505067, "page": 5}], "section": "Questionnaire", "id": "4892"}, {"text": "Visual Guidance Condition", "bboxes": [{"left": 0.08811928104575163, "top": 0.8093712121212122, "width": 0.17263235294117651, "height": 0.011321969696969636, "page": 5}], "section": "Apparatus and Task", "id": "4893"}, {"text": "Fifteen of the eighteen participants faced difculty in using the diagonal gestures and six out of these fteen also faced difculty in using the vertical swipe for GAT3.", "bboxes": [{"left": 0.08811928104575163, "top": 0.2146111111111111, "width": 0.3971732026143791, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.22844823232323233, "width": 0.3998774509803921, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.24228409090909092, "width": 0.2753137254901961, "height": 0.012579545454545427, "page": 6}], "section": "Comments from the participants", "id": "4894"}, {"text": "Owing to the differences in remote and wearable eye tracking environments, and the slippage and HWD wobbling problems of wearable eye trackers, we conducted Experiment 2 utilizing an eye-trackable HWD, FOVE VR 4 .", "bboxes": [{"left": 0.5246633986928104, "top": 0.6991464646464647, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7129835858585859, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7268207070707071, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.738655303030303, "width": 0.23382679738562107, "height": 0.014580808080808083, "page": 6}], "section": "Implementation", "id": "4895"}, {"text": "We analyzed the key input time to better understand the typing skills of users utilizing GAT, with only characters that were not erased and typed correctly (27,383 key entries).", "bboxes": [{"left": 0.08735457516339869, "top": 0.3511691919191919, "width": 0.3979379084967321, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3650063131313131, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3788434343434343, "width": 0.3345767973856209, "height": 0.012579545454545482, "page": 6}], "section": "Micro Analysis on Key Input Time", "id": "4896"}, {"text": "Considering these optimal values, the value of SWITCH is still far from an optimal level, even in the VGC (0.8, 1.0, and 1.0 for GAT3, GAT6, and GAT9, respectively).", "bboxes": [{"left": 0.5246633986928104, "top": 0.5068901515151515, "width": 0.39717483660130715, "height": 0.012666666666666715, "page": 6}, {"left": 0.5246633986928104, "top": 0.520814393939394, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 6}, {"left": 0.5234428104575163, "top": 0.5346515151515152, "width": 0.29698529411764707, "height": 0.012579545454545427, "page": 6}], "section": "Micro Analysis on Key Input Time", "id": "4897"}, {"text": "To understand the observation on T1 , we calculated the average number of sub-keyboard switches per letter inputted ( SWITCH , Figure 11(e)).", "bboxes": [{"left": 0.08761437908496732, "top": 0.6429166666666667, "width": 0.4003790849673202, "height": 0.012666666666666604, "page": 6}, {"left": 0.08811437908496732, "top": 0.6568409090909091, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 6}, {"left": 0.08758169934640524, "top": 0.670590909090909, "width": 0.17174836601307192, "height": 0.012666666666666715, "page": 6}], "section": "Micro Analysis on Key Input Time", "id": "4898"}, {"text": "We decomposed the key input time into four parts considering eyeand touch input phase; Eye movement data was logged every 30 ms. T1 is the time from the moment the previous key was entered to the moment the eye cursor entered a target sub-keyboard containing a target key.", "bboxes": [{"left": 0.08735457516339869, "top": 0.40022727272727276, "width": 0.39794281045751634, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.414064393939394, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.42781439393939397, "width": 0.39717810457516345, "height": 0.01266666666666666, "page": 6}, {"left": 0.08811928104575163, "top": 0.4417386363636364, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.4555757575757575, "width": 0.24215032679738563, "height": 0.012579545454545482, "page": 6}], "section": "Micro Analysis on Key Input Time", "id": "4899"}, {"text": "To validate that GAT has better usability than the monomodal text entry methods, we compared GAT to eye-only typing (adjustable dwell time [22]) and touch-only typing (SwipeZone [11]) utilizing eye-trackable HWD.", "bboxes": [{"left": 0.5241601307189543, "top": 0.61689898989899, "width": 0.3976781045751634, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6307361111111112, "width": 0.3998725490196079, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6445732323232324, "width": 0.39716830065359465, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6584103535353536, "width": 0.22940032679738565, "height": 0.012579545454545427, "page": 6}], "section": "EXPERIMENT 2: \"GAT\" EVALUATION ON \"HWD\"", "id": "4900"}, {"text": "The FOVE headset has 100 FoV and a resolution of the display is 2560  1440 pixels.", "bboxes": [{"left": 0.5241601307189543, "top": 0.8035530303030303, "width": 0.40037418300653593, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.8171136363636363, "width": 0.17844444444444452, "height": 0.012856060606060704, "page": 6}], "section": "Implementation", "id": "4901"}, {"text": "Wilcoxon-signed rank tests with Bonferroni correction showed that GAT6 got signicantly higher ratings than GAT3 for the different options; i.e. , Easy to learn (Z = -2.449, p < 0.05), Easy to use (Z = -3.372, p < 0.01), Prefer to use (Z = -2.864, p < 0.05).", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.3979379084967321, "height": 0.012579545454545468, "page": 6}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.39717810457516345, "height": 0.012579545454545468, "page": 6}, {"left": 0.08811928104575163, "top": 0.10884217171717173, "width": 0.3992091503267974, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.12269065656565657, "width": 0.3992042483660131, "height": 0.01285606060606058, "page": 6}, {"left": 0.08811928104575163, "top": 0.13652777777777778, "width": 0.0648545751633987, "height": 0.01285606060606062, "page": 6}], "section": "Questionnaire", "id": "4902"}, {"text": "The task was same as that of the previous studies.", "bboxes": [{"left": 0.5241601307189543, "top": 0.7385681818181818, "width": 0.31185457516339865, "height": 0.012579545454545427, "page": 7}], "section": "Task and Participants", "id": "4903"}, {"text": "We used plain QWERTY layout for the eye only typing as shown in gure 13 (d).", "bboxes": [{"left": 0.5238986928104574, "top": 0.5712272727272727, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5850643939393939, "width": 0.14745588235294116, "height": 0.012579545454545538, "page": 7}], "section": "Eye-only typing: Adjustable dwell time selection", "id": "4904"}, {"text": "We used the within-subject design with the technique as a factor; touch-only (SwipeZone), eye-only (A-Dwell), and GAT.", "bboxes": [{"left": 0.5238986928104574, "top": 0.8782335858585859, "width": 0.4006339869281046, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.39323039215686273, "height": 0.012579545454545427, "page": 7}], "section": "Design and Procedure", "id": "4905"}, {"text": "As shown in Figure 13(c), SwipeZone is nearly the same as the original.", "bboxes": [{"left": 0.5240784313725491, "top": 0.3324090909090909, "width": 0.39775490196078434, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246601307189542, "top": 0.3462462121212121, "width": 0.10204901960784318, "height": 0.012579545454545482, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "id": "4906"}, {"text": "We constructed a VR environment that was similar to the smart glass-based environment.", "bboxes": [{"left": 0.08735457516339869, "top": 0.5704444444444444, "width": 0.3979379084967321, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.5842815656565656, "width": 0.16820098039215686, "height": 0.012579545454545427, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "id": "4907"}, {"text": "SwipeZone is a two-step key selection method, similar to SwipeBoard, but using different sub-keyboard selection gestures.", "bboxes": [{"left": 0.5246633986928104, "top": 0.1449810606060606, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 7}, {"left": 0.5246633986928104, "top": 0.15881818181818183, "width": 0.39988235294117647, "height": 0.012579545454545454, "page": 7}, {"left": 0.5246633986928104, "top": 0.17265530303030302, "width": 0.03642973856209153, "height": 0.012579545454545454, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "id": "4908"}, {"text": "We chose the Adjustable dwell time selection method [22] as the eye only typing baseline, i.e. , A-Dwell.", "bboxes": [{"left": 0.5238986928104574, "top": 0.4668219696969697, "width": 0.39793790849673216, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246617647058823, "top": 0.4803699494949495, "width": 0.31614542483660124, "height": 0.012868686868686863, "page": 7}], "section": "Eye-only typing: Adjustable dwell time selection", "id": "4909"}, {"text": "There were few options for the touch-only typing baseline for smart glasses.", "bboxes": [{"left": 0.5241601307189543, "top": 0.0959229797979798, "width": 0.3976715686274508, "height": 0.01257954545454544, "page": 7}, {"left": 0.5246584967320261, "top": 0.10976010101010102, "width": 0.11596405228758166, "height": 0.01257954545454544, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "id": "4910"}, {"text": "In Experiment 1, GAT3, GAT6, and GAT9 were equally effective; however, GAT6 was the most preferred.", "bboxes": [{"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.3998839869281046, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.2932320261437909, "height": 0.012579545454545538, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "id": "4911"}, {"text": "Touch-only typing: SwipeZone", "bboxes": [{"left": 0.5246633986928104, "top": 0.0824090909090909, "width": 0.196264705882353, "height": 0.011321969696969691, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "id": "4912"}, {"text": "Eye+Touch typing: GAT9", "bboxes": [{"left": 0.08811928104575163, "top": 0.6911275252525253, "width": 0.1612483660130719, "height": 0.011321969696969636, "page": 7}], "section": "Simulated gaze tracking smart glasses environment in VR", "id": "4913"}, {"text": "In the eye-only technique, four participants did not complete all the blocks.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5404027777777778, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.554239898989899, "width": 0.09339869281045753, "height": 0.012579545454545427, "page": 8}], "section": "Results", "id": "4914"}, {"text": "The order of sessions was counter balanced with a full factorial within subjects.", "bboxes": [{"left": 0.08761437908496732, "top": 0.22108459595959595, "width": 0.4003774509803922, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811274509803921, "top": 0.23492171717171717, "width": 0.1456356209150327, "height": 0.012579545454545454, "page": 8}], "section": "Design and Procedure", "id": "4915"}, {"text": "The post-hoc tests showed that GAT is signicantly faster than both touch-only (t 11 = 6.919, p < 0.01) and eye-only (t 11 = 3.882, p < 0.01) techniques.", "bboxes": [{"left": 0.08761437908496732, "top": 0.7093181818181817, "width": 0.39795424836601306, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.7228787878787879, "width": 0.3971748366013072, "height": 0.014049242424242436, "page": 8}, {"left": 0.08811928104575163, "top": 0.7367159090909091, "width": 0.1858382352941177, "height": 0.012856060606060482, "page": 8}], "section": "Text entry speed", "id": "4916"}, {"text": "The analysis methods were nearly the same as those for Experiment 1; however, we conducted three post-hoc tests only in the last block, and we used a one-tailed assumption for the tests because we assumed that GAT outperforms the others.", "bboxes": [{"left": 0.08761437908496732, "top": 0.47750757575757574, "width": 0.4003774509803921, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.49134343434343436, "width": 0.39773692810457517, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.5051805555555555, "width": 0.397171568627451, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5190176767676767, "width": 0.38745098039215686, "height": 0.012579545454545427, "page": 8}], "section": "Results", "id": "4917"}, {"text": "Finally, we gathered 12 participants  2 days  3 sessions (techniques)  3 blocks  4 tasks = 864 phrases.", "bboxes": [{"left": 0.08811928104575163, "top": 0.3805631313131313, "width": 0.39717156862745095, "height": 0.012856060606060649, "page": 8}, {"left": 0.08758660130718955, "top": 0.39439898989898986, "width": 0.3307173202614379, "height": 0.012857323232323303, "page": 8}], "section": "Design and Procedure", "id": "4918"}, {"text": "Friedman tests did not show signicant effects on the other two questions.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6257462121212122, "width": 0.39745588235294116, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6395833333333334, "width": 0.09668627450980405, "height": 0.012579545454545427, "page": 8}], "section": "Questionnaire", "id": "4919"}, {"text": "The experiment consisted of three sessions per day; the participants participated for two days in sequence.", "bboxes": [{"left": 0.08761437908496732, "top": 0.0814570707070707, "width": 0.4003774509803922, "height": 0.012579545454545468, "page": 8}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.29101470588235295, "height": 0.012579545454545468, "page": 8}], "section": "Design and Procedure", "id": "4920"}, {"text": "At the start of each session, we calibrated the eye tracker with the default calibration method of FOVE.", "bboxes": [{"left": 0.08753267973856209, "top": 0.14435227272727272, "width": 0.3977549019607843, "height": 0.012579545454545454, "page": 8}, {"left": 0.08811928104575163, "top": 0.15818939393939394, "width": 0.26888888888888896, "height": 0.012579545454545454, "page": 8}], "section": "Design and Procedure", "id": "4921"}, {"text": "The RM-ANOVA analysis on text entry speed showed significant effects of the technique (F ( 2 , 22 ) = 11.020, p < 0.01) and block (F ( 2 . 696 , 29 . 654 ) = 48.822, p < 0.01).", "bboxes": [{"left": 0.08761437908496732, "top": 0.6464229797979798, "width": 0.40037581699346403, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6599823232323233, "width": 0.3971764705882353, "height": 0.014804292929292862, "page": 8}, {"left": 0.08811928104575163, "top": 0.6738194444444444, "width": 0.2563562091503268, "height": 0.014803030303030318, "page": 8}], "section": "Text entry speed", "id": "4922"}, {"text": "The RM-ANOVA analysis on CER showed signicant effects of block (F ( 5 , 55 ) = 3.260, p < 0.05); however, the effects of technique and interaction were not signicant.", "bboxes": [{"left": 0.08761437908496732, "top": 0.8291742424242424, "width": 0.39767320261437905, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8427348484848485, "width": 0.3971650326797386, "height": 0.014803030303030318, "page": 8}, {"left": 0.08811928104575163, "top": 0.8568484848484849, "width": 0.3011078431372549, "height": 0.012579545454545427, "page": 8}], "section": "Error rate", "id": "4923"}, {"text": "The RM-ANOVA with ART analysis on UER showed that both the effects and the interaction were not signicant.", "bboxes": [{"left": 0.08761437908496732, "top": 0.8782335858585859, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.36173039215686276, "height": 0.012579545454545427, "page": 8}], "section": "Error rate", "id": "4924"}, {"text": "GAT got signicantly better ratings than touch-only for Easy to learn (Z= -2.565, p < 0.05), and better ratings than eyeonly for Prefer to use (Z= -2.280, p< 0.05) and Eye fatigue (Z= -2.980, p < 0.01).", "bboxes": [{"left": 0.5246650326797386, "top": 0.5350883838383839, "width": 0.39773856209150316, "height": 0.012666666666666604, "page": 8}, {"left": 0.5246650326797386, "top": 0.5487373737373737, "width": 0.399875816993464, "height": 0.012856060606060593, "page": 8}, {"left": 0.5246617647058823, "top": 0.5627626262626263, "width": 0.39717810457516356, "height": 0.012667929292929259, "page": 8}, {"left": 0.5241339869281045, "top": 0.5764103535353535, "width": 0.14168790849673207, "height": 0.012856060606060593, "page": 8}], "section": "Questionnaire", "id": "4925"}, {"text": "Friedman tests showed signicant effects of the technique on Easy to learn , Prefer to use , and Eye fatigue :  ( 22 ) = 8.667, 6.826, and 18.050, p < 0.05, 0.05, and 0.01, respectively.", "bboxes": [{"left": 0.5246633986928104, "top": 0.45573989898989903, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.4675757575757576, "width": 0.39718790849673213, "height": 0.018300505050505067, "page": 8}, {"left": 0.5246633986928104, "top": 0.4858409090909091, "width": 0.40002450980392157, "height": 0.012856060606060593, "page": 8}], "section": "Questionnaire", "id": "4926"}, {"text": "In our study, we considered rst the input modalities for the conventional gesture input and nalized on a touchpad.", "bboxes": [{"left": 0.5246633986928104, "top": 0.5162007575757576, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5300378787878788, "width": 0.3582450980392158, "height": 0.012579545454545427, "page": 9}], "section": "Scalability of GAT", "id": "4927"}, {"text": "Three of the ve participants chose touch-only as the best technique.", "bboxes": [{"left": 0.08761437908496732, "top": 0.32204292929292927, "width": 0.39767156862745096, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.3358800505050505, "width": 0.06601470588235293, "height": 0.012579545454545482, "page": 9}], "section": "Touch-only: SwipeZone", "id": "4928"}, {"text": "We presented the concept of GAT and the rst adaptation of a complementary combination of the gaze and touch input modalities for wearable text entry.", "bboxes": [{"left": 0.5238986928104574, "top": 0.6706780303030303, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246617647058823, "top": 0.6845151515151515, "width": 0.39717483660130715, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.6983522727272727, "width": 0.21854248366013063, "height": 0.012579545454545427, "page": 9}], "section": "CONCLUSION", "id": "4929"}, {"text": "However, when compared to eye-only typing, the typing speed may be questionable as the previous studies were conducted in a desk-based environment.", "bboxes": [{"left": 0.5246633986928104, "top": 0.2434810606060606, "width": 0.39717320261437905, "height": 0.01257954545454551, "page": 9}, {"left": 0.5246633986928104, "top": 0.25731818181818183, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.27115530303030305, "width": 0.19403758169934648, "height": 0.012579545454545427, "page": 9}], "section": "Performance Comparison with Literature", "id": "4930"}, {"text": "The other two participants chose eye-only as the best.", "bboxes": [{"left": 0.08761437908496732, "top": 0.48353156565656563, "width": 0.35825326797385615, "height": 0.012579545454545482, "page": 9}], "section": "Touch-only: SwipeZone", "id": "4931"}, {"text": "For the \"Prefer to use\" question, we asked the reason for the highest/lowest rating.", "bboxes": [{"left": 0.08811928104575163, "top": 0.0959229797979798, "width": 0.3971748366013072, "height": 0.01257954545454544, "page": 9}, {"left": 0.08811928104575163, "top": 0.10976010101010102, "width": 0.14101143790849674, "height": 0.01257954545454544, "page": 9}], "section": "Questionnaire", "id": "4932"}, {"text": "Seven out of twelve participants chose GAT as the best technique because it was the fastest.", "bboxes": [{"left": 0.08811928104575163, "top": 0.1743901515151515, "width": 0.39986928104575165, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.18822727272727272, "width": 0.20704411764705882, "height": 0.012579545454545454, "page": 9}], "section": "Questionnaire", "id": "4933"}, {"text": "Even though the learning curve of GAT was steeper than A-Dwell, a longitudinal study was required to compare both techniques after long-term training.", "bboxes": [{"left": 0.5246633986928104, "top": 0.3893989898989899, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.5240784313725491, "top": 0.40323611111111113, "width": 0.3977630718954248, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.41707196969696975, "width": 0.22383986928104582, "height": 0.012579545454545427, "page": 9}], "section": "Performance Comparison with Literature", "id": "4934"}, {"text": "If we consider that the eye-tracking accuracy may affect the performance of GAT variations, the results of Exp 1 may differ in the wearable context, because a wearable eye tracker may be less accurate than a stationary one.", "bboxes": [{"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.3974591503267974, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.22814215686274514, "height": 0.012579545454545538, "page": 9}], "section": "Validity of the Simulated Environment", "id": "4935"}, {"text": "We conducted the experiments in simulated environments.", "bboxes": [{"left": 0.08735457516339869, "top": 0.6417462121212121, "width": 0.4007941176470589, "height": 0.012579545454545427, "page": 9}], "section": "Validity of the Simulated Environment", "id": "4936"}, {"text": "Reasons for Preference", "bboxes": [{"left": 0.08811928104575163, "top": 0.08245454545454546, "width": 0.16403267973856211, "height": 0.011321969696969691, "page": 9}], "section": "Questionnaire", "id": "4937"}, {"text": "Although Exp 2 was conducted with a VR HWD rather than with an augmented reality (AR) glasses, it was effective", "bboxes": [{"left": 0.08753267973856209, "top": 0.8782335858585859, "width": 0.39776797385620916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08753267973856209, "top": 0.892070707070707, "width": 0.3977549019607843, "height": 0.012579545454545427, "page": 9}], "section": "Validity of the Simulated Environment", "id": "4938"}, {"text": "We demonstrated that GAT (11.04 wpm, after 40 min of usage) is faster than SwipeZone (8.53 wpm, after 50 min of usage) in Experiment 2.", "bboxes": [{"left": 0.5238986928104574, "top": 0.1805858585858586, "width": 0.39903921568627454, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246650326797386, "top": 0.19442297979797982, "width": 0.39717156862745084, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.208260101010101, "width": 0.09251307189542479, "height": 0.012579545454545454, "page": 9}], "section": "Performance Comparison with Literature", "id": "4939"}, {"text": "Eye-only: A-Dwell", "bboxes": [{"left": 0.08811928104575163, "top": 0.4700176767676768, "width": 0.11612418300653594, "height": 0.011321969696969691, "page": 9}], "section": "Touch-only: SwipeZone", "id": "4940"}, {"text": "Eye + Touch: GAT", "bboxes": [{"left": 0.08811928104575163, "top": 0.16087626262626262, "width": 0.11808660130718952, "height": 0.011321969696969691, "page": 9}], "section": "Questionnaire", "id": "4941"}, {"text": "This research was supported by Basic Science ResearchProgram through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF2015R1D1A1A01058992).", "bboxes": [{"left": 0.08761437908496732, "top": 0.0959229797979798, "width": 0.4003774509803922, "height": 0.01257954545454544, "page": 10}, {"left": 0.08811274509803921, "top": 0.10976010101010102, "width": 0.3998790849673203, "height": 0.01257954545454544, "page": 10}, {"left": 0.08811274509803921, "top": 0.12359722222222222, "width": 0.3998790849673203, "height": 0.012579545454545454, "page": 10}, {"left": 0.08811928104575163, "top": 0.13743434343434344, "width": 0.17769934640522878, "height": 0.012579545454545454, "page": 10}], "section": "CONCLUSION", "id": "4942"}, {"text": "ACKNOWLEDGMENTS", "bboxes": [{"left": 0.08811928104575163, "top": 0.08245454545454546, "width": 0.15820098039215685, "height": 0.011321969696969691, "page": 10}], "section": "CONCLUSION", "id": "4943"}], "uist-4": [{"text": "Todays smartphones enable a wide range of functionalities.", "bboxes": [{"left": 0.08761437908496732, "top": 0.646929292929293, "width": 0.4005457516339869, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "id": "4944"}, {"text": "In order to augment the input ability on a mobile device, several approaches have been explored previously, such as onscreen gestures [35, 1], keyboard shortcuts [32, 2], and extending the input space to the edge [7], back [49, 5], and above area [20, 9] of the device.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6631313131313132, "width": 0.39989052287581706, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.6769671717171717, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.6908042929292929, "width": 0.39989869281045753, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.7046414141414141, "width": 0.3971911764705881, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.16650980392156856, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "id": "4945"}, {"text": "In this paper, we introduce Lip-Interact, which enables users to access functionality on the smartphone by issuing silent speech commands (Figure 1).", "bboxes": [{"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.3971944444444445, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.2023153594771242, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "id": "4947"}, {"text": "The goal of Lip-Interact is to provide a simple, yet powerful approach to issuing commands on the smartphone.", "bboxes": [{"left": 0.08761437908496732, "top": 0.21353661616161618, "width": 0.39768954248366006, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.2273737373737374, "width": 0.32528431372549016, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "id": "4949"}, {"text": "One of the main challenges when designing interfaces for touch-based mobile devices is the limited size of the display.", "bboxes": [{"left": 0.5246633986928104, "top": 0.0959229797979798, "width": 0.3974738562091503, "height": 0.01257954545454544, "page": 1}, {"left": 0.5246633986928104, "top": 0.10976010101010102, "width": 0.40003594771241846, "height": 0.01257954545454544, "page": 1}], "section": "Accessing Functionality with Gestures", "id": "4950"}, {"text": "A silent speech interface (SSI) [16] is a system that allows speech communication without using the sound made when people vocalize their speech sounds.", "bboxes": [{"left": 0.5240784313725491, "top": 0.593945707070707, "width": 0.3977728758169934, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6077828282828283, "width": 0.3971911764705882, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.6216199494949495, "width": 0.23902941176470593, "height": 0.012579545454545427, "page": 1}], "section": "Silent Speech Interface and Lip Reading", "id": "4951"}, {"text": "Multimodal interfaces process two or more combined user inputs to create new interaction possibilities, such as Pen+Voice [15] and Gaze+Touch [38].", "bboxes": [{"left": 0.5246633986928104, "top": 0.2619116161616162, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.27545959595959596, "width": 0.39717973856209154, "height": 0.012868686868686863, "page": 1}, {"left": 0.5246633986928104, "top": 0.2892967171717172, "width": 0.18323692810457526, "height": 0.012868686868686863, "page": 1}], "section": "Multimodal Interfaces and Voice Interaction", "id": "4952"}, {"text": "Lip-Interact intersects with three key literatures: gesture interaction, voice input and silent speech interfaces.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39989542483660134, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.30694771241830066, "height": 0.012579545454545427, "page": 1}], "section": "RELATED WORK", "id": "4953"}, {"text": "The concept of silent speech is not new.", "bboxes": [{"left": 0.08761437908496732, "top": 0.4286401515151515, "width": 0.26773856209150326, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "id": "4954"}, {"text": "With the recent advance of computer vision technologies, lipreading from vision has achieved signicant improvement", "bboxes": [{"left": 0.5238986928104574, "top": 0.8782335858585859, "width": 0.40065849673202614, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 1}], "section": "Silent Speech Interface and Lip Reading", "id": "4955"}, {"text": "With the development of computer vision technology, the front camera on mobile devices has enabled a variety of new features, such as sele beautication, face-based AR animation and face unlock.", "bboxes": [{"left": 0.08735457516339869, "top": 0.2585063131313131, "width": 0.39795261437908497, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.27234343434343433, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.28618055555555555, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.30001767676767677, "width": 0.10995751633986929, "height": 0.012579545454545427, "page": 2}], "section": "LIP-INTERACT DESIGNS", "id": "4956"}, {"text": "With Lip-Interact on a smartphone, the user should be clearly informed about commands that are currently available.", "bboxes": [{"left": 0.5238986928104574, "top": 0.7537007575757576, "width": 0.3985310457516342, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.3535539215686275, "height": 0.012579545454545427, "page": 2}], "section": "User Learning and System Feedback", "id": "4957"}, {"text": "The commands silently spoken by the user must be recognized accurately and robustly.", "bboxes": [{"left": 0.08761437908496732, "top": 0.5572575757575757, "width": 0.3976977124183007, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.571094696969697, "width": 0.15885947712418302, "height": 0.012579545454545427, "page": 2}], "section": "High Level of Recognition Accuracy", "id": "4958"}, {"text": "Since touch is straightforward and performs well in many contexts on mobile devices, the goal of Lip-Interact is to offer users an alternative based on their current cognitive and motor conditions when touch is inferior.", "bboxes": [{"left": 0.08811928104575163, "top": 0.4217184343434343, "width": 0.39775653594771243, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.43555555555555553, "width": 0.3974673202614379, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.44939267676767675, "width": 0.3974738562091504, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.46322979797979796, "width": 0.21304738562091502, "height": 0.012579545454545482, "page": 2}], "section": "Compatible with Existing Touch Interaction", "id": "4959"}, {"text": "Please note that in this work, we cannot cover all the functionalities on smartphones in Lip-Interact because of their immense quantity, especially the built-in features of various apps.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6014924242424242, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6153295454545454, "width": 0.3974738562091502, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6291666666666667, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.6430037878787879, "width": 0.0345882352941177, "height": 0.012579545454545427, "page": 2}], "section": "Context-Aware Functionality Support", "id": "4960"}, {"text": "Figure 2 illustrates how Lip-Interact is integrated into smartphone usage.", "bboxes": [{"left": 0.5246633986928104, "top": 0.37255303030303033, "width": 0.39988725490196086, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.38639015151515155, "width": 0.08378431372549022, "height": 0.012579545454545427, "page": 2}], "section": "Context-Aware Functionality Support", "id": "4961"}, {"text": "Users are instructed to issue commands in a more \"standard\" way than when they are casually talking by exaggerating their", "bboxes": [{"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 2}, {"left": 0.08753267973856209, "top": 0.892070707070707, "width": 0.39806045751633984, "height": 0.012579545454545427, "page": 2}], "section": "High Level of Recognition Accuracy", "id": "4962"}, {"text": "Lip-Interact only considers short and visually distinguishable commands rather than long sentences such as those in a conversational system.", "bboxes": [{"left": 0.08811928104575163, "top": 0.7869305555555556, "width": 0.3971846405228758, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8007676767676767, "width": 0.39989052287581695, "height": 0.012579545454545538, "page": 2}, {"left": 0.0877124183006536, "top": 0.814604797979798, "width": 0.12161601307189546, "height": 0.012579545454545427, "page": 2}], "section": "High Level of Recognition Accuracy", "id": "4963"}, {"text": "Concise and Differentiable Commands", "bboxes": [{"left": 0.08811928104575163, "top": 0.7734179292929293, "width": 0.2500375816993464, "height": 0.011320707070707092, "page": 2}], "section": "High Level of Recognition Accuracy", "id": "4964"}, {"text": "Properly Exaggerating Lip Movements", "bboxes": [{"left": 0.08811928104575163, "top": 0.8647209595959596, "width": 0.24782516339869287, "height": 0.011320707070707092, "page": 2}], "section": "High Level of Recognition Accuracy", "id": "4965"}, {"text": "To implement the Lip-Interact prototype, we equipped a smartphone (Huawei P9 Plus with a 5.5 inch screen running Android 7.0) with a 720p/30-fps camera at the top (see Figure 5.a).", "bboxes": [{"left": 0.08761437908496732, "top": 0.45580934343434343, "width": 0.4003937908496732, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.46964646464646465, "width": 0.3971862745098039, "height": 0.012579545454545482, "page": 3}, {"left": 0.0877124183006536, "top": 0.48348358585858586, "width": 0.3678627450980393, "height": 0.012579545454545482, "page": 3}], "section": "SYSTEM IMPLEMENTATION", "id": "4966"}, {"text": "We used Standard Chinese as the experiment language to facilitate data collection and user evaluation locally.", "bboxes": [{"left": 0.08735457516339869, "top": 0.6529608585858586, "width": 0.4006650326797386, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6667967171717171, "width": 0.3008267973856209, "height": 0.012579545454545538, "page": 3}], "section": "Command Set", "id": "4967"}, {"text": "The frames from the front camera are processed with the DLib face detector [29].", "bboxes": [{"left": 0.5241601307189543, "top": 0.512195707070707, "width": 0.39769771241830065, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5260315656565657, "width": 0.11969607843137253, "height": 0.012579545454545427, "page": 3}], "section": "Segmenting Mouth Sequence of Silent Speaking", "id": "4968"}, {"text": "Our recognition method built on the latest research [4, 12] on lip reading in computer vision and was improved to adapt to", "bboxes": [{"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3971944444444445, "height": 0.012579545454545427, "page": 3}], "section": "End-to-End Deep-Learning Command Recognition", "id": "4969"}, {"text": "We also chose two representative apps for evaluation: WeChat 1 and Notepad .", "bboxes": [{"left": 0.08735457516339869, "top": 0.7692020202020201, "width": 0.39802777777777776, "height": 0.014580808080808194, "page": 3}, {"left": 0.08811928104575163, "top": 0.7847512626262626, "width": 0.08617320261437907, "height": 0.012868686868686918, "page": 3}], "section": "Command Set", "id": "4970"}, {"text": "To segment the command issuing sequence, an online sliding window algorithm [28] is used to to detect and transfer between the following four states: begin speaking, continue speaking, stop speaking and other (Figure 5.c).", "bboxes": [{"left": 0.5241601307189543, "top": 0.6166010101010101, "width": 0.4003970588235293, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6304381313131313, "width": 0.3974738562091503, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.643986111111111, "width": 0.39717973856209154, "height": 0.01286868686868703, "page": 3}, {"left": 0.5246633986928104, "top": 0.6578232323232323, "width": 0.3123169934640523, "height": 0.012868686868686918, "page": 3}], "section": "Segmenting Mouth Sequence of Silent Speaking", "id": "4971"}, {"text": "The output frames of each Spatial Transformer module are concatenated along the time dimension, forming a T  H  W  3input for the following 3  (3D Convolutions, 3D maxpooling, spatial dropout) layers.", "bboxes": [{"left": 0.08761437908496732, "top": 0.584320707070707, "width": 0.39768954248366006, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.5978686868686869, "width": 0.397187908496732, "height": 0.012868686868686918, "page": 4}, {"left": 0.08730555555555555, "top": 0.6117058080808081, "width": 0.40069771241830066, "height": 0.012868686868686807, "page": 4}, {"left": 0.08811928104575163, "top": 0.6258320707070707, "width": 0.21192973856209152, "height": 0.012579545454545538, "page": 4}], "section": "Representation Learning and Dynamic Modelling", "id": "4972"}, {"text": "The architecture starts with a Spatial Transformer network [24] on each input frame whose dimension is H  W  3.", "bboxes": [{"left": 0.08761437908496732, "top": 0.3457563131313131, "width": 0.39810457516339864, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.35930429292929295, "width": 0.36691830065359476, "height": 0.012868686868686863, "page": 4}], "section": "Spatial Transformer", "id": "4973"}, {"text": "Table 1 summarizes the hyperparameters of the recognition model.", "bboxes": [{"left": 0.08761437908496732, "top": 0.7675366161616162, "width": 0.39768954248366006, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7813737373737373, "width": 0.04405228758169934, "height": 0.012579545454545538, "page": 4}], "section": "Implementation and Training Details", "id": "4974"}, {"text": "Lip-Interact in mobile interactions.", "bboxes": [{"left": 0.08811928104575163, "top": 0.2593989898989899, "width": 0.23540849673202613, "height": 0.012579545454545482, "page": 4}], "section": "End-to-End Deep-Learning Command Recognition", "id": "4975"}, {"text": "After the recognizer returns the result, a vocal check is applied to distinguish Lip-Interact from users normal speaking.", "bboxes": [{"left": 0.5240784313725491, "top": 0.6393901515151516, "width": 0.3977761437908496, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.6532272727272728, "width": 0.3697140522875818, "height": 0.012579545454545427, "page": 4}], "section": "Post- Vocal Check", "id": "4976"}, {"text": "We rst apply a band-pass lter on the audio signal to remove part of the background noise, and then calculate the peaks (syllables) of the signal.", "bboxes": [{"left": 0.5238986928104574, "top": 0.7437967171717171, "width": 0.39794934640522894, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7576338383838385, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7714709595959596, "width": 0.1291454248366013, "height": 0.012579545454545427, "page": 4}], "section": "Post- Vocal Check", "id": "4977"}, {"text": "Our implementation on the smartphone uses Android Accessibility Service 2 .", "bboxes": [{"left": 0.5246633986928104, "top": 0.8425037878787879, "width": 0.39989215686274504, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8543383838383838, "width": 0.11296895424836595, "height": 0.014582070707070738, "page": 4}], "section": "Device Control and Feedback Implementation", "id": "4978"}, {"text": "Participants were rst given a short introduction to our task.", "bboxes": [{"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.40003104575163395, "height": 0.012579545454545427, "page": 5}], "section": "Experimental Setup and Design", "id": "4979"}, {"text": "We conducted the experiment in a laboratory space with participants sitting or standing comfortably (Figure 5.a) near a window.", "bboxes": [{"left": 0.08735457516339869, "top": 0.6559722222222222, "width": 0.40064869281045756, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6698093434343435, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 5}, {"left": 0.08753267973856209, "top": 0.6836464646464646, "width": 0.05397875816993465, "height": 0.012579545454545427, "page": 5}], "section": "Experimental Setup and Design", "id": "4980"}, {"text": "We recruited 22 participants (12M/10F, P1-P22), aged between 20 and 28 years ( M = 24.5).", "bboxes": [{"left": 0.08735457516339869, "top": 0.5582436868686869, "width": 0.39795261437908497, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5717916666666666, "width": 0.1799673202614379, "height": 0.012868686868686918, "page": 5}], "section": "Participants", "id": "4981"}, {"text": "Table 2 summarizes the overall classication accuracies of the four groups.", "bboxes": [{"left": 0.5241601307189543, "top": 0.7277954545454546, "width": 0.39769771241830065, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7416325757575758, "width": 0.07713562091503268, "height": 0.012579545454545427, "page": 5}], "section": "Leave-One-Subject-Out Cross-Validation", "id": "4982"}, {"text": "The computing is conducted on a host with CPU of Intel Core i7 4.20 GHz  8 and GPU of GeForce GTX 1080 Ti.", "bboxes": [{"left": 0.08761437908496732, "top": 0.3904621212121212, "width": 0.3976977124183007, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.40402272727272726, "width": 0.32462254901960785, "height": 0.012856060606060649, "page": 5}], "section": "Real-time considerations", "id": "4983"}, {"text": "The goal of our rst study was to verify the feasibility of using Lip-Interact on a mobile device.", "bboxes": [{"left": 0.08761437908496732, "top": 0.4881906565656566, "width": 0.3976895424836601, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.5020265151515152, "width": 0.24889215686274513, "height": 0.012579545454545427, "page": 5}], "section": "Real-time considerations", "id": "4984"}, {"text": "Before feeding the data to our model, we augmented the dataset by applying a horizontally mirrored transformation on the mouth crop images.", "bboxes": [{"left": 0.5246633986928104, "top": 0.397030303030303, "width": 0.3971911764705882, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.41086742424242423, "width": 0.3971960784313725, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.42470454545454545, "width": 0.1525424836601308, "height": 0.012579545454545482, "page": 5}], "section": "Data Augmentation", "id": "4985"}, {"text": "Of the 22 participants, the samples of one participant were retained as the validation data for evaluating the model, and the samples of the other 21 participants were used as the training data.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6233888888888889, "width": 0.3971911764705882, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.6372260101010101, "width": 0.39718627450980404, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.6510631313131314, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.6649002525252525, "width": 0.03078594771241827, "height": 0.012579545454545427, "page": 5}], "section": "Leave-One-Subject-Out Cross-Validation", "id": "4986"}, {"text": "For training and evaluation, we divided the 44 commands into four groups based on the principle of [ system functionality + current context ].", "bboxes": [{"left": 0.5246633986928104, "top": 0.47561742424242426, "width": 0.3971977124183006, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.48916540404040404, "width": 0.39947058823529424, "height": 0.012868686868686863, "page": 5}, {"left": 0.5246633986928104, "top": 0.5030025252525252, "width": 0.10742973856209148, "height": 0.012868686868686918, "page": 5}], "section": "Recognition Group", "id": "4987"}, {"text": "We quantied the exaggeration of lip movements by comparing the Maximum Mouth Opening Degree ( MMOD ) of the \"exaggerated\" silent speech vs. normal audible speech when the participants were issuing the same commands.", "bboxes": [{"left": 0.5238986928104574, "top": 0.263094696969697, "width": 0.40065359477124196, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.2766426767676768, "width": 0.3971911764705882, "height": 0.012868686868686863, "page": 5}, {"left": 0.5246633986928104, "top": 0.2907689393939394, "width": 0.3971895424836601, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.3046060606060606, "width": 0.3180228758169935, "height": 0.012579545454545482, "page": 5}], "section": "Quantifying the Exaggeration of Lip Movements", "id": "4988"}, {"text": "This section evaluates recognition accuracy across time to simulate real-world scenarios when the pre-trained model improves as the number of uses increases.", "bboxes": [{"left": 0.08761437908496732, "top": 0.45731818181818185, "width": 0.39768954248366006, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.471155303030303, "width": 0.39989705882352944, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.4849924242424242, "width": 0.25863725490196077, "height": 0.012579545454545482, "page": 6}], "section": "Progress over Time", "id": "4989"}, {"text": "Each participant was rst exposed to a practice session.", "bboxes": [{"left": 0.5246633986928104, "top": 0.7813737373737373, "width": 0.36640522875816983, "height": 0.012579545454545538, "page": 6}], "section": "Task and Procedure", "id": "4990"}, {"text": "The experiment was conducted in a laboratory space with participants sitting comfortably.", "bboxes": [{"left": 0.5241601307189543, "top": 0.6300631313131313, "width": 0.39768954248366006, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6439002525252525, "width": 0.20310784313725505, "height": 0.012579545454545538, "page": 6}], "section": "Experimental Design", "id": "4991"}, {"text": "The above results verify the feasibility of Lip-Interact from two aspects.", "bboxes": [{"left": 0.08761437908496732, "top": 0.7952108585858586, "width": 0.39768954248366006, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.08126470588235295, "height": 0.012579545454545538, "page": 6}], "section": "Summary", "id": "4992"}, {"text": "Figure 8 shows the results.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6309090909090909, "width": 0.17119444444444443, "height": 0.012579545454545427, "page": 6}], "section": "Progress over Time", "id": "4993"}, {"text": "Current smartphones use buttons, icons and menus to trigger functionality because they are easy to perceive and use.", "bboxes": [{"left": 0.5246633986928104, "top": 0.4029166666666667, "width": 0.39989869281045753, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.4167525252525252, "width": 0.40003104575163406, "height": 0.012579545454545482, "page": 6}], "section": "STUDY 2: EVALUATE LIP-INTERACT VS. TOUCH", "id": "4994"}, {"text": "We recruited 10 right-handed participants (P1-P10, 5M/5F), aged between 22 and 30.", "bboxes": [{"left": 0.5238986928104574, "top": 0.56177398989899, "width": 0.3999983660130719, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.5756111111111112, "width": 0.15590032679738564, "height": 0.012579545454545427, "page": 6}], "section": "Participants", "id": "4995"}, {"text": "The participant was then exposed to the experimental session.", "bboxes": [{"left": 0.08761437908496732, "top": 0.0814570707070707, "width": 0.40054248366013073, "height": 0.012579545454545468, "page": 7}], "section": "Task and Procedure", "id": "4996"}, {"text": "Table 4 lists participants quantitative rating of overall input easiness for the three input conditions , from strongly disagree 1 to strongly agree 5.", "bboxes": [{"left": 0.5241601307189543, "top": 0.4817209595959596, "width": 0.3976895424836602, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.4952689393939394, "width": 0.3971960784313726, "height": 0.012868686868686863, "page": 7}, {"left": 0.5234428104575163, "top": 0.5093952020202019, "width": 0.13561764705882362, "height": 0.012579545454545427, "page": 7}], "section": "Input Easiness", "id": "4997"}, {"text": "Table 3 shows the mean input time of accessing the seven types of functionality under three input conditions .", "bboxes": [{"left": 0.08761437908496732, "top": 0.7537007575757576, "width": 0.39768954248366006, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7672474747474747, "width": 0.3586176470588236, "height": 0.012868686868686918, "page": 7}], "section": "Measures", "id": "4998"}, {"text": "In particular, participants enjoyed input with two channels: touch + Lip-Interact together in the Notepad app.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6983522727272727, "width": 0.3994493464052288, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.7119002525252526, "width": 0.33777450980392165, "height": 0.012868686868686918, "page": 7}], "section": "User Behavior and Feedback with Lip-Interact", "id": "4999"}, {"text": "A total of 735 valid instances were collected.", "bboxes": [{"left": 0.08753267973856209, "top": 0.2374810606060606, "width": 0.29102124183006534, "height": 0.012579545454545427, "page": 7}], "section": "Results and Analysis", "id": "5000"}, {"text": "Lip-Interact also had the smallest standard deviation of input time on all interaction types, showing that Lip-Interact provided a more stable and dependable way to access functionality.", "bboxes": [{"left": 0.5246633986928104, "top": 0.3833320707070707, "width": 0.39989869281045753, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.3971691919191919, "width": 0.3971911764705882, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.41100631313131314, "width": 0.39989869281045753, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.42484343434343436, "width": 0.0313006535947713, "height": 0.012579545454545482, "page": 7}], "section": "Measures", "id": "5001"}, {"text": "We measured input time in seconds to quantify the efciency.", "bboxes": [{"left": 0.08735457516339869, "top": 0.31793560606060606, "width": 0.40079901960784314, "height": 0.012868686868686863, "page": 7}], "section": "Measures", "id": "5002"}, {"text": "All participants expressed that Lip-Interact was easy to understand and learn.", "bboxes": [{"left": 0.5240784313725491, "top": 0.593945707070707, "width": 0.4004820261437909, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6077828282828283, "width": 0.10073202614379084, "height": 0.012579545454545538, "page": 7}], "section": "User Behavior and Feedback with Lip-Interact", "id": "5003"}, {"text": "We collected a total of 90 blocks of task (10 participants  3 input conditions  3 replications).", "bboxes": [{"left": 0.08735457516339869, "top": 0.704364898989899, "width": 0.39793954248366015, "height": 0.012856060606060593, "page": 7}, {"left": 0.08811928104575163, "top": 0.7182020202020203, "width": 0.22248202614379087, "height": 0.012856060606060593, "page": 7}], "section": "Measures", "id": "5004"}, {"text": "Type II : Accessing between pages across different levels of the hierarchy.", "bboxes": [{"left": 0.08768954248366012, "top": 0.45030555555555557, "width": 0.39761274509803923, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.4641426767676768, "width": 0.06570588235294118, "height": 0.012579545454545482, "page": 7}], "section": "Measures", "id": "5005"}, {"text": "Type I : Accessing between multiple pages at the same level of the hierarchy (e.g. on the home screen, scroll left and right to locate and then tap to launch an app).", "bboxes": [{"left": 0.08768954248366012, "top": 0.4087941919191919, "width": 0.3976143790849673, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.42263131313131314, "width": 0.397186274509804, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.43646843434343435, "width": 0.24323039215686276, "height": 0.012579545454545482, "page": 7}], "section": "Measures", "id": "5006"}, {"text": "The denition of each input type was determined based on the participants actual operation, which was recorded on video and labelled manually later.", "bboxes": [{"left": 0.08761437908496732, "top": 0.6377335858585859, "width": 0.3976977124183007, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6515707070707071, "width": 0.39719117647058827, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6654078282828283, "width": 0.179952614379085, "height": 0.012579545454545427, "page": 7}], "section": "Measures", "id": "5007"}, {"text": "Type VI : Single tap out of the comfortable nger range (e.g. tap the back button in the upper left corner).", "bboxes": [{"left": 0.08768954248366012, "top": 0.5748383838383838, "width": 0.39762091503267977, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.588675505050505, "width": 0.2627990196078432, "height": 0.012579545454545427, "page": 7}], "section": "Measures", "id": "5008"}, {"text": "Type VII : Precise pointing which is difcult for touch because of the \" fat nger \"problem [42] (e.g. adjust a cursor in text).", "bboxes": [{"left": 0.08768954248366012, "top": 0.6025126262626262, "width": 0.3976143790849673, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6160606060606061, "width": 0.38723366013071897, "height": 0.012868686868686807, "page": 7}], "section": "Measures", "id": "5009"}, {"text": "Type V : Single tap within the comfortable nger range (e.g. tap the home button).", "bboxes": [{"left": 0.08768954248366012, "top": 0.5471641414141414, "width": 0.39762418300653596, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.5610012626262626, "width": 0.11498202614379087, "height": 0.012579545454545427, "page": 7}], "section": "Measures", "id": "5010"}, {"text": "Type IV : Accessing a functionality on the Quick Settings Dropdown panel (e.g. ashlight).", "bboxes": [{"left": 0.08768954248366012, "top": 0.5194911616161616, "width": 0.40032026143790844, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.5333282828282828, "width": 0.18324183006535955, "height": 0.012579545454545427, "page": 7}], "section": "Measures", "id": "5011"}, {"text": "The ten participants were divided into ve groups.", "bboxes": [{"left": 0.08761437908496732, "top": 0.6097739898989899, "width": 0.3197467320261438, "height": 0.012579545454545538, "page": 8}], "section": "Experimental Design and Task Procedure", "id": "5012"}, {"text": "Theoretically, issuing audible commands can achieve similar results to Lip-Interact in Study 2 with a higher and more stable input speed than touch.", "bboxes": [{"left": 0.08761437908496732, "top": 0.42261994949494947, "width": 0.3979803921568628, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.4364570707070707, "width": 0.3971911764705882, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.4502941919191919, "width": 0.19046568627450983, "height": 0.012579545454545482, "page": 8}], "section": "STUDY 3: EVALUATE LIP-INTERACT VS. VOICE INPUT", "id": "5013"}, {"text": "When watching videos on a smartphone, the users hands are not always available.", "bboxes": [{"left": 0.5238986928104574, "top": 0.7681148989898989, "width": 0.39795261437908513, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.7819520202020203, "width": 0.13182679738562098, "height": 0.012579545454545427, "page": 8}], "section": "MORE EXAMPLE APPLICATIONS", "id": "5014"}, {"text": "Finally, we encouraged participants to suggest more applications for Lip-Interact.", "bboxes": [{"left": 0.5246633986928104, "top": 0.5893914141414142, "width": 0.39989052287581706, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.6032272727272727, "width": 0.13901470588235298, "height": 0.012579545454545427, "page": 8}], "section": "MORE EXAMPLE APPLICATIONS", "id": "5015"}, {"text": "This example application comes from a real-life experience of one of our participants.", "bboxes": [{"left": 0.5241601307189543, "top": 0.8782335858585859, "width": 0.39768954248366006, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.1733562091503268, "height": 0.012579545454545427, "page": 8}], "section": "Shared Bike", "id": "5016"}, {"text": "For Comfort as user , participants expressed two concerns about voice control.", "bboxes": [{"left": 0.5246633986928104, "top": 0.3745770202020202, "width": 0.397187908496732, "height": 0.012868686868686863, "page": 8}, {"left": 0.5246633986928104, "top": 0.3887032828282828, "width": 0.12713725490196082, "height": 0.012579545454545427, "page": 8}], "section": "Experimental Design and Task Procedure", "id": "5017"}, {"text": "However, participants also thought that there was room for improvement for Lip-Interact.", "bboxes": [{"left": 0.08811928104575163, "top": 0.2983611111111111, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.3121982323232323, "width": 0.1755702614379085, "height": 0.012579545454545427, "page": 8}], "section": "User Behavior and Feedback with Lip-Interact", "id": "5018"}, {"text": "After \" open camera \", the use can quickly switch shooting mode and settings by issuing \" photo \", \" video \", \" panorama \", \" sele \", \" reverse camera \", \" ash on/off \" lip commands instead of recalling where the buttons are.", "bboxes": [{"left": 0.5240784313725491, "top": 0.6438712121212121, "width": 0.3977777777777778, "height": 0.012868686868686807, "page": 8}, {"left": 0.5246633986928104, "top": 0.6577083333333333, "width": 0.3992026143790849, "height": 0.012868686868686807, "page": 8}, {"left": 0.5246633986928104, "top": 0.6715454545454546, "width": 0.39717320261437916, "height": 0.012868686868686918, "page": 8}, {"left": 0.5246633986928104, "top": 0.6856717171717172, "width": 0.2266584967320262, "height": 0.012579545454545427, "page": 8}], "section": "MORE EXAMPLE APPLICATIONS", "id": "5019"}, {"text": "For Privacy , P4 commented that \" If there are other people around me, there is no privacy when using the voice interaction. People will know what I am doing on my device. Using the silent speech can largely protect my interaction privacy, unless others are looking at me very carefully. \"", "bboxes": [{"left": 0.5246633986928104, "top": 0.29784469696969695, "width": 0.39717973856209154, "height": 0.012868686868686863, "page": 8}, {"left": 0.5246633986928104, "top": 0.31168181818181817, "width": 0.39989705882352955, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.3255189393939394, "width": 0.39718300653594774, "height": 0.012579545454545482, "page": 8}, {"left": 0.5246633986928104, "top": 0.3393560606060606, "width": 0.40003104575163406, "height": 0.012579545454545482, "page": 8}, {"left": 0.5246633986928104, "top": 0.3531931818181818, "width": 0.3064117647058824, "height": 0.012868686868686863, "page": 8}], "section": "Experimental Design and Task Procedure", "id": "5020"}, {"text": "We chose the subway as our experimental environment.", "bboxes": [{"left": 0.08735457516339869, "top": 0.5607159090909091, "width": 0.354828431372549, "height": 0.012579545454545427, "page": 8}], "section": "Experimental Design and Task Procedure", "id": "5021"}, {"text": "During this study, participants issued a total of 367 Lip-Interact commands.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8585669191919192, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8724040404040404, "width": 0.07610130718954249, "height": 0.012579545454545538, "page": 8}], "section": "Experimental Design and Task Procedure", "id": "5022"}, {"text": "As for Comfort as people in the surrounding environment , most participants rated 3 (neutral) on voice input.", "bboxes": [{"left": 0.5240784313725491, "top": 0.47898358585858586, "width": 0.397767973856209, "height": 0.012868686868686918, "page": 8}, {"left": 0.5246633986928104, "top": 0.4931098484848485, "width": 0.29228104575163405, "height": 0.012579545454545427, "page": 8}], "section": "Experimental Design and Task Procedure", "id": "5023"}, {"text": "An analysis using the Friedman test showed that participants reported signicantly higher levels of satisfaction for LipInteract than for voice input on all three measures ( p = . 00157,  2 r = 10).", "bboxes": [{"left": 0.5240784313725491, "top": 0.23523863636363637, "width": 0.3977696078431373, "height": 0.012579545454545454, "page": 8}, {"left": 0.5246633986928104, "top": 0.2490757575757576, "width": 0.39989869281045753, "height": 0.012579545454545454, "page": 8}, {"left": 0.5246633986928104, "top": 0.26262373737373734, "width": 0.3992107843137255, "height": 0.012868686868686863, "page": 8}, {"left": 0.5249901960784313, "top": 0.2764734848484849, "width": 0.06581045751633996, "height": 0.013821969696969638, "page": 8}], "section": "Experimental Design and Task Procedure", "id": "5024"}, {"text": "Although in our study, Lip-Interact outperformed touch-based input on most of the investigated tasks with a shorter completion time and higher input easiness, our purpose is not to substitute touch with Lip-Interact but to provide an alternative input approach.", "bboxes": [{"left": 0.08753267973856209, "top": 0.5989027777777778, "width": 0.39776960784313725, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.612739898989899, "width": 0.3998986928104575, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6265770202020202, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6404141414141414, "width": 0.3998986928104575, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.6542512626262627, "width": 0.12908006535947714, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "id": "5025"}, {"text": "For the intention recognition, at present we solve it by detecting the changes of the mouth opening degree and the audio signal.", "bboxes": [{"left": 0.5246633986928104, "top": 0.40182449494949496, "width": 0.39990032679738563, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.4156616161616162, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.4294987373737374, "width": 0.04231699346405238, "height": 0.012579545454545427, "page": 9}], "section": "System Robustness", "id": "5026"}, {"text": "Some commercial smartphones like the iPhone X are already equipped with special hardwares to ensure the robustness of face detection.", "bboxes": [{"left": 0.5246633986928104, "top": 0.31125505050505053, "width": 0.39775490196078434, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.32509217171717175, "width": 0.3971911764705881, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.33892929292929297, "width": 0.09680718954248368, "height": 0.012579545454545427, "page": 9}], "section": "System Robustness", "id": "5027"}, {"text": "This work focuses on the implementation and evaluation of Lip-Interact for smartphone, but we also foresee the potential of Lip-Interact on other platforms.", "bboxes": [{"left": 0.08761437908496732, "top": 0.19111742424242426, "width": 0.3976895424836601, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.20495454545454547, "width": 0.3971911764705882, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.21879166666666666, "width": 0.22865032679738562, "height": 0.012579545454545482, "page": 9}], "section": "Smart Watch and Head-worn Device", "id": "5028"}, {"text": "Our implementation uses an additional mounted camera and host server for computing.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6316224747474748, "width": 0.3971830065359476, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.645459595959596, "width": 0.17984803921568637, "height": 0.012579545454545538, "page": 9}], "section": "Other Limitations", "id": "5029"}, {"text": "We have described Lip-Interact, a novel interaction technique that repurposes the front camera of smartphones to detect lip movement and recognize it into commands.", "bboxes": [{"left": 0.08735457516339869, "top": 0.3976376262626262, "width": 0.39795261437908497, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.41147474747474744, "width": 0.39719444444444446, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.42531186868686865, "width": 0.28545751633986927, "height": 0.012579545454545482, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "id": "5030"}, {"text": "This work is supported by the National Key Research and Development Plan under Grant No. 2016YFB1001200, the Natural Science Foundation of China under Grant No. 61672314 and No. 61572276, Tsinghua University Research Funding No. 20151080408, and also by Beijing Key Lab of Networked Multimedia.", "bboxes": [{"left": 0.5241601307189543, "top": 0.822885101010101, "width": 0.4003986928104576, "height": 0.012579545454545427, "page": 9}, {"left": 0.5242565359477125, "top": 0.8367222222222221, "width": 0.400299019607843, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.3971911764705882, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.3971911764705882, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.3971862745098039, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.08004575163398697, "height": 0.012579545454545427, "page": 9}], "section": "ACKNOWLEDGEMENT", "id": "5031"}, {"text": "In order to raise the recognition accuracy of silent speech to a practically usable level in a mobile input system, we add two", "bboxes": [{"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39719444444444446, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.3971960784313725, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "id": "5032"}, {"text": "Lastly, we discuss the main limitations of our work, which also points to the directions of future work.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8278421717171717, "width": 0.39719117647058827, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.841679292929293, "width": 0.2810800653594771, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "id": "5033"}, {"text": "Improve Recognition Accuracy of Silent Speech Commands", "bboxes": [{"left": 0.08811928104575163, "top": 0.8647209595959596, "width": 0.38764705882352934, "height": 0.011320707070707092, "page": 9}], "section": "DISCUSSION AND LIMITATIONS", "id": "5034"}], "uist-5": [{"text": "Despite the difficulty they experience, PLV use their residual vision extensively when navigating stairs [73].", "bboxes": [{"left": 0.5176797385620915, "top": 0.503361111111111, "width": 0.3941078431372549, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176633986928104, "top": 0.5179078282828282, "width": 0.31419281045751646, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION", "id": "5035"}, {"text": "As many as 1.2 billion people worldwide have low vision , a visual impairment that cannot be corrected with eyeglasses or contact lenses [11, 72].", "bboxes": [{"left": 0.08823529411764706, "top": 0.653354797979798, "width": 0.3941748366013072, "height": 0.01272727272727281, "page": 0}, {"left": 0.08823529411764706, "top": 0.6679027777777778, "width": 0.3941078431372549, "height": 0.012727272727272587, "page": 0}, {"left": 0.08823529411764706, "top": 0.6824494949494949, "width": 0.16803267973856212, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "id": "5036"}, {"text": "Our research explores AR visualization designs to facilitate stair navigation by leveraging PLVs residual vision.", "bboxes": [{"left": 0.5176470588235295, "top": 0.7945984848484848, "width": 0.3941503267973855, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176307189542484, "top": 0.8091464646464647, "width": 0.33934313725490184, "height": 0.012727272727272587, "page": 0}], "section": "INTRODUCTION", "id": "5037"}, {"text": "Stair navigation is one of the most dangerous mobility challenges for PLV [5].", "bboxes": [{"left": 0.5176470588235295, "top": 0.40880934343434344, "width": 0.394156862745098, "height": 0.012727272727272754, "page": 0}, {"left": 0.5176470588235295, "top": 0.4233573232323232, "width": 0.1299493464052287, "height": 0.012727272727272754, "page": 0}], "section": "RELATED WORK", "id": "5038"}, {"text": "Advances in augmented reality (AR) present a unique opportunity to address this problem.", "bboxes": [{"left": 0.5176797385620915, "top": 0.6997424242424243, "width": 0.3941584967320261, "height": 0.012727272727272587, "page": 0}, {"left": 0.5176633986928104, "top": 0.7142891414141413, "width": 0.19740032679738573, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "id": "5039"}, {"text": "In the humancomputer interaction field, researchers also explored the challenges that PLV face during navigation, including navigating stairs.", "bboxes": [{"left": 0.5176078431372549, "top": 0.07004545454545455, "width": 0.39412418300653584, "height": 0.012727272727272726, "page": 1}, {"left": 0.5176078431372549, "top": 0.08459217171717172, "width": 0.3942385620915032, "height": 0.012727272727272726, "page": 1}, {"left": 0.5176078431372549, "top": 0.09913888888888889, "width": 0.1725392156862745, "height": 0.012727272727272726, "page": 1}], "section": "RELATED WORK", "id": "5040"}, {"text": "In addition to navigation systems, researchers have also proposed stair detection algorithms [20, 30, 5760, 68, 79].", "bboxes": [{"left": 0.517531045751634, "top": 0.7797474747474746, "width": 0.3942091503267974, "height": 0.01272727272727281, "page": 1}, {"left": 0.5175147058823529, "top": 0.7942954545454546, "width": 0.36731045751633995, "height": 0.01272727272727281, "page": 1}], "section": "Safe Navigation for Blind People", "id": "5041"}, {"text": "Stair navigation is one of the most dangerous mobility challenges for PLV [5].", "bboxes": [{"left": 0.08820261437908496, "top": 0.7079103535353535, "width": 0.3941078431372549, "height": 0.01272727272727281, "page": 1}, {"left": 0.08820261437908496, "top": 0.7224570707070708, "width": 0.13253594771241833, "height": 0.012727272727272587, "page": 1}], "section": "RELATED WORK", "id": "5042"}, {"text": "Mobility problems for people who are blind and PLV can be divided into two categories: wayfinding ( i.e ., the global problem of planning and following routes from place to place) and safe navigation ( i.e ., the local problem of taking the next step safely without bumping into things or tripping) [75].", "bboxes": [{"left": 0.5176470588235295, "top": 0.2939608585858586, "width": 0.3941911764705882, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176633986928104, "top": 0.30850883838383836, "width": 0.3941584967320262, "height": 0.012727272727272754, "page": 1}, {"left": 0.5176633986928104, "top": 0.3230555555555556, "width": 0.3941584967320262, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176633986928104, "top": 0.3376035353535354, "width": 0.3941584967320261, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.35184469696969695, "width": 0.3940588235294119, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.3663926767676768, "width": 0.03134313725490201, "height": 0.012727272727272698, "page": 1}], "section": "Safe Navigation for Blind and PLV", "id": "5043"}, {"text": "We designed visualizations on two AR platforms that can generate immersive virtual content in the physical environment: projection-based AR and smartglasses.", "bboxes": [{"left": 0.08827450980392157, "top": 0.17916035353535356, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 1}, {"left": 0.08827450980392157, "top": 0.19370833333333334, "width": 0.39412581699346405, "height": 0.012727272727272754, "page": 1}, {"left": 0.08827450980392157, "top": 0.2082550505050505, "width": 0.3062107843137255, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION", "id": "5044"}, {"text": "Since perceiving stairs is essential for safe navigation, many obstacle avoidance systems also detected stairs [7, 17, 28, 34].", "bboxes": [{"left": 0.5175980392156863, "top": 0.5400303030303031, "width": 0.39418300653594773, "height": 0.012727272727272698, "page": 1}, {"left": 0.5175980392156863, "top": 0.5545770202020203, "width": 0.3942058823529412, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176143790849673, "top": 0.5688181818181819, "width": 0.025892156862745153, "height": 0.012727272727272698, "page": 1}], "section": "Safe Navigation for Blind People", "id": "5045"}, {"text": "In summary, we contribute the first exploration of AR visualizations to facilitate stair navigation for PLV.", "bboxes": [{"left": 0.08825816993464053, "top": 0.48494570707070705, "width": 0.39414215686274506, "height": 0.012727272727272698, "page": 1}, {"left": 0.08824183006535948, "top": 0.49918813131313133, "width": 0.3113937908496732, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "id": "5046"}, {"text": "To facilitate safe navigation, researchers designed obstacle avoidance systems for people who are blind ( e.g ., [1, 24, 48, 77]).", "bboxes": [{"left": 0.5176470588235295, "top": 0.4451729797979798, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.4597209595959596, "width": 0.3941911764705882, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176307189542484, "top": 0.4742676767676768, "width": 0.03132679738562083, "height": 0.012727272727272698, "page": 1}], "section": "Safe Navigation for Blind People", "id": "5047"}, {"text": "Mobility is critical but challenging for PLV.", "bboxes": [{"left": 0.08823529411764706, "top": 0.5985063131313131, "width": 0.29912254901960783, "height": 0.012727272727272698, "page": 1}], "section": "RELATED WORK", "id": "5048"}, {"text": "We evaluated our visualizations on each platform with 12 PLV.", "bboxes": [{"left": 0.08824183006535948, "top": 0.3464482323232323, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 1}, {"left": 0.08822549019607843, "top": 0.36099494949494954, "width": 0.03498366013071895, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "id": "5049"}, {"text": "Stair Navigation Experiences of PLV", "bboxes": [{"left": 0.08823529411764706, "top": 0.5851628787878788, "width": 0.2534885620915032, "height": 0.011515151515151478, "page": 1}], "section": "RELATED WORK", "id": "5050"}, {"text": "We sought to facilitate stair navigation by augmenting the stairs with AR visualizations.", "bboxes": [{"left": 0.08823529411764706, "top": 0.6130517676767676, "width": 0.39428594771241826, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.6275997474747476, "width": 0.1897565359477124, "height": 0.012727272727272698, "page": 2}], "section": "INITIAL EXPLORATION", "id": "5051"}, {"text": "Although there are no popular commercial devices in the market, researchers have prototyped different hand-held projection-based AR platforms [16, 18, 63, 82].", "bboxes": [{"left": 0.5176797385620915, "top": 0.48336616161616164, "width": 0.39426307189542487, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176797385620915, "top": 0.4979141414141414, "width": 0.39422385620915046, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176797385620915, "top": 0.5124608585858587, "width": 0.28818627450980394, "height": 0.012727272727272698, "page": 2}], "section": "VISUALIZATIONS FOR PROJECTION-BASED AR", "id": "5052"}, {"text": "Some research has contributed obstacle avoidance systems for PLV [26, 33, 41, 64].", "bboxes": [{"left": 0.08820261437908496, "top": 0.3891262626262626, "width": 0.3941830065359478, "height": 0.012727272727272698, "page": 2}, {"left": 0.08820261437908496, "top": 0.4036729797979798, "width": 0.165545751633987, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for PLV", "id": "5053"}, {"text": "In terms of low-tech tools, some PLV use optical devices to enhance their visual abilities.", "bboxes": [{"left": 0.08820261437908496, "top": 0.23638636363636362, "width": 0.3941895424836601, "height": 0.012727272727272754, "page": 2}, {"left": 0.08820261437908496, "top": 0.2509330808080808, "width": 0.20230718954248367, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for PLV", "id": "5054"}, {"text": "From an interaction perspective, we aimed to simulate use of a flashlight, which is commonly used by PLV in dark places [79]: when a user points the projection-based AR phone at the stairs, it recognizes several stairs in front of her and projects visualizations on those stairs in real time (Figure 1a).", "bboxes": [{"left": 0.5176470588235295, "top": 0.6345669191919192, "width": 0.39411274509803906, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.649114898989899, "width": 0.394142156862745, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.6636616161616161, "width": 0.3941454248366014, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176307189542484, "top": 0.6782095959595961, "width": 0.3941078431372549, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176307189542484, "top": 0.6927563131313131, "width": 0.39412581699346394, "height": 0.01272727272727281, "page": 2}], "section": "Visualization (and Sonification) Design", "id": "5055"}, {"text": "Based on the formative study, we narrowed down our target platforms to immersive AR platforms, specifically (1) handheld projection-based AR, and (2) optical see-through smartglasses.", "bboxes": [{"left": 0.5175898692810458, "top": 0.16459343434343435, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.17914141414141413, "width": 0.39422222222222214, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176078431372549, "top": 0.19368813131313134, "width": 0.39416503267973846, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176078431372549, "top": 0.20823611111111112, "width": 0.0503986928104575, "height": 0.012727272727272698, "page": 2}], "section": "INITIAL EXPLORATION", "id": "5056"}, {"text": "According to Zhao et al .", "bboxes": [{"left": 0.5176307189542484, "top": 0.7582121212121212, "width": 0.1633382352941175, "height": 0.012727272727272587, "page": 2}], "section": "Visualization (and Sonification) Design", "id": "5057"}, {"text": "We first explored the design space of hand-held projectionbased AR, which combines a camera that recognizes the environment and a projector that projects visual contents into that environment [61].", "bboxes": [{"left": 0.5176470588235295, "top": 0.359415404040404, "width": 0.3942401960784314, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176470588235295, "top": 0.3739633838383838, "width": 0.39407516339869286, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176470588235295, "top": 0.38851010101010097, "width": 0.39420751633986917, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.4030580808080808, "width": 0.14836764705882355, "height": 0.012727272727272754, "page": 2}], "section": "VISUALIZATIONS FOR PROJECTION-BASED AR", "id": "5058"}, {"text": "To determine what platforms would be appropriate, we began by conducting a formative study with 11 PLV (7 female, 4 male; age: 2870, mean = 40) to evaluate prototype visualizations for a smartphone.", "bboxes": [{"left": 0.08820261437908496, "top": 0.7803396464646465, "width": 0.39422385620915035, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.7948863636363636, "width": 0.3942107843137255, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.8094343434343434, "width": 0.3942336601307189, "height": 0.01272727272727281, "page": 2}, {"left": 0.08820261437908496, "top": 0.8239810606060606, "width": 0.18408823529411766, "height": 0.01272727272727281, "page": 2}], "section": "INITIAL EXPLORATION", "id": "5059"}, {"text": "There has been little research on navigation systems for low vision.", "bboxes": [{"left": 0.08823529411764706, "top": 0.19971843434343434, "width": 0.39423039215686273, "height": 0.012727272727272754, "page": 2}, {"left": 0.08823529411764706, "top": 0.21426641414141415, "width": 0.044058823529411775, "height": 0.012727272727272698, "page": 2}], "section": "Safe Navigation for PLV", "id": "5060"}, {"text": "To alert users of the presence of stairs as they approach, we first generate auditory feedback to provide an overview of", "bboxes": [{"left": 0.5176797385620915, "top": 0.8676161616161616, "width": 0.39416503267973846, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176633986928104, "top": 0.8821641414141413, "width": 0.39417483660130725, "height": 0.01272727272727281, "page": 2}], "section": "Visualization (and Sonification) Design", "id": "5061"}, {"text": "This prior research addressed only auditory feedback for people who are blind, overlooking PLVs preference to use their remaining vision.", "bboxes": [{"left": 0.08830555555555555, "top": 0.10670328282828283, "width": 0.3941960784313725, "height": 0.012727272727272726, "page": 2}, {"left": 0.08830555555555555, "top": 0.12125126262626264, "width": 0.39417320261437905, "height": 0.01272727272727274, "page": 2}, {"left": 0.08830555555555555, "top": 0.1357979797979798, "width": 0.15093954248366015, "height": 0.012727272727272726, "page": 2}], "section": "Safe Navigation for Blind People", "id": "5062"}, {"text": "We evaluated the visualizations for projection-based AR, aiming to answer three questions: (1) How do PLV perceive the different visualization designs?", "bboxes": [{"left": 0.5176470588235295, "top": 0.4975972222222222, "width": 0.39420751633986917, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.5121452020202021, "width": 0.394156862745098, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.5266919191919192, "width": 0.2338006535947713, "height": 0.01272727272727281, "page": 3}], "section": "Evaluation of Projection-Based AR Visualizations", "id": "5063"}, {"text": "Since locating the first and last stairs was most important but challenging for PLV [86], we distinguish the first and last stairs from the rest by projecting thick highlights on them (Figure 2a), while projecting thin highlights on the middle stairs (Figure 3a).", "bboxes": [{"left": 0.08823529411764706, "top": 0.3545820707070707, "width": 0.3941388888888889, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3691287878787879, "width": 0.39412745098039215, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.38367676767676767, "width": 0.39422058823529404, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.3982234848484848, "width": 0.3942075163398692, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.4127714646464647, "width": 0.11595261437908495, "height": 0.012727272727272698, "page": 3}], "section": "Visualization (and Sonification) Design", "id": "5064"}, {"text": "Since a staircase typically has stairs of uniform size, the middle stairs usually do not require much of the users attention.", "bboxes": [{"left": 0.08815359477124184, "top": 0.8724924242424242, "width": 0.39414052287581697, "height": 0.01272727272727281, "page": 3}, {"left": 0.08813725490196078, "top": 0.887040404040404, "width": 0.39421241830065357, "height": 0.01272727272727281, "page": 3}], "section": "Visualization (and Sonification) Design", "id": "5065"}, {"text": "Beyond these highlights, we sought ways to further emphasize the first and last stairs so that a user will notice them and perceive their exact location from a distance.", "bboxes": [{"left": 0.08820261437908496, "top": 0.49307954545454546, "width": 0.394124183006536, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5076275252525253, "width": 0.3941993464052288, "height": 0.012727272727272698, "page": 3}, {"left": 0.08820261437908496, "top": 0.5218686868686868, "width": 0.3023006535947713, "height": 0.012727272727272698, "page": 3}], "section": "Visualization (and Sonification) Design", "id": "5066"}, {"text": "We designed two middle highlights to support the user in a minimally obtrusive way.", "bboxes": [{"left": 0.5176470588235295, "top": 0.2945669191919192, "width": 0.39420751633986917, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.30911489898989897, "width": 0.16747385620915034, "height": 0.012727272727272754, "page": 3}], "section": "Visualization (and Sonification) Design", "id": "5067"}, {"text": "To support a range of visual abilities, the design alternatives can be selected and combined by a user to optimize her experience for a particular environment.", "bboxes": [{"left": 0.5176633986928104, "top": 0.4333585858585859, "width": 0.3941437908496731, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176633986928104, "top": 0.447905303030303, "width": 0.39422875816993463, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176633986928104, "top": 0.4624532828282828, "width": 0.24727287581699353, "height": 0.012727272727272754, "page": 3}], "section": "Visualization (and Sonification) Design", "id": "5068"}, {"text": "In each phase, we presented all design options to the participant and asked about their experiences, including whether or not they liked the design, whether the design distracted them from seeing the environment, and how they wanted to improve it.", "bboxes": [{"left": 0.0882843137254902, "top": 0.7594419191919192, "width": 0.39417156862745095, "height": 0.01272727272727281, "page": 4}, {"left": 0.08826797385620916, "top": 0.7739886363636364, "width": 0.39415849673202613, "height": 0.012727272727272587, "page": 4}, {"left": 0.08826797385620916, "top": 0.7885366161616161, "width": 0.3942761437908497, "height": 0.01272727272727281, "page": 4}, {"left": 0.08826797385620916, "top": 0.8030833333333334, "width": 0.39415849673202613, "height": 0.012727272727272587, "page": 4}, {"left": 0.08826797385620916, "top": 0.8176313131313131, "width": 0.055999999999999994, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "id": "5069"}, {"text": "During the visualization experience, we gave the participant our prototype smartphone and explained how to use it.", "bboxes": [{"left": 0.0882516339869281, "top": 0.6357967171717172, "width": 0.39419444444444446, "height": 0.012727272727272698, "page": 4}, {"left": 0.0882516339869281, "top": 0.6503434343434343, "width": 0.36376307189542484, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "id": "5070"}, {"text": "We ended the study with an exit interview, asking about the participants general experience with the prototype.", "bboxes": [{"left": 0.5177058823529412, "top": 0.7382638888888889, "width": 0.3941830065359476, "height": 0.012727272727272587, "page": 4}, {"left": 0.5176895424836602, "top": 0.752810606060606, "width": 0.3531781045751634, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "id": "5071"}, {"text": "We indicated the start points with yellow stickers on the landings, three feet away from the top and bottom stairs.", "bboxes": [{"left": 0.5176895424836602, "top": 0.5634040404040405, "width": 0.39417320261437894, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.577645202020202, "width": 0.3675245098039215, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "id": "5072"}, {"text": "After the participant experienced all design alternatives in all three phases, we asked them to select one alternative from", "bboxes": [{"left": 0.0882516339869281, "top": 0.8688459595959596, "width": 0.394140522875817, "height": 0.01272727272727281, "page": 4}, {"left": 0.0882516339869281, "top": 0.8833926767676769, "width": 0.39422712418300654, "height": 0.012727272727272587, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "id": "5073"}, {"text": "To reduce order effects, we used a simultaneous within-subjects design, switching the task condition after each walking up and down task.", "bboxes": [{"left": 0.5177222222222222, "top": 0.6725012626262626, "width": 0.3941405228758169, "height": 0.01272727272727281, "page": 4}, {"left": 0.5177058823529412, "top": 0.6870492424242425, "width": 0.3941813725490195, "height": 0.012727272727272587, "page": 4}, {"left": 0.5177058823529412, "top": 0.7015959595959596, "width": 0.1301013071895425, "height": 0.01272727272727281, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "id": "5074"}, {"text": "During the stair navigation portion of the study, participants conducted two stair navigation tasks: walking upstairs and walking downstairs.", "bboxes": [{"left": 0.517673202614379, "top": 0.45399999999999996, "width": 0.39417320261437905, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176895424836602, "top": 0.4685467171717172, "width": 0.39420915032679726, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176895424836602, "top": 0.483094696969697, "width": 0.1314215686274509, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "id": "5075"}, {"text": "We asked the participant to hold a regular phone with the back camera facing the stairs, assuming the projected visualizations were from the smartphone.", "bboxes": [{"left": 0.08823529411764706, "top": 0.4172941919191919, "width": 0.39416666666666667, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4318421717171717, "width": 0.3940915032679739, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4463888888888889, "width": 0.22724183006535947, "height": 0.012727272727272698, "page": 4}], "section": "Evaluation of Projection-Based AR Visualizations", "id": "5076"}, {"text": "Although we provided different visualizations (flash or movement) to further enhance the end highlights, most participants (seven out of 12) liked the original design.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8230833333333333, "width": 0.3942663398692811, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8376313131313131, "width": 0.3941421568627451, "height": 0.01272727272727281, "page": 5}, {"left": 0.08811928104575163, "top": 0.8521780303030303, "width": 0.331998366013072, "height": 0.01272727272727281, "page": 5}], "section": "Results", "id": "5077"}, {"text": "Two participants (P2, P3) liked the Moving Vertical Zebra the most.", "bboxes": [{"left": 0.5175081699346404, "top": 0.4021931818181818, "width": 0.39417483660130737, "height": 0.012727272727272754, "page": 5}, {"left": 0.5175081699346404, "top": 0.416739898989899, "width": 0.06097058823529411, "height": 0.012727272727272698, "page": 5}], "section": "Results", "id": "5078"}, {"text": "In terms of color, most participants preferred the bright yellow (seven out of 12), wanting to be alert on each step.", "bboxes": [{"left": 0.5174264705882353, "top": 0.8176742424242424, "width": 0.3942892156862746, "height": 0.01272727272727281, "page": 5}, {"left": 0.5174264705882353, "top": 0.8322222222222222, "width": 0.3575098039215686, "height": 0.01272727272727281, "page": 5}], "section": "Results", "id": "5079"}, {"text": "Three participants (P6, P4, P11) felt the flash effect grabbed their attention more and alerted them.", "bboxes": [{"left": 0.5174918300653595, "top": 0.3076414141414141, "width": 0.3941911764705883, "height": 0.012727272727272754, "page": 5}, {"left": 0.5175081699346404, "top": 0.32218939393939394, "width": 0.2536633986928106, "height": 0.012727272727272754, "page": 5}], "section": "Results", "id": "5080"}, {"text": "Although no participants chose the Moving Edge in the study, P6 felt it could be helpful since it indicated direction.", "bboxes": [{"left": 0.5175245098039216, "top": 0.4970492424242424, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 5}, {"left": 0.5175081699346404, "top": 0.5115959595959596, "width": 0.39425980392156856, "height": 0.012727272727272698, "page": 5}], "section": "Results", "id": "5081"}, {"text": "Even on a typical set of stairs, participants wanted the middle highlights to confirm that they are still on the stairs, which made them feel safe.", "bboxes": [{"left": 0.5174428104575164, "top": 0.7519128787878788, "width": 0.39416339869281036, "height": 0.01272727272727281, "page": 5}, {"left": 0.5174428104575164, "top": 0.7664595959595959, "width": 0.394156862745098, "height": 0.01272727272727281, "page": 5}, {"left": 0.5174428104575164, "top": 0.7810075757575757, "width": 0.13691339869281038, "height": 0.01272727272727281, "page": 5}], "section": "Results", "id": "5082"}, {"text": "We analyzed the participants qualitative feedback by coding the interview transcripts based on grounded theory [66].", "bboxes": [{"left": 0.08823366013071895, "top": 0.3366868686868687, "width": 0.3941911764705883, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823366013071895, "top": 0.3512348484848485, "width": 0.3666683006535948, "height": 0.012727272727272698, "page": 5}], "section": "Evaluation of Projection-Based AR Visualizations", "id": "5083"}, {"text": "Next, we report participants responses on all design alternatives in the three design phases.", "bboxes": [{"left": 0.08818627450980392, "top": 0.5536729797979798, "width": 0.39420261437908494, "height": 0.012727272727272698, "page": 5}, {"left": 0.08816993464052288, "top": 0.568219696969697, "width": 0.20695261437908502, "height": 0.01272727272727281, "page": 5}], "section": "Results", "id": "5084"}, {"text": "Effectiveness of the Visualizations (and Sonification).", "bboxes": [{"left": 0.08823529411764706, "top": 0.386385101010101, "width": 0.36806535947712415, "height": 0.012727272727272698, "page": 5}], "section": "Results", "id": "5085"}, {"text": "However, current optical see-through smartglasses have a very limited FOV [88] ( e.g ., ca. 30 wide  17 high for HoloLens v1), largely limiting the area for presenting AR visualizations.", "bboxes": [{"left": 0.5176960784313726, "top": 0.5124570707070707, "width": 0.39417483660130725, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176960784313726, "top": 0.526699494949495, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176797385620915, "top": 0.541246212121212, "width": 0.3941584967320261, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176797385620915, "top": 0.555794191919192, "width": 0.06672222222222224, "height": 0.012727272727272698, "page": 6}], "section": "VISUALIZATIONS FOR SMARTGLASSES", "id": "5086"}, {"text": "Social Acceptance.", "bboxes": [{"left": 0.517625816993464, "top": 0.20824242424242423, "width": 0.12995098039215702, "height": 0.012727272727272754, "page": 6}], "section": "Results", "id": "5087"}, {"text": "Psychological Security.", "bboxes": [{"left": 0.517625816993464, "top": 0.07005050505050504, "width": 0.15717810457516346, "height": 0.01272727272727274, "page": 6}], "section": "Evaluation of Smartglasses Visualizations", "id": "5088"}, {"text": "Behavior Change .", "bboxes": [{"left": 0.08823692810457516, "top": 0.5066843434343434, "width": 0.12211274509803925, "height": 0.01272727272727281, "page": 6}], "section": "Evaluation of Smartglasses Visualizations", "id": "5089"}, {"text": "The second platform we explored was optical see-through smartglasses.", "bboxes": [{"left": 0.5176470588235295, "top": 0.3885063131313131, "width": 0.3941503267973855, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.4030542929292929, "width": 0.08672058823529405, "height": 0.012727272727272698, "page": 6}], "section": "VISUALIZATIONS FOR SMARTGLASSES", "id": "5090"}, {"text": "Some participants ( e.g ., P6, P3, P11) hesitated at the first and last stairs and felt the stairs with their feet when walking without our visualizations (especially in the first two trials of the walking tasks).", "bboxes": [{"left": 0.08822058823529412, "top": 0.6597297979797979, "width": 0.3941748366013072, "height": 0.01272727272727281, "page": 6}, {"left": 0.08822058823529412, "top": 0.6742777777777778, "width": 0.3941437908496732, "height": 0.012727272727272587, "page": 6}, {"left": 0.08822058823529412, "top": 0.6885189393939394, "width": 0.3941584967320262, "height": 0.012727272727272587, "page": 6}, {"left": 0.08822058823529412, "top": 0.7030669191919191, "width": 0.13227614379084968, "height": 0.01272727272727281, "page": 6}], "section": "Results", "id": "5091"}, {"text": "For ascending stairs, participants navigation time was reduced by 5.78% when using their preferred visualizations ( mean =5.84s, SD =1.59s) than when not using them ( mean =6.20s, SD =1.81s).", "bboxes": [{"left": 0.08822712418300653, "top": 0.4121489898989899, "width": 0.3941748366013072, "height": 0.012727272727272698, "page": 6}, {"left": 0.08824509803921568, "top": 0.42669696969696974, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 6}, {"left": 0.08824509803921568, "top": 0.4412436868686868, "width": 0.39421895424836595, "height": 0.012727272727272754, "page": 6}, {"left": 0.08824509803921568, "top": 0.45579040404040405, "width": 0.16658660130718952, "height": 0.012727272727272698, "page": 6}], "section": "Results", "id": "5092"}, {"text": "Meanwhile, four participants felt that the middle highlights should be a different color from the end highlights.", "bboxes": [{"left": 0.08823529411764706, "top": 0.07002146464646465, "width": 0.3941372549019607, "height": 0.01272727272727274, "page": 6}, {"left": 0.08823529411764706, "top": 0.08456944444444445, "width": 0.34987418300653594, "height": 0.012727272727272726, "page": 6}], "section": "Results", "id": "5093"}, {"text": "P6 was the only one who did not want the middle highlights.", "bboxes": [{"left": 0.08820261437908496, "top": 0.1500252525252525, "width": 0.3941307189542484, "height": 0.012727272727272754, "page": 6}], "section": "Results", "id": "5094"}, {"text": "We conducted a user study to evaluate the visualizations we designed for commercial smartglasses.", "bboxes": [{"left": 0.5176470588235295, "top": 0.7388093434343433, "width": 0.3941911764705881, "height": 0.01272727272727281, "page": 7}, {"left": 0.5176633986928104, "top": 0.7533573232323232, "width": 0.25916503267973856, "height": 0.01272727272727281, "page": 7}], "section": "Evaluation of Smartglasses Visualizations", "id": "5095"}, {"text": "According to Zhao et al .", "bboxes": [{"left": 0.08823529411764706, "top": 0.2709330808080808, "width": 0.16288071895424833, "height": 0.012727272727272698, "page": 7}], "section": "Visualization (and Sonification) Design", "id": "5096"}, {"text": "Similar to projection-based AR, when the user stands on the landing, our system verbally notifies the user of the existence of the stairs with stair direction and the number of stairs.", "bboxes": [{"left": 0.08823529411764706, "top": 0.21971843434343433, "width": 0.3941911764705882, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.23426641414141414, "width": 0.3941601307189542, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.24881313131313132, "width": 0.3698970588235294, "height": 0.012727272727272698, "page": 7}], "section": "Visualization (and Sonification) Design", "id": "5097"}, {"text": "Specifically, the following are the seven stages we used in our design, described for descending stairs as an example (Figure 6): (1) Upper landing: the flat surface that is more than 3' away from the edge of the top stair; (2) Upper preparation area: 1.5'3' away from the top stair edge where the person should prepare to step down; (3) Upper alert area: within 1.5' from the top stair edge where the persons next step would be stepping down; (4) Middle stairs: between the edge of the top stair and the edge of the second-to-last stair, where the person is stepping down repeatedly; (5) Lower preparation area: the last stair, where the person is one step away from the flat surface and should prepare for the imminent flat surface; (6) Lower alert area: within 1.5' from the last stair edge on the landing where the persons next step is on the flat surface (not stepping down); (7) Lower landing: 1.5' away from the last stair edge where the person is walking on flat surface again.", "bboxes": [{"left": 0.08823529411764706, "top": 0.42367297979797974, "width": 0.39420098039215684, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.43822095959595964, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.45276767676767676, "width": 0.39417320261437905, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.46731565656565655, "width": 0.3942124183006535, "height": 0.012727272727272754, "page": 7}, {"left": 0.08821895424836601, "top": 0.48186237373737373, "width": 0.3942091503267974, "height": 0.012727272727272698, "page": 7}, {"left": 0.08821895424836601, "top": 0.4964090909090909, "width": 0.39410784313725494, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5109570707070707, "width": 0.3942794117647059, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5255037878787879, "width": 0.39420424836601314, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5400517676767677, "width": 0.394140522875817, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5545984848484848, "width": 0.3941568627450981, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5688409090909091, "width": 0.3941568627450981, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.5833876262626263, "width": 0.39419281045751636, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.597935606060606, "width": 0.3942042483660131, "height": 0.012727272727272698, "page": 7}, {"left": 0.08821895424836601, "top": 0.6124823232323232, "width": 0.3941666666666667, "height": 0.012727272727272698, "page": 7}, {"left": 0.08821895424836601, "top": 0.627030303030303, "width": 0.39409803921568626, "height": 0.012727272727272698, "page": 7}, {"left": 0.08820261437908496, "top": 0.6415770202020202, "width": 0.39420424836601314, "height": 0.012727272727272587, "page": 7}, {"left": 0.08820261437908496, "top": 0.6561237373737373, "width": 0.1398186274509804, "height": 0.01272727272727281, "page": 7}], "section": "Visualization (and Sonification) Design", "id": "5098"}, {"text": "We conducted the design exploration session at an emergency staircase with 12 stairs (different stairs than those in the projection study).", "bboxes": [{"left": 0.08818627450980392, "top": 0.6873409090909092, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 8}, {"left": 0.08818627450980392, "top": 0.7018876262626262, "width": 0.3941764705882353, "height": 0.01272727272727281, "page": 8}, {"left": 0.08820261437908496, "top": 0.7161300505050504, "width": 0.14408496732026144, "height": 0.01272727272727281, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "id": "5099"}, {"text": "To minimize the confounding effect of general computer vision accuracy, we marked the position of the stairs with two Vuforia image targets [37] (on the side walls at the top and bottom landing of the stairs) that can be recognized by HoloLens.", "bboxes": [{"left": 0.08821895424836601, "top": 0.3815555555555556, "width": 0.39415686274509804, "height": 0.012727272727272698, "page": 8}, {"left": 0.08821895424836601, "top": 0.3961022727272728, "width": 0.39419444444444446, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.41065025252525256, "width": 0.39425326797385624, "height": 0.012727272727272698, "page": 8}, {"left": 0.08821895424836601, "top": 0.4251969696969697, "width": 0.39415686274509804, "height": 0.012727272727272754, "page": 8}, {"left": 0.08821895424836601, "top": 0.43974368686868687, "width": 0.048620915032679735, "height": 0.012727272727272754, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "id": "5100"}, {"text": "Experience with the Smartglasses .", "bboxes": [{"left": 0.5176470588235295, "top": 0.7748699494949495, "width": 0.2298464052287581, "height": 0.01272727272727281, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "id": "5101"}, {"text": "The study ended with a final interview asking about the participants general experience with the prototype.", "bboxes": [{"left": 0.5175571895424836, "top": 0.36858585858585863, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175408496732027, "top": 0.3831338383838384, "width": 0.32315032679738565, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "id": "5102"}, {"text": "We indicated the start and end points on the stairs with stickers that were three feet away from the top and bottom steps on the landings.", "bboxes": [{"left": 0.5175898692810458, "top": 0.19372601010101012, "width": 0.39417483660130714, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175898692810458, "top": 0.20827272727272728, "width": 0.3941813725490195, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175898692810458, "top": 0.22282070707070706, "width": 0.10745424836601292, "height": 0.012727272727272754, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "id": "5103"}, {"text": "To validate counterbalancing, we added another betweensubject factor, Order (six levels based on the three conditions), into our model.", "bboxes": [{"left": 0.5175245098039216, "top": 0.601635101010101, "width": 0.3941584967320262, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175245098039216, "top": 0.6161818181818182, "width": 0.3941078431372549, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175081699346404, "top": 0.630729797979798, "width": 0.1468709150326798, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "id": "5104"}, {"text": "The stair navigation session was conducted at another staircase with 14 stairsa wider set of access stairs in a more", "bboxes": [{"left": 0.08816993464052288, "top": 0.869175505050505, "width": 0.3942124183006536, "height": 0.01272727272727281, "page": 8}, {"left": 0.08816993464052288, "top": 0.8834166666666667, "width": 0.39419771241830065, "height": 0.012727272727272587, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "id": "5105"}, {"text": "To reduce the effect of order on the results, we used a simultaneous within-subjects design by switching the task condition after each round of walking up and down.", "bboxes": [{"left": 0.5175735294117647, "top": 0.30313005050505054, "width": 0.39399346405228763, "height": 0.012727272727272698, "page": 8}, {"left": 0.5175735294117647, "top": 0.31767676767676767, "width": 0.3942075163398693, "height": 0.012727272727272754, "page": 8}, {"left": 0.5175571895424836, "top": 0.3319191919191919, "width": 0.2976928104575164, "height": 0.012727272727272698, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "id": "5106"}, {"text": "We analyzed participants qualitative responses with the same method we used in the previous study.", "bboxes": [{"left": 0.517656862745098, "top": 0.7251717171717171, "width": 0.39416993464052286, "height": 0.01272727272727281, "page": 8}, {"left": 0.517640522875817, "top": 0.7397196969696969, "width": 0.28866339869281055, "height": 0.01272727272727281, "page": 8}], "section": "Evaluation of Smartglasses Visualizations", "id": "5107"}, {"text": "On the other hand, P17 felt that Beep may not be distinguishable from environmental sounds: The world around you is so full of noise. I mean, if I use this in the city you have cars honking and everything like that, Im not sure if I would react in time.", "bboxes": [{"left": 0.5176895424836602, "top": 0.7322601010101011, "width": 0.39426470588235285, "height": 0.012727272727272587, "page": 9}, {"left": 0.5177058823529412, "top": 0.7468068181818182, "width": 0.3941405228758168, "height": 0.01272727272727281, "page": 9}, {"left": 0.5177058823529412, "top": 0.761354797979798, "width": 0.3942401960784313, "height": 0.012727272727272587, "page": 9}, {"left": 0.5177058823529412, "top": 0.7759015151515152, "width": 0.3942075163398693, "height": 0.01272727272727281, "page": 9}, {"left": 0.5177058823529412, "top": 0.7904494949494949, "width": 0.09646895424836599, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "id": "5108"}, {"text": "Nevertheless, some participants ( e.g ., P6, P10, P13) felt this design was helpful because it provided a preview for future steps, especially when they looked downstairs from the top landing.", "bboxes": [{"left": 0.0882516339869281, "top": 0.29609595959595963, "width": 0.3942075163398693, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.3106439393939394, "width": 0.39416993464052286, "height": 0.012727272727272754, "page": 9}, {"left": 0.0882516339869281, "top": 0.32519065656565654, "width": 0.3941650326797385, "height": 0.012727272727272698, "page": 9}, {"left": 0.0882516339869281, "top": 0.3394330808080808, "width": 0.05317647058823531, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "id": "5109"}, {"text": "Moreover, some participants ( e.g ., P10, P12, P17) mentioned that the blue glow on the middle stairs was difficult to notice, especially in the bright environment for the walking tasks.", "bboxes": [{"left": 0.0882843137254902, "top": 0.7473421717171717, "width": 0.39416830065359476, "height": 0.01272727272727281, "page": 9}, {"left": 0.08826797385620916, "top": 0.761888888888889, "width": 0.3941584967320262, "height": 0.012727272727272587, "page": 9}, {"left": 0.08826797385620916, "top": 0.7764368686868687, "width": 0.39415849673202613, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "id": "5110"}, {"text": "Interestingly, we found that participants had different preferences for Paths position in their visual field.", "bboxes": [{"left": 0.517640522875817, "top": 0.3176818181818182, "width": 0.3941078431372549, "height": 0.012727272727272754, "page": 9}, {"left": 0.517640522875817, "top": 0.33192424242424245, "width": 0.28760947712418305, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "id": "5111"}, {"text": "However, two participants (P6, P14) had difficulty using Glow because of difficulty distinguishing colors.", "bboxes": [{"left": 0.08826797385620916, "top": 0.681885101010101, "width": 0.3941911764705882, "height": 0.01272727272727281, "page": 9}, {"left": 0.08826797385620916, "top": 0.6961275252525252, "width": 0.31449019607843137, "height": 0.01272727272727281, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "id": "5112"}, {"text": "Effectiveness of the visualizations (and sonification) .", "bboxes": [{"left": 0.08823529411764706, "top": 0.1357840909090909, "width": 0.3654820261437909, "height": 0.012727272727272726, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "id": "5113"}, {"text": "Behavior Change .", "bboxes": [{"left": 0.5176503267973857, "top": 0.7606654040404041, "width": 0.12422058823529403, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "id": "5114"}, {"text": "Participants gave scores to their psychological security during stair navigation in three conditions (Figure 9): (1) walking as they typically would ( mean =4.8, SD =1.60); (2) with HoloLens but no visualizations ( mean =3.9, SD =1.44); (3) with preferred visualizations or sonification ( mean =6.1, SD =1.38).", "bboxes": [{"left": 0.5176830065359477, "top": 0.6079242424242425, "width": 0.3941405228758169, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176666666666667, "top": 0.6224722222222222, "width": 0.39412418300653584, "height": 0.012727272727272698, "page": 10}, {"left": 0.5176503267973857, "top": 0.6370189393939394, "width": 0.39421568627450976, "height": 0.01272727272727281, "page": 10}, {"left": 0.5176503267973857, "top": 0.6515669191919192, "width": 0.3942075163398693, "height": 0.012727272727272587, "page": 10}, {"left": 0.5176503267973857, "top": 0.6658080808080808, "width": 0.39417483660130725, "height": 0.012727272727272587, "page": 10}, {"left": 0.5176503267973857, "top": 0.6803560606060606, "width": 0.0674035947712418, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "id": "5115"}, {"text": "We found that most participants (10 out of 12) combined a visualization with a sonification (Beep).", "bboxes": [{"left": 0.0882516339869281, "top": 0.3242638888888889, "width": 0.39417483660130725, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882516339869281, "top": 0.3388118686868687, "width": 0.2609101307189543, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "id": "5116"}, {"text": "The most commonly chosen visualization was Glow, which was preferred by eight participants.", "bboxes": [{"left": 0.0882843137254902, "top": 0.49185732323232323, "width": 0.3941911764705882, "height": 0.012727272727272698, "page": 10}, {"left": 0.0882843137254902, "top": 0.5060984848484849, "width": 0.24099836601307187, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "id": "5117"}, {"text": "Psychological Security.", "bboxes": [{"left": 0.5176503267973857, "top": 0.48427904040404035, "width": 0.16008823529411764, "height": 0.012727272727272754, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "id": "5118"}, {"text": "With the condition of wearing HoloLens without visualizations as the baseline, we analyzed the effect of our visualiza-", "bboxes": [{"left": 0.08822222222222222, "top": 0.8627487373737374, "width": 0.39427450980392165, "height": 0.012727272727272587, "page": 10}, {"left": 0.08822222222222222, "top": 0.8772967171717171, "width": 0.3941584967320262, "height": 0.01272727272727281, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "id": "5119"}, {"text": "In general, participants felt that our prototype was helpful, especially in unfamiliar places.", "bboxes": [{"left": 0.08831699346405228, "top": 0.6155025252525252, "width": 0.3941584967320261, "height": 0.012727272727272698, "page": 10}, {"left": 0.08831699346405228, "top": 0.6300492424242424, "width": 0.22053104575163393, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "id": "5120"}, {"text": "Preferences for visualizations (and sonification).", "bboxes": [{"left": 0.08823529411764706, "top": 0.273354797979798, "width": 0.3391764705882353, "height": 0.012727272727272698, "page": 10}], "section": "Evaluation of Smartglasses Visualizations", "id": "5121"}, {"text": "The system implementation should also take into account different real-world situations.", "bboxes": [{"left": 0.517656862745098, "top": 0.5776717171717172, "width": 0.39412418300653596, "height": 0.012727272727272698, "page": 11}, {"left": 0.517656862745098, "top": 0.5922184343434344, "width": 0.2094624183006536, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "id": "5122"}, {"text": "Participants had some common choices on the visualizations on each platform.", "bboxes": [{"left": 0.08823529411764706, "top": 0.47458080808080805, "width": 0.3941650326797385, "height": 0.012727272727272698, "page": 11}, {"left": 0.08823529411764706, "top": 0.4891275252525253, "width": 0.1162156862745098, "height": 0.012727272727272754, "page": 11}], "section": "DISCUSSION", "id": "5123"}, {"text": "As with any study, ours had some limitations.", "bboxes": [{"left": 0.5175571895424836, "top": 0.8031477272727272, "width": 0.30143790849673213, "height": 0.01272727272727281, "page": 11}], "section": "DISCUSSION", "id": "5124"}, {"text": "Our research is the first to explore AR visualizations for people with low vision in the context of stair navigation.", "bboxes": [{"left": 0.08823529411764706, "top": 0.33608207070707075, "width": 0.3942238562091503, "height": 0.012727272727272698, "page": 11}, {"left": 0.0882516339869281, "top": 0.35063005050505053, "width": 0.3627483660130719, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "id": "5125"}, {"text": "We compared users experiences with the visualizations on both platforms given that seven participated in both studies.", "bboxes": [{"left": 0.08821895424836601, "top": 0.6709621212121213, "width": 0.39417320261437905, "height": 0.012727272727272587, "page": 11}, {"left": 0.08821895424836601, "top": 0.6852045454545455, "width": 0.39422385620915035, "height": 0.012727272727272587, "page": 11}], "section": "DISCUSSION", "id": "5126"}, {"text": "While our study focused on the design and evaluation of the AR visualizations, we discuss the technical feasibility and challenges for our AR stair navigation systems.", "bboxes": [{"left": 0.517673202614379, "top": 0.3521957070707071, "width": 0.394140522875817, "height": 0.012727272727272698, "page": 11}, {"left": 0.517673202614379, "top": 0.3667424242424242, "width": 0.3941862745098039, "height": 0.012727272727272754, "page": 11}, {"left": 0.5176895424836602, "top": 0.3812891414141414, "width": 0.315937908496732, "height": 0.012727272727272698, "page": 11}], "section": "DISCUSSION", "id": "5127"}, {"text": "In this paper, we designed AR visualizations to facilitate stair navigation for people with low vision.", "bboxes": [{"left": 0.08823529411764706, "top": 0.2357790404040404, "width": 0.39419444444444446, "height": 0.012727272727272754, "page": 12}, {"left": 0.08823529411764706, "top": 0.2503270202020202, "width": 0.2511928104575163, "height": 0.012727272727272754, "page": 12}], "section": "CONCLUSIONS", "id": "5128"}, {"text": "This work was supported in part by the National Science Foundation under grant no.", "bboxes": [{"left": 0.08823529411764706, "top": 0.4015366161616162, "width": 0.3941748366013072, "height": 0.012727272727272698, "page": 12}, {"left": 0.08823529411764706, "top": 0.41608459595959596, "width": 0.17816993464052283, "height": 0.012727272727272754, "page": 12}], "section": "CONCLUSIONS", "id": "5129"}, {"text": "ACKNOWLEDGMENTS", "bboxes": [{"left": 0.08823529411764706, "top": 0.3881982323232323, "width": 0.16026797385620917, "height": 0.011515151515151534, "page": 12}], "section": "CONCLUSIONS", "id": "5130"}, {"text": "However, half of the participants felt Path was distracting and hard to understand.", "bboxes": [{"left": 0.5176895424836602, "top": 0.484969696969697, "width": 0.3941911764705882, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176895424836602, "top": 0.49921085858585856, "width": 0.160390522875817, "height": 0.012727272727272698, "page": 9}], "section": "Evaluation of Smartglasses Visualizations", "id": "5131"}], "uist-2": [{"text": "In recent years, various technologies have been developed to help transform speech and sound into visual representations, which provide benefit to the deaf and hard-of-hearing population.", "bboxes": [{"left": 0.5176470588235295, "top": 0.5030517676767676, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.5172941919191919, "width": 0.39430555555555546, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5318409090909091, "width": 0.39440522875816997, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5463888888888889, "width": 0.0743153594771242, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "id": "5132"}, {"text": "In an effort to reduce visual dispersion, various research teams have investigated the use of captions on custom [19] and commercially available head-worn displays (HWDs) [16, 29, 35, 42].", "bboxes": [{"left": 0.5176470588235295, "top": 0.6703396464646465, "width": 0.3943888888888889, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.6848863636363636, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.699128787878788, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 0}, {"left": 0.5176470588235295, "top": 0.713675505050505, "width": 0.10252941176470587, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "id": "5134"}, {"text": "Profita et al. found HWDs to be socially acceptable as assistive technologies from the interlocutor and bystander perspectives [33].", "bboxes": [{"left": 0.5176470588235295, "top": 0.8376262626262626, "width": 0.39465196078431364, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.8521742424242424, "width": 0.394470588235294, "height": 0.012727272727272587, "page": 0}, {"left": 0.5176470588235295, "top": 0.8664154040404041, "width": 0.11783006535947704, "height": 0.012727272727272587, "page": 0}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "id": "5135"}, {"text": "Most prior work on technologies for captions or the display of sound awareness cues focus on stationary contexts and the use of commercially available devices [14, 17, 18, 24, 29, 32].", "bboxes": [{"left": 0.517640522875817, "top": 0.18822853535353534, "width": 0.39447058823529413, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.20277525252525255, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 1}, {"left": 0.517640522875817, "top": 0.2173219696969697, "width": 0.3933594771241832, "height": 0.012727272727272754, "page": 1}, {"left": 0.517640522875817, "top": 0.23186868686868686, "width": 0.026071895424836677, "height": 0.012727272727272754, "page": 1}], "section": "RELATED WORK", "id": "5137"}, {"text": "Having distinctive technology that signifies a disability, such as a white cane for people who are blind, can increase bystanders acceptance of that technology [23, 33].", "bboxes": [{"left": 0.5176470588235295, "top": 0.7755909090909091, "width": 0.39445751633986914, "height": 0.012728535353535464, "page": 1}, {"left": 0.5176470588235295, "top": 0.7901325757575757, "width": 0.3943316993464051, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176470588235295, "top": 0.8046729797979798, "width": 0.39446241830065365, "height": 0.012727272727272587, "page": 1}], "section": "Social Acceptability of Wearable Assistive Devices", "id": "5138"}, {"text": "While we acknowledge that not all deaf/hard-of-hearing (DHH) individuals want to use sound or captioning technologies, prior work has demonstrated that many people would find such technologies desirable and useful in everyday activities [16].", "bboxes": [{"left": 0.08823529411764706, "top": 0.7672941919191919, "width": 0.39440522875816986, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.7818421717171717, "width": 0.3944705882352941, "height": 0.012727272727272587, "page": 1}, {"left": 0.08823529411764706, "top": 0.7963888888888889, "width": 0.3943055555555556, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.8109368686868687, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.8251780303030304, "width": 0.1660392156862745, "height": 0.012727272727272587, "page": 1}], "section": "RELATED WORK", "id": "5139"}, {"text": "Our HWD prototype augments the users perception of speech and sounds in the environment (Figure 1).", "bboxes": [{"left": 0.08821895424836601, "top": 0.2976287878787879, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 1}, {"left": 0.08821895424836601, "top": 0.31217550505050506, "width": 0.3327058823529412, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "id": "5140"}, {"text": "Captioning efforts on Google Glass [6] and Epson BT-200 frames [29] also highlight the limitations in current HWDs.", "bboxes": [{"left": 0.517640522875817, "top": 0.6100770202020201, "width": 0.39231862745098034, "height": 0.01272727272727281, "page": 1}, {"left": 0.517640522875817, "top": 0.6245896464646464, "width": 0.39431862745098045, "height": 0.012727272727272698, "page": 1}], "section": "RELATED WORK", "id": "5141"}, {"text": "In the following sections, we review related work suggesting initial design requirements for this project.", "bboxes": [{"left": 0.08823529411764706, "top": 0.6097247474747475, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.6242714646464647, "width": 0.279406862745098, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "id": "5142"}, {"text": "Academic evaluations of HWDs suggest that commercially available solutions do not have a suitable form factor for sustained captioning.", "bboxes": [{"left": 0.517640522875817, "top": 0.34096338383838387, "width": 0.39322058823529416, "height": 0.012727272727272698, "page": 1}, {"left": 0.517640522875817, "top": 0.3554924242424243, "width": 0.39202614379084966, "height": 0.012727272727272698, "page": 1}, {"left": 0.517640522875817, "top": 0.3700214646464647, "width": 0.1383398692810457, "height": 0.012727272727272698, "page": 1}], "section": "RELATED WORK", "id": "5143"}, {"text": "Technologies for the d/Deaf and Hard of Hearing", "bboxes": [{"left": 0.08823529411764706, "top": 0.7539507575757576, "width": 0.3384509803921568, "height": 0.011515151515151478, "page": 1}], "section": "RELATED WORK", "id": "5144"}, {"text": "The major contributions of this work include", "bboxes": [{"left": 0.08823529411764706, "top": 0.4197184343434343, "width": 0.2931764705882353, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "id": "5145"}, {"text": "Despite the benefits of HWDs for captioning, current commercially available solutions inhibit mobility and are not socially acceptable due to their poor fit or large form factor [16, 29].", "bboxes": [{"left": 0.08821895424836601, "top": 0.1303409090909091, "width": 0.3945032679738562, "height": 0.012727272727272726, "page": 1}, {"left": 0.08821895424836601, "top": 0.14488888888888887, "width": 0.394421568627451, "height": 0.012727272727272726, "page": 1}, {"left": 0.08821895424836601, "top": 0.15913005050505052, "width": 0.3944542483660131, "height": 0.012727272727272698, "page": 1}, {"left": 0.08821895424836601, "top": 0.1736780303030303, "width": 0.108031045751634, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION: CAPTIONING FOR ACCESSIBILITY", "id": "5146"}, {"text": "However, existing commercially available HWD systems were not designed to support these continuous usage scenarios as they run high level operating systems to support generic applications and drivers.", "bboxes": [{"left": 0.08821895424836601, "top": 0.3542790404040404, "width": 0.39419117647058827, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.3685214646464647, "width": 0.39432189542483664, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.3830681818181818, "width": 0.3943218954248366, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.3976161616161616, "width": 0.21233823529411766, "height": 0.012727272727272754, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "id": "5147"}, {"text": "To learn more about challenges in mobile contexts, we conducted a brief large-scale online survey with participants who used hearing aids, TDD/TTY (telecommunications device for the deaf/teletypewriter) [13], CART [31], and cochlear implants.", "bboxes": [{"left": 0.08823529411764706, "top": 0.8039608585858585, "width": 0.3943218954248366, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8185088383838384, "width": 0.39453594771241834, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8330555555555555, "width": 0.3946192810457516, "height": 0.01272727272727281, "page": 2}, {"left": 0.08823529411764706, "top": 0.8476035353535354, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.8621502525252525, "width": 0.11901633986928102, "height": 0.01272727272727281, "page": 2}], "section": "MOBILE SCENARIOS SURVEY: 501 RESPONDENTS", "id": "5148"}, {"text": "We used Google Surveys [7] to deploy a short, ten-question survey.", "bboxes": [{"left": 0.5176470588235295, "top": 0.49803535353535355, "width": 0.39449509803921556, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.5125782828282828, "width": 0.04764215686274509, "height": 0.012727272727272698, "page": 2}], "section": "Survey Design", "id": "5149"}, {"text": "In addition to physical and social comfort, prior work [16, 25] suggests that battery life is a pertinent measure for the usability of a captioning device.", "bboxes": [{"left": 0.08823529411764706, "top": 0.20123358585858586, "width": 0.3943153594771242, "height": 0.012727272727272698, "page": 2}, {"left": 0.08823529411764706, "top": 0.21578156565656567, "width": 0.39432189542483653, "height": 0.012727272727272754, "page": 2}, {"left": 0.08823529411764706, "top": 0.2303282828282828, "width": 0.23391503267973857, "height": 0.012727272727272754, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "id": "5150"}, {"text": "Participants were recruited from Google Opinion Rewards App users [8] who are compensated up to 1 USD for each completed survey in Google Play or PayPal credit.", "bboxes": [{"left": 0.5176470588235295, "top": 0.7512159090909091, "width": 0.394578431372549, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.7658459595959596, "width": 0.3944754901960784, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.78047601010101, "width": 0.33268954248366, "height": 0.01272727272727281, "page": 2}], "section": "Survey Design", "id": "5151"}, {"text": "Comfort, of course, is a primary concern for eyewear that might be worn all day.", "bboxes": [{"left": 0.08821895424836601, "top": 0.5649027777777778, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.5794507575757576, "width": 0.15391830065359477, "height": 0.012727272727272698, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "id": "5152"}, {"text": "We received 501 responses indicating usage of assistive technologies such as a hearing aid, cochlear implant, or realtime captioning/transcription services (i.e., CART).", "bboxes": [{"left": 0.5176470588235295, "top": 0.815530303030303, "width": 0.3931895424836601, "height": 0.012626262626262652, "page": 2}, {"left": 0.5176470588235295, "top": 0.830050505050505, "width": 0.3926225490196078, "height": 0.012626262626262652, "page": 2}, {"left": 0.5176470588235295, "top": 0.844570707070707, "width": 0.379656862745098, "height": 0.012626262626262652, "page": 2}], "section": "Survey Design", "id": "5153"}, {"text": "Thermal comfort is another engineering issue due to the small volume of eyeglasses HWDs.", "bboxes": [{"left": 0.08821895424836601, "top": 0.6597588383838384, "width": 0.3943545751633986, "height": 0.01272727272727281, "page": 2}, {"left": 0.08821895424836601, "top": 0.6743068181818183, "width": 0.2295669934640523, "height": 0.012727272727272587, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "id": "5154"}, {"text": "Our eyewear prototype weighs 54 g with 30 g on the nose.", "bboxes": [{"left": 0.08821895424836601, "top": 0.725215909090909, "width": 0.3942075163398693, "height": 0.01272727272727281, "page": 2}], "section": "Lightweight, Low-power, Head-worn Display Systems", "id": "5155"}, {"text": "For graphics effects that require animation, such as our smoothly-scrolling transcript text, we implement interpolation primitives which are executed on the device to drive position and scale parameters of other primitives.", "bboxes": [{"left": 0.5176470588235295, "top": 0.6842601010101009, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.6988080808080809, "width": 0.394388888888889, "height": 0.012727272727272587, "page": 3}, {"left": 0.5176797385620915, "top": 0.713354797979798, "width": 0.3944869281045751, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.7279027777777778, "width": 0.36096895424836595, "height": 0.012727272727272587, "page": 3}], "section": "Embedded System and Communication protocol", "id": "5156"}, {"text": "However, we acknowledge limitations in our survey.", "bboxes": [{"left": 0.08823529411764706, "top": 0.7451906565656566, "width": 0.3675898692810457, "height": 0.012727272727272587, "page": 3}], "section": "Discussion and Limitations", "id": "5157"}, {"text": "The embedded system and phone communicate using BLE.", "bboxes": [{"left": 0.5176470588235295, "top": 0.5606275252525252, "width": 0.39431209150326807, "height": 0.012727272727272698, "page": 3}], "section": "Embedded System and Communication protocol", "id": "5158"}, {"text": "To overcome the limitations of existing platforms, we developed a hybrid approach adapted to our specific application that consists of a thin-client low-power eyewear prototype coupled with a mobile phone over a wireless connection (Figure 3).", "bboxes": [{"left": 0.5176470588235295, "top": 0.27850631313131313, "width": 0.39435457516339867, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.2930542929292929, "width": 0.39430555555555546, "height": 0.012727272727272754, "page": 3}, {"left": 0.5176470588235295, "top": 0.3076010101010101, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.3221489898989899, "width": 0.3941911764705881, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.33669570707070706, "width": 0.15742647058823522, "height": 0.012727272727272698, "page": 3}], "section": "SYSTEM ARCHITECTURE AND IMPLEMENTATION", "id": "5159"}, {"text": "Our large-scale survey enabled us to identify common mobile experiences, scenarios that frequently exacerbate hearing difficulties, and attitudes toward assistive technologies.", "bboxes": [{"left": 0.08823529411764706, "top": 0.5345669191919192, "width": 0.39405882352941174, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.549114898989899, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.5636616161616161, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.5779040404040404, "width": 0.08686601307189544, "height": 0.012727272727272698, "page": 3}], "section": "Discussion and Limitations", "id": "5160"}, {"text": "The most common scenarios that participants experienced on a daily basis were text or phone conversations (88.5%), oneon-one conversations (87.2%), and conversations in cars (72.2%).", "bboxes": [{"left": 0.08823529411764706, "top": 0.143354797979798, "width": 0.394421568627451, "height": 0.012727272727272726, "page": 3}, {"left": 0.08823529411764706, "top": 0.15790277777777778, "width": 0.39450326797385615, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.17244949494949496, "width": 0.39456862745098037, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.18699747474747475, "width": 0.0574330065359477, "height": 0.012727272727272698, "page": 3}], "section": "Survey Design", "id": "5161"}, {"text": "Participants also indicated the perceived social acceptability of their assistive technologies.", "bboxes": [{"left": 0.08821895424836601, "top": 0.4412626262626263, "width": 0.3945196078431372, "height": 0.012727272727272698, "page": 3}, {"left": 0.08821895424836601, "top": 0.45580934343434343, "width": 0.2021764705882353, "height": 0.012727272727272754, "page": 3}], "section": "Survey Design", "id": "5162"}, {"text": "Survey R esults", "bboxes": [{"left": 0.08823529411764706, "top": 0.13001641414141413, "width": 0.10672058823529411, "height": 0.011515151515151534, "page": 3}], "section": "Survey Design", "id": "5163"}, {"text": "Our prototype system uses a 1-lane MIPI-DSI compatible microdisplay engine with 30 fps graphics rendering.", "bboxes": [{"left": 0.08823529411764706, "top": 0.25093055555555555, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.2654785353535353, "width": 0.34133660130718957, "height": 0.012727272727272754, "page": 4}], "section": "Display System", "id": "5164"}, {"text": "All studies used our eyewear prototype with transcribed speech sent wirelessly from an Android Pixel 3 phone.", "bboxes": [{"left": 0.5176470588235295, "top": 0.36183964646464645, "width": 0.3944379084967321, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.3760820707070707, "width": 0.3742941176470589, "height": 0.012727272727272698, "page": 4}], "section": "Prototype Apparatus and Fitting", "id": "5165"}, {"text": "Participants were asked to wear the prototype throughout various pre-planned activities, which included single and multi-party conversations (locally and over video call), communication while working on manual tasks (card sorting, assembly tasks on a computer), watching a movie, and mingling with other participants in a happy hour.", "bboxes": [{"left": 0.5176633986928104, "top": 0.6221502525252525, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176633986928104, "top": 0.6366969696969698, "width": 0.3944379084967321, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176633986928104, "top": 0.6512449494949494, "width": 0.3942565359477125, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176633986928104, "top": 0.6657916666666667, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 4}, {"left": 0.5176633986928104, "top": 0.6803396464646464, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176633986928104, "top": 0.6948863636363637, "width": 0.3452728758169935, "height": 0.012727272727272587, "page": 4}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "id": "5166"}, {"text": "Our first prototype frame included the board in the right temple while the battery was embedded in the left temple (Figure 5 left).", "bboxes": [{"left": 0.08823529411764706, "top": 0.4824482323232323, "width": 0.394421568627451, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4969949494949495, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.5115429292929293, "width": 0.09823039215686273, "height": 0.012727272727272698, "page": 4}], "section": "MECHANICAL DESIGN FOR RAPID PROTOTYPING", "id": "5167"}, {"text": "We ran formative in-lab pilot studies with 14 participants in the U.S. ( New York, NY =6, Mountain View, CA =8), whose self-reported hearing loss ranged from moderate (41-70 dB) to profound (>95 dB).", "bboxes": [{"left": 0.5176470588235295, "top": 0.5130517676767676, "width": 0.3945359477124182, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.5275997474747475, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.5421464646464647, "width": 0.3944215686274509, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.5566944444444444, "width": 0.1425196078431371, "height": 0.01272727272727281, "page": 4}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "id": "5168"}, {"text": "A core service application manages the BLE connection and exposes an API for controlling the display and capturing sensor data through an Android Interface Definition Language (AIDL) interface.", "bboxes": [{"left": 0.08823529411764706, "top": 0.143354797979798, "width": 0.39445424836601306, "height": 0.012727272727272726, "page": 4}, {"left": 0.08823529411764706, "top": 0.15790277777777778, "width": 0.3945359477124183, "height": 0.012727272727272754, "page": 4}, {"left": 0.08823529411764706, "top": 0.17244949494949496, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.18699747474747475, "width": 0.18918137254901957, "height": 0.012727272727272698, "page": 4}], "section": "Embedded System and Communication protocol", "id": "5169"}, {"text": "Our plastic parts are 3D printed with a biocompatible material and use a multi-step finishing process.", "bboxes": [{"left": 0.08823529411764706, "top": 0.4169911616161616, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4315391414141414, "width": 0.30447549019607845, "height": 0.012727272727272698, "page": 4}], "section": "MECHANICAL DESIGN FOR RAPID PROTOTYPING", "id": "5170"}, {"text": "We are particularly interested in our proof-of-concepts potential to augment communication and perception for DHH individuals.", "bboxes": [{"left": 0.08823529411764706, "top": 0.8588093434343435, "width": 0.39456862745098037, "height": 0.012727272727272587, "page": 4}, {"left": 0.08823529411764706, "top": 0.8733573232323232, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 4}, {"left": 0.08823529411764706, "top": 0.8875984848484848, "width": 0.11745098039215686, "height": 0.01272727272727281, "page": 4}], "section": "USABILITY EVALUATIONS: THREE STUDIES WITH DEAF/HARD-OF-HEARING PARTICIPANTS", "id": "5171"}, {"text": "APIs: Android, JavaScript and Python for Prototyping", "bboxes": [{"left": 0.08823529411764706, "top": 0.13001641414141413, "width": 0.37545424836601304, "height": 0.011515151515151534, "page": 4}], "section": "Embedded System and Communication protocol", "id": "5172"}, {"text": "Throughout the three days, participants were encouraged to wear the eyewear prototype during several activities: ordering and consuming beverages/food, watching TV or movies, walking around campus, and participating in meetings, presentations, and conversations.", "bboxes": [{"left": 0.08823529411764706, "top": 0.6360820707070707, "width": 0.3944052287581699, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.650324494949495, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 5}, {"left": 0.08823529411764706, "top": 0.6648712121212121, "width": 0.39448692810457514, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.679419191919192, "width": 0.39448692810457514, "height": 0.012727272727272587, "page": 5}, {"left": 0.08823529411764706, "top": 0.693965909090909, "width": 0.3056274509803922, "height": 0.01272727272727281, "page": 5}], "section": "Task and Procedure", "id": "5173"}, {"text": "To provide visual feedback to the wearer about when the prototype was connected to the phone, we implemented a real-time audio level meter in the lower right corner of the display (Figure 1).", "bboxes": [{"left": 0.5176470588235295, "top": 0.8127487373737374, "width": 0.39430555555555546, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8272967171717173, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.8418434343434343, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8563914141414141, "width": 0.12137254901960781, "height": 0.01272727272727281, "page": 5}], "section": "Connectivity and Pairing", "id": "5174"}, {"text": "Participants used Version 1 of the prototype in a variety of settings and scenarios.", "bboxes": [{"left": 0.08823529411764706, "top": 0.8309305555555555, "width": 0.39443790849673205, "height": 0.01272727272727281, "page": 5}, {"left": 0.08823529411764706, "top": 0.8454785353535353, "width": 0.1492238562091503, "height": 0.012727272727272587, "page": 5}], "section": "Results", "id": "5175"}, {"text": "A total of five DHH participants were recruited from our institution.", "bboxes": [{"left": 0.08823529411764706, "top": 0.5136578282828282, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.5282058080808081, "width": 0.07041176470588235, "height": 0.012727272727272698, "page": 5}], "section": "Participants", "id": "5176"}, {"text": "All participants experienced issues with Version 1 of the prototype.", "bboxes": [{"left": 0.5176470588235295, "top": 0.18547601010101009, "width": 0.39435457516339856, "height": 0.012727272727272754, "page": 5}, {"left": 0.5176470588235295, "top": 0.20002398989898992, "width": 0.06687091503267972, "height": 0.012727272727272698, "page": 5}], "section": "Display of Transcriptions", "id": "5177"}, {"text": "Participants limited themselves mainly to scenarios in the office work environment (at desk, meetings, video calls) and in conversation.", "bboxes": [{"left": 0.5176470588235295, "top": 0.09214267676767678, "width": 0.3943888888888889, "height": 0.012727272727272726, "page": 5}, {"left": 0.5176470588235295, "top": 0.10669065656565656, "width": 0.3944869281045751, "height": 0.012727272727272726, "page": 5}, {"left": 0.5176470588235295, "top": 0.12123737373737373, "width": 0.11036928104575161, "height": 0.012727272727272726, "page": 5}], "section": "Results", "id": "5178"}, {"text": "We received feedback that the eyewear temple overlapped with cochlear implants and all four cochlear implant users experienced discomfort.", "bboxes": [{"left": 0.5176470588235295, "top": 0.29305176767676766, "width": 0.39442156862745104, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.3075997474747475, "width": 0.39460294117647066, "height": 0.012727272727272754, "page": 5}, {"left": 0.5176470588235295, "top": 0.3221464646464646, "width": 0.1593856209150326, "height": 0.012727272727272754, "page": 5}], "section": "Display of Transcriptions", "id": "5179"}, {"text": "Participants suggested that testing in group conversations and more challenging auditory environments could be helpful to fully understand the benefits of the HWD.", "bboxes": [{"left": 0.08823529411764706, "top": 0.3127487373737374, "width": 0.3942565359477125, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.32729671717171716, "width": 0.39437254901960783, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.34184343434343434, "width": 0.36487254901960786, "height": 0.012727272727272698, "page": 5}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "id": "5180"}, {"text": "To gather feedback from in-the-wild scenarios, we conducted a study where participants could use Version 1 of our prototype over three days.", "bboxes": [{"left": 0.08823529411764706, "top": 0.44941540404040403, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.46396338383838387, "width": 0.39440522875816986, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.47851010101010105, "width": 0.19660947712418297, "height": 0.012727272727272698, "page": 5}], "section": "PILOT STUDIES: FORMATIVE IN-LAB RESEARCH", "id": "5181"}, {"text": "For physical button ergonomics, one participant felt that it was difficult to use the button to change the display settings.", "bboxes": [{"left": 0.5176470588235295, "top": 0.5827689393939394, "width": 0.39435457516339856, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.5973169191919192, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 5}], "section": "User Experience", "id": "5182"}, {"text": "Participants gave feedback on problems that affected their user experience, noting challenges in five main areas: text visibility, speed of transcription, connectivity, physical button ergonomics, and text placement.", "bboxes": [{"left": 0.5176470588235295, "top": 0.37183964646464646, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.3863876262626263, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.4009343434343435, "width": 0.3943888888888889, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.41548232323232326, "width": 0.27350980392156854, "height": 0.012727272727272698, "page": 5}], "section": "User Experience", "id": "5183"}, {"text": "Study 1 identified several important issues that we focused on addressing before Study 2.", "bboxes": [{"left": 0.5176470588235295, "top": 0.7630517676767676, "width": 0.3944379084967321, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.7775997474747475, "width": 0.1946830065359476, "height": 0.012727272727272587, "page": 5}], "section": "DISCUSSION AND DESIGN IMPROVEMENTS", "id": "5184"}, {"text": "Manual adjustment of the nose pads also supports final adjustments of the eyewear height, nose bridge fit, and monocular display visibility.", "bboxes": [{"left": 0.5176470588235295, "top": 0.5982260101010102, "width": 0.3943725490196077, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.6127727272727272, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.6273207070707071, "width": 0.20138562091503254, "height": 0.012727272727272698, "page": 6}], "section": "Display of Transcriptions", "id": "5185"}, {"text": "Minimal in-lens displays with small eyeboxes, such as North Focals 1.0 and our prototype, require alignment of the optics to the users face and eye geometry.", "bboxes": [{"left": 0.5176470588235295, "top": 0.3651767676767677, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.3797247474747475, "width": 0.39460294117647055, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.3942714646464646, "width": 0.24409477124183, "height": 0.012727272727272754, "page": 6}], "section": "Display of Transcriptions", "id": "5186"}, {"text": "To offset the weight of the optics and the display engine in the front, which can lead to slipping and nose pressure, we redesigned the frame to place the batteries at the back of the temple (Figure 5, right), similar to other head-worn systems such as Vuzix Blade and Google Glass.", "bboxes": [{"left": 0.5176470588235295, "top": 0.09214267676767678, "width": 0.39445424836601306, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.10669065656565656, "width": 0.3942892156862744, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.12123737373737373, "width": 0.39432189542483653, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.13578535353535354, "width": 0.39422385620915046, "height": 0.012727272727272726, "page": 6}, {"left": 0.5176470588235295, "top": 0.15033207070707072, "width": 0.2668235294117647, "height": 0.012727272727272726, "page": 6}], "section": "Display of Transcriptions", "id": "5187"}, {"text": "With the revisions to the Version 1 prototype discussed in the previous section, we conducted an additional study to understand the physical and social comfort of Version 2 and associated software improvements.", "bboxes": [{"left": 0.5176470588235295, "top": 0.8297184343434343, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.8442664141414141, "width": 0.3945359477124182, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.8588131313131312, "width": 0.394470588235294, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.8733611111111111, "width": 0.22836437908496732, "height": 0.01272727272727281, "page": 6}], "section": "STUDY 2: MOBILE AND GROUP CONVERSATIONS", "id": "5188"}, {"text": "To accommodate different and/or asymmetric interpupillary distances (IPDs), we prototyped a more adaptable nose bridge and nose piece.", "bboxes": [{"left": 0.5176470588235295, "top": 0.2851729797979798, "width": 0.39447058823529413, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.29972095959595957, "width": 0.39435457516339856, "height": 0.012727272727272754, "page": 6}, {"left": 0.5176470588235295, "top": 0.31426767676767675, "width": 0.14919117647058822, "height": 0.012727272727272754, "page": 6}], "section": "Display of Transcriptions", "id": "5189"}, {"text": "Study 1 also helped us discover and address antenna issues.", "bboxes": [{"left": 0.08823529411764706, "top": 0.07911237373737373, "width": 0.3942892156862745, "height": 0.012727272727272726, "page": 6}], "section": "Connectivity and Pairing", "id": "5190"}, {"text": "Interacting with our participants also highlighted the importance of smooth scrolling in order to mitigate eye fatigue.", "bboxes": [{"left": 0.08823529411764706, "top": 0.15790025252525253, "width": 0.39435457516339867, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.17244823232323234, "width": 0.39438888888888884, "height": 0.012727272727272698, "page": 6}, {"left": 0.08823529411764706, "top": 0.1869949494949495, "width": 0.049609477124183024, "height": 0.012727272727272754, "page": 6}], "section": "Display of Transcriptions", "id": "5191"}, {"text": "By exchanging the location of the battery from the temple (Version 1) to the temple tip (Version 2), most of the left temple earpiece remained empty.", "bboxes": [{"left": 0.5176470588235295, "top": 0.18700000000000003, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.20124116161616162, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.21578787878787878, "width": 0.24918300653594772, "height": 0.012727272727272754, "page": 6}], "section": "Display of Transcriptions", "id": "5192"}, {"text": "We generated 12 nose bridges, which we 3D-printed to cover interlens distances from 39 mm and 0, 5, and 15 wrap.", "bboxes": [{"left": 0.5176470588235295, "top": 0.7218712121212121, "width": 0.3943888888888889, "height": 0.01272727272727281, "page": 6}, {"left": 0.5176470588235295, "top": 0.7364191919191919, "width": 0.39420751633986917, "height": 0.012727272727272587, "page": 6}], "section": "Display of Transcriptions", "id": "5193"}, {"text": "Eyebox Adjustments: Personalized 3DOF Nose Bridge", "bboxes": [{"left": 0.5176470588235295, "top": 0.27183459595959597, "width": 0.37961274509803933, "height": 0.011515151515151534, "page": 6}], "section": "Display of Transcriptions", "id": "5194"}, {"text": "To enable personalization in a single frame design without the need for 3D scanning, we developed an interchangeable nose bridge, which provides independent adjustments of IPD, wrap, and cyclo-rotation.", "bboxes": [{"left": 0.5176470588235295, "top": 0.5324646464646464, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5470113636363636, "width": 0.39435457516339856, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5615593434343434, "width": 0.39447058823529413, "height": 0.012727272727272698, "page": 6}, {"left": 0.5176470588235295, "top": 0.5761060606060606, "width": 0.1989803921568628, "height": 0.012727272727272698, "page": 6}], "section": "Display of Transcriptions", "id": "5195"}, {"text": "Comfort: Nose Weight and Balance", "bboxes": [{"left": 0.5176470588235295, "top": 0.07880429292929292, "width": 0.24626307189542473, "height": 0.01151515151515152, "page": 6}], "section": "Display of Transcriptions", "id": "5196"}, {"text": "Participants also provided specific feedback on the eyewear at the end of the study.", "bboxes": [{"left": 0.5176470588235295, "top": 0.706385101010101, "width": 0.39447058823529413, "height": 0.01272727272727281, "page": 7}, {"left": 0.5176470588235295, "top": 0.7209330808080808, "width": 0.1542957516339869, "height": 0.012727272727272587, "page": 7}], "section": "Prototype Eyewear Experience and Desired Use", "id": "5197"}, {"text": "Five newly-recruited participants completed all parts of the study.", "bboxes": [{"left": 0.08823529411764706, "top": 0.2448699494949495, "width": 0.39437254901960783, "height": 0.012727272727272698, "page": 7}, {"left": 0.08823529411764706, "top": 0.2594179292929293, "width": 0.03980882352941177, "height": 0.012727272727272698, "page": 7}], "section": "Participants", "id": "5198"}, {"text": "Participants rated the text rate on the phone while on-the-go slightly more positive (x  =4, IQR =1) compared to the neutral ratings for the eyewear (x  =3, IQR =1).", "bboxes": [{"left": 0.5176470588235295, "top": 0.5842638888888889, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.5988118686868688, "width": 0.39432189542483653, "height": 0.014241161616161513, "page": 7}, {"left": 0.5176307189542484, "top": 0.6130530303030303, "width": 0.2656699346405228, "height": 0.014242424242424279, "page": 7}], "section": "Comprehension of Presented Contents", "id": "5199"}, {"text": "After each task, participants completed a feedback questionnaire regarding device comfort, ranking a series of statements on 5-point ordinal scales, ranging from 1 = Not", "bboxes": [{"left": 0.08823529411764706, "top": 0.6873131313131313, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 7}, {"left": 0.08823529411764706, "top": 0.7018611111111112, "width": 0.39440522875816986, "height": 0.012727272727272587, "page": 7}, {"left": 0.08823529411764706, "top": 0.7164078282828282, "width": 0.3943055555555555, "height": 0.01272727272727281, "page": 7}], "section": "Task and Procedure", "id": "5200"}, {"text": "Participants found that in mobile contexts (on-the-go), the eyewear transcriptions were more discreet (x  =4, IQR =1) compared to a handheld mobile phone (x  =2, IQR =1).", "bboxes": [{"left": 0.5176470588235295, "top": 0.2075972222222222, "width": 0.39435457516339856, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176470588235295, "top": 0.22214520202020202, "width": 0.39450326797385615, "height": 0.014241161616161596, "page": 7}, {"left": 0.5176470588235295, "top": 0.23638636363636362, "width": 0.3429346405228759, "height": 0.014242424242424279, "page": 7}], "section": "Social D ynamics: Discreetness and Awareness", "id": "5201"}, {"text": "Participants provided feedback after completing the following activities:", "bboxes": [{"left": 0.08823529411764706, "top": 0.45426388888888886, "width": 0.39433823529411755, "height": 0.012727272727272754, "page": 7}, {"left": 0.08823529411764706, "top": 0.46881186868686864, "width": 0.13125490196078432, "height": 0.012727272727272754, "page": 7}], "section": "Task and Procedure", "id": "5202"}, {"text": "Participants ratings also suggest that the eyewear prototypes helped participants become more aware of their surroundings (x   eyewear =4 vs. x phone =3), and who was currently speaking (x   eyewear =4 vs. x phone =2) while using the eyewear, whereas awareness of body language was similar for both conditions (Figure 9, center).", "bboxes": [{"left": 0.5176470588235295, "top": 0.374885101010101, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.3894318181818182, "width": 0.39460294117647066, "height": 0.012727272727272698, "page": 7}, {"left": 0.5176470588235295, "top": 0.4036578282828283, "width": 0.39441503267973854, "height": 0.014257575757575691, "page": 7}, {"left": 0.5176421568627452, "top": 0.4182032828282828, "width": 0.39459803921568626, "height": 0.014243686868686878, "page": 7}, {"left": 0.5176535947712417, "top": 0.4327512626262626, "width": 0.3945866013071897, "height": 0.012727272727272754, "page": 7}, {"left": 0.5176535947712417, "top": 0.4472979797979798, "width": 0.12291993464052298, "height": 0.012727272727272698, "page": 7}], "section": "Social D ynamics: Discreetness and Awareness", "id": "5203"}, {"text": "The feedback questionnaires revealed the benefits of the eyewear prototype in comparison to the experience on the mobile phone.", "bboxes": [{"left": 0.5176470588235295, "top": 0.12880934343434344, "width": 0.39430555555555546, "height": 0.012727272727272726, "page": 7}, {"left": 0.5176470588235295, "top": 0.14335732323232322, "width": 0.39435457516339856, "height": 0.012727272727272726, "page": 7}, {"left": 0.5176470588235295, "top": 0.1579040404040404, "width": 0.0927450980392156, "height": 0.012727272727272698, "page": 7}], "section": "Task and Procedure", "id": "5204"}, {"text": "Our power consumption modeling from device specifications maps well to measured values.", "bboxes": [{"left": 0.5176470588235295, "top": 0.7069911616161616, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.7215391414141413, "width": 0.2951029411764705, "height": 0.01272727272727281, "page": 8}], "section": "System Power Consumption", "id": "5205"}, {"text": "Study 2 incorporated both a mobile phone and our prototype eyewear in walking and multi-speaker interactions to bring further insights into the potential for a HWD for captions.", "bboxes": [{"left": 0.08823529411764706, "top": 0.6060820707070707, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 8}, {"left": 0.08823529411764706, "top": 0.6206300505050505, "width": 0.39443790849673205, "height": 0.01272727272727281, "page": 8}, {"left": 0.08823529411764706, "top": 0.6351767676767677, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 8}], "section": "Discussion", "id": "5206"}, {"text": "These preliminary results also suggest that Version 2 of the prototype addresses some of the main technical challenges identified in the previous studies.", "bboxes": [{"left": 0.5176470588235295, "top": 0.42578787878787877, "width": 0.39435457516339856, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.44033585858585855, "width": 0.3946356209150327, "height": 0.012727272727272754, "page": 8}, {"left": 0.5176470588235295, "top": 0.45488257575757574, "width": 0.21902614379084961, "height": 0.012727272727272698, "page": 8}], "section": "Discussion", "id": "5207"}, {"text": "As discussed in the Related Work, prior research indicated a need for reliable transcription and sufficient battery throughout daily tasks [16, 25].", "bboxes": [{"left": 0.5176470588235295, "top": 0.6136578282828283, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.6282058080808081, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.6427525252525252, "width": 0.20843464052287586, "height": 0.012727272727272698, "page": 8}], "section": "TECHNICAL EVALUATION", "id": "5208"}, {"text": "With these improvements and positive feedback, we are excited about opportunities to further validate the potential through quantitative methods for attention, as well as through more extended usage in continued diary studies.", "bboxes": [{"left": 0.5176470588235295, "top": 0.5348863636363637, "width": 0.3943382352941176, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.5494343434343434, "width": 0.39435457516339856, "height": 0.01272727272727281, "page": 8}, {"left": 0.5176470588235295, "top": 0.5639810606060606, "width": 0.3946290849673202, "height": 0.012727272727272698, "page": 8}, {"left": 0.5176470588235295, "top": 0.5785290404040404, "width": 0.3695669934640523, "height": 0.012727272727272698, "page": 8}], "section": "Discussion", "id": "5209"}, {"text": "When asked about how long they would like to use the system, all participants expressed a desire to use it for multiple hours on a daily basis.", "bboxes": [{"left": 0.0882516339869281, "top": 0.39669444444444446, "width": 0.39433823529411766, "height": 0.012727272727272698, "page": 8}, {"left": 0.0882516339869281, "top": 0.4112411616161616, "width": 0.39445424836601306, "height": 0.012727272727272754, "page": 8}, {"left": 0.0882516339869281, "top": 0.42578787878787877, "width": 0.20883006535947712, "height": 0.012727272727272698, "page": 8}], "section": "Prototype Eyewear Experience and Desired Use", "id": "5210"}, {"text": "We thank our many collaborators for contributing their expertise in hearing accessibility, in particular Dimitri Kanevsky, who is continuously helping us improve this vision while evangelizing the DHH perspective, and also Chet Gnegy, Pascal Getreuer and Dick Lyon; software development, Eric Bouchard; optics, human factors and hardware, especially Omar Negrete, Kiet Tang, Ozan Cakmakci, David Hoffman and Ella Zhang; mobile perception, especially Eunyoung Kim, Jeff Gilbert and Alec Go; user experience and product thinking, Robin Dua, Chelsey Fleming, JD Velasquez, Jin Kim, Leo Szrejter, and many others.", "bboxes": [{"left": 0.5176470588235295, "top": 0.6727487373737374, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.6872967171717173, "width": 0.39441176470588224, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7018434343434343, "width": 0.3946356209150327, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.716391414141414, "width": 0.3945196078431372, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7309381313131313, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7451805555555555, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7597272727272727, "width": 0.39445424836601306, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.7742739898989899, "width": 0.39432189542483653, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.7888219696969696, "width": 0.3943725490196077, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.8033686868686869, "width": 0.39427287581699344, "height": 0.012727272727272587, "page": 9}, {"left": 0.5176470588235295, "top": 0.8179166666666666, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 9}, {"left": 0.5176470588235295, "top": 0.8324633838383839, "width": 0.09076960784313715, "height": 0.012727272727272587, "page": 9}], "section": "CONCLUSIONS", "id": "5211"}, {"text": "In this paper, we introduce Wearable Subtitles, a lightweight headworn prototype system for all-day hearing accessibility.", "bboxes": [{"left": 0.5176470588235295, "top": 0.3975972222222222, "width": 0.3943382352941175, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.412145202020202, "width": 0.39430555555555546, "height": 0.012727272727272754, "page": 9}], "section": "CONCLUSIONS", "id": "5212"}, {"text": "Privacy issues are important for continuously captured audio.", "bboxes": [{"left": 0.517640522875817, "top": 0.24644065656565659, "width": 0.39427287581699344, "height": 0.012727272727272726, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "id": "5213"}, {"text": "We evaluated BLE power consumption under various usage scenarios.", "bboxes": [{"left": 0.08823529411764706, "top": 0.12880934343434344, "width": 0.3943545751633987, "height": 0.012727272727272726, "page": 9}, {"left": 0.08823529411764706, "top": 0.14335732323232322, "width": 0.06489379084967321, "height": 0.012727272727272726, "page": 9}], "section": "System Power Consumption", "id": "5214"}, {"text": "Our user studies included 24 DHH participants, 19 from outside our institution, and five from within.", "bboxes": [{"left": 0.08823529411764706, "top": 0.45426388888888886, "width": 0.3943218954248366, "height": 0.012727272727272754, "page": 9}, {"left": 0.08823529411764706, "top": 0.46881186868686864, "width": 0.30177450980392156, "height": 0.012727272727272754, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "id": "5215"}, {"text": "Multiple speakers in group conversations can cause confusion in the transcription, whether a phone or HWD is used.", "bboxes": [{"left": 0.08823529411764706, "top": 0.5924570707070707, "width": 0.3943218954248366, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.607003787878788, "width": 0.39458660130718953, "height": 0.012727272727272698, "page": 9}, {"left": 0.08823529411764706, "top": 0.6215517676767677, "width": 0.03430882352941177, "height": 0.012727272727272698, "page": 9}], "section": "LIMITATIONS AND FUTURE WORK", "id": "5216"}, {"text": "To validate our proposed approach for hearing accessibility, we conducted a pilot and two studies with 24 DHH participants who provided feedback on our prototypes in various scenarios and tasks.", "bboxes": [{"left": 0.5176470588235295, "top": 0.5503371212121212, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.564885101010101, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.5794318181818182, "width": 0.394437908496732, "height": 0.012727272727272698, "page": 9}, {"left": 0.5176470588235295, "top": 0.593979797979798, "width": 0.1836797385620914, "height": 0.012727272727272698, "page": 9}], "section": "CONCLUSIONS", "id": "5217"}, {"text": "ACKNOWLEDGMENTS", "bboxes": [{"left": 0.5176470588235295, "top": 0.6594103535353536, "width": 0.1605392156862745, "height": 0.011515151515151478, "page": 9}], "section": "CONCLUSIONS", "id": "5218"}, {"text": "Power Consumption for Communications Scenarios", "bboxes": [{"left": 0.08823529411764706, "top": 0.1154709595959596, "width": 0.3647728758169934, "height": 0.011515151515151506, "page": 9}], "section": "System Power Consumption", "id": "5219"}], "uist-3": [{"text": "Another difference from conventional markers is that LightAnchors can transmit dynamic payloads, without the need for WiFi, Bluetooth or indeed, any connectivity.", "bboxes": [{"left": 0.5176470588235295, "top": 0.5248699494949495, "width": 0.394375816993464, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5394179292929293, "width": 0.3943382352941175, "height": 0.012727272727272698, "page": 0}, {"left": 0.5176470588235295, "top": 0.5539646464646465, "width": 0.36368627450980384, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION", "id": "5220"}, {"text": "As smartphones are the most pervasive AR platform at present, we created a proof-of-concept LightAnchors implementation for iOS.", "bboxes": [{"left": 0.5176470588235295, "top": 0.7212525252525253, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 0}, {"left": 0.5176470588235295, "top": 0.7357992424242424, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 0}, {"left": 0.5176470588235295, "top": 0.7503472222222222, "width": 0.12451797385620911, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "id": "5221"}, {"text": "In this paper, we present LightAnchors, a new method to display spatially-anchored data in augmented reality applications.", "bboxes": [{"left": 0.08821895424836601, "top": 0.6173030303030304, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.6318510101010101, "width": 0.3945032679738562, "height": 0.012727272727272698, "page": 0}, {"left": 0.08821895424836601, "top": 0.6463977272727273, "width": 0.03588888888888887, "height": 0.01272727272727281, "page": 0}], "section": "INTRODUCTION", "id": "5222"}, {"text": "Augmented reality (AR) allows for the overlay of digital information and interactive content onto scenes and objects.", "bboxes": [{"left": 0.08823529411764706, "top": 0.4936578282828282, "width": 0.39448692810457514, "height": 0.01272727272727281, "page": 0}, {"left": 0.08823529411764706, "top": 0.5082058080808081, "width": 0.37702777777777774, "height": 0.012727272727272698, "page": 0}], "section": "INTRODUCTION", "id": "5223"}, {"text": "At a high level, for every incoming frame of video, our algorithm creates an image pyramid, such that lights  big or small, close or far  are guaranteed to be contained within a single pixel at least one level.", "bboxes": [{"left": 0.5176470588235295, "top": 0.38699116161616165, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.40153914141414143, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.4160858585858586, "width": 0.3943055555555556, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.4306338383838384, "width": 0.20018300653594767, "height": 0.012727272727272698, "page": 1}], "section": "IMPLEMENTATION", "id": "5224"}, {"text": "We encode all data as a binary sequence, prefixed with a known pattern.", "bboxes": [{"left": 0.5176470588235295, "top": 0.5821426767676768, "width": 0.3943807189542482, "height": 0.012727272727272698, "page": 1}, {"left": 0.5176470588235295, "top": 0.596385101010101, "width": 0.09902124183006533, "height": 0.012727272727272698, "page": 1}], "section": "Encoding & Point Lights", "id": "5225"}, {"text": "With specialized equipment, it is possible to transmit data at high speeds with visible light (VLC).", "bboxes": [{"left": 0.5176470588235295, "top": 0.17790025252525252, "width": 0.394388888888889, "height": 0.012727272727272754, "page": 1}, {"left": 0.5176470588235295, "top": 0.19214267676767677, "width": 0.24292810457516334, "height": 0.012727272727272754, "page": 1}], "section": "Visible Light Communication with Commodity Devices", "id": "5226"}, {"text": "There are a wide variety of successful fiducial marking schemes.", "bboxes": [{"left": 0.08823529411764706, "top": 0.16183964646464646, "width": 0.3945196078431373, "height": 0.012727272727272754, "page": 1}, {"left": 0.08823529411764706, "top": 0.17638762626262627, "width": 0.05978758169934641, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "id": "5227"}, {"text": "Many light-based schemes have been previously considered.", "bboxes": [{"left": 0.08823529411764706, "top": 0.3566881313131313, "width": 0.3942401960784313, "height": 0.012727272727272698, "page": 1}], "section": "INTRODUCTION", "id": "5228"}, {"text": "Rarer are systems that support dynamic payloads.", "bboxes": [{"left": 0.08821895424836601, "top": 0.49488131313131317, "width": 0.32290522875816996, "height": 0.012727272727272754, "page": 1}], "section": "INTRODUCTION", "id": "5229"}, {"text": "Augmented reality systems can also track objects using innate features.", "bboxes": [{"left": 0.08823529411764706, "top": 0.8424457070707071, "width": 0.3944705882352941, "height": 0.01272727272727281, "page": 1}, {"left": 0.08823529411764706, "top": 0.8569936868686869, "width": 0.08645424836601305, "height": 0.012727272727272587, "page": 1}], "section": "INTRODUCTION", "id": "5230"}, {"text": "LightAnchors is closer in spirit to approaches that use active point lights as markers.", "bboxes": [{"left": 0.08821895424836601, "top": 0.6767159090909091, "width": 0.3943709150326798, "height": 0.01272727272727281, "page": 1}, {"left": 0.08821895424836601, "top": 0.6912626262626262, "width": 0.15312745098039215, "height": 0.012727272727272587, "page": 1}], "section": "INTRODUCTION", "id": "5231"}, {"text": "Unlike prior approaches that synchronized light modulation with e.g., RF triggers [12], our lights and smartphones are totally unsynchronized.", "bboxes": [{"left": 0.5176470588235295, "top": 0.7348825757575758, "width": 0.394470588235294, "height": 0.012727272727272587, "page": 1}, {"left": 0.5176470588235295, "top": 0.7494305555555555, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 1}, {"left": 0.5176470588235295, "top": 0.7639772727272728, "width": 0.15274836601307185, "height": 0.012727272727272587, "page": 1}], "section": "Encoding & Point Lights", "id": "5232"}, {"text": "LightAnchors overlaps with several disparate literatures, including marker-based tracking approaches, marker-less computer vision techniques, and visible light communication.", "bboxes": [{"left": 0.08823529411764706, "top": 0.08305176767676768, "width": 0.3944705882352941, "height": 0.012727272727272726, "page": 1}, {"left": 0.08823529411764706, "top": 0.09759974747474748, "width": 0.3941519607843137, "height": 0.012727272727272726, "page": 1}, {"left": 0.08823529411764706, "top": 0.11214646464646463, "width": 0.39450326797385615, "height": 0.01272727272727274, "page": 1}, {"left": 0.08823529411764706, "top": 0.12669444444444444, "width": 0.02961437908496732, "height": 0.012727272727272726, "page": 1}], "section": "INTRODUCTION", "id": "5233"}, {"text": "Marker-Less Strategies", "bboxes": [{"left": 0.08823529411764706, "top": 0.8291073232323233, "width": 0.16241830065359475, "height": 0.011515151515151478, "page": 1}], "section": "INTRODUCTION", "id": "5234"}, {"text": "Light-Based Markers", "bboxes": [{"left": 0.08823529411764706, "top": 0.34334974747474745, "width": 0.14592973856209146, "height": 0.011515151515151534, "page": 1}], "section": "INTRODUCTION", "id": "5235"}, {"text": "Fiducial Markers", "bboxes": [{"left": 0.08823529411764706, "top": 0.14850126262626262, "width": 0.11651307189542483, "height": 0.011515151515151534, "page": 1}], "section": "INTRODUCTION", "id": "5236"}, {"text": "After each frame is tracked, we attempt to decode all candidate anchors.", "bboxes": [{"left": 0.08823529411764706, "top": 0.7112335858585859, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.08823529411764706, "top": 0.7257815656565656, "width": 0.08683333333333333, "height": 0.01272727272727281, "page": 2}], "section": "Detection", "id": "5237"}, {"text": "We profiled our implementation using Xcode on both a iPhone 7 and iPhone X. We tested different base resolutions (i.e., largest pyramid size), and for each, ran three trials of 500 frames each.", "bboxes": [{"left": 0.5176470588235295, "top": 0.6260820707070708, "width": 0.3944869281045751, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176470588235295, "top": 0.6406300505050505, "width": 0.3945686274509803, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.6548712121212121, "width": 0.39450326797385615, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.6694191919191919, "width": 0.11157189542483659, "height": 0.01272727272727281, "page": 2}], "section": "Performance Analysis", "id": "5238"}, {"text": "To evaluate the robustness of our approach, we tested point lights of different size, across varying rooms, lighting conditions, and sensing distances.", "bboxes": [{"left": 0.5176470588235295, "top": 0.8354760101010101, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 2}, {"left": 0.5176470588235295, "top": 0.85002398989899, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 2}, {"left": 0.5176470588235295, "top": 0.8642651515151516, "width": 0.18759967320261428, "height": 0.012727272727272587, "page": 2}], "section": "EVALUATION", "id": "5239"}, {"text": "Our detection process passes all candidate anchors to our tracker on every frame, which must be computationally inexpensive in order to maintain a high frame rate.", "bboxes": [{"left": 0.08823529411764706, "top": 0.4582032828282828, "width": 0.39448692810457514, "height": 0.012727272727272698, "page": 2}, {"left": 0.08821895424836601, "top": 0.4727512626262626, "width": 0.3944379084967321, "height": 0.012727272727272754, "page": 2}, {"left": 0.08821895424836601, "top": 0.4872979797979798, "width": 0.33000490196078436, "height": 0.012727272727272754, "page": 2}], "section": "Detection", "id": "5240"}, {"text": "Our LightAnchor detection algorithm is designed to have high recall.", "bboxes": [{"left": 0.08823529411764706, "top": 0.263354797979798, "width": 0.39438888888888884, "height": 0.012727272727272754, "page": 2}, {"left": 0.08823529411764706, "top": 0.2779027777777778, "width": 0.07508986928104573, "height": 0.012727272727272754, "page": 2}], "section": "Detection", "id": "5241"}, {"text": "An interesting corner case that must be handled are reflections from LightAnchors (e.g., glints off specular objects, which also appear as point lights).", "bboxes": [{"left": 0.5176633986928104, "top": 0.460030303030303, "width": 0.39450326797385626, "height": 0.012727272727272698, "page": 2}, {"left": 0.5176633986928104, "top": 0.4745782828282828, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 2}, {"left": 0.5176633986928104, "top": 0.489125, "width": 0.22016176470588245, "height": 0.012727272727272754, "page": 2}], "section": "Detection", "id": "5242"}, {"text": "In our proof-of-concept iOS app, we use the AVCaptureSession API to grab video frames and OpenCV for image processing.", "bboxes": [{"left": 0.08823529411764706, "top": 0.08305176767676768, "width": 0.3945196078431373, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.09759974747474748, "width": 0.39443790849673205, "height": 0.012727272727272726, "page": 2}, {"left": 0.08823529411764706, "top": 0.11214646464646463, "width": 0.052343137254901986, "height": 0.01272727272727274, "page": 2}], "section": "Encoding & Point Lights", "id": "5243"}, {"text": "We captured study data using an iPhone 7 (720p at 120 FPS) in three environments: workshop, classroom and office.", "bboxes": [{"left": 0.08823529411764706, "top": 0.29153661616161614, "width": 0.39407516339869275, "height": 0.012727272727272754, "page": 3}, {"left": 0.08823529411764706, "top": 0.3057790404040404, "width": 0.37464052287581695, "height": 0.012727272727272754, "page": 3}], "section": "Apparatus & Procedure", "id": "5244"}, {"text": "Across all conditions and distances, we found a mean bit error rate (BER) of 5.2%, or roughly 1 error in every 20 transmitted bits.", "bboxes": [{"left": 0.5176470588235295, "top": 0.4151729797979798, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.4297209595959596, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 3}, {"left": 0.5176470588235295, "top": 0.4442676767676768, "width": 0.07512254901960769, "height": 0.012727272727272698, "page": 3}], "section": "Bit Error Rate", "id": "5245"}, {"text": "There are several effects that can cause incorrect rejection of LightAnchors, including poor tracking, motion blur, suboptimal camera-light synchronization, camera sensor noise, and variations in ambient lighting.", "bboxes": [{"left": 0.5176470588235295, "top": 0.6682032828282828, "width": 0.39442156862745104, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176470588235295, "top": 0.6824457070707071, "width": 0.3944869281045752, "height": 0.01272727272727281, "page": 3}, {"left": 0.5176633986928104, "top": 0.6969924242424242, "width": 0.39445424836601306, "height": 0.012727272727272587, "page": 3}, {"left": 0.5176633986928104, "top": 0.711540404040404, "width": 0.19824019607843135, "height": 0.01272727272727281, "page": 3}], "section": "Recognition Latency", "id": "5246"}, {"text": "Our detection rate did not change substantially across study conditions, and so we combine detection results for brevity.", "bboxes": [{"left": 0.08823529411764706, "top": 0.6024457070707071, "width": 0.3945196078431373, "height": 0.012727272727272698, "page": 3}, {"left": 0.08823529411764706, "top": 0.6169936868686868, "width": 0.39432189542483653, "height": 0.012727272727272698, "page": 3}], "section": "LightAnchor Detection", "id": "5247"}, {"text": "After the pre/postamble filtering process, the true positive rate was still 100%, but our system found 3.1% false", "bboxes": [{"left": 0.08823529411764706, "top": 0.6969974747474748, "width": 0.3943382352941176, "height": 0.012727272727272587, "page": 3}, {"left": 0.08823529411764706, "top": 0.7115441919191919, "width": 0.3943382352941176, "height": 0.01272727272727281, "page": 3}], "section": "LightAnchor Detection", "id": "5248"}, {"text": "Across all conditions, we found a mean recognition time of 464 ms. As our test LightAnchors were 22 bits long (6 bit preamble, 10 bit payload, 6 bit postamble), they take a minimum of 183ms to transmit at 120 FPS.", "bboxes": [{"left": 0.08823529411764706, "top": 0.4436666666666667, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4582146464646465, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.4727613636363637, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 4}, {"left": 0.08823529411764706, "top": 0.48730934343434346, "width": 0.25980718954248366, "height": 0.012727272727272754, "page": 4}], "section": "Recognition Latency", "id": "5249"}, {"text": "The data payload of LightAnchors can be used in three distinct ways: fixed payloads, dynamic payloads, and connection payloads.", "bboxes": [{"left": 0.5176470588235295, "top": 0.33305176767676764, "width": 0.39450326797385615, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.3475997474747474, "width": 0.39450326797385615, "height": 0.012727272727272754, "page": 4}, {"left": 0.5176470588235295, "top": 0.36214646464646466, "width": 0.09156045751633979, "height": 0.012727272727272698, "page": 4}], "section": "PAYLOAD TYPES & EXAMPLE USES", "id": "5250"}, {"text": "The simplest use of LightAnchors is a fixed payload (similar to fiducial markers).", "bboxes": [{"left": 0.5176470588235295, "top": 0.46850631313131313, "width": 0.39445424836601306, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.4830542929292929, "width": 0.13464705882352945, "height": 0.012727272727272698, "page": 4}], "section": "Fixed Payloads", "id": "5251"}, {"text": "As a demonstration of a fixed payload, we instrumented a street parking meter (Figure 5, left) with a light that transmits its enforcement zone ID (from which the rate schedule could", "bboxes": [{"left": 0.5176470588235295, "top": 0.6070050505050505, "width": 0.39435457516339867, "height": 0.012727272727272698, "page": 4}, {"left": 0.5176470588235295, "top": 0.6212462121212121, "width": 0.39460294117647055, "height": 0.01272727272727281, "page": 4}, {"left": 0.5176470588235295, "top": 0.6357941919191918, "width": 0.39440522875816997, "height": 0.01272727272727281, "page": 4}], "section": "Fixed Payloads", "id": "5252"}, {"text": "Finally, LightAnchor payloads could be used to provide connection information.", "bboxes": [{"left": 0.08823529411764706, "top": 0.7121426767676768, "width": 0.39447058823529413, "height": 0.012727272727272587, "page": 5}, {"left": 0.08823529411764706, "top": 0.7266906565656566, "width": 0.13313235294117648, "height": 0.01272727272727281, "page": 5}], "section": "Dynamic Payloads", "id": "5253"}, {"text": "We have presented our work on LightAnchors, a new approach for overlaying information and interfaces onto everyday objects in mobile AR.", "bboxes": [{"left": 0.5176470588235295, "top": 0.8215366161616162, "width": 0.3943888888888889, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176470588235295, "top": 0.8360845959595959, "width": 0.39440522875816997, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176470588235295, "top": 0.8506313131313132, "width": 0.1707843137254902, "height": 0.012727272727272587, "page": 5}], "section": "CONCLUSION", "id": "5254"}, {"text": "The biggest drawback of our method is limited bitrate, which is chiefly set by smartphone processors and camera FPS.", "bboxes": [{"left": 0.5176470588235295, "top": 0.36456691919191925, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176470588235295, "top": 0.37911489898989903, "width": 0.3942892156862745, "height": 0.012727272727272698, "page": 5}], "section": "LIMITATIONS", "id": "5255"}, {"text": "There are also challenges in controlling the exposure and focus of the camera to enable robust tracking.", "bboxes": [{"left": 0.5176307189542484, "top": 0.560949494949495, "width": 0.39442156862745104, "height": 0.012727272727272698, "page": 5}, {"left": 0.5176307189542484, "top": 0.5754962121212122, "width": 0.2962075163398693, "height": 0.012727272727272698, "page": 5}], "section": "LIMITATIONS", "id": "5256"}, {"text": "More interesting are dynamic payloads, which contain a fixed UID that denotes the object, along with a dynamic value.", "bboxes": [{"left": 0.08823529411764706, "top": 0.4372941919191919, "width": 0.39433823529411755, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.45184217171717167, "width": 0.3943382352941176, "height": 0.012727272727272754, "page": 5}, {"left": 0.08823529411764706, "top": 0.4663888888888889, "width": 0.039792483660130704, "height": 0.012727272727272698, "page": 5}], "section": "Dynamic Payloads", "id": "5257"}, {"text": "Finally, at present, our LightAnchor widgets are flat with respect to the smartphone screen, as a single LightAnchor cannot provide 3D orientation.", "bboxes": [{"left": 0.5176307189542484, "top": 0.6700467171717172, "width": 0.39450326797385615, "height": 0.012727272727272587, "page": 5}, {"left": 0.5176307189542484, "top": 0.6845946969696969, "width": 0.39450326797385626, "height": 0.01272727272727281, "page": 5}, {"left": 0.5176307189542484, "top": 0.6991414141414142, "width": 0.17979248366013068, "height": 0.012727272727272587, "page": 5}], "section": "LIMITATIONS", "id": "5258"}, {"text": "Of course, many devices already contain microprocessors that can control status lights and could be LightAnchor-enabled with a firmware update.", "bboxes": [{"left": 0.08823529411764706, "top": 0.590034090909091, "width": 0.3946192810457516, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.6045820707070707, "width": 0.39450326797385615, "height": 0.012727272727272698, "page": 5}, {"left": 0.08823529411764706, "top": 0.6191287878787879, "width": 0.19194771241830066, "height": 0.012727272727272698, "page": 5}], "section": "Dynamic Payloads", "id": "5259"}, {"text": "Connection Payloads", "bboxes": [{"left": 0.08823529411764706, "top": 0.6988042929292929, "width": 0.1498692810457516, "height": 0.011515151515151478, "page": 5}], "section": "Dynamic Payloads", "id": "5260"}, {"text": "This research was generously supported with funds from the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.", "bboxes": [{"left": 0.08823529411764706, "top": 0.1342638888888889, "width": 0.39428921568627445, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.1488118686868687, "width": 0.3945196078431373, "height": 0.012727272727272726, "page": 6}, {"left": 0.08823529411764706, "top": 0.16335858585858584, "width": 0.3944705882352941, "height": 0.012727272727272754, "page": 6}, {"left": 0.08823529411764706, "top": 0.17790656565656565, "width": 0.08057352941176472, "height": 0.012727272727272754, "page": 6}], "section": "CONCLUSION", "id": "5261"}, {"text": "ACKNOWLEDGMENTS", "bboxes": [{"left": 0.08823529411764706, "top": 0.12092550505050505, "width": 0.1605392156862745, "height": 0.01151515151515152, "page": 6}], "section": "CONCLUSION", "id": "5262"}], "2104.03820": [{"text": "Recent advances in deep generative models have enabled them to produce content of a quality indistinguishable from that produced by a human being.", "bboxes": [{"left": 0.1200016339869281, "top": 0.6748825757575757, "width": 0.6999918300653596, "height": 0.011320707070707092, "page": 0}, {"left": 0.1200016339869281, "top": 0.6921792929292929, "width": 0.23094607843137255, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "id": "5263"}, {"text": "Recently, generative techniques have been applied to the realm of software engineering.", "bboxes": [{"left": 0.1362794117647059, "top": 0.7613636363636364, "width": 0.5201307189542483, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "id": "5264"}, {"text": "Our research focuses on understanding the extent to which generative models are still useful to human stakeholders despite their potential to produce imperfect output.", "bboxes": [{"left": 0.196281045751634, "top": 0.2942222222222222, "width": 0.6837222222222222, "height": 0.013969696969696965, "page": 1}, {"left": 0.18000163398692812, "top": 0.31151893939393943, "width": 0.3034836601307189, "height": 0.011320707070707037, "page": 1}], "section": "1 INTRODUCTION", "id": "5265"}, {"text": "We address these questions by considering the use of an NMT model within the context of application modernization [27, 64, 72].", "bboxes": [{"left": 0.196281045751634, "top": 0.3807032828282828, "width": 0.6861993464052287, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.398, "width": 0.11356862745098037, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5266"}, {"text": "To explore the utility of NMT models, we conducted a series of scenario-based design interviews with 11 professional software engineers who work across a variety of technology areas.", "bboxes": [{"left": 0.196281045751634, "top": 0.4671843434343434, "width": 0.6837124183006537, "height": 0.011320707070707092, "page": 1}, {"left": 0.18000163398692812, "top": 0.4844810606060606, "width": 0.3955375816993464, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5267"}, {"text": "Several studies have examined the notion of an imperfect AI system.", "bboxes": [{"left": 0.1200016339869281, "top": 0.4369166666666667, "width": 0.405014705882353, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Imperfect AI", "id": "5268"}, {"text": "In recent years, several efforts have focused on the use of AI and machine learning techniques for various tasks related to software engineering, including code completion [15, 41, 75, 84], code classification [49, 68], API recommendation [16, 33], variable and method naming [3, 5], type inference [39, 93], bug detection and repair [25, 40, 71, 74, 89, 95], comment description and generation [4, 44, 48, 65, 80, 91], code change summarization [66], and code clone detection [96].", "bboxes": [{"left": 0.1200016339869281, "top": 0.23800883838383838, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.25530555555555556, "width": 0.7016078431372549, "height": 0.011320707070707092, "page": 2}, {"left": 0.11963562091503267, "top": 0.2726022727272727, "width": 0.7003627450980393, "height": 0.011320707070707037, "page": 2}, {"left": 0.1200016339869281, "top": 0.2898977272727273, "width": 0.6862254901960784, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.1 AI Techniques for Software Engineering", "id": "5269"}, {"text": "Given the imperfect output of state-of-the-art NMT models, we posit that such systems will act in concert with human software engineers as a collaborative partner or teammate.", "bboxes": [{"left": 0.1200016339869281, "top": 0.7396010101010101, "width": 0.7, "height": 0.011320707070707092, "page": 2}, {"left": 0.1200016339869281, "top": 0.7568964646464647, "width": 0.34180555555555553, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "id": "5270"}, {"text": "We discuss three areas relevant to our work: recent advances in the use of AI techniques, and specifically deep generative models, in software engineering; studies of the utility of imperfect AI; and studies of human-AI co-creation with generative AI.", "bboxes": [{"left": 0.11929738562091505, "top": 0.1428800505050505, "width": 0.7006944444444443, "height": 0.011320707070707065, "page": 2}, {"left": 0.1200016339869281, "top": 0.16017676767676767, "width": 0.7, "height": 0.011320707070707065, "page": 2}, {"left": 0.11945915032679738, "top": 0.17747222222222223, "width": 0.11533333333333334, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "id": "5271"}, {"text": "Despite identifying favorable outcomes of human-AI partnerships, we note that these outcomes are all subjective: the quality of the generated output lies in the perceptions of the people using the system.", "bboxes": [{"left": 0.196281045751634, "top": 0.5790757575757576, "width": 0.6854493464052289, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.5963724747474748, "width": 0.5234330065359476, "height": 0.011320707070707092, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "id": "5272"}, {"text": "Much of the recent focus of the generative AI community has been on developing new capabilities and exploring their applications in creative domains such as art [29, 51, 70, 92], photography [10], music [45, 57], video games [37], and literature [21], as well as scientific domains such as drug discovery [24], materials discovery [98], and software engineering [69].", "bboxes": [{"left": 0.196281045751634, "top": 0.4234090909090909, "width": 0.6837222222222223, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.44070580808080806, "width": 0.7016078431372548, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.45800252525252527, "width": 0.6999918300653596, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.4752979797979798, "width": 0.10299183006535947, "height": 0.011320707070707092, "page": 3}], "section": "2 RELATED WORK @@ 2.3 Co-Creation with Generative AI", "id": "5273"}, {"text": "We developed an exploratory design scenario in order to engage software engineers in a discussion about the role of generative AI in application modernization.", "bboxes": [{"left": 0.17929901960784314, "top": 0.7062714646464646, "width": 0.7007042483660132, "height": 0.011320707070707092, "page": 3}, {"left": 0.18000163398692812, "top": 0.7235669191919193, "width": 0.2531307189542483, "height": 0.011320707070706981, "page": 3}], "section": "3 DESIGN SCENARIO", "id": "5274"}, {"text": "Each interview was attended by at least three of the authors.", "bboxes": [{"left": 0.1362794117647059, "top": 0.6843686868686868, "width": 0.367361111111111, "height": 0.011320707070707092, "page": 4}], "section": "4 METHOD", "id": "5275"}, {"text": "The UX variants were designed to resemble a simple programming environment with two code panes: a source pane on the left for input code, and a target pane on the right for the code output by the NMT model.", "bboxes": [{"left": 0.1362794117647059, "top": 0.12126010101010101, "width": 0.6853725490196079, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.13855555555555557, "width": 0.6204558823529411, "height": 0.011320707070707065, "page": 4}], "section": "3 DESIGN SCENARIO", "id": "5276"}, {"text": "Our scenario used real output from TransCoder that contained flaws.", "bboxes": [{"left": 0.1362794117647059, "top": 0.31151893939393943, "width": 0.405109477124183, "height": 0.011320707070707037, "page": 4}], "section": "3 DESIGN SCENARIO", "id": "5277"}, {"text": "We conducted an interview study with software engineers in which we used the design scenario to spark discussions around the role of generative AI in application modernization.", "bboxes": [{"left": 0.11929738562091505, "top": 0.4422209595959596, "width": 0.7006944444444444, "height": 0.011320707070707092, "page": 4}, {"left": 0.1200016339869281, "top": 0.45951767676767674, "width": 0.3838970588235294, "height": 0.011320707070707092, "page": 4}], "section": "4 METHOD", "id": "5278"}, {"text": "We conducted a thematic analysis of our data to identify important ideas and themes from our interviews.", "bboxes": [{"left": 0.17929901960784314, "top": 0.4566830808080808, "width": 0.6401535947712419, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "id": "5279"}, {"text": "We begin by highlighting the difficulties faced by our software engineers in modernizing legacy applications, motivating the need for AI support.", "bboxes": [{"left": 0.196281045751634, "top": 0.6642386363636363, "width": 0.6853316993464053, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.6815340909090909, "width": 0.20596568627450976, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "id": "5280"}, {"text": "Our interviews generated a wealth of material: 11 hours of recorded videos, approximately 63 pages of notes, and a corpus of approximately 400 pages of interview transcripts containing about 89k words.", "bboxes": [{"left": 0.196281045751634, "top": 0.5258686868686868, "width": 0.6837222222222223, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.5431641414141414, "width": 0.5334330065359476, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "id": "5281"}, {"text": "We recruited 11 full-time software engineers within our organization, an international information technology company.", "bboxes": [{"left": 0.17929901960784314, "top": 0.1428800505050505, "width": 0.7029509803921569, "height": 0.011320707070707065, "page": 5}], "section": "4 METHOD @@ 4.1 Participants", "id": "5282"}, {"text": "For each UX variant, we asked participants about what they liked and disliked and how they might improve the design.", "bboxes": [{"left": 0.18000163398692812, "top": 0.2824848484848485, "width": 0.7022565359477124, "height": 0.011320707070707037, "page": 5}], "section": "4 METHOD @@ 4.2 Variant-Specific Questions", "id": "5283"}, {"text": "P4 described application modernization as a rats nest [of] very impossible, undocumented, uncommitted, lots of dead unit tests that hadnt been run [in] year[s] kind of situation, and complained that there was an architecture at some point... then you had other folks come along and write additional pieces that... kind of go their own way.", "bboxes": [{"left": 0.5011683006535947, "top": 0.7433068181818181, "width": 0.3792091503267975, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.7606035353535354, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.7778989898989899, "width": 0.6999967320261437, "height": 0.011320707070707092, "page": 5}, {"left": 0.18000163398692812, "top": 0.795195707070707, "width": 0.2411062091503268, "height": 0.011320707070707092, "page": 5}], "section": "5 RESULTS", "id": "5284"}, {"text": "In discussing whether they would accept code produced by the NMT model, participants felt that they would treat it in the same fashion that they would treat the output of their fellow software developers: by reviewing and testing it.", "bboxes": [{"left": 0.3613709150326797, "top": 0.48463636363636364, "width": 0.460233660130719, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.5019330808080809, "width": 0.7000000000000001, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.519229797979798, "width": 0.23117483660130722, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "id": "5285"}, {"text": "We recognize that trust is a complex, multi-faceted construct that has been described as being an attitude [78], an intention [62], and a behavior [1].", "bboxes": [{"left": 0.1362794117647059, "top": 0.35429040404040407, "width": 0.683720588235294, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.37158712121212123, "width": 0.19429738562091503, "height": 0.011320707070706981, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "id": "5286"}, {"text": "Some participants had questions about the underlying mechanics of the NMT model, as they were curious how it works (P5) and would like to know what its thinking (P10).", "bboxes": [{"left": 0.33141666666666664, "top": 0.5976868686868687, "width": 0.49019444444444443, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.6149835858585858, "width": 0.561625816993464, "height": 0.011320707070707092, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "id": "5287"}, {"text": "P9 offered a more nuanced view on who would need to have an understanding of how the AI model operated.", "bboxes": [{"left": 0.1362794117647059, "top": 0.7782045454545454, "width": 0.6625980392156862, "height": 0.013969696969697076, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "id": "5288"}, {"text": "Many participants discussed the issue of trust in software engineering and how practices such as testing and code review help establish trust amongst human teams of software engineers.", "bboxes": [{"left": 0.1200016339869281, "top": 0.22566161616161615, "width": 0.6999918300653596, "height": 0.011320707070707092, "page": 6}, {"left": 0.1200016339869281, "top": 0.2429570707070707, "width": 0.4484689542483661, "height": 0.011320707070707065, "page": 6}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "id": "5289"}, {"text": "One way of producing such explanations is via the alternate translations shown in Figure 1C.", "bboxes": [{"left": 0.196281045751634, "top": 0.5476982323232323, "width": 0.5615604575163399, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "id": "5290"}, {"text": "Understanding the models operation could also eliminate some of the confusion caused by the confidence highlights in Figure 1B.", "bboxes": [{"left": 0.196281045751634, "top": 0.2250366161616162, "width": 0.6837124183006537, "height": 0.011320707070707037, "page": 7}, {"left": 0.18000163398692812, "top": 0.24233333333333332, "width": 0.07773856209150323, "height": 0.011320707070707065, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "id": "5291"}, {"text": "This discrepancy in confidence, in which participants were more confident in their own abilities to translate the code than the NMT model, caused many participants to desire additional insight into why the NMT model was not confident.", "bboxes": [{"left": 0.196281045751634, "top": 0.44392045454545453, "width": 0.6837124183006537, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.4612171717171717, "width": 0.7022598039215685, "height": 0.01396969696969702, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "id": "5292"}, {"text": "Despite the feelings that having an understanding of the NMT models operation wasnt important, we observed that having such understanding does have benefits.", "bboxes": [{"left": 0.196281045751634, "top": 0.15585227272727273, "width": 0.6837124183006537, "height": 0.011320707070707092, "page": 7}, {"left": 0.18000163398692812, "top": 0.17314898989898989, "width": 0.28227450980392155, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "id": "5293"}, {"text": "Participants identified the importance of being able to give feedback (P4), such as by unflag[ing] (P7) low-confidence tokens identified by the NMT model.", "bboxes": [{"left": 0.35497712418300653, "top": 0.679611111111111, "width": 0.5254019607843138, "height": 0.011320707070707092, "page": 7}, {"left": 0.17834640522875816, "top": 0.6969078282828283, "width": 0.4203398692810458, "height": 0.011320707070707092, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "id": "5294"}, {"text": "P6 further imagined that Im just going to rewrite this and through my action the AI can learn.", "bboxes": [{"left": 0.196281045751634, "top": 0.7947171717171718, "width": 0.5606781045751634, "height": 0.011320707070706981, "page": 7}], "section": "5 RESULTS @@ 5.1 Acceptance Through Verification, Not Understanding", "id": "5295"}, {"text": "Participants also appreciated the feature of the alternate translation UI in which downstream edits could be made when selecting an alternate translation (e.g. because selecting an alternate may have renamed a variable or changed a loop index).", "bboxes": [{"left": 0.1362794117647059, "top": 0.6210934343434343, "width": 0.6837173202614379, "height": 0.011320707070707092, "page": 8}, {"left": 0.11945915032679738, "top": 0.6383901515151515, "width": 0.7005359477124182, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.6556868686868687, "width": 0.08280392156862747, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "id": "5296"}, {"text": "Participants responded positively to the confidence highlights (Figure 1B) and the alternate translations (Figure 1C) and felt that both views were helpful.", "bboxes": [{"left": 0.1200016339869281, "top": 0.1428800505050505, "width": 0.7008709150326797, "height": 0.011320707070707065, "page": 8}, {"left": 0.1200016339869281, "top": 0.16017676767676767, "width": 0.22180882352941178, "height": 0.011320707070707065, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "id": "5297"}, {"text": "We initially thought that the translation would produce only content in the form of the translated code.", "bboxes": [{"left": 0.5411143790849673, "top": 0.2564772727272727, "width": 0.27888725490196087, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.2737727272727273, "width": 0.34687254901960785, "height": 0.013969696969696965, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "id": "5298"}, {"text": "Software developers typically work with deterministic tools and a great deal of work goes into ensuring that these tools are highly reliable [19].", "bboxes": [{"left": 0.1200016339869281, "top": 0.7779318181818181, "width": 0.6999918300653596, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.7952272727272728, "width": 0.16895915032679737, "height": 0.011320707070706981, "page": 8}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "id": "5299"}, {"text": "Many participants were initially skeptical of the accuracy and value of the confidence highlights (P1, P2, P3, P5, P7, P9).", "bboxes": [{"left": 0.48274999999999996, "top": 0.45655555555555555, "width": 0.3376274509803922, "height": 0.011320707070707092, "page": 8}, {"left": 0.1200016339869281, "top": 0.4738510101010101, "width": 0.374781045751634, "height": 0.011320707070707037, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "id": "5300"}, {"text": "In addition, the alternate translations had additional explanatory power in helping participants understand the nature of the underlying code.", "bboxes": [{"left": 0.1362794117647059, "top": 0.525739898989899, "width": 0.6837173202614379, "height": 0.011320707070706981, "page": 8}, {"left": 0.1200016339869281, "top": 0.5430366161616161, "width": 0.18079738562091507, "height": 0.011320707070707092, "page": 8}], "section": "5 RESULTS @@ 5.2 Human-AI Patterns of Interaction", "id": "5301"}, {"text": "Thus, although many participants expressed a willingness to accept imperfectly-translated code as a starting point, acceptance may depend on how many errors the translated code contains as well as the nature of those errors (e.g. if they are easy to spot and fix, or if they take attention away from the central problem).", "bboxes": [{"left": 0.196281045751634, "top": 0.725564393939394, "width": 0.6853316993464054, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.7428598484848485, "width": 0.7000049019607842, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.7601565656565656, "width": 0.5177058823529412, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "id": "5302"}, {"text": "Participants also expressed mixed feelings about their tolerance for easy-to-spot or easy-to-fix errors.", "bboxes": [{"left": 0.196281045751634, "top": 0.4895366161616162, "width": 0.5857467320261438, "height": 0.011320707070707037, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "id": "5303"}, {"text": "For large codebases, the downside of imperfectly-translated code may be less than the upside of simply having automatically-produced translations.", "bboxes": [{"left": 0.196281045751634, "top": 0.5760176767676768, "width": 0.6837173202614379, "height": 0.011320707070707092, "page": 9}, {"left": 0.18000163398692812, "top": 0.5933143939393939, "width": 0.21612581699346403, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "id": "5304"}, {"text": "In some cases, although not an explicit part of the study, participants actually found the errors in the translation.", "bboxes": [{"left": 0.196281045751634, "top": 0.15585227272727273, "width": 0.6859738562091504, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "id": "5305"}, {"text": "Overall, many participants felt that even the erroneous output was desirable and that it would be something they would find useful to have.", "bboxes": [{"left": 0.196281045751634, "top": 0.2250366161616162, "width": 0.6840931372549021, "height": 0.011320707070707037, "page": 9}, {"left": 0.17945915032679738, "top": 0.24233333333333332, "width": 0.16245588235294117, "height": 0.011320707070707065, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "id": "5306"}, {"text": "Despite the positive reception of potentially error-prone code, there did seem to be a threshold regarding how many errors would be acceptable.", "bboxes": [{"left": 0.196281045751634, "top": 0.33999116161616166, "width": 0.6841045751633988, "height": 0.011320707070707037, "page": 9}, {"left": 0.18000163398692812, "top": 0.35728661616161617, "width": 0.16003431372549018, "height": 0.011320707070707092, "page": 9}], "section": "5 RESULTS @@ 5.3 Utility of Imperfect AI", "id": "5307"}, {"text": "Participants described the lifecycle of modernizing applications as consisting of three general phases: (1) creating an understanding of the legacy application and its architecture by reviewing code and documentation; (2) performing the migration work, which may include translating and/or refactoring the code; and (3) reviewing and testing the migrated code.", "bboxes": [{"left": 0.1200016339869281, "top": 0.1428800505050505, "width": 0.6999967320261439, "height": 0.011320707070707065, "page": 10}, {"left": 0.1200016339869281, "top": 0.16017676767676767, "width": 0.7, "height": 0.011320707070707065, "page": 10}, {"left": 0.1200016339869281, "top": 0.17747222222222223, "width": 0.7000032679738563, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.1947689393939394, "width": 0.03042647058823529, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "id": "5308"}, {"text": "Generating documentation from source is an active area of research in the machine learning community (e.g. [44, 48, 65, 91]) and our results highlight the importance of this functionality in user experiences, even when generated documentation may be imperfect.", "bboxes": [{"left": 0.1362794117647059, "top": 0.6726565656565657, "width": 0.685330065359477, "height": 0.011320707070707092, "page": 10}, {"left": 0.11966339869281045, "top": 0.6899532828282828, "width": 0.7003284313725489, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.7072487373737373, "width": 0.2015539215686275, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "id": "5309"}, {"text": "One helpful improvement to the code translation user interface would be to show a stronger visual correspondence between input and output source code.", "bboxes": [{"left": 0.3282320261437908, "top": 0.7344722222222222, "width": 0.49176633986928114, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.7517676767676768, "width": 0.4176830065359477, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "id": "5310"}, {"text": "Participants also described how generative methods could be used to fill in missing documentation to help others understand ones own code, to create an understanding of poorly-documented code, or to re-align documentation with code when drift has occurred [30, 85].", "bboxes": [{"left": 0.1362794117647059, "top": 0.4559722222222222, "width": 0.6837140522875816, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.4732689393939394, "width": 0.7000000000000001, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.49056439393939394, "width": 0.2220196078431373, "height": 0.011320707070707037, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "id": "5311"}, {"text": "As described in Section 5.0.1, one of the main challenges in migrating legacy applications is due to the fact that code and code architectures are undocumented, (P4) with specifications sometimes living in the gray matter storage (P7) of engineers that dont work here anymore (P4).", "bboxes": [{"left": 0.3043169934640523, "top": 0.23928787878787877, "width": 0.5156797385620916, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.25658333333333333, "width": 0.6999901960784314, "height": 0.011320707070707092, "page": 10}, {"left": 0.11834640522875817, "top": 0.27388005050505054, "width": 0.4432303921568628, "height": 0.011320707070707037, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "id": "5312"}, {"text": "Translation could also help software engineers better understand codebases in languages with which they are less familiar: Ive got a bunch of engineers that know Python, but they dont know Java. So, it might be easier to delve in and spelunk through... auto gen[erated] Python code than it would be to go back and spelunk the original Java. (P4).", "bboxes": [{"left": 0.1362794117647059, "top": 0.40408333333333335, "width": 0.6837156862745097, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.4213800505050505, "width": 0.6999983660130719, "height": 0.011320707070707092, "page": 10}, {"left": 0.1200016339869281, "top": 0.438675505050505, "width": 0.6986323529411764, "height": 0.011320707070707092, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "id": "5313"}, {"text": "P6: Its very tedious to write these comments... It really takes so much time... If the AI generated this for me I think that is gonna be really helpful.", "bboxes": [{"left": 0.15988398692810457, "top": 0.5496666666666666, "width": 0.6202254901960784, "height": 0.011320707070707092, "page": 10}, {"left": 0.15988398692810457, "top": 0.5669633838383838, "width": 0.21548692810457515, "height": 0.011320707070706981, "page": 10}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "id": "5314"}, {"text": "Generating a whole slew of automated unit tests (P4) as well as more realistic test values (P3) ensures that the right hand side code not only runs and doesnt crash but produces expected output (P4).", "bboxes": [{"left": 0.196281045751634, "top": 0.5964709595959595, "width": 0.6837173202614378, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.6137676767676767, "width": 0.5337369281045752, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "id": "5315"}, {"text": "Many participants described how code translation could be used to take advantage of functionality defined in third party libraries, because the same libraries in Java dont exist in Python, or similar libraries exist but theyre not... syntactically the same. (P0).", "bboxes": [{"left": 0.196281045751634, "top": 0.17314898989898989, "width": 0.6837238562091503, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.19044444444444444, "width": 0.7022483660130718, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.20774116161616163, "width": 0.16701960784313727, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "id": "5316"}, {"text": "Participants were eager to accept help from an imperfect AI assistant (e.g. as discussed by P2 & P3).", "bboxes": [{"left": 0.18000163398692812, "top": 0.7250416666666666, "width": 0.5942875816993464, "height": 0.011320707070707092, "page": 11}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "id": "5317"}, {"text": "Finally, P7 discussed how an AI partner might monitor his work and make proactive suggestions: It would be cool if the AI knew what I was trying to build, like a controller or view, and it would finish it for me. Like give me a shell of what Im trying to do so I can fill in the details.", "bboxes": [{"left": 0.196281045751634, "top": 0.398, "width": 0.6837124183006538, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.4152954545454545, "width": 0.7000000000000001, "height": 0.011320707070707092, "page": 11}, {"left": 0.17945915032679738, "top": 0.43259217171717174, "width": 0.2852254901960784, "height": 0.011320707070707092, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "id": "5318"}, {"text": "As discussed in Section 5.1.1, testing seems to be one of the primary ways that software engineers will accept and use AI-translated code: Im very much a supporter of test-driven development, which typically means you write tests that validate behavior. (P3).", "bboxes": [{"left": 0.3488790849673203, "top": 0.46170959595959593, "width": 0.5311241830065359, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.47900505050505054, "width": 0.7003807189542482, "height": 0.011320707070707092, "page": 11}, {"left": 0.18000163398692812, "top": 0.4963017676767677, "width": 0.29514215686274514, "height": 0.011320707070706981, "page": 11}], "section": "5 RESULTS @@ 5.4 Beyond our Design Scenario: Opportunities for Generative AI Across the Modernization Lifecycle", "id": "5319"}, {"text": "We note that our system only examined one kind of human-AI interaction pattern, in which the human provided code and the translator produced a translation.", "bboxes": [{"left": 0.1362794117647059, "top": 0.17314898989898989, "width": 0.6837254901960784, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.19044444444444444, "width": 0.25008986928104576, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.1 Human-AI Partnerships", "id": "5320"}, {"text": "While conventional software development tools are generally quite reliable, software engineers themselves are not.", "bboxes": [{"left": 0.11929738562091505, "top": 0.3807032828282828, "width": 0.7029509803921569, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.2 Toward Explainable Generative AI", "id": "5321"}, {"text": "Our results motivate the need for explainable generative AI.", "bboxes": [{"left": 0.1362794117647059, "top": 0.5709621212121212, "width": 0.35481535947712417, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.2 Toward Explainable Generative AI", "id": "5322"}, {"text": "Our study identified a number of areas that could be pursued for the use of generative AI in application modernization.", "bboxes": [{"left": 0.1200016339869281, "top": 0.7093320707070708, "width": 0.7022598039215686, "height": 0.011320707070706981, "page": 12}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "id": "5323"}, {"text": "Incorporating feedback to the generative model from the edits made by software engineers is another fruitful research opportunity.", "bboxes": [{"left": 0.1362794117647059, "top": 0.7958131313131313, "width": 0.6837254901960784, "height": 0.011320707070707092, "page": 12}, {"left": 0.1200016339869281, "top": 0.8131098484848485, "width": 0.07378758169934639, "height": 0.011320707070707092, "page": 12}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "id": "5324"}, {"text": "Participants identified many opportunities for generative models to aid the overall application modernization workflow of building understanding, performing the migration, and reviewing & testing.", "bboxes": [{"left": 0.196281045751634, "top": 0.19044444444444444, "width": 0.6837173202614379, "height": 0.011320707070707092, "page": 13}, {"left": 0.17945915032679738, "top": 0.20774116161616163, "width": 0.5505816993464052, "height": 0.011320707070707092, "page": 13}], "section": "6 DISCUSSION @@ 6.3 Future Directions for Generative AI in Application Modernization", "id": "5325"}, {"text": "Our study is a preliminary examination of how probabilistic generative models can be applied to a use case that requires an objective level of quality.", "bboxes": [{"left": 0.18000163398692812, "top": 0.3841628787878788, "width": 0.6999934640522876, "height": 0.011320707070707092, "page": 13}, {"left": 0.18000163398692812, "top": 0.4014583333333333, "width": 0.16816013071895422, "height": 0.011320707070707092, "page": 13}], "section": "7 LIMITATIONS", "id": "5326"}, {"text": "Generative AI techniques are enabling new forms of human-AI co-creation.", "bboxes": [{"left": 0.18000163398692812, "top": 0.5951767676767676, "width": 0.4724248366013072, "height": 0.011320707070707092, "page": 13}], "section": "8 CONCLUSION", "id": "5327"}], "uist-0": [{"text": "We are interested in designing camera interfaces that can encourage users to incorporate these exploratory stages of the design process into their photographic process.", "bboxes": [{"left": 0.5358137254901961, "top": 0.1649078282828283, "width": 0.3787549019607842, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.19257954545454545, "width": 0.2330882352941177, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5328"}, {"text": "Storytelling is an important aspect of photography.", "bboxes": [{"left": 0.08790522875816993, "top": 0.1268560606060606, "width": 0.30207843137254903, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5329"}, {"text": "Our fnal annotation design involves color-coded edge highlighting (very similar to the focus peaking feature that can be found on a number of commercial cameras [2, 32]) focused on regions around the subject(s) and image borders where clutter can be most distracting.", "bboxes": [{"left": 0.5358137254901961, "top": 0.3309520202020202, "width": 0.3787647058823528, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.3447891414141414, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.3586237373737373, "width": 0.39255555555555544, "height": 0.011323232323232402, "page": 1}, {"left": 0.5195343137254902, "top": 0.37246338383838384, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.38630050505050506, "width": 0.06763725490196082, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5330"}, {"text": "Amateurs frequently make the mistake of taking too few photos in the moment, relying on editing to improve their photos.", "bboxes": [{"left": 0.10418464052287582, "top": 0.29290025252525254, "width": 0.376281045751634, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790196078431373, "top": 0.3067348484848485, "width": 0.3358055555555556, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5331"}, {"text": "Designers have long known the benefts of quickly testing many ideas.", "bboxes": [{"left": 0.10418464052287582, "top": 0.5558030303030304, "width": 0.3766683006535947, "height": 0.011320707070706981, "page": 1}, {"left": 0.08790196078431373, "top": 0.5696376262626263, "width": 0.032895424836601295, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5332"}, {"text": "On the other hand, experienced photographers tend to capture many photos of a given scene: they know how to consider diferent options (e.g., composition, lighting, or pose), and recognize the challenges of not having the option to physically move the camera or elements in the image at edit-time.", "bboxes": [{"left": 0.10418464052287582, "top": 0.4451073232323232, "width": 0.3762761437908496, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.45894444444444443, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.47278156565656565, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.48661868686868687, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.500455808080808, "width": 0.2307794117647059, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5333"}, {"text": "One signifcant diference between sketching ideas and trying them out in the camera is this level of fdelity.", "bboxes": [{"left": 0.10418464052287582, "top": 0.7495214646464646, "width": 0.3762826797385621, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.763358585858586, "width": 0.26817810457516345, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5334"}, {"text": "In order to realize the proposed interaction, we additionally present a proposal of an algorithm for visually annotating potential clutter by highlighting relevant edges around salient objects and around the image border.", "bboxes": [{"left": 0.5195343137254902, "top": 0.6303383838383838, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6441527777777778, "width": 0.39283660130718945, "height": 0.011343434343434322, "page": 1}, {"left": 0.5195343137254902, "top": 0.6580126262626262, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.6718497474747475, "width": 0.10806045751633997, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5335"}, {"text": "Image Abstraction and Simplifcation.", "bboxes": [{"left": 0.5195343137254902, "top": 0.8290984848484848, "width": 0.2566111111111111, "height": 0.011321969696969858, "page": 1}], "section": "2 RELATED WORK", "id": "5336"}, {"text": "We contextualize our work of designing an abstraction-based camera overlay within the most relevant work in image manipulation and camera guidance.", "bboxes": [{"left": 0.5188316993464052, "top": 0.7739381313131313, "width": 0.3957385620915034, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.78777398989899, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.801611111111111, "width": 0.12963235294117648, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK", "id": "5337"}, {"text": "Specifcally in this paper, we contribute:", "bboxes": [{"left": 0.5195343137254902, "top": 0.48190151515151514, "width": 0.23960294117647052, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5338"}, {"text": "Most similar to our work is the focus peaking feature in commercial cameras and the work of E et al.", "bboxes": [{"left": 0.10418464052287582, "top": 0.6764747474747476, "width": 0.3787549019607843, "height": 0.011320707070707092, "page": 2}, {"left": 0.08791013071895425, "top": 0.6903093434343435, "width": 0.25865032679738564, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "id": "5339"}, {"text": "Capture-time Guidance.", "bboxes": [{"left": 0.08790522875816993, "top": 0.5104457070707071, "width": 0.15743954248366016, "height": 0.011321969696969636, "page": 2}], "section": "2 RELATED WORK", "id": "5340"}, {"text": "The goal of this line of work tends to be to serve one of two purposes, generating an artistic result, or reducing data for visual communication [22].", "bboxes": [{"left": 0.10418464052287582, "top": 0.1925820707070707, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.20641919191919192, "width": 0.3925555555555555, "height": 0.011320707070707037, "page": 2}, {"left": 0.08790522875816993, "top": 0.22025631313131314, "width": 0.12822222222222224, "height": 0.011320707070707037, "page": 2}], "section": "2 RELATED WORK", "id": "5341"}, {"text": "Image Declutering.", "bboxes": [{"left": 0.08790522875816993, "top": 0.31692929292929295, "width": 0.1261013071895425, "height": 0.011321969696969691, "page": 2}], "section": "2 RELATED WORK", "id": "5342"}, {"text": "In this section, we describe the steps we took to design our abstraction interface for decluttering images.", "bboxes": [{"left": 0.5195343137254902, "top": 0.2787487373737374, "width": 0.39502777777777776, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.2925858585858586, "width": 0.25750816993464054, "height": 0.011320707070707037, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS", "id": "5343"}, {"text": "To test out the concept of abstraction guidance, we started with a low-fdelity Wizard-of-Oz (WoZ) prototype [14] where experimenters manually drew abstraction overlays.", "bboxes": [{"left": 0.51909477124183, "top": 0.47561237373737375, "width": 0.39299673202614394, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195277777777778, "top": 0.4894469696969697, "width": 0.395047385620915, "height": 0.011323232323232346, "page": 2}, {"left": 0.5195310457516339, "top": 0.5032840909090909, "width": 0.27888398692810457, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "id": "5344"}, {"text": "E et al. begins to tackle this for overall photo composition by interactively highlighting the perceived composition.", "bboxes": [{"left": 0.5358137254901961, "top": 0.1372348484848485, "width": 0.3766617647058822, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.3230228758169934, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK", "id": "5345"}, {"text": "We informally tested our low-fdelity prototype with 19 participants (9 male, 10 female), 18 to 41 years old (  = 24).", "bboxes": [{"left": 0.5358137254901961, "top": 0.5586338383838384, "width": 0.3787581699346404, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.5698333333333333, "width": 0.3122205882352942, "height": 0.013969696969697076, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "id": "5346"}, {"text": "From our WoZ prototype studies, we saw promising signs that the participants in fact noticed high level", "bboxes": [{"left": 0.6788071895424836, "top": 0.8705959595959595, "width": 0.23328921568627448, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.3925686274509804, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "id": "5347"}, {"text": "Given the observations from our low-fdelity prototype, we were motivated to continue with this concept and move onto the step of answering the second question: What is an abstraction of a photo?", "bboxes": [{"left": 0.5195343137254902, "top": 0.4595542929292929, "width": 0.3925408496732027, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.4733914141414141, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.4872260101010101, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5010656565656566, "width": 0.04084313725490207, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5348"}, {"text": "Moving the camera instead of the objects in the scene was often more practical because the objects could not be moved.", "bboxes": [{"left": 0.10418464052287582, "top": 0.6492032828282828, "width": 0.37625980392156855, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.31932189542483663, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "id": "5349"}, {"text": "Participants tended to either adjust the camera position/angle (17) such that the clutter was no longer in frame, or move the clutter (3) out of the scene (or a mix of both).", "bboxes": [{"left": 0.10418464052287582, "top": 0.5661818181818182, "width": 0.37625816993464056, "height": 0.011320707070707092, "page": 3}, {"left": 0.0874656862745098, "top": 0.5800189393939394, "width": 0.3932614379084967, "height": 0.011320707070707092, "page": 3}, {"left": 0.0874656862745098, "top": 0.593854797979798, "width": 0.2356111111111111, "height": 0.011320707070706981, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "id": "5350"}, {"text": "Note that while these participants behaviors matched what we hoped for, there were limitations to our study design such that they cannot be directly mapped to how a user might respond to seeing this style of overlay interactively in the camera.", "bboxes": [{"left": 0.5358137254901961, "top": 0.31581944444444443, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.3296565656565657, "width": 0.3929183006535949, "height": 0.011320707070707037, "page": 3}, {"left": 0.5195343137254902, "top": 0.3434936868686869, "width": 0.3925392156862745, "height": 0.011320707070707037, "page": 3}, {"left": 0.5195343137254902, "top": 0.357330808080808, "width": 0.2899150326797386, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.1 Wizard-of-Oz Prototype", "id": "5351"}, {"text": "Photography books describe that contrast is key to directing attention [20].", "bboxes": [{"left": 0.5358137254901961, "top": 0.6353661616161616, "width": 0.37625816993464045, "height": 0.011320707070707203, "page": 3}, {"left": 0.5195408496732026, "top": 0.6492007575757576, "width": 0.08829248366013087, "height": 0.01132323232323229, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5352"}, {"text": "On the other hand, contrast in other regions especially the border of the image, will distract, causing the eye to be attracted away from the focal subject.", "bboxes": [{"left": 0.5358137254901961, "top": 0.8014103535353535, "width": 0.37655065359477113, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.8152474747474748, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.8290820707070707, "width": 0.1380457516339869, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5353"}, {"text": "To answer our questions on how to design an abstraction overlay for decluttering, we looked to existing literature to better understand how photographers think about directing the viewers attention for efective storytelling [9, 20, 30].", "bboxes": [{"left": 0.7007532679738562, "top": 0.5800189393939394, "width": 0.21133823529411766, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.593854797979798, "width": 0.3925441176470589, "height": 0.011320707070706981, "page": 3}, {"left": 0.5195343137254902, "top": 0.6076919191919191, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.6215290404040403, "width": 0.3946633986928104, "height": 0.011320707070707092, "page": 3}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5354"}, {"text": "Given these principles, we wondered what annotation methods photographers currently used for highlighting clutter.", "bboxes": [{"left": 0.10418464052287582, "top": 0.41673611111111114, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.43057323232323236, "width": 0.3299183006535948, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5355"}, {"text": "We additionally were inspired by a line of research in nonphotorealistic rendering to generate stylized image abstractions [15, 22, 36].", "bboxes": [{"left": 0.10418464052287582, "top": 0.8152474747474748, "width": 0.37873856209150325, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.394171568627451, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.04186274509803921, "height": 0.011320707070706981, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5356"}, {"text": "Inspired by this idea of using outlines to highlight contrast and the lack of contrast, we hoped to recreate this outlining as an overlay directly in the camera (see Figure 5), while also extending it to contrast along the image borders.", "bboxes": [{"left": 0.33908660130718954, "top": 0.6492032828282828, "width": 0.1413643790849673, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6630378787878788, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.6768775252525252, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 4}, {"left": 0.0874656862745098, "top": 0.6907146464646465, "width": 0.39298529411764704, "height": 0.011320707070706981, "page": 4}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.0474656862745098, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5357"}, {"text": "Combining concepts from these two abstraction ideas, we have three components in total to consider: line drawing, location context, and color fattening.", "bboxes": [{"left": 0.5358137254901961, "top": 0.6698838383838384, "width": 0.37625980392156844, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6837209595959596, "width": 0.3950245098039217, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6975580808080808, "width": 0.14877450980392148, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5358"}, {"text": "Figure 5 shows two examples of images that do not satisfy the decluttering principles, along with approximations of Glovers suggested outlining to emphasize contrast around the subject [20].", "bboxes": [{"left": 0.10418464052287582, "top": 0.5412689393939394, "width": 0.37625816993464056, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5551060606060606, "width": 0.3950163398692811, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5689431818181818, "width": 0.3700343137254902, "height": 0.011320707070707092, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5359"}, {"text": "Adjusting these parameters, we designed and implemented a range of potential overlay proposals.", "bboxes": [{"left": 0.5358137254901961, "top": 0.7667424242424242, "width": 0.37625816993464045, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7805795454545456, "width": 0.21088071895424831, "height": 0.011320707070706981, "page": 4}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5360"}, {"text": "The survey consisted of 12 pieces of visual media (8 photos, 4 videos) shown with the set of diferent abstractions overlaid.", "bboxes": [{"left": 0.5358137254901961, "top": 0.6929760101010101, "width": 0.37787091503267967, "height": 0.011320707070707092, "page": 5}, {"left": 0.5191960784313725, "top": 0.7068106060606061, "width": 0.39513071895424845, "height": 0.011320707070707092, "page": 5}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5361"}, {"text": "We ran an informal design survey through Qualtrics with 29 participants (demographics information not collected) to try to understand if these overlay visualizations were interpretable by novice photographers, and if there were strong preferences between the overlay options.", "bboxes": [{"left": 0.2217156862745098, "top": 0.8152474747474748, "width": 0.25874183006535956, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.39501307189542484, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.3925457516339869, "height": 0.011320707070706981, "page": 5}, {"left": 0.08790522875816993, "top": 0.8567563131313132, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8705959595959595, "width": 0.24487581699346406, "height": 0.011320707070707092, "page": 5}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5362"}, {"text": "Given the elements supported by the qualitative feedback, (f) appeared to be the best candidate for our overlayit included the color-coded line drawings to help defne the subject and possible distractors along the borders, while being less noisy than showing all outlines.", "bboxes": [{"left": 0.5358137254901961, "top": 0.6076919191919191, "width": 0.37715522875816987, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6215290404040403, "width": 0.39255555555555555, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6353661616161616, "width": 0.39255882352941185, "height": 0.011320707070707203, "page": 6}, {"left": 0.5195343137254902, "top": 0.6492032828282828, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.0717908496732026, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5363"}, {"text": "Further discussing the latter observation that (a)(c) were more helpful for SBS, several (8) participants specifcally noted that the color fattening was helpful for noticing SBS.", "bboxes": [{"left": 0.5358137254901961, "top": 0.4139747474747475, "width": 0.3762777777777778, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.42781186868686866, "width": 0.39256209150326804, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.4416489898989899, "width": 0.2679607843137255, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5364"}, {"text": "Many (10) participants also mentioned that the darkening of the image was particularly helpful for seeing the lines due to the contrast, but with the caveat that it made the original image harder to see.", "bboxes": [{"left": 0.5358137254901961, "top": 0.317114898989899, "width": 0.3762745098039215, "height": 0.011320707070707037, "page": 6}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.39255555555555544, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.3447891414141414, "width": 0.39283496732026146, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.3586262626262626, "width": 0.03891339869281052, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5365"}, {"text": "Overall, almost all (23) participants expressed interest in some form of line drawing.", "bboxes": [{"left": 0.5358137254901961, "top": 0.17874494949494948, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.12901307189542477, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5366"}, {"text": "Both of these conclusions were also supported by the qualitative feedback from participants general impressions.", "bboxes": [{"left": 0.10418464052287582, "top": 0.8014103535353535, "width": 0.376281045751634, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.28612418300653597, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5367"}, {"text": "Results of both steps of our design prototyping process made us hopeful of the potential of an abstraction-based overlay.", "bboxes": [{"left": 0.5195343137254902, "top": 0.7875732323232324, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8014103535353535, "width": 0.3338431372549021, "height": 0.011320707070707092, "page": 6}], "section": "4 IMPLEMENTATION", "id": "5368"}, {"text": "Participants were presented with some training describing the two decluttering principles (see Section 3.2.1).", "bboxes": [{"left": 0.10418464052287582, "top": 0.3420353535353535, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.35587247474747474, "width": 0.28457843137254907, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5369"}, {"text": "Finally at the end of the survey, we asked participants to summarize their choices: Please provide a brief explanation for your choices why did you fnd these overlays most helpful? Are there specifc characteristics of the overlays that you like (e.g., line drawing, color fattening, image darkening, or color)?", "bboxes": [{"left": 0.08790522875816993, "top": 0.5576111111111111, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5714457070707071, "width": 0.39418464052287583, "height": 0.011320707070707092, "page": 6}, {"left": 0.08736437908496732, "top": 0.5852853535353535, "width": 0.3931013071895425, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5991224747474747, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.6129595959595959, "width": 0.24230065359477126, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5370"}, {"text": "Table 1 presents the results from our design survey.", "bboxes": [{"left": 0.2654705882352941, "top": 0.6907146464646465, "width": 0.21526960784313726, "height": 0.011320707070706981, "page": 6}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.0867434640522876, "height": 0.011320707070707092, "page": 6}], "section": "3 ABSTRACTION DESIGN PROCESS @@ 3.2 Visualization Designs", "id": "5371"}, {"text": "We choose the most abstracted form of the overlay (solid black, minimal edges) as the default as we imagine users starting in a more", "bboxes": [{"left": 0.10418464052287582, "top": 0.8705959595959595, "width": 0.37788071895424835, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8844318181818183, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "id": "5372"}, {"text": "As a reminder, the lines are color-coded such that lines within and immediately around the subject are yellow, lines along the image border are cyan, and remaining lines in the background are white.", "bboxes": [{"left": 0.10418464052287582, "top": 0.593854797979798, "width": 0.37625816993464056, "height": 0.011320707070706981, "page": 7}, {"left": 0.08790522875816993, "top": 0.6076919191919191, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6215290404040403, "width": 0.392547385620915, "height": 0.011320707070707092, "page": 7}, {"left": 0.08736437908496732, "top": 0.6353661616161616, "width": 0.036310457516339864, "height": 0.011320707070707203, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "id": "5373"}, {"text": "Figure 8 walks through our algorithm for generating our abstraction overlay.", "bboxes": [{"left": 0.5195343137254902, "top": 0.37707575757575756, "width": 0.3950196078431373, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.3909128787878788, "width": 0.07646568627450978, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "id": "5374"}, {"text": "From the default state, the user can adjust the slider at the top to adjust the opacity of the black layer, bringing in more or less of the image color.", "bboxes": [{"left": 0.10418464052287582, "top": 0.7875732323232324, "width": 0.37626797385620914, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8014103535353535, "width": 0.392547385620915, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.09441339869281044, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "id": "5375"}, {"text": "To implement this, our fnal camera tool has 3 layers: the camera view, a black layer of varying degrees of opacity, and a color-coded outlines layer.", "bboxes": [{"left": 0.10418464052287582, "top": 0.5108333333333334, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 7}, {"left": 0.08754411764705881, "top": 0.5246679292929293, "width": 0.3929035947712418, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.5385075757575758, "width": 0.08537745098039216, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "id": "5376"}, {"text": "Given this saliency map, we segment the image into regions describing the subject, subject border, image border, and remaining background.", "bboxes": [{"left": 0.5358137254901961, "top": 0.5292828282828282, "width": 0.37625816993464045, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195408496732026, "top": 0.5431174242424243, "width": 0.3950114379084968, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.5569570707070707, "width": 0.09839542483660124, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.3 Context-Aware Line Drawing Algorithm", "id": "5377"}, {"text": "We wanted to study how users would react to our abstraction guidance tool.", "bboxes": [{"left": 0.5188316993464052, "top": 0.8290845959595959, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8429217171717173, "width": 0.08177941176470593, "height": 0.011320707070706981, "page": 7}], "section": "5 USER EVALUATION", "id": "5378"}, {"text": "Our overlay tool is build on top of a basic iOS camera app.", "bboxes": [{"left": 0.5195343137254902, "top": 0.18796969696969698, "width": 0.3393496732026142, "height": 0.011320707070707037, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.2 Mobile Implementation", "id": "5379"}, {"text": "Based on our learnings from the design survey (Section 3.2.3), we chose to go with a design inspired by a combination of overlay options (d) and (f).", "bboxes": [{"left": 0.08790522875816993, "top": 0.45548484848484855, "width": 0.39253758169934644, "height": 0.011320707070707037, "page": 7}, {"left": 0.08790522875816993, "top": 0.46932196969696965, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.48315909090909087, "width": 0.11031209150326797, "height": 0.011320707070707092, "page": 7}], "section": "4 IMPLEMENTATION @@ 4.1 Interaction", "id": "5380"}, {"text": "After each condition, participants were asked to complete surveys with a number of Likert questions (on a 7-point scale) about their experience using the tool along with the Creativity Support Index (CSI) questions (0 to 100) [13].", "bboxes": [{"left": 0.5358137254901961, "top": 0.7304381313131313, "width": 0.37874183006535944, "height": 0.011320707070707092, "page": 8}, {"left": 0.5191683006535948, "top": 0.7442752525252525, "width": 0.3929101307189542, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.7581123737373737, "width": 0.39254738562091496, "height": 0.011320707070707092, "page": 8}, {"left": 0.5195343137254902, "top": 0.7719494949494949, "width": 0.22514542483660138, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "id": "5381"}, {"text": "We ran these user studies over Zoom, asking the participant to adjust the webcam when possible to keep their photographing within view.", "bboxes": [{"left": 0.10418464052287582, "top": 0.75989898989899, "width": 0.37625816993464056, "height": 0.011320707070707092, "page": 8}, {"left": 0.08790522875816993, "top": 0.7737361111111111, "width": 0.392547385620915, "height": 0.011320707070707092, "page": 8}, {"left": 0.08736437908496732, "top": 0.7875732323232324, "width": 0.07515359477124182, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "id": "5382"}, {"text": "Due to the COVID-19 pandemic, we had to run our studies remotely.", "bboxes": [{"left": 0.08790522875816993, "top": 0.5385075757575758, "width": 0.3947990196078432, "height": 0.011320707070707092, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "id": "5383"}, {"text": "Participants were provided with a document describing the two decluttering principles (see Section 3.2.1 as training for how to think about decluttering their photo compositions).", "bboxes": [{"left": 0.5358137254901961, "top": 0.49520959595959596, "width": 0.37626470588235283, "height": 0.011320707070707037, "page": 8}, {"left": 0.5195343137254902, "top": 0.5090467171717172, "width": 0.3929183006535948, "height": 0.011320707070706981, "page": 8}, {"left": 0.5195343137254902, "top": 0.5228838383838385, "width": 0.27107516339869286, "height": 0.011320707070706981, "page": 8}], "section": "5 USER EVALUATION @@ 5.1 Study Procedure", "id": "5384"}, {"text": "When evaluating their favorited photos per task, participants did believe that the photos captured using our tool had better subjectbackground separation (Mdn = 6, IQR = 4-5), than those captured using the no guidance baseline (Mdn = 4, IQR = 2-5) [ V = 0, p = . 003].", "bboxes": [{"left": 0.10418464052287582, "top": 0.5338383838383839, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5476755050505051, "width": 0.39501470588235305, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5615126262626263, "width": 0.39254084967320263, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5727121212121212, "width": 0.3925571895424837, "height": 0.013969696969696965, "page": 9}, {"left": 0.0881421568627451, "top": 0.5888888888888889, "width": 0.03333496732026142, "height": 0.011618686868686834, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "id": "5385"}, {"text": "Therefore we see that while participants are more confdent in their ability to take clear photos, and found the tool helpful for achieving their favorite resulting photos, they didnt necessarily fnd that their photos were better with regard to their personal preferences or the decluttering principles.", "bboxes": [{"left": 0.5358137254901961, "top": 0.275604797979798, "width": 0.37625980392156855, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.2894419191919192, "width": 0.3928153594771241, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.30327651515151516, "width": 0.3929199346405229, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3171098484848485, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3309520202020202, "width": 0.2501111111111112, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "id": "5386"}, {"text": "Thus, even though we did not fnd signifcant changes in CSI, we decided that we should compare against a baseline that provided a little more assistance.", "bboxes": [{"left": 0.10418464052287582, "top": 0.6583712121212121, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6722083333333333, "width": 0.39253758169934644, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.6860454545454545, "width": 0.1452941176470588, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "id": "5387"}, {"text": "Again we saw that the tool made the participants more confdent in their ability to address the decluttering principles of subject-background separation and image border ficker (Mdn = 6, IQR = 5-7), versus no guidance (Mdn = 5, IQR = 4-6) [ V = 8, p = . 03].", "bboxes": [{"left": 0.7150800653594771, "top": 0.10956060606060607, "width": 0.19701143790849684, "height": 0.011320707070707065, "page": 9}, {"left": 0.5195343137254902, "top": 0.12339772727272727, "width": 0.3950114379084967, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1372348484848485, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.1510719696969697, "width": 0.3941535947712419, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.162270202020202, "width": 0.1635751633986927, "height": 0.013969696969696965, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "id": "5388"}, {"text": "We found that overall, this study design worked reasonably well in the Zoom environment.", "bboxes": [{"left": 0.24475000000000002, "top": 0.4231426767676768, "width": 0.23571078431372544, "height": 0.011320707070707092, "page": 9}, {"left": 0.08736437908496732, "top": 0.436979797979798, "width": 0.301578431372549, "height": 0.011320707070706981, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "id": "5389"}, {"text": "Another participant described that the external representation provided by the interface assisted in the process of exploring the scene and quickly evaluating diferent options: It caused me to experiment more... didnt see it as a rule that I needed to minimize lines, but the tool made it easy to move around and check by that metric, how good it was (P9).", "bboxes": [{"left": 0.5358137254901961, "top": 0.75989898989899, "width": 0.37626960784313734, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7737361111111111, "width": 0.3925375816993464, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.7875732323232324, "width": 0.39256045751633983, "height": 0.011343434343434211, "page": 9}, {"left": 0.5195343137254902, "top": 0.8014330808080808, "width": 0.3925408496732028, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.815270202020202, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.8290845959595959, "width": 0.1808725490196078, "height": 0.011343434343434433, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "id": "5390"}, {"text": "For example in Figure 10, this participant (P7) refnes the camera angle until the edges in the overlay look the way she wants.", "bboxes": [{"left": 0.5358137254901961, "top": 0.5387083333333333, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.5525454545454546, "width": 0.3544297385620915, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "id": "5391"}, {"text": "Figure 11 shows how an unexpected edge highlight encourages the participant to explore diferent backgrounds and compositions.", "bboxes": [{"left": 0.5358137254901961, "top": 0.8429217171717173, "width": 0.37626307189542485, "height": 0.011320707070706981, "page": 9}, {"left": 0.5195343137254902, "top": 0.8567588383838384, "width": 0.3948022875816992, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "id": "5392"}, {"text": "We also did not fnd signifcant diferences in CSI, but did fnd support for the increased confdence and descriptions of how the tool encouraged participants to explore more in the qualitative feedback.", "bboxes": [{"left": 0.7312189542483659, "top": 0.3590290404040404, "width": 0.18085947712418315, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.37286363636363634, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.3866969696969697, "width": 0.3925441176470589, "height": 0.011320707070707092, "page": 9}, {"left": 0.5195343137254902, "top": 0.40054040404040403, "width": 0.19635457516339871, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "id": "5393"}, {"text": "We ran remote studies over Zoom with 18 participants (6 male, 10 female), 24 to 32 years old (  = 29), to understand if the tool would help users declutter photos, and if users felt creative while using the tool.", "bboxes": [{"left": 0.08720261437908497, "top": 0.8152474747474748, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8264469696969697, "width": 0.39254901960784316, "height": 0.013969696969696965, "page": 9}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.392545751633987, "height": 0.011320707070706981, "page": 9}, {"left": 0.08790522875816993, "top": 0.8567588383838384, "width": 0.04986111111111112, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "id": "5394"}, {"text": "We ran a pilot study (n = 5) to test this study design in a remote setting.", "bboxes": [{"left": 0.08720261437908497, "top": 0.3587222222222222, "width": 0.3932483660130719, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.3725568181818182, "width": 0.04485947712418299, "height": 0.011320707070707092, "page": 9}], "section": "5 USER EVALUATION @@ 5.2 Pilot Study", "id": "5395"}, {"text": "One participant (P1) observed that the tool jumps back and forth in its highlighting of a subject, She interprets this to mean that there is no clear subject in her photo, and confrms that this is consistent with her own perception.", "bboxes": [{"left": 0.10418464052287582, "top": 0.5979570707070707, "width": 0.37626307189542485, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790359477124182, "top": 0.6117916666666666, "width": 0.3925441176470588, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790359477124182, "top": 0.625625, "width": 0.3925441176470588, "height": 0.011320707070707092, "page": 10}, {"left": 0.0873578431372549, "top": 0.6394583333333334, "width": 0.1477516339869281, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "id": "5396"}, {"text": "On the other hand, another participant (P18) had a slightly different interpretation of the lack of a consistent subject or in this case, no identifed subject.", "bboxes": [{"left": 0.10418464052287582, "top": 0.6671426767676767, "width": 0.37873529411764717, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.680979797979798, "width": 0.3925457516339869, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.6948169191919191, "width": 0.16211928104575163, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "id": "5397"}, {"text": "For either of these interpretations, the tool has given the participant another perspective from which to consider what it means to clearly capture a subject.", "bboxes": [{"left": 0.10418464052287582, "top": 0.7363270202020202, "width": 0.37873366013071896, "height": 0.011320707070706981, "page": 10}, {"left": 0.08790522875816993, "top": 0.7501641414141414, "width": 0.392545751633987, "height": 0.011320707070707092, "page": 10}, {"left": 0.08790522875816993, "top": 0.7639987373737374, "width": 0.16725980392156864, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "id": "5398"}, {"text": "Additionally, we found in our low-fdelity prototype that the abstraction also encouraged participants to be more aware of composition.", "bboxes": [{"left": 0.5190212418300654, "top": 0.7460618686868686, "width": 0.395531045751634, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195375816993464, "top": 0.7598964646464647, "width": 0.394799019607843, "height": 0.011320707070707092, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.2 Abstraction for Composition?", "id": "5399"}, {"text": "Our design survey (Section 3.2.3) provided us with a lot of interesting insight on how participants might imagine an abstraction overlay to assist with decluttering.", "bboxes": [{"left": 0.5195343137254902, "top": 0.5150921717171717, "width": 0.3950114379084968, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.5289267676767677, "width": 0.39254575163398686, "height": 0.011320707070706981, "page": 10}, {"left": 0.5195343137254902, "top": 0.542760101010101, "width": 0.21325653594771243, "height": 0.011320707070707092, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.1 Other Abstraction Designs", "id": "5400"}, {"text": "In fact, we would be interested in comparing how such an interface might compare to other grid-based composition guidance [17], as well as understanding how these diferent camera interfaces", "bboxes": [{"left": 0.5358137254901961, "top": 0.8567588383838384, "width": 0.37874509803921563, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.8705959595959595, "width": 0.39417156862745106, "height": 0.011320707070707092, "page": 10}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.39254575163398686, "height": 0.011320707070707092, "page": 10}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.2 Abstraction for Composition?", "id": "5401"}, {"text": "Subject Identifcation.", "bboxes": [{"left": 0.08790522875816993, "top": 0.5426237373737374, "width": 0.1436323529411765, "height": 0.011320707070707092, "page": 10}], "section": "5 USER EVALUATION @@ 5.3 Summative Evaluation", "id": "5402"}, {"text": "It additionally could be interesting to experiment more thoroughly with diferent types of edge detection to see which would best match what humans actually perceive as noisee.g., in our user studies, we often found that textures like carpet ended up appearing as a lot of noise.", "bboxes": [{"left": 0.10418464052287582, "top": 0.43472979797979794, "width": 0.37875490196078443, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.44856691919191916, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4624040404040404, "width": 0.3928349673202615, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4762411616161616, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.4900782828282828, "width": 0.15979901960784315, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "id": "5403"}, {"text": "If we remove the requirement of our camera guidance running interactively, there are further approaches that can be considered both for identifying objects in the scene for location context and for generating line drawings.", "bboxes": [{"left": 0.10418464052287582, "top": 0.503915404040404, "width": 0.3762761437908496, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.5177525252525252, "width": 0.39256699346405227, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.5315883838383838, "width": 0.392562091503268, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.545425505050505, "width": 0.17374673202614382, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "id": "5404"}, {"text": "We would like to thank Grifn Dietz for her assistance in transitioning to virtual user studies, Mitchell Gordon for his feedback and insightful discussions, and Matthew Cong and Charlene Seto Kung for their help with fgure photos.", "bboxes": [{"left": 0.5188316993464052, "top": 0.30579419191919194, "width": 0.3957303921568628, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.31962878787878785, "width": 0.39293627450980395, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.3334684343434344, "width": 0.39255228758169936, "height": 0.011320707070707037, "page": 11}, {"left": 0.5195343137254902, "top": 0.34730555555555553, "width": 0.2267565359477124, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ ACKNOWLEDGMENTS", "id": "5405"}, {"text": "Much of the efort photographers put into designing a photo is to help more clearly communicate their intended story.", "bboxes": [{"left": 0.5195343137254902, "top": 0.4466805555555556, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 11}, {"left": 0.5195343137254902, "top": 0.46051641414141414, "width": 0.3307467320261438, "height": 0.011320707070707092, "page": 11}], "section": "7 CONCLUSION", "id": "5406"}, {"text": "In this paper, we have described a type of annotation-based guidance.", "bboxes": [{"left": 0.08790522875816993, "top": 0.7322260101010101, "width": 0.3950359477124183, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.7460606060606061, "width": 0.03092973856209151, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.4 More Directed Guidance", "id": "5407"}, {"text": "While there isnt necessarily a clear defnition of failure, the main failure case with our current algorithm is when the main subject is not properly captured in the saliency map.", "bboxes": [{"left": 0.10418464052287582, "top": 0.28252272727272726, "width": 0.3762745098039215, "height": 0.011320707070707037, "page": 11}, {"left": 0.08791013071895425, "top": 0.2963573232323232, "width": 0.3925637254901961, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.3101969696969697, "width": 0.30394444444444446, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "id": "5408"}, {"text": "In designing our overlays, we were somewhat limited in the methods that we used in order to produce something that could be computed interactively.", "bboxes": [{"left": 0.08790522875816993, "top": 0.1856641414141414, "width": 0.395031045751634, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.1995012626262626, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 11}, {"left": 0.08790522875816993, "top": 0.21333838383838383, "width": 0.11609640522875818, "height": 0.011320707070707092, "page": 11}], "section": "6 LIMITATIONS AND FUTURE WORK @@ 6.3 Algorithmic Implementation", "id": "5409"}, {"text": "Through our design process, we aimed to answer two questions: (1) Will an abstracted visualization be efective in encouraging the user to see parts of the image outside of the main subject?", "bboxes": [{"left": 0.5358137254901961, "top": 0.3617714646464647, "width": 0.37801797385620906, "height": 0.011320707070706981, "page": 2}, {"left": 0.51909477124183, "top": 0.3756085858585859, "width": 0.3930032679738562, "height": 0.011320707070706981, "page": 2}, {"left": 0.5195343137254902, "top": 0.389445707070707, "width": 0.3600441176470588, "height": 0.011320707070707092, "page": 2}], "section": "3 ABSTRACTION DESIGN PROCESS", "id": "5410"}], "2102.09039": [{"text": "To battle the issue, previous work has attempted to apply Artificial Intelligence algorithms to enable efficient search of a large design space [5, 13, 16, 19, 22].", "bboxes": [{"left": 0.5358137254901961, "top": 0.39347853535353533, "width": 0.3787614379084967, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.40731565656565655, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.42115277777777776, "width": 0.18356045751633987, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "id": "5411"}, {"text": "To address challenge 1), We designed the HTML annotation as a simple extension of the existing HTML and CSS grammar, where instead of specifying a single value for an attribute, a designer can provide multiple candidate values for it, which are to be explored by Spacewalker.", "bboxes": [{"left": 0.5358137254901961, "top": 0.7255656565656566, "width": 0.3762794117647059, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7394027777777777, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7532398989898991, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7670770202020202, "width": 0.39255392156862734, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.7809141414141415, "width": 0.09825653594771244, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "id": "5412"}, {"text": "To ease the effort for exploring a design space, previous work has extensively investigated using crowdsourcing as an essential component in UI design and evaluation [3, 911, 14, 15, 17, 25, 26], which lowers the threshold for acquiring user feedback at scale.", "bboxes": [{"left": 0.5358137254901961, "top": 0.2827828282828283, "width": 0.3766633986928105, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.29661868686868686, "width": 0.39255718954248375, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.3104558080808081, "width": 0.39417156862745095, "height": 0.011320707070707092, "page": 0}, {"left": 0.5189918300653594, "top": 0.32429292929292924, "width": 0.39535784313725497, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "id": "5413"}, {"text": "In this paper, we present Spacewalker, a tool that allows designers to rapidly search a design space of a web UI for an optimal design within that space (see Figure 1).", "bboxes": [{"left": 0.5358137254901961, "top": 0.545685606060606, "width": 0.3787565359477124, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5595227272727272, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 0}, {"left": 0.5195343137254902, "top": 0.5733598484848484, "width": 0.24064379084967324, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "id": "5414"}, {"text": "User interface design is a complex task that often requires designers to explore a wide range of options, which is expensive and time consuming.", "bboxes": [{"left": 0.08790522875816993, "top": 0.6707891414141414, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.6846262626262627, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 0}, {"left": 0.08790522875816993, "top": 0.6984633838383838, "width": 0.0705065359477124, "height": 0.011320707070707092, "page": 0}], "section": "1 INTRODUCTION", "id": "5415"}, {"text": "Although existing methods are widely adopted, they often require substantial engineering effort to build and instrument a test.", "bboxes": [{"left": 0.5358137254901961, "top": 0.5736401515151515, "width": 0.3787549019607843, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.5874772727272727, "width": 0.3948153594771242, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "id": "5416"}, {"text": "Usability testing [2] is a commonly used approach for evaluating a UI design, which often requires a user experience researcher to", "bboxes": [{"left": 0.08790522875816993, "top": 0.8340012626262626, "width": 0.39256699346405227, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.847838383838384, "width": 0.39255718954248364, "height": 0.011320707070706981, "page": 1}], "section": "2 RELATED WORK @@ 2.1 Traditional UI Evaluation Methods", "id": "5417"}, {"text": "We evaluated Spacewalker by asking interaction designers to use it for exploring a set of UI design tasks, and Spacewalker received positive feedback.", "bboxes": [{"left": 0.10418464052287582, "top": 0.46294444444444444, "width": 0.3762794117647058, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.47678156565656565, "width": 0.39255882352941174, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.49061742424242427, "width": 0.10406699346405228, "height": 0.011320707070707092, "page": 1}], "section": "1 INTRODUCTION", "id": "5418"}, {"text": "Previous work has incorporated crowdsourcing for UI design and evaluation.", "bboxes": [{"left": 0.5195343137254902, "top": 0.798324494949495, "width": 0.39256209150326793, "height": 0.011320707070707092, "page": 1}, {"left": 0.5195343137254902, "top": 0.8121616161616161, "width": 0.06723366013071896, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK @@ 2.2 Crowd-Powered UI Design & Evaluation", "id": "5419"}, {"text": "Our work is related to three areas of the literature, including UI evaluation methods, crowdsourcing-based design support, and interactive UI design optimization.", "bboxes": [{"left": 0.08790522875816993, "top": 0.761003787878788, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.7748409090909091, "width": 0.39502941176470585, "height": 0.011320707070707092, "page": 1}, {"left": 0.08790522875816993, "top": 0.7886780303030303, "width": 0.19503431372549018, "height": 0.011320707070707092, "page": 1}], "section": "2 RELATED WORK", "id": "5420"}, {"text": "This instructs Spacewalker to explore three different alternatives for the page background: image \" bg1.jpg \", image \" bg2.jpg \", and a solid background with a dark gray color \" #333 \".", "bboxes": [{"left": 0.51909477124183, "top": 0.23409343434343433, "width": 0.3930049019607845, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.24726262626262627, "width": 0.39255392156862745, "height": 0.011988636363636368, "page": 2}, {"left": 0.5195343137254902, "top": 0.26109974747474746, "width": 0.28547549019607843, "height": 0.011988636363636396, "page": 2}], "section": "3 USING SPACEWALKER", "id": "5421"}, {"text": "The crowd can be more actively involved in UI design tasks to provide feedback [14, 17, 25, 26] or participate in the design process [10, 11, 15].", "bboxes": [{"left": 0.10418464052287582, "top": 0.20641919191919192, "width": 0.3762745098039215, "height": 0.011320707070707037, "page": 2}, {"left": 0.08790522875816993, "top": 0.22025631313131314, "width": 0.3925653594771242, "height": 0.011320707070707037, "page": 2}, {"left": 0.08790522875816993, "top": 0.23409343434343433, "width": 0.07009313725490195, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.2 Crowd-Powered UI Design & Evaluation", "id": "5422"}, {"text": "To address the issue, Salem [19] combined crowdsourcing and genetic programming [12] for the design of landing pages.", "bboxes": [{"left": 0.10418464052287582, "top": 0.5246704545454546, "width": 0.3762859477124183, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.5385075757575758, "width": 0.3408872549019608, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Interactive UI Design Optimization", "id": "5423"}, {"text": "After creating the specifications for all the design aspects in questioning, Alex launches a Spacewalker task by specifying 50", "bboxes": [{"left": 0.5358137254901961, "top": 0.8705959595959595, "width": 0.3762745098039214, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8844318181818183, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "id": "5424"}, {"text": "This ensures that Spacewalker would globally apply these options: either color1 for titles and color2 for body text, or color3 for titles and color4 for body text.", "bboxes": [{"left": 0.51909477124183, "top": 0.8152474747474748, "width": 0.39473202614379077, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8290845959595959, "width": 0.39255555555555544, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.8429217171717173, "width": 0.14893464052287586, "height": 0.011320707070706981, "page": 2}], "section": "3 USING SPACEWALKER", "id": "5425"}, {"text": "We here describe how UI designers or developers would use Spacewalker to explore the design space of their user interfaces.", "bboxes": [{"left": 0.08720261437908497, "top": 0.8014103535353535, "width": 0.3957352941176471, "height": 0.011320707070707092, "page": 2}, {"left": 0.08736437908496732, "top": 0.8152474747474748, "width": 0.341718954248366, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "id": "5426"}, {"text": "Using Artificial Intelligence algorithms to optimize interface design is a longstanding topic.", "bboxes": [{"left": 0.08790522875816993, "top": 0.4139747474747475, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 2}, {"left": 0.08790522875816993, "top": 0.42781186868686866, "width": 0.13661111111111113, "height": 0.011320707070707092, "page": 2}], "section": "2 RELATED WORK @@ 2.3 Interactive UI Design Optimization", "id": "5427"}, {"text": "In this example, Spacewalker uses either \" nav-1 \" or \" nav-2 \" at a time, while the rest children are unaffected.", "bboxes": [{"left": 0.5195343137254902, "top": 0.5240025252525252, "width": 0.39255555555555544, "height": 0.011988636363636451, "page": 2}, {"left": 0.5195343137254902, "top": 0.5385075757575758, "width": 0.27861601307189554, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "id": "5428"}, {"text": "In addition to exploring individual CSS properties, Alex wants to determine which design of the navigation bar she should use for the Product page.", "bboxes": [{"left": 0.5358137254901961, "top": 0.3447891414141414, "width": 0.3762794117647058, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.3586262626262626, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 2}, {"left": 0.5195343137254902, "top": 0.37246338383838384, "width": 0.08324673202614385, "height": 0.011320707070707092, "page": 2}], "section": "3 USING SPACEWALKER", "id": "5429"}, {"text": "Instead of specifying exploration strategies based on nodes, which can be tedious, designers can directly explore at the level of CSS specification using the explore-css tag.", "bboxes": [{"left": 0.5358137254901961, "top": 0.5800189393939394, "width": 0.37788888888888883, "height": 0.011320707070707092, "page": 2}, {"left": 0.5189918300653594, "top": 0.593854797979798, "width": 0.3930996732026145, "height": 0.011320707070706981, "page": 2}, {"left": 0.5195343137254902, "top": 0.6070239898989899, "width": 0.2660130718954249, "height": 0.01198863636363634, "page": 2}], "section": "3 USING SPACEWALKER", "id": "5430"}, {"text": "In addition to explore individual elements, Spacewalker allows a designer to easily explore a large component of a design as a whole, which might contain a branch of elements and sub-trees, such as a side bar or a navigation bar.", "bboxes": [{"left": 0.5358137254901961, "top": 0.7322260101010101, "width": 0.3762843137254901, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.7460618686868686, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 3}, {"left": 0.5189918300653594, "top": 0.75989898989899, "width": 0.3947156862745098, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.7737361111111111, "width": 0.23681699346405227, "height": 0.011320707070707092, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "id": "5431"}, {"text": "Alex can monitor the progress of the task in the Progress Viewer (see Figure 4), which allows her to see the sample designs of the current generation (iteration).", "bboxes": [{"left": 0.10418464052287582, "top": 0.6829886363636364, "width": 0.3765669934640523, "height": 0.011320707070707092, "page": 3}, {"left": 0.0874656862745098, "top": 0.6968257575757575, "width": 0.3929967320261438, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.7106628787878788, "width": 0.17824509803921568, "height": 0.011320707070707092, "page": 3}], "section": "3 USING SPACEWALKER", "id": "5432"}, {"text": "In this section, we discuss the system design and algorithmic details that underline the Spacewalker.", "bboxes": [{"left": 0.08790522875816993, "top": 0.8100050505050506, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 3}, {"left": 0.08790522875816993, "top": 0.8238421717171717, "width": 0.18967810457516343, "height": 0.011320707070707092, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM", "id": "5433"}, {"text": "As shown in the above example, Spacewalker supports a rich set of methods for exploring a design space through simple HTML extensions, which are intuitive to designers as shown in our experiments.", "bboxes": [{"left": 0.5190212418300654, "top": 0.469530303030303, "width": 0.39307843137254905, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.48336742424242424, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.49720454545454545, "width": 0.3948153594771242, "height": 0.011320707070707037, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "id": "5434"}, {"text": "Spacewalker supports all CSS properties and any number of them for an element.", "bboxes": [{"left": 0.5358137254901961, "top": 0.6262121212121212, "width": 0.3762745098039214, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.6400492424242424, "width": 0.12963888888888886, "height": 0.011320707070707092, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "id": "5435"}, {"text": "To explore a property of an individual element, a designer follows a simple syntax by prefixing \" explore\" to the property, and specifying the alternative values for the property delimited by spaces:", "bboxes": [{"left": 0.6100898692810458, "top": 0.5340366161616161, "width": 0.30200326797385624, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5472058080808081, "width": 0.3925669934640523, "height": 0.01198863636363634, "page": 3}, {"left": 0.5195343137254902, "top": 0.5617108585858586, "width": 0.39293790849673205, "height": 0.011320707070707092, "page": 3}, {"left": 0.5195343137254902, "top": 0.5755479797979798, "width": 0.1201372549019607, "height": 0.011320707070707092, "page": 3}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "id": "5436"}, {"text": "In our early exploration, we found conventional GA sensitive to the random initialization of design options.", "bboxes": [{"left": 0.5358137254901961, "top": 0.75989898989899, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.7737361111111111, "width": 0.27940849673202606, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "id": "5437"}, {"text": "A specific genetic sequence indicates a UI configuration, which can be rendered as a design instance shown to a crowd worker for feedback.", "bboxes": [{"left": 0.5358137254901961, "top": 0.1649078282828283, "width": 0.3762777777777778, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.17874494949494948, "width": 0.39283333333333337, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.1925820707070707, "width": 0.05510457516339873, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "id": "5438"}, {"text": "The pairwise comparison of designs eases the rater task and yields more reliable feedback than rating a design individually on an absolute scale.", "bboxes": [{"left": 0.7451078431372549, "top": 0.6492032828282828, "width": 0.16697875816993468, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6630404040404041, "width": 0.3925669934640523, "height": 0.011320707070707092, "page": 4}, {"left": 0.5195343137254902, "top": 0.6768775252525252, "width": 0.28088398692810457, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "id": "5439"}, {"text": "As the number of attributes and nodes to be explored increases, the search space for a design grows combinatorially.", "bboxes": [{"left": 0.0873921568627451, "top": 0.3271136363636364, "width": 0.39307843137254905, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.3409507575757576, "width": 0.29160294117647056, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "id": "5440"}, {"text": "The Spacewalker parser analyzes a design specification file by parsing its HTML structure, which derives an internal representation for the design search space.", "bboxes": [{"left": 0.17905228758169933, "top": 0.15827146464646463, "width": 0.3038856209150327, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.17210858585858585, "width": 0.39256045751633994, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.18594570707070707, "width": 0.2536960784313726, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.1 Spacewalker Markup Syntax & Parsing", "id": "5441"}, {"text": "We refer an instance of a UI design, which is acquired by selecting a specific option for each attribute to be explored, as a configuration of the design.", "bboxes": [{"left": 0.4615653594771242, "top": 0.7427878787878788, "width": 0.018895424836601227, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.756625, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.7704621212121212, "width": 0.39256045751633994, "height": 0.011332070707070763, "page": 4}, {"left": 0.08790522875816993, "top": 0.7842992424242424, "width": 0.06349019607843136, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "id": "5442"}, {"text": "A typical genetic algorithm (GA) follows an iterative process, where potential solutions evolve from a multi-generation process.", "bboxes": [{"left": 0.3167761437908497, "top": 0.4726843434343434, "width": 0.16369281045751638, "height": 0.011320707070707092, "page": 4}, {"left": 0.0874656862745098, "top": 0.4865214646464646, "width": 0.3930016339869281, "height": 0.011320707070707092, "page": 4}, {"left": 0.08790522875816993, "top": 0.5003585858585858, "width": 0.19708169934640524, "height": 0.011320707070707092, "page": 4}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "id": "5443"}, {"text": "Initialization.", "bboxes": [{"left": 0.10418464052287582, "top": 0.35860353535353534, "width": 0.09064542483660129, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "id": "5444"}, {"text": "To do so, we introduce a bit mask, named feedback mask , for each genetic sequencethat corresponds a design instance, which has the same length as a genetic sequence.", "bboxes": [{"left": 0.10418464052287582, "top": 0.10956060606060607, "width": 0.3762859477124183, "height": 0.011332070707070707, "page": 5}, {"left": 0.08790522875816993, "top": 0.12339772727272727, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.1372348484848485, "width": 0.22974673202614382, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "id": "5445"}, {"text": "We invited five participants for this remote user study.", "bboxes": [{"left": 0.7086290849673202, "top": 0.5293876262626263, "width": 0.20346732026143788, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.5432247474747475, "width": 0.10915196078431366, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "id": "5446"}, {"text": "We evaluate Spacewalker in multiple dimensions.", "bboxes": [{"left": 0.5188316993464052, "top": 0.3218320707070707, "width": 0.3049624183006536, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS", "id": "5447"}, {"text": "The Spacewalker system is built as a web service based on AppEngine 7 .", "bboxes": [{"left": 0.0874656862745098, "top": 0.7910239898989899, "width": 0.3954787581699347, "height": 0.011320707070707092, "page": 5}, {"left": 0.08790522875816993, "top": 0.8023914141414141, "width": 0.06010784313725491, "height": 0.013790404040404103, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.3 Architectures", "id": "5448"}, {"text": "In this study, we evaluate the usability of our proposed HTML extensions by gather informal feedback from web designers.", "bboxes": [{"left": 0.5195343137254902, "top": 0.45108333333333334, "width": 0.39293790849673205, "height": 0.011320707070707092, "page": 5}, {"left": 0.5195343137254902, "top": 0.46491919191919195, "width": 0.36536274509803923, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "id": "5449"}, {"text": "Finally, we consider nested designs (where one option value depend on another parent value).", "bboxes": [{"left": 0.10418464052287582, "top": 0.6907146464646465, "width": 0.37627450980392163, "height": 0.011320707070706981, "page": 5}, {"left": 0.08790522875816993, "top": 0.7045517676767676, "width": 0.20579411764705885, "height": 0.011320707070707092, "page": 5}], "section": "4 THE SPACEWALKER SYSTEM @@ 4.2 Spacewalker Genetic Algorithms", "id": "5450"}, {"text": "All participants were able to learn the Spacewalker markup syntax using the description we provided and were able to create syntactically correct specifications.", "bboxes": [{"left": 0.6833088235294118, "top": 0.8429217171717173, "width": 0.22878104575163383, "height": 0.011320707070706981, "page": 5}, {"left": 0.5195343137254902, "top": 0.8567588383838384, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 5}, {"left": 0.5189918300653594, "top": 0.8705959595959595, "width": 0.33919934640522875, "height": 0.011320707070707092, "page": 5}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "id": "5451"}, {"text": "We conducted two experiments to evaluate whether Spacewalker was able to efficiently search a design space and generate better designs by utilizing the responses from the crowd workers.", "bboxes": [{"left": 0.08720261437908497, "top": 0.4878926767676768, "width": 0.39354901960784316, "height": 0.011320707070707092, "page": 6}, {"left": 0.08736437908496732, "top": 0.501729797979798, "width": 0.3933856209150327, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.5155669191919192, "width": 0.36811601307189545, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "id": "5452"}, {"text": "On average, each task took about 1 hour to finish.", "bboxes": [{"left": 0.10418464052287582, "top": 0.8567588383838384, "width": 0.28203267973856205, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "id": "5453"}, {"text": "We received largely positive feedback from the participants.", "bboxes": [{"left": 0.10418464052287582, "top": 0.34482323232323236, "width": 0.3582385620915032, "height": 0.011320707070707037, "page": 6}], "section": "5 EXPERIMENTS @@ 5.1 User Study", "id": "5454"}, {"text": "We conducted both experiments following the same procedure.", "bboxes": [{"left": 0.3279264705882353, "top": 0.6076919191919191, "width": 0.1550081699346405, "height": 0.011320707070707092, "page": 6}, {"left": 0.08790522875816993, "top": 0.6215290404040403, "width": 0.22123692810457518, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "id": "5455"}, {"text": "For each search specification, we calculate the percentage of votes received by Spacewalker genetic method (the rest of the votes are received by uniform sampling).", "bboxes": [{"left": 0.689547385620915, "top": 0.7958598484848485, "width": 0.22502287581699354, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8096969696969697, "width": 0.3925637254901959, "height": 0.011320707070707092, "page": 6}, {"left": 0.5195343137254902, "top": 0.8235340909090909, "width": 0.3948202614379086, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "id": "5456"}, {"text": "Experiment #2: Effects of Web Page Designs.", "bboxes": [{"left": 0.5195343137254902, "top": 0.400114898989899, "width": 0.29360620915032687, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "id": "5457"}, {"text": "Experiment #1: Effects of Search Space Sizes.", "bboxes": [{"left": 0.5195343137254902, "top": 0.2894191919191919, "width": 0.3021029411764705, "height": 0.011320707070707092, "page": 6}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "id": "5458"}, {"text": "Another challenge lies in how well designers can understand the effect when complex design alternatives exist in one design space, e.g., design options nested within a parent option or global options via CSS.", "bboxes": [{"left": 0.5358137254901961, "top": 0.3789532828282828, "width": 0.3762745098039214, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.392790404040404, "width": 0.392563725490196, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.4066275252525252, "width": 0.39255392156862745, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.42046464646464643, "width": 0.0941307189542483, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "id": "5459"}, {"text": "Dependency between elements and designer specified options also presents two challenges to Spacewalker.", "bboxes": [{"left": 0.5358137254901961, "top": 0.517324494949495, "width": 0.3762875816993464, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.5311603535353535, "width": 0.268483660130719, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "id": "5460"}, {"text": "Crowd raters, from the cross-method evaluation, showed significant preference for the designs generated by Spacewalker for all the search space sizes in Experiment 1 (see Table 2).", "bboxes": [{"left": 0.10418464052287582, "top": 0.33744318181818184, "width": 0.3787630718954249, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.35128030303030305, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.36511742424242427, "width": 0.30173202614379085, "height": 0.011320707070707092, "page": 7}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "id": "5461"}, {"text": "Our quantitative experiments for examining the performance of Spacewalker algorithms for searching a design space indicate that it improves designs over time by producing better design candidates, particularly when the search space is large.", "bboxes": [{"left": 0.5358137254901961, "top": 0.8355744949494949, "width": 0.3762794117647058, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8494116161616162, "width": 0.39256045751633983, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8632487373737373, "width": 0.39417320261437905, "height": 0.011320707070707092, "page": 7}, {"left": 0.5195343137254902, "top": 0.8770858585858586, "width": 0.25522549019607843, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "id": "5462"}, {"text": "For the experiment where Spacewalker is used to search for different web page types, we find that crowd raters, from the crossmethod evaluation, preferred the results produced by Spacewalker genetic method in all cases we tested when they are compared with those from the uniform sampling method (see Table 3).", "bboxes": [{"left": 0.10418464052287582, "top": 0.43430176767676765, "width": 0.3765588235294117, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.44813888888888886, "width": 0.39503104575163406, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4619760101010101, "width": 0.3928333333333333, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4758131313131313, "width": 0.3925604575163399, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.4896502525252525, "width": 0.3386454248366013, "height": 0.011320707070707092, "page": 7}], "section": "5 EXPERIMENTS @@ 5.2 Exploration Performance Evaluation", "id": "5463"}, {"text": "The Spacewalker markup extension is easy to understand and use for specifying design exploration.", "bboxes": [{"left": 0.10418464052287582, "top": 0.8014103535353535, "width": 0.3762794117647058, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.8152474747474748, "width": 0.23391830065359476, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "id": "5464"}, {"text": "In this section, we discuss the strengths and limitation of our work, and our plan for future work.", "bboxes": [{"left": 0.08790522875816993, "top": 0.6492032828282828, "width": 0.3941683006535948, "height": 0.011320707070707092, "page": 7}, {"left": 0.08790522875816993, "top": 0.6630404040404041, "width": 0.17506535947712415, "height": 0.011320707070707092, "page": 7}], "section": "6 DISCUSSION & FUTURE WORK", "id": "5465"}, {"text": "For each of the tasks, our GA-based algorithm only visited a small portion of the design space.", "bboxes": [{"left": 0.10418464052287582, "top": 0.17874494949494948, "width": 0.3762745098039215, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.1925820707070707, "width": 0.19990686274509806, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "id": "5466"}, {"text": "Finally, we want to emphasize that Spacewalker is able to identify an optimal design in a defined design space, instead of finding a globally optimal design.", "bboxes": [{"left": 0.10418464052287582, "top": 0.5385075757575758, "width": 0.3787516339869281, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.552344696969697, "width": 0.3925571895424837, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.5661818181818182, "width": 0.15059640522875817, "height": 0.011320707070707092, "page": 9}], "section": "6 DISCUSSION & FUTURE WORK", "id": "5467"}, {"text": "Spacewalker provides integrated support to enable designers to rapidly explore a large design space to improve their web UI design.", "bboxes": [{"left": 0.08790522875816993, "top": 0.8290845959595959, "width": 0.39256372549019614, "height": 0.011320707070707092, "page": 9}, {"left": 0.08790522875816993, "top": 0.8429217171717173, "width": 0.3948153594771242, "height": 0.011320707070706981, "page": 9}], "section": "7 CONCLUSION", "id": "5468"}, {"text": "We would like to thank anonymous reviewers for their insightful feedback for improving the paper.", "bboxes": [{"left": 0.5188316993464052, "top": 0.24478535353535355, "width": 0.39325816993464047, "height": 0.011320707070707065, "page": 9}, {"left": 0.5195343137254902, "top": 0.25862247474747474, "width": 0.2007859477124183, "height": 0.011320707070707037, "page": 9}], "section": "ACKNOWLEDGEMENTS", "id": "5469"}], "uist-1": [{"text": "Making touchscreen interfaces accessible has been a longstanding challenge in accessibility [14, 17, 30], and some current platforms are quite accessible ( e.g. , iOS).", "bboxes": [{"left": 0.5246633986928104, "top": 0.6354570707070707, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.6492941919191919, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.6628421717171717, "width": 0.3214444444444444, "height": 0.012868686868686918, "page": 0}], "section": "INTRODUCTION", "id": "5470"}, {"text": "Most prior work on making touchscreens accessible has as sumed access to change or add to the touchscreen hardware or software.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.07756862745098048, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "id": "5472"}, {"text": "Inaccessible touchscreen interfaces in the world represent a long-standing and frustrating problem for people who are", "bboxes": [{"left": 0.08811928104575163, "top": 0.7651818181818182, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 0}, {"left": 0.08811928104575163, "top": 0.7790189393939393, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "id": "5473"}, {"text": "This paper introduces StateLens , a reverse engineering solution for making existing dynamic touchscreens accessible.", "bboxes": [{"left": 0.08761437908496732, "top": 0.6423472222222222, "width": 0.39768300653594774, "height": 0.012868686868686918, "page": 1}, {"left": 0.08811928104575163, "top": 0.6564734848484849, "width": 0.35609477124183003, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "id": "5474"}, {"text": "Our work is related to prior work on", "bboxes": [{"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.24262091503267968, "height": 0.012579545454545427, "page": 1}], "section": "RELATED WORK", "id": "5475"}, {"text": "A known challenge for touchscreen interfaces is that they can not easily be explored non-visually without the risk of acciden tally triggering functions on the screen.", "bboxes": [{"left": 0.08753267973856209, "top": 0.8577386363636363, "width": 0.4004673202614379, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.8715757575757576, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.8854128787878788, "width": 0.2543382352941177, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "id": "5476"}, {"text": "StateLens builds on this rich literature, and applies a hybrid crowd-computer vision pipeline to automatically extract state diagrams about the underlying interface structures from point of-view usage videos.", "bboxes": [{"left": 0.08811928104575163, "top": 0.7273926767676768, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7412297979797979, "width": 0.3971764705882353, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.7550669191919193, "width": 0.3998725490196079, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.7689040404040404, "width": 0.14699673202614377, "height": 0.012579545454545427, "page": 2}], "section": "Reverse Engineering User Interfaces", "id": "5477"}, {"text": "VizLens::State Detection is able to do limited adaptation to dynamic interfaces by matching against every possible state and providing feedback based on the best match.", "bboxes": [{"left": 0.5240784313725491, "top": 0.755695707070707, "width": 0.39775490196078434, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246601307189542, "top": 0.7695328282828283, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246601307189542, "top": 0.7833699494949494, "width": 0.32848202614379096, "height": 0.012579545454545538, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "id": "5478"}, {"text": "Some of the prior work has gone beyond the task of identifying individual GUI components from static photos, and looked instead to extract interaction ows from screencast videos and screen metadata provided by the system API.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5676388888888889, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.5814760101010101, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5953131313131313, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.6091502525252526, "width": 0.3071290849673203, "height": 0.012579545454545427, "page": 2}], "section": "Reverse Engineering User Interfaces", "id": "5479"}, {"text": "A core feature of StateLens is its ability to reverse engineer user interfaces in-the-wild based on videos of their use.", "bboxes": [{"left": 0.08753267973856209, "top": 0.0959229797979798, "width": 0.3980375816993464, "height": 0.01257954545454544, "page": 2}, {"left": 0.08811928104575163, "top": 0.1094709595959596, "width": 0.36365849673202616, "height": 0.012868686868686863, "page": 2}], "section": "Reverse Engineering User Interfaces", "id": "5480"}, {"text": "Static physical interfaces can be augmented with tactile over lays to make them accessible.", "bboxes": [{"left": 0.5246633986928104, "top": 0.4500239898989899, "width": 0.3998709150326799, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4638611111111111, "width": 0.19946732026143799, "height": 0.012579545454545482, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "id": "5481"}, {"text": "VizLens [17] is a screen reader to help blind people use inac cessible static interfaces in the real world ( e.g. , the buttons on a microwave).", "bboxes": [{"left": 0.5240784313725491, "top": 0.5959419191919192, "width": 0.4004705882352939, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.609489898989899, "width": 0.39717973856209154, "height": 0.012868686868686807, "page": 2}, {"left": 0.5246633986928104, "top": 0.6236161616161616, "width": 0.09436274509803921, "height": 0.012579545454545538, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "id": "5482"}, {"text": "Prior work on reverse engineering of user interfaces has mainly used sceenshots or screencast videos.", "bboxes": [{"left": 0.08811928104575163, "top": 0.2833510101010101, "width": 0.39774673202614386, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.2971881313131313, "width": 0.24211764705882355, "height": 0.012579545454545482, "page": 2}], "section": "Reverse Engineering User Interfaces", "id": "5483"}, {"text": "Crowd-powered systems robustly make visual information accessible to blind people.", "bboxes": [{"left": 0.5246633986928104, "top": 0.1305151515151515, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 2}, {"left": 0.5246633986928104, "top": 0.14435227272727272, "width": 0.17784803921568626, "height": 0.012579545454545454, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "id": "5484"}, {"text": "Other systems provide more continuous support.", "bboxes": [{"left": 0.5246633986928104, "top": 0.2764330808080808, "width": 0.3103709150326798, "height": 0.012579545454545427, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "id": "5485"}, {"text": "Many physical interfaces in the real world are inaccessible to blind people, which has led to substantial prior work on sys tems for making them accessible.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.21489869281045754, "height": 0.012579545454545427, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "id": "5486"}, {"text": "LCD panels on appliances [14, 30, 33].", "bboxes": [{"left": 0.5246535947712418, "top": 0.0814520202020202, "width": 0.2605702614379085, "height": 0.01257954545454544, "page": 2}], "section": "Improving Accessibility for Physical Interfaces", "id": "5487"}, {"text": "We conducted a formative study to identify the key challenges and design considerations for a system to provide access to dy namic touchscreen interfaces in the real world.", "bboxes": [{"left": 0.08735457516339869, "top": 0.13697853535353535, "width": 0.39794281045751634, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.15081565656565657, "width": 0.3998839869281046, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.16465151515151516, "width": 0.3008382352941177, "height": 0.012579545454545454, "page": 3}], "section": "FORMATIVE STUDY", "id": "5488"}, {"text": "Informed by the participants feedback to our initial prototype, we designed variations of 3D-printed accessories (Figure 3DG) that focus on improving stability and comfort during use.", "bboxes": [{"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.3992140522875818, "height": 0.012579545454545427, "page": 3}, {"left": 0.5240735294117647, "top": 0.7813737373737373, "width": 0.39884803921568623, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.7952108585858586, "width": 0.36716176470588247, "height": 0.012579545454545427, "page": 3}], "section": "Design Variations", "id": "5489"}, {"text": "We rst conducted an exploratory search on Thingiverse to understand what openly available solutions exist for people to interact with touchscreens and see if they can enable risk-free exploration for blind people.", "bboxes": [{"left": 0.5238986928104574, "top": 0.16255555555555556, "width": 0.39793790849673205, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.17639267676767678, "width": 0.39717810457516345, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.190229797979798, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.20406691919191922, "width": 0.19151307189542488, "height": 0.012579545454545454, "page": 3}], "section": "Thingiverse Survey", "id": "5490"}, {"text": "Participants often resorted to sighted help when accessing pub lic touchscreen appliances, and raised serious privacy concerns when asking others (often strangers) to help with entering sen sitive information, e.g. , using credit card machines to complete nancial transactions, or using sign-in kiosks at pharmacies and doctors ofces.", "bboxes": [{"left": 0.08811928104575163, "top": 0.4093371212121212, "width": 0.3998839869281046, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.4231742424242424, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 3}, {"left": 0.08753267973856209, "top": 0.4370113636363636, "width": 0.40046568627450974, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.45055934343434345, "width": 0.39717320261437916, "height": 0.012868686868686863, "page": 3}, {"left": 0.08811928104575163, "top": 0.4646843434343434, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.4785214646464646, "width": 0.13170588235294117, "height": 0.012579545454545482, "page": 3}], "section": "Supporting Independence", "id": "5491"}, {"text": "We tested this design in a pilot study with two blind partic ipants (one female, age 48; one male, age 57).", "bboxes": [{"left": 0.5238986928104574, "top": 0.5763712121212121, "width": 0.4006437908496734, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246617647058823, "top": 0.5902083333333333, "width": 0.32037908496732026, "height": 0.012579545454545538, "page": 3}], "section": "Finger Ring Prototype", "id": "5492"}, {"text": "For unfamiliar dynamic touchscreen devices, the amount of time and cognitive effort needed for blind people to explore, understand, and activate functions became quite heavy.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5708257575757576, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5846628787878788, "width": 0.3991993464052287, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.5985, "width": 0.3523382352941177, "height": 0.012579545454545427, "page": 3}], "section": "Supporting Independence", "id": "5493"}, {"text": "Participants remarked that interfaces are becoming much less accessible as at touch pads and touchscreens replace physical buttons.", "bboxes": [{"left": 0.08811928104575163, "top": 0.3170328282828283, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.33086994949494947, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.3447058080808081, "width": 0.05271895424836602, "height": 0.012579545454545482, "page": 3}], "section": "Design Considerations", "id": "5494"}, {"text": "Inspired by the nger cap designs from Thingiverse, we rst created a 3D-printed ring that allows users to explore without touching the screen, and tilt their nger forward to perform a touch at a desired position (Figure 3A-C).", "bboxes": [{"left": 0.5246633986928104, "top": 0.5134760101010101, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5273131313131313, "width": 0.39717973856209154, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.5411502525252525, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.5549873737373737, "width": 0.2726372549019608, "height": 0.012579545454545427, "page": 3}], "section": "Finger Ring Prototype", "id": "5495"}, {"text": "Using an approach akin to afnity diagramming [5, 10, 21], we classied these items into ve main categories of devices: styluses, prosthetic accessories, nger caps, buttons and joy sticks.", "bboxes": [{"left": 0.5246633986928104, "top": 0.33614646464646464, "width": 0.3991993464052288, "height": 0.012579545454545427, "page": 3}, {"left": 0.5240784313725491, "top": 0.34998358585858586, "width": 0.40002614379084966, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3638207070707071, "width": 0.3998823529411766, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3776578282828283, "width": 0.041316993464052265, "height": 0.012579545454545427, "page": 3}], "section": "Thingiverse Survey", "id": "5496"}, {"text": "Risk-free exploration allows blind users to freely explore with out accidentally triggering functions on the screen, all without modifying the underlying hardware or software of the device.", "bboxes": [{"left": 0.5246633986928104, "top": 0.0959229797979798, "width": 0.39988398692810456, "height": 0.01257954545454544, "page": 3}, {"left": 0.5246633986928104, "top": 0.10976010101010102, "width": 0.39717320261437905, "height": 0.01257954545454544, "page": 3}, {"left": 0.5246633986928104, "top": 0.12359722222222222, "width": 0.39989215686274515, "height": 0.012579545454545454, "page": 3}], "section": "RISK-FREE EXPLORATION", "id": "5497"}, {"text": "Participants shared their concerns and fears of accidentally triggering functions on inaccessible touchscreens.", "bboxes": [{"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.3977385620915034, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.3272843137254903, "height": 0.012579545454545427, "page": 3}], "section": "Supporting Independence", "id": "5498"}, {"text": "When attempting to use existing inaccessible touchscreen de vices, participants found holding their ngers in mid-air while trying to explore and locate the buttons to be very awkward and unusable, which also often resulted in accidental touches.", "bboxes": [{"left": 0.08735457516339869, "top": 0.8090479797979797, "width": 0.4006372549019608, "height": 0.012579545454545538, "page": 3}, {"left": 0.0877124183006536, "top": 0.822885101010101, "width": 0.3975751633986928, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.4000212418300654, "height": 0.012579545454545427, "page": 3}], "section": "Supporting Independence", "id": "5499"}, {"text": "Reducing Cognitive Effort", "bboxes": [{"left": 0.08811928104575163, "top": 0.5573118686868687, "width": 0.1658937908496732, "height": 0.011321969696969636, "page": 3}], "section": "Supporting Independence", "id": "5500"}, {"text": "Enabling Risk-Free Exploration", "bboxes": [{"left": 0.08811928104575163, "top": 0.7049646464646464, "width": 0.2008202614379085, "height": 0.011321969696969636, "page": 3}], "section": "Supporting Independence", "id": "5501"}, {"text": "StateLens uses a hybrid crowd-computer vision pipeline to dynamically generate state diagrams about interface structures from point-of-view usage videos, and to provide interactive feedback and guidance to help blind users access the interfaces through these diagrams.", "bboxes": [{"left": 0.5246633986928104, "top": 0.4004621212121212, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.41429924242424243, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.42813510101010105, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.4419722222222222, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.45580934343434343, "width": 0.15161928104575162, "height": 0.012579545454545482, "page": 4}], "section": "STATELENS", "id": "5502"}, {"text": "Our design variations consist of a nger cap (Figure 3D) and a conductive stylus (Figure 3G).", "bboxes": [{"left": 0.08811928104575163, "top": 0.329939393939394, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.3437765151515152, "width": 0.21460947712418305, "height": 0.012579545454545427, "page": 4}], "section": "Design Variations", "id": "5503"}, {"text": "StateLens represents the interface structure with a state diagram, as shown in Figure 2 and the instantiation of the coffee machine shown in Figure 4.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.28474673202614387, "height": 0.012579545454545538, "page": 4}], "section": "Representing State Diagram", "id": "5504"}, {"text": "StateLens takes point-of-view usage videos of dynamic inter faces from various sources as input to build up state diagrams about interface structures.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6054242424242424, "width": 0.3998709150326798, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.619260101010101, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6330972222222222, "width": 0.17276797385620912, "height": 0.012579545454545427, "page": 4}], "section": "Capturing Point-of-View Usage Video", "id": "5505"}, {"text": "The architecture of StateLens to generate state diagrams (Fig ure 2) involves capturing point-of-view usage videos from a variety of sources, representing state diagrams, detecting screen regions, identifying existing and new states, soliciting labels from the crowd, as well as recognizing user interactions.", "bboxes": [{"left": 0.5241601307189543, "top": 0.5124949494949496, "width": 0.4003839869281045, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.5263320707070708, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.540169191919192, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.5540063131313132, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.5678434343434344, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 4}], "section": "Generating the State Diagram", "id": "5506"}, {"text": "If the highest matched ratio across existing reference images is not high enough, meaning the match using only SURF features is not so condent, StateLens then uses the Google Cloud Vision API [16] to compute OCR results for the input image and compares to the pre-computed OCR results of the state reference image.", "bboxes": [{"left": 0.5246633986928104, "top": 0.40785353535353536, "width": 0.39718137254901964, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.4216906565656566, "width": 0.3976225490196078, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.43552777777777774, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.44936489898989895, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.46320202020202017, "width": 0.39717810457516345, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.4770391414141414, "width": 0.14830882352941177, "height": 0.012579545454545482, "page": 5}], "section": "Representing State Diagram", "id": "5507"}, {"text": "When a transition happens on the dynamic interface, the new state might not have been seen before.", "bboxes": [{"left": 0.5238986928104574, "top": 0.6291666666666667, "width": 0.3985310457516342, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.6430037878787879, "width": 0.2577810457516341, "height": 0.012579545454545427, "page": 5}], "section": "Adding New States", "id": "5508"}, {"text": "StateLens extracts two kinds of features and intelligently com bines them (Figure 2): SURF (Speeded-Up Robust Features) [3] and OCR.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.3998839869281046, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.39826307189542487, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.08942810457516341, "height": 0.012579545454545538, "page": 5}], "section": "Representing State Diagram", "id": "5509"}, {"text": "StateLens detects whether a screen is present and its bound ing box in the cameras eld of view to lter out irrelevant video frames and random background content.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5354924242424243, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5493295454545455, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.0877124183006536, "top": 0.5631666666666667, "width": 0.3043643790849673, "height": 0.012579545454545427, "page": 5}], "section": "Representing State Diagram", "id": "5510"}, {"text": "Identifying Existing States", "bboxes": [{"left": 0.08811928104575163, "top": 0.7955340909090909, "width": 0.1677532679738562, "height": 0.011321969696969636, "page": 5}], "section": "Representing State Diagram", "id": "5511"}, {"text": "Detecting the Screen", "bboxes": [{"left": 0.08811928104575163, "top": 0.5219785353535353, "width": 0.1368104575163399, "height": 0.011321969696969747, "page": 5}], "section": "Representing State Diagram", "id": "5512"}, {"text": "In the next section of Accessing the State Diagram, using the user interaction information, StateLens predicts the state that the interface could be transitioning to, and reduces the process ing latency and errors by narrowing down the search space.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5080366161616162, "width": 0.3971699346405229, "height": 0.012578282828282772, "page": 6}, {"left": 0.08811928104575163, "top": 0.5218737373737374, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5357108585858587, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5495479797979799, "width": 0.4000130718954249, "height": 0.012579545454545427, "page": 6}], "section": "Recognizing User Interaction", "id": "5513"}, {"text": "StateLens allows users to interact with a natural language con versational agent to prespecify the task they want to achieve.", "bboxes": [{"left": 0.5246633986928104, "top": 0.34317045454545453, "width": 0.399874183006536, "height": 0.012579545454545482, "page": 6}, {"left": 0.5242565359477125, "top": 0.35700631313131315, "width": 0.4004313725490195, "height": 0.012579545454545427, "page": 6}], "section": "Enabling Natural Language Queries", "id": "5514"}, {"text": "StateLens builds upon the crowdsourcing workow in VizLens [17], and uses a two-step workow to label the area of the image that contains the interface assisted with screen detection results, and then label the individual interaction components assisted with OCR output (Figure 2).", "bboxes": [{"left": 0.08811928104575163, "top": 0.0959229797979798, "width": 0.39717320261437916, "height": 0.01257954545454544, "page": 6}, {"left": 0.08811928104575163, "top": 0.10976010101010102, "width": 0.39717320261437916, "height": 0.01257954545454544, "page": 6}, {"left": 0.08811928104575163, "top": 0.12359722222222222, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.13743434343434344, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.15127020202020203, "width": 0.2553725490196079, "height": 0.012579545454545454, "page": 6}], "section": "Adding New States", "id": "5515"}, {"text": "StateLens employs several techniques to enable efcient searching of states to reduce latency and prevent errors.", "bboxes": [{"left": 0.08811928104575163, "top": 0.7813737373737373, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.36030392156862745, "height": 0.012579545454545427, "page": 6}], "section": "Accessing the State Diagram", "id": "5516"}, {"text": "Crowd workers are then instructed to provide labels to the individual interaction components ( e.g. , buttons) assisted with OCR output.", "bboxes": [{"left": 0.08811928104575163, "top": 0.24183964646464648, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.25538762626262623, "width": 0.39716830065359476, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.2695138888888889, "width": 0.08576143790849673, "height": 0.012579545454545427, "page": 6}], "section": "Adding New States", "id": "5517"}, {"text": "Finally, StateLens captures the interaction component that triggered a state transition, e.g. , a button b n that contributes to the transition E i j = V i  V j = ( { b n } , V i , V j ) .", "bboxes": [{"left": 0.08811928104575163, "top": 0.3897929292929293, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.40334090909090914, "width": 0.3971748366013072, "height": 0.013834595959595908, "page": 6}, {"left": 0.08812091503267974, "top": 0.41717803030303025, "width": 0.3142107843137255, "height": 0.014047979797979837, "page": 6}], "section": "Recognizing User Interaction", "id": "5518"}, {"text": "To help blind users access the dynamic interfaces, StateLens takes advantage of the state diagram to efciently identify states, integrates natural language agents, and interactively provides feedback and guidance (Figure 1).", "bboxes": [{"left": 0.08761437908496732, "top": 0.7026060606060607, "width": 0.397686274509804, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.7164431818181818, "width": 0.3977385620915034, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.730280303030303, "width": 0.3977385620915033, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.7441174242424242, "width": 0.2827450980392157, "height": 0.012579545454545427, "page": 6}], "section": "Accessing the State Diagram", "id": "5519"}, {"text": "Soliciting Labels from the Crowd", "bboxes": [{"left": 0.08811928104575163, "top": 0.0824090909090909, "width": 0.20984640522875814, "height": 0.011321969696969691, "page": 6}], "section": "Adding New States", "id": "5520"}, {"text": "Identifying States Efciently and Robustly", "bboxes": [{"left": 0.08811928104575163, "top": 0.7678598484848485, "width": 0.26843464052287586, "height": 0.011321969696969636, "page": 6}], "section": "Accessing the State Diagram", "id": "5521"}, {"text": "StateLens identies the current state of the dynamic interface, and recognizes the users touchpoint location to provide realtime feedback and guidance for blind users through the iOS application.", "bboxes": [{"left": 0.08811928104575163, "top": 0.593945707070707, "width": 0.3992075163398694, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6077828282828283, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6216199494949495, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6354570707070707, "width": 0.07793954248366013, "height": 0.012579545454545538, "page": 7}], "section": "Providing Interactive Feedback and Guidance", "id": "5522"}, {"text": "StateLens uses the state diagram and the associated aggrega tion of interaction traces to automatically generate a natural language summary of the devices popular use cases.", "bboxes": [{"left": 0.08811928104575163, "top": 0.3918510101010101, "width": 0.39987418300653593, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.4056881313131313, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4195252525252525, "width": 0.34787254901960785, "height": 0.012579545454545427, "page": 7}], "section": "Generating Natural Language Summary", "id": "5523"}, {"text": "We collected a total of 28 videos from a diverse set of eight dy namic touchscreen interfaces, in different lighting conditions, and with both stationary and hand-held cameras, resulting in a total of 40,140 video frames.", "bboxes": [{"left": 0.5238986928104574, "top": 0.38839141414141415, "width": 0.40065032679738566, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.40222853535353537, "width": 0.399202614379085, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4160656565656566, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4299027777777778, "width": 0.1881176470588235, "height": 0.012579545454545482, "page": 7}], "section": "Dataset", "id": "5524"}, {"text": "We rst evaluated the effectiveness of StateLens in reconstruct ing interface structures from stationary, hand-held, and web usage videos.", "bboxes": [{"left": 0.5238986928104574, "top": 0.531050505050505, "width": 0.40065032679738566, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.5448876262626262, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.5587247474747474, "width": 0.0854624183006536, "height": 0.012579545454545427, "page": 7}], "section": "Generating the State Diagram", "id": "5525"}, {"text": "StateLens then looks up the coordinates of the touchpoint in the current states labeled interaction components, and an nounces feedback and guidance to the blind user, e.g. , state: coffee drinks, select strength; target: regular, move up, move left slowly and at regular, press it.", "bboxes": [{"left": 0.08811928104575163, "top": 0.7537007575757576, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7675366161616162, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.781084595959596, "width": 0.39943137254901956, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.39919934640522875, "height": 0.012579545454545427, "page": 7}, {"left": 0.08596078431372549, "top": 0.8090479797979797, "width": 0.29621895424836603, "height": 0.012579545454545538, "page": 7}], "section": "Providing Interactive Feedback and Guidance", "id": "5526"}, {"text": "Regarding the effect of our screen detection approach, a com bination of Screen Detection+SURF+OCR features generally yielded higher performance compared to SURF+OCR fea tures.", "bboxes": [{"left": 0.5246633986928104, "top": 0.7952108585858586, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39774346405228767, "height": 0.012579545454545538, "page": 7}, {"left": 0.5242565359477125, "top": 0.822885101010101, "width": 0.40027941176470594, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.0361078431372549, "height": 0.012579545454545538, "page": 7}], "section": "Generating the State Diagram", "id": "5527"}, {"text": "We conducted a multi-part technical evaluation in order to understand how each key component of StateLens performs across a wide range of interfaces and usage scenarios.", "bboxes": [{"left": 0.5238986928104574, "top": 0.3149179292929293, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246617647058823, "top": 0.32875505050505055, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.34259217171717177, "width": 0.35015686274509805, "height": 0.012579545454545427, "page": 7}], "section": "TECHNICAL EVALUATION", "id": "5528"}, {"text": "For each interface and video source, we computed the preci sion, recall, and F1 scores for the extracted states using four congurations of features:", "bboxes": [{"left": 0.5246633986928104, "top": 0.6631313131313132, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6769684343434343, "width": 0.39745588235294127, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6908042929292929, "width": 0.16960784313725497, "height": 0.012579545454545538, "page": 7}], "section": "Generating the State Diagram", "id": "5529"}, {"text": "Regarding OCR features, a combination of Screen Detec tion+SURF+OCR features generally had better performance compared to Screen Detection+SURF features.", "bboxes": [{"left": 0.08811928104575163, "top": 0.504270202020202, "width": 0.3998807189542484, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.5181073232323232, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5319444444444444, "width": 0.31607843137254904, "height": 0.012579545454545427, "page": 8}], "section": "Generating the State Diagram", "id": "5530"}, {"text": "We then evaluated the robustness of our techniques in identify ing states compared to the baseline approach.", "bboxes": [{"left": 0.5238986928104574, "top": 0.5751654040404041, "width": 0.40065032679738566, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.5890025252525253, "width": 0.30075326797385626, "height": 0.012579545454545427, "page": 8}], "section": "Using State Diagram to Reduce Search Error", "id": "5531"}, {"text": "We evaluated the efciency of our techniques in identifying states compared to the naive approach in VizLens::State De tection [17] which compares against every possible reference image.", "bboxes": [{"left": 0.08735457516339869, "top": 0.822885101010101, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811764705882352, "top": 0.8367222222222221, "width": 0.3998807189542483, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.04297875816993464, "height": 0.012579545454545538, "page": 8}], "section": "Using State Diagram to Reduce Search Time", "id": "5532"}, {"text": "We next evaluated the effectiveness of using state diagrams to reduce latency and prevent errors in the state detection process.", "bboxes": [{"left": 0.08735457516339869, "top": 0.7727929292929293, "width": 0.3979362745098039, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7866300505050504, "width": 0.40002777777777776, "height": 0.012579545454545538, "page": 8}], "section": "Accessing the State Diagram", "id": "5533"}, {"text": "Parameters can be chosen to further maximize recall (sacri cing some precision), as post-hoc crowd validation can be applied in the future to further lter out duplicates.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6778623737373737, "width": 0.3998807189542484, "height": 0.012579545454545538, "page": 8}, {"left": 0.08811928104575163, "top": 0.691699494949495, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7055366161616161, "width": 0.3291143790849673, "height": 0.012579545454545538, "page": 8}], "section": "Generating the State Diagram", "id": "5534"}, {"text": "In order to enable repeated testing without wasting coffee, we built a simulated interactive prototype of the coffee machine in Figure 4 with InVision [23], which we displayed on an iPad tablet of similar size as the coffee machines interface (iPad Pro 3rd generation, 11-inch, running iOS 12.2 without VoiceOver enabled).", "bboxes": [{"left": 0.08811928104575163, "top": 0.49769191919191924, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5115277777777778, "width": 0.3971764705882353, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.525364898989899, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.5392020202020202, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 9}, {"left": 0.08758169934640524, "top": 0.5530391414141415, "width": 0.3977140522875816, "height": 0.012579545454545538, "page": 9}, {"left": 0.08753267973856209, "top": 0.5668762626262627, "width": 0.13685294117647062, "height": 0.012579545454545427, "page": 9}], "section": "Apparatus and Participants", "id": "5535"}, {"text": "The goal of our user study was to evaluate how the components of StateLens (the 3D-printed accessories, the conversational agent, and the iOS application) perform in enabling blind people to accomplish realistic tasks that involve otherwise inaccessible dynamic touchscreen interfaces.", "bboxes": [{"left": 0.08761437908496732, "top": 0.4014368686868687, "width": 0.39768300653594774, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4152739898989899, "width": 0.39716830065359476, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4291111111111111, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.4429482323232323, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 9}, {"left": 0.08811928104575163, "top": 0.45678535353535354, "width": 0.2918611111111111, "height": 0.012579545454545482, "page": 9}], "section": "USER EVALUATION", "id": "5536"}, {"text": "Following a brief introduction of the study and demographic questions, participants rst completed tasks using the 3D printed accessories.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6769671717171717, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.6908042929292929, "width": 0.3998807189542484, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.7046414141414141, "width": 0.12411601307189545, "height": 0.012579545454545427, "page": 9}], "section": "Procedure", "id": "5537"}, {"text": "Next, according to the three interaction traces prespecied through the conversational agent, participants were asked to use the 3D-printed accessories to perform the tasks following the guidance and feedback of the iOS application.", "bboxes": [{"left": 0.5246633986928104, "top": 0.1305151515151515, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.14435227272727272, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.15818939393939394, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 9}, {"left": 0.5246633986928104, "top": 0.17202651515151515, "width": 0.3137369281045752, "height": 0.012579545454545454, "page": 9}], "section": "Procedure", "id": "5538"}, {"text": "All participants except P12 completed tasks using the 3D printed accessories.", "bboxes": [{"left": 0.5240784313725491, "top": 0.5033762626262626, "width": 0.40046078431372545, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246601307189542, "top": 0.5172133838383838, "width": 0.13211928104575166, "height": 0.012579545454545538, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "id": "5539"}, {"text": "In general, participants found both accessories to be com fortable to use ( M = 5 . 9 , SD = 1 . 1 ) and highly useful ( M = 6 . 4 , SD = 0 . 8 ) .", "bboxes": [{"left": 0.5246633986928104, "top": 0.6631313131313132, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.6766780303030303, "width": 0.39718790849673213, "height": 0.012869949494949462, "page": 9}, {"left": 0.5246699346405229, "top": 0.6905151515151515, "width": 0.09928921568627447, "height": 0.012868686868686918, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "id": "5540"}, {"text": "After each step of the study, we collected Likert scale ratings and subjective feedback from the participants.", "bboxes": [{"left": 0.5240784313725491, "top": 0.3041060606060606, "width": 0.397766339869281, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.3179431818181818, "width": 0.3123758169934642, "height": 0.012579545454545427, "page": 9}], "section": "Procedure", "id": "5541"}, {"text": "We now detail our user study results and summarize user feedback and preferences.", "bboxes": [{"left": 0.5238986928104574, "top": 0.425885101010101, "width": 0.3982189542483662, "height": 0.012579545454545482, "page": 9}, {"left": 0.5246633986928104, "top": 0.43972222222222224, "width": 0.17071895424836614, "height": 0.012579545454545482, "page": 9}], "section": "Results", "id": "5542"}, {"text": "We observed that participants sometimes held the accessories in awkward postures, likely due to unfamiliarity.", "bboxes": [{"left": 0.5238986928104574, "top": 0.8505593434343435, "width": 0.39794607843137264, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.33000980392156853, "height": 0.012579545454545538, "page": 9}], "section": "Exploration and Activation with 3D-Printed Accessories", "id": "5543"}, {"text": "Next, participants were asked to talk to the conversational agent to prespecify drinks they want to order from the coffee machine for three times.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.15759640522875817, "height": 0.012579545454545427, "page": 9}], "section": "Procedure", "id": "5544"}, {"text": "Participants spent an average of 122.3 seconds ( SD = 41 . 9 ) completing the rst task, 110.4 seconds ( SD = 36 . 9 ) for the second, the 97.6 seconds ( SD = 30 . 7 ) for the third, as they got familiar with the audio feedback and guidance.", "bboxes": [{"left": 0.5246633986928104, "top": 0.4324936868686869, "width": 0.39846405228758164, "height": 0.012868686868686807, "page": 10}, {"left": 0.5246633986928104, "top": 0.4463308080808081, "width": 0.39717973856209154, "height": 0.012868686868686807, "page": 10}, {"left": 0.5246633986928104, "top": 0.46016792929292927, "width": 0.39717647058823546, "height": 0.012868686868686863, "page": 10}, {"left": 0.5246633986928104, "top": 0.47429292929292927, "width": 0.31458333333333344, "height": 0.012579545454545482, "page": 10}], "section": "Completing Realistic Tasks", "id": "5545"}, {"text": "Another interesting observation was that 8 of 13 participants who completed the tasks for the printed accessories would occasionally perform a double-click, or two taps in quick succession to activate the screen.", "bboxes": [{"left": 0.08753267973856209, "top": 0.3998787878787879, "width": 0.39776960784313725, "height": 0.012579545454545427, "page": 10}, {"left": 0.08753267973856209, "top": 0.4137159090909091, "width": 0.3977549019607843, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811437908496732, "top": 0.4275530303030303, "width": 0.3975718954248366, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.44139015151515154, "width": 0.21523529411764708, "height": 0.012579545454545427, "page": 10}], "section": "Exploration and Activation with 3D-Printed Accessories", "id": "5546"}, {"text": "Participants spent an average of 53.7 seconds ( SD = 11 . 6 ) to prespecify tasks with the conversational agent, with an overall task completion rate of 100%, and found it to be ex tremely easy to learn ( M = 6 . 6 , SD = 0 . 6 ) , comfortable to use ( M = 6 . 8 , SD = 0 . 4 ) , and useful ( M = 6 . 7 , SD = 0 . 6 ) .", "bboxes": [{"left": 0.08811928104575163, "top": 0.5241161616161616, "width": 0.3984640522875817, "height": 0.012868686868686918, "page": 10}, {"left": 0.08811928104575163, "top": 0.5382424242424243, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.5520795454545455, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.5656262626262626, "width": 0.39717156862745095, "height": 0.012868686868686807, "page": 10}, {"left": 0.08748366013071895, "top": 0.5794633838383838, "width": 0.345702614379085, "height": 0.012868686868686918, "page": 10}], "section": "Prespecifying Tasks with the Conversational Agent", "id": "5547"}, {"text": "In subjective ratings, participants found the StateLens iOS ap plication to be easy to learn ( M = 5 . 5 , SD = 0 . 9 ) , comfortable to use ( M = 5 . 6 , SD = 1 . 2 ) , and very useful ( M = 6 . 1 , SD = 1 . 1 ) .", "bboxes": [{"left": 0.5246633986928104, "top": 0.647885101010101, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.6614330808080808, "width": 0.39717483660130715, "height": 0.012868686868686807, "page": 10}, {"left": 0.5246633986928104, "top": 0.6752702020202019, "width": 0.39717647058823546, "height": 0.012868686868686918, "page": 10}, {"left": 0.5242516339869281, "top": 0.6891199494949495, "width": 0.031259803921568685, "height": 0.012856060606060593, "page": 10}], "section": "Completing Realistic Tasks", "id": "5548"}, {"text": "Better affordances could further improve learnability as one participant (P14) noted that a conductive stylus design which incorporates a physical button to trigger, instead of a conduc tive region, would be benecial.", "bboxes": [{"left": 0.08811928104575163, "top": 0.3369835858585859, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.3508207070707071, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.36465782828282833, "width": 0.3998709150326797, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.37849494949494944, "width": 0.20872549019607844, "height": 0.012579545454545482, "page": 10}], "section": "Exploration and Activation with 3D-Printed Accessories", "id": "5549"}, {"text": "Overall, participants were very excited about the potential of StateLens, and felt that it could help them access other inaccessible interface in the future ( M = 6 . 6 , SD = 0 . 9 ) :", "bboxes": [{"left": 0.5246633986928104, "top": 0.7522916666666667, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.7661287878787879, "width": 0.39745588235294116, "height": 0.012579545454545427, "page": 10}, {"left": 0.5246633986928104, "top": 0.7796767676767677, "width": 0.3654738562091504, "height": 0.012868686868686807, "page": 10}], "section": "Completing Realistic Tasks", "id": "5550"}, {"text": "Our 3D-printed accessories elegantly add risk-free explo ration to existing capacitive touchscreen devices without modifying the underlying hardware or software, which has been a major hurdle for past efforts.", "bboxes": [{"left": 0.5246633986928104, "top": 0.4037411616161616, "width": 0.3998807189542485, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.4175782828282828, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.431415404040404, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.44525252525252523, "width": 0.2448627450980393, "height": 0.012579545454545482, "page": 11}], "section": "Assistive Hardware for Automatic Screen Actuation", "id": "5551"}, {"text": "People who are blind were involved throughout the research, including several people with visual impairments on our ex tended research team, and multiple sessions of design and study with a total of 30 outside participants.", "bboxes": [{"left": 0.08811928104575163, "top": 0.64325, "width": 0.3992009803921569, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.6570871212121212, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.6709242424242424, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 11}, {"left": 0.08811928104575163, "top": 0.6847613636363636, "width": 0.28585457516339874, "height": 0.012579545454545427, "page": 11}], "section": "Technical Approach to Accessibility", "id": "5552"}, {"text": "As with most systems, StateLens currently has some limita tions, which we believe could be explored in future work.", "bboxes": [{"left": 0.5240784313725491, "top": 0.6216199494949495, "width": 0.40046078431372534, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246633986928104, "top": 0.6354570707070707, "width": 0.37029901960784317, "height": 0.012579545454545538, "page": 11}], "section": "Limitations", "id": "5553"}, {"text": "The completeness of the state diagram is limited by the cov erage of the videos collected for the device.", "bboxes": [{"left": 0.5241601307189543, "top": 0.7675366161616162, "width": 0.4003774509803921, "height": 0.012579545454545427, "page": 11}, {"left": 0.5246584967320261, "top": 0.7813737373737373, "width": 0.293264705882353, "height": 0.012579545454545538, "page": 11}], "section": "Limitations", "id": "5554"}, {"text": "StateLens is not the ideal solution.", "bboxes": [{"left": 0.08811928104575163, "top": 0.4834962121212121, "width": 0.23119771241830067, "height": 0.012579545454545482, "page": 11}], "section": "Technical Approach to Accessibility", "id": "5555"}, {"text": "In this paper, we developed a hybrid crowd-computer vision system to enable access to dynamic touchscreens in-the-wild.", "bboxes": [{"left": 0.08811928104575163, "top": 0.7675366161616162, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.7813737373737373, "width": 0.40002777777777776, "height": 0.012579545454545538, "page": 11}], "section": "Technical Approach to Accessibility", "id": "5556"}, {"text": "A low vision user (P12) mentioned that even though he might not always need assistance, if the interfaces contrast or bright ness is poor, a system like StateLens would be greatly helpful as a conrmation.", "bboxes": [{"left": 0.08753267973856209, "top": 0.22108459595959595, "width": 0.3977598039215686, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.23492171717171717, "width": 0.3998839869281046, "height": 0.012579545454545454, "page": 11}, {"left": 0.08811928104575163, "top": 0.2487588383838384, "width": 0.3971797385620915, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.2625959595959596, "width": 0.12067973856209151, "height": 0.012579545454545427, "page": 11}], "section": "Completing Realistic Tasks", "id": "5557"}, {"text": "We motivated our approach as a benet to improve accessibil ity for blind users.", "bboxes": [{"left": 0.5238986928104574, "top": 0.0814570707070707, "width": 0.40064215686274507, "height": 0.012579545454545468, "page": 11}, {"left": 0.5246633986928104, "top": 0.09529419191919192, "width": 0.11714542483660129, "height": 0.012579545454545468, "page": 11}], "section": "Technical Approach to Accessibility", "id": "5558"}, {"text": "Using StateLens, we envision building a queryable map of state diagrams for many of the devices in the world using existing point-of-view videos that have been shared online.", "bboxes": [{"left": 0.5246633986928104, "top": 0.2273737373737374, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 11}, {"left": 0.5246633986928104, "top": 0.24121085858585856, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 11}, {"left": 0.5246633986928104, "top": 0.2550479797979798, "width": 0.3745196078431372, "height": 0.012579545454545482, "page": 11}], "section": "Technical Approach to Accessibility", "id": "5559"}, {"text": "In this section, we discuss how the approaches used in StateLens might generalize to extract information from existing online videos to, for instance, assist sighted users and con struct a queryable map of devices.", "bboxes": [{"left": 0.08811928104575163, "top": 0.3868825757575758, "width": 0.399874183006536, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.40071969696969695, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 11}, {"left": 0.08811928104575163, "top": 0.41455681818181817, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 11}, {"left": 0.08811928104575163, "top": 0.4283939393939394, "width": 0.21855555555555561, "height": 0.012579545454545427, "page": 11}], "section": "DISCUSSION AND FUTURE WORK", "id": "5560"}, {"text": "Generalizability", "bboxes": [{"left": 0.08811928104575163, "top": 0.7540681818181818, "width": 0.10830065359477124, "height": 0.011321969696969636, "page": 11}], "section": "Technical Approach to Accessibility", "id": "5561"}, {"text": "We evaluated StateLens across a number of touchscreen inter faces and with blind users in the lab, but we did not deeply study how StateLens works in the real world, which is often much more complicated and messier than in-lab studies.", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.40064542483660137, "height": 0.012579545454545468, "page": 12}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.3977385620915033, "height": 0.012579545454545468, "page": 12}, {"left": 0.08811928104575163, "top": 0.10913131313131313, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 12}, {"left": 0.08811928104575163, "top": 0.12296717171717171, "width": 0.36700326797385624, "height": 0.01257954545454544, "page": 12}], "section": "Limitations", "id": "5562"}, {"text": "We have presented StateLens , a reverse engineering solution that makes existing dynamic touchscreens accessible.", "bboxes": [{"left": 0.08735457516339869, "top": 0.2045378787878788, "width": 0.3979362745098039, "height": 0.01286868686868689, "page": 12}, {"left": 0.08811928104575163, "top": 0.2186641414141414, "width": 0.3426601307189543, "height": 0.012579545454545482, "page": 12}], "section": "CONCLUSION", "id": "5563"}, {"text": "This work has been supported by the National Science Foun dation (#IIS-1816012), Google, and the National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR).", "bboxes": [{"left": 0.08761437908496732, "top": 0.4665669191919192, "width": 0.4003839869281045, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.48040404040404044, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 12}, {"left": 0.08811928104575163, "top": 0.49424116161616166, "width": 0.39717483660130726, "height": 0.012579545454545371, "page": 12}, {"left": 0.08758169934640524, "top": 0.5080770202020202, "width": 0.07978921568627452, "height": 0.012579545454545427, "page": 12}], "section": "CONCLUSION", "id": "5564"}, {"text": "ACKNOWLEDGMENTS", "bboxes": [{"left": 0.08811928104575163, "top": 0.4530984848484848, "width": 0.15820098039215685, "height": 0.011321969696969747, "page": 12}], "section": "CONCLUSION", "id": "5565"}], "1602.06979": [{"text": "Language is rich in subtle signals.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6492941919191919, "width": 0.2205114379084968, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "id": "5566"}, {"text": "High quality lexicons allow us to analyze language at scale and across a broad range of signals.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.2300833333333333, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "id": "5567"}, {"text": "To build Empath, we extend a deep learning skip-gram network to capture words in a neural embedding [23].", "bboxes": [{"left": 0.08811928104575163, "top": 0.75, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.7638371212121211, "width": 0.33259150326797393, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "id": "5571"}, {"text": "Text analysis via dictionary categories has a long history in academic research.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.12651797385620922, "height": 0.012579545454545427, "page": 1}], "section": "Extracting Signal from Text", "id": "5572"}, {"text": "Empath inherits from a rich ecosystem of tools and applications for text analysis, and draws on the insights of prior work in data mining and unsupervised language modeling.", "bboxes": [{"left": 0.5246633986928104, "top": 0.7397840909090909, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7536212121212121, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7674583333333334, "width": 0.3440833333333334, "height": 0.012579545454545427, "page": 1}], "section": "RELATED WORK", "id": "5573"}, {"text": "Our evaluation validates Empath by comparing its analyses against LIWC, a lexicon of gold standard categories that have been psychometrically validated.", "bboxes": [{"left": 0.5246633986928104, "top": 0.34746843434343433, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.36130555555555555, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.37514267676767676, "width": 0.21515686274509804, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "id": "5574"}, {"text": "While Empath presents an approach that can be trained on any text corpora, in this paper we use 1.8 billion words of modern amateur ction.", "bboxes": [{"left": 0.08811928104575163, "top": 0.562570707070707, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.5764078282828282, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.5902449494949494, "width": 0.10133496732026144, "height": 0.012579545454545538, "page": 1}], "section": "INTRODUCTION", "id": "5575"}, {"text": "To address these problems, we present Empath : a living lexicon mined from modern text on the web.", "bboxes": [{"left": 0.08811928104575163, "top": 0.34717929292929295, "width": 0.39716830065359476, "height": 0.012868686868686863, "page": 1}, {"left": 0.08811928104575163, "top": 0.36130555555555555, "width": 0.30892156862745096, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "id": "5576"}, {"text": "This papers contributions include:", "bboxes": [{"left": 0.5246633986928104, "top": 0.5210606060606061, "width": 0.22620915032679734, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "id": "5577"}, {"text": "We show how the open-ended nature of Empaths model can replicate and extend classic work in classifying deceptive language [31], identifying the patterns of language in movie reviews [32], and analyzing mood on twitter [13].", "bboxes": [{"left": 0.5246633986928104, "top": 0.21538888888888888, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.22922474747474747, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.2430618686868687, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.2568989898989899, "width": 0.3074411764705882, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "id": "5578"}, {"text": "Other work in human-computer interaction has relied upon text analysis tools to build new interactive systems.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6849507575757575, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6987878787878788, "width": 0.3407303921568628, "height": 0.012579545454545427, "page": 2}], "section": "Applications for Text Analysis", "id": "5579"}, {"text": "As social computing and computational social science researchers have gained access to large textual datasets, they have increasingly adopted analyses that cover a wide range of textual signal.", "bboxes": [{"left": 0.08811928104575163, "top": 0.4975227272727273, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5113598484848485, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5251969696969697, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5390340909090909, "width": 0.09083496732026146, "height": 0.012579545454545427, "page": 2}], "section": "Applications for Text Analysis", "id": "5580"}, {"text": "To motivate the opportunities that Empath creates, we rst present three example analyses that illustrate its breadth and exibility.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6780328282828282, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.6918699494949495, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.7057070707070706, "width": 0.06609150326797386, "height": 0.012579545454545538, "page": 2}], "section": "EMPATH APPLICATIONS", "id": "5581"}, {"text": "What kinds of words accompany our lies?", "bboxes": [{"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.2762336601307189, "height": 0.012579545454545538, "page": 2}], "section": "Example 1: Understanding deception in hotel reviews", "id": "5582"}, {"text": "A large body of prior work has investigated unsupervised language modeling.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.1301977124183007, "height": 0.012579545454545538, "page": 2}], "section": "Data Mining and Modeling", "id": "5583"}, {"text": "Empath also takes inspiration from techniques for mining human patterns from data.", "bboxes": [{"left": 0.5246633986928104, "top": 0.3042411616161616, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.3180782828282828, "width": 0.15669934640522876, "height": 0.012579545454545427, "page": 2}], "section": "Data Mining and Modeling", "id": "5584"}, {"text": "Finally, Empath also benets from prior work in commonsense knowledge representation.", "bboxes": [{"left": 0.5246633986928104, "top": 0.4916691919191919, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 2}, {"left": 0.5246633986928104, "top": 0.5055063131313131, "width": 0.21481372549019606, "height": 0.012579545454545427, "page": 2}], "section": "Data Mining and Modeling", "id": "5585"}, {"text": "Work in sentiment analysis, in combination with deep learning, has developed powerful techniques to classify text across positive and negative polarity [39], but has also beneted from simpler, transparent models and rules [15].", "bboxes": [{"left": 0.08811928104575163, "top": 0.3457512626262626, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3595883838383838, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.37342550505050504, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.38726262626262625, "width": 0.3325751633986928, "height": 0.012579545454545482, "page": 2}], "section": "Extracting Signal from Text", "id": "5586"}, {"text": "The movie review dataset reveals, unsurprisingly, a strong correlation between negative reviews and negatively charged categories (Figure 5).", "bboxes": [{"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 3}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.14530392156862737, "height": 0.012579545454545427, "page": 3}], "section": "Exploring the movie dataset", "id": "5587"}, {"text": "What kinds of movies do reviewers enjoy?", "bboxes": [{"left": 0.5246633986928104, "top": 0.7088724747474747, "width": 0.28575653594771244, "height": 0.012579545454545427, "page": 3}], "section": "Example 2: Understanding language in movie reviews", "id": "5588"}, {"text": "We ran Empaths full set of categories over the truthful and deceptive reviews, and produced aggregate statistics for each.", "bboxes": [{"left": 0.08811928104575163, "top": 0.40651767676767675, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 3}, {"left": 0.08811928104575163, "top": 0.42035479797979797, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "id": "5589"}, {"text": "For exploratory research questions, Empath provides a highlevel view over many potential categories, some of which a researcher may not have thought to investigate.", "bboxes": [{"left": 0.08811928104575163, "top": 0.7952108585858586, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3195522875816994, "height": 0.012579545454545427, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "id": "5590"}, {"text": "Our results provide new evidence in support of the Ott et al. study, suggesting that deceptive reviews convey stronger sentiment across both positively and negatively charged categories, and tend towards exaggerated language (Figure 4).", "bboxes": [{"left": 0.08811928104575163, "top": 0.5109242424242424, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5247613636363636, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 3}, {"left": 0.08811928104575163, "top": 0.5385972222222223, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5524343434343435, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "id": "5591"}, {"text": "Exploring the deception dataset", "bboxes": [{"left": 0.08811928104575163, "top": 0.3930037878787879, "width": 0.20545098039215687, "height": 0.011321969696969691, "page": 3}], "section": "Example 1: Understanding deception in hotel reviews", "id": "5592"}, {"text": "While the original study provided some evidence that liars use less spatially descriptive language, it wasnt able to test the theory directly.", "bboxes": [{"left": 0.5246633986928104, "top": 0.40482954545454547, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4186666666666667, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4325037878787879, "width": 0.12611111111111117, "height": 0.012579545454545427, "page": 3}], "section": "Spatial language in lies", "id": "5593"}, {"text": "When we then add the new spatial category to our analysis, we nd it favors truthful reviews by 1.2 odds ( p < 0 . 001 ).", "bboxes": [{"left": 0.5246633986928104, "top": 0.5856792929292929, "width": 0.39717320261437905, "height": 0.012868686868686918, "page": 3}, {"left": 0.5246633986928104, "top": 0.5970883838383838, "width": 0.39717483660130715, "height": 0.0161262626262626, "page": 3}], "section": "Spatial language in lies", "id": "5594"}, {"text": "In our nal example, we use Empath to investigate the relationship between mood on twitter and time of day, replicating the work of Golder and Macy [13].", "bboxes": [{"left": 0.08811928104575163, "top": 0.8090479797979797, "width": 0.39716993464052286, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.2584248366013072, "height": 0.012579545454545538, "page": 4}], "section": "Exploring the movie dataset", "id": "5595"}, {"text": "Empath analyzes text across hundreds of topics and emotions.", "bboxes": [{"left": 0.5246633986928104, "top": 0.2880138888888889, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}], "section": "EMPATH", "id": "5596"}, {"text": "When analyzing textual data, researchers collectively engage with many possible linguistic categories.", "bboxes": [{"left": 0.5246633986928104, "top": 0.383875, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.39771212121212124, "width": 0.2726372549019608, "height": 0.012579545454545482, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "id": "5597"}, {"text": "The original paper shows a low of negative sentiment in the morning that rises over the rest of the day.", "bboxes": [{"left": 0.5246633986928104, "top": 0.0814570707070707, "width": 0.39716993464052297, "height": 0.012579545454545468, "page": 4}, {"left": 0.5246633986928104, "top": 0.09529419191919192, "width": 0.2778300653594771, "height": 0.012579545454545468, "page": 4}], "section": "Exploring the movie dataset", "id": "5598"}, {"text": "The movie dataset also allows us to demonstrate convergent validity between Empath and gold standard tools like LIWC.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6681047979797979, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.6819419191919192, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 4}], "section": "Exploring the movie dataset", "id": "5599"}, {"text": "Empath aims to make possible all of these analyses (and more) through its 200 human validated categories, which cover topics like violence , depression , or femininity .", "bboxes": [{"left": 0.5246633986928104, "top": 0.47444444444444445, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.48828156565656566, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.5018295454545455, "width": 0.34544444444444444, "height": 0.012868686868686807, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "id": "5600"}, {"text": "Example 3: Mood on Twitter and time of day", "bboxes": [{"left": 0.08811928104575163, "top": 0.7955795454545456, "width": 0.30589705882352947, "height": 0.011321969696969636, "page": 4}], "section": "Exploring the movie dataset", "id": "5601"}, {"text": "Specically, to generate Empaths category names and seed terms, we selected 200 common dependency relationships in ConceptNet, conditioned on 10,000 common words in our corpus.", "bboxes": [{"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.759989898989899, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7876641414141414, "width": 0.04746895424836595, "height": 0.012579545454545427, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "id": "5602"}, {"text": "Beyond these obviously polarized categories, we nd interesting trends in the topics associated with positive and negative reviews.", "bboxes": [{"left": 0.08811928104575163, "top": 0.43916540404040405, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.45300252525252527, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.4668396464646465, "width": 0.09200816993464052, "height": 0.012579545454545427, "page": 4}], "section": "Exploring the movie dataset", "id": "5603"}, {"text": "For emotional analyses, Empath likewise draws upon the hierarchy of emotions introduced by Parrott [36], in which emotions are dened by other emotions.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.24690032679738572, "height": 0.012579545454545427, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "id": "5604"}, {"text": "For example, when a researcher provides shirt and hat as seed words, ConceptNet tells us shirts and hats are articles of clothing .", "bboxes": [{"left": 0.5246633986928104, "top": 0.6341982323232324, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.6480353535353536, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.6615833333333334, "width": 0.0768398692810458, "height": 0.012868686868686807, "page": 4}], "section": "Designing Empath\u2019s data-driven categories", "id": "5605"}, {"text": "To train Empaths model, we adapt the deep learning skipgram architecture introduced by Mikolov et al.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6823926767676768, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.696229797979798, "width": 0.30761928104575165, "height": 0.012579545454545427, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "id": "5606"}, {"text": "We train our skip-gram network on the ction corpus from Wattpad, lemmatizing all words through a preprocessing step.", "bboxes": [{"left": 0.5246633986928104, "top": 0.23107323232323232, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.24490909090909088, "width": 0.39716993464052297, "height": 0.01257954545454551, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "id": "5607"}, {"text": "VSMs encode concepts as vectors, where each dimension of the vector v  R n conveys a feature relevant to the concept.", "bboxes": [{"left": 0.5246633986928104, "top": 0.5148611111111111, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.525979797979798, "width": 0.39717973856209154, "height": 0.015862373737373647, "page": 5}, {"left": 0.5246633986928104, "top": 0.5425340909090909, "width": 0.031189542483660126, "height": 0.012579545454545427, "page": 5}], "section": "Building categories with a vector space", "id": "5608"}, {"text": "We use the neural embeddings created by our skip-gram network to construct a vector space model (VSM).", "bboxes": [{"left": 0.5246633986928104, "top": 0.41045454545454546, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.4242916666666667, "width": 0.30719607843137264, "height": 0.012579545454545482, "page": 5}], "section": "Building categories with a vector space", "id": "5609"}, {"text": "While Empaths topical and emotional categories stem from different sources of knowledge, we generate member terms for both kinds of categories in the same way.", "bboxes": [{"left": 0.08811928104575163, "top": 0.2587462121212121, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.27258333333333334, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.28642045454545456, "width": 0.3085800653594772, "height": 0.012579545454545427, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "id": "5610"}, {"text": "Where do category terms come from?", "bboxes": [{"left": 0.08811928104575163, "top": 0.3583371212121212, "width": 0.24836437908496733, "height": 0.012579545454545482, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "id": "5611"}, {"text": "Empaths VSM selects member terms for its categories (e.g., social media, violence, shame) by using cosine similarity  a similarity measure over vector spaces  to nd nearby terms in the space.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6192664141414141, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.6331035353535354, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.6469406565656565, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 5}, {"left": 0.5246633986928104, "top": 0.6607777777777778, "width": 0.08126307189542492, "height": 0.012579545454545427, "page": 5}], "section": "Building categories with a vector space", "id": "5612"}, {"text": "We can also search the vector spaces on multiple terms by querying on the vector sum of those terms  a kind of reasoning by analogy.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8230176767676768, "width": 0.39717320261437905, "height": 0.012868686868686918, "page": 5}, {"left": 0.5246633986928104, "top": 0.8371439393939394, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.8509810606060606, "width": 0.12503758169934642, "height": 0.012579545454545538, "page": 5}], "section": "Building categories with a vector space", "id": "5613"}, {"text": "Training a neural vector space model", "bboxes": [{"left": 0.08811928104575163, "top": 0.6688787878787879, "width": 0.23860620915032682, "height": 0.011321969696969858, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "id": "5614"}, {"text": "Learning category terms from a text corpus", "bboxes": [{"left": 0.08811928104575163, "top": 0.3448686868686869, "width": 0.3024983660130719, "height": 0.011321969696969691, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "id": "5615"}, {"text": "Empath generates these category terms by querying a vector space model (VSM) trained by a neural network on a large corpus of text.", "bboxes": [{"left": 0.08811928104575163, "top": 0.49796464646464644, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.5118017676767677, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5256388888888889, "width": 0.09269117647058826, "height": 0.012579545454545538, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "id": "5616"}, {"text": "While Empath can be trained on any text corpus, for the analyses in this paper we use a dataset of modern ction from Wattpad, 1 a community of amateur writers.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5885340909090909, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6023712121212121, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6146022727272727, "width": 0.2821045751633987, "height": 0.014185606060606148, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "id": "5617"}, {"text": "As we have discussed, each of Empaths categories is dened by seed words (e.g., lust : desire, passion; clothing : shirt, hat; social media : facebook, twitter).", "bboxes": [{"left": 0.08811928104575163, "top": 0.42123232323232324, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 5}, {"left": 0.08811928104575163, "top": 0.434780303030303, "width": 0.3971732026143791, "height": 0.012868686868686918, "page": 5}, {"left": 0.08811928104575163, "top": 0.44861742424242423, "width": 0.213718954248366, "height": 0.012868686868686918, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "id": "5618"}, {"text": "Using an embedding function v that maps a word to the vector space, we can nd the eight terms nearest to v ( depressed ) , by comparing its cosine similarity with all other terms in the space, and selecting the ones that are most similar:", "bboxes": [{"left": 0.5246633986928104, "top": 0.725189393939394, "width": 0.39716830065359465, "height": 0.013145202020201863, "page": 5}, {"left": 0.5246633986928104, "top": 0.7390265151515151, "width": 0.39717483660130715, "height": 0.013409090909090926, "page": 5}, {"left": 0.5246633986928104, "top": 0.7528636363636364, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.7667007575757575, "width": 0.3295964052287582, "height": 0.012579545454545538, "page": 5}], "section": "Building categories with a vector space", "id": "5619"}, {"text": "More formally, for word w and context C in a network with negative sampling, a skip-gram network will learn weights that maximize the dot product w  w c and minimize w  w n for w c  C and w n sampled randomly from the vocabulary.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8283093434343434, "width": 0.3971813725490197, "height": 0.013145202020202085, "page": 5}, {"left": 0.08811928104575163, "top": 0.8421464646464647, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8532790404040405, "width": 0.3971748366013072, "height": 0.01675252525252524, "page": 5}, {"left": 0.08811928104575163, "top": 0.8671161616161616, "width": 0.36533660130718953, "height": 0.016751262626262697, "page": 5}], "section": "Designing Empath\u2019s data-driven categories", "id": "5620"}, {"text": "Finally, to help researchers analyze text over new kinds of categories, we have released Empath as a web service and open source library.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6740404040404041, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6878775252525252, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7017146464646465, "width": 0.09282189542483665, "height": 0.012579545454545427, "page": 6}], "section": "Empath API and web service", "id": "5621"}, {"text": "An acceptance score of 96% allows us to efciently collect validated words for Empaths categories.", "bboxes": [{"left": 0.5246633986928104, "top": 0.38680050505050506, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.4006376262626263, "width": 0.30924836601307193, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "id": "5622"}, {"text": "We divide the total number of words to be ltered across many separate tasks, where each task consists of twenty words to be rated for a given category.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5291906565656566, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.5430277777777778, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 6}, {"left": 0.08811928104575163, "top": 0.556864898989899, "width": 0.2661895424836601, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "id": "5623"}, {"text": "We limit tasks to Masters workers to ensure quality [26] and we aggregate crowdworker feedback by majority vote.", "bboxes": [{"left": 0.08811928104575163, "top": 0.7166186868686869, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.730455808080808, "width": 0.3532156862745098, "height": 0.012579545454545538, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "id": "5624"}, {"text": "How well do human workers agree with Empath?", "bboxes": [{"left": 0.5246633986928104, "top": 0.29623106060606064, "width": 0.3385490196078431, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "id": "5625"}, {"text": "Our experience conrms the ndings of prior work that category construction is somewhat subjective.", "bboxes": [{"left": 0.5246633986928104, "top": 0.5742285353535354, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5880656565656566, "width": 0.28999019607843135, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "id": "5626"}, {"text": "Prior work suggests that category construction is subjective, often resulting in low agreement among humans [41, 27].", "bboxes": [{"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "id": "5627"}, {"text": "Can ction teach computers about the emotional and topical connotations of words?", "bboxes": [{"left": 0.5246633986928104, "top": 0.8153623737373739, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.829199494949495, "width": 0.1504477124183008, "height": 0.012579545454545427, "page": 6}], "section": "EVALUATION", "id": "5628"}, {"text": "Human-validated categories can ensure that accidental terms do not slip into a lexicon.", "bboxes": [{"left": 0.08811928104575163, "top": 0.30528282828282827, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3191199494949495, "width": 0.17292973856209148, "height": 0.012579545454545482, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "id": "5629"}, {"text": "Prior work has adopted a similar question and scale [27].", "bboxes": [{"left": 0.08811928104575163, "top": 0.5078068181818182, "width": 0.37057026143790855, "height": 0.012579545454545538, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "id": "5630"}, {"text": "Where v is the embedding function, c is the name a category, and S is a set of seed words that belong to the category.", "bboxes": [{"left": 0.08811928104575163, "top": 0.1542070707070707, "width": 0.39717156862745095, "height": 0.01314520202020203, "page": 6}, {"left": 0.08811928104575163, "top": 0.1680429292929293, "width": 0.3597761437908497, "height": 0.01314520202020203, "page": 6}], "section": "Building categories with a vector space", "id": "5631"}, {"text": "From a small seed of words, Empath can gather hundreds of terms related to a given category, and then use these terms for textual analysis.", "bboxes": [{"left": 0.08811928104575163, "top": 0.23555050505050507, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.24938762626262626, "width": 0.39716993464052297, "height": 0.01257954545454551, "page": 6}, {"left": 0.08811928104575163, "top": 0.26322474747474744, "width": 0.10465686274509804, "height": 0.012579545454545482, "page": 6}], "section": "Building categories with a vector space", "id": "5632"}, {"text": "Contents and Efciency", "bboxes": [{"left": 0.08811928104575163, "top": 0.8647196969696969, "width": 0.15390849673202617, "height": 0.011321969696969858, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "id": "5633"}, {"text": "For each word, tell us how strongly it relates to the topic.", "bboxes": [{"left": 0.10439869281045752, "top": 0.4320593434343434, "width": 0.33873039215686274, "height": 0.011321969696969747, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "id": "5634"}, {"text": "For example, to generate the terms for clothing :", "bboxes": [{"left": 0.08811928104575163, "top": 0.18789267676767676, "width": 0.3105653594771242, "height": 0.012868686868686863, "page": 6}], "section": "Building categories with a vector space", "id": "5635"}, {"text": "To validate each of Empaths categories, we have created a crowdsourcing pipeline on Amazon Mechanical Turk.", "bboxes": [{"left": 0.08811928104575163, "top": 0.3820151515151515, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.3958522727272727, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 6}], "section": "Re\ufb01ning categories with crowd validation", "id": "5636"}, {"text": "Empath shares overall average Pearson correlations of 0.90 (unsupervised) and 0.906 (crowd) with LIWC (Table 3).", "bboxes": [{"left": 0.5246633986928104, "top": 0.4542878787878788, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.468125, "width": 0.36010294117647057, "height": 0.012579545454545482, "page": 7}], "section": "Results", "id": "5637"}, {"text": "Finally, to test the importance of choosing seed terms, we re-ran our evaluation while permuting the seed words in Empaths categories.", "bboxes": [{"left": 0.5246633986928104, "top": 0.3475542929292929, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.3613914141414141, "width": 0.39716993464052297, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.37522853535353534, "width": 0.11367483660130717, "height": 0.012579545454545482, "page": 7}], "section": "Method", "id": "5638"}, {"text": "We then ran all tools over the documents in the test corpus, recorded their category word counts, then used these counts to compute Pearson correlations between all shared categories, as well as aggregate overall correlations.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6077828282828283, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6216199494949495, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.6354570707070707, "width": 0.3971699346405229, "height": 0.012579545454545538, "page": 7}, {"left": 0.08811928104575163, "top": 0.6492941919191919, "width": 0.2732222222222222, "height": 0.012579545454545427, "page": 7}], "section": "Method", "id": "5639"}, {"text": "To compare all tools, we created a mixed textual dataset evenly divided among tweets [28], StackExchange opinions [5], movie reviews [32], hotel reviews [31], and chapters sampled from four classic novels on Project Gutenberg (David Coppereld, Moby Dick, Anna Karenina, and The Count of Monte Cristo) [1].", "bboxes": [{"left": 0.08811928104575163, "top": 0.3574583333333334, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.37129545454545454, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.3851325757575757, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.3989696969696969, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.41280681818181814, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.42664393939393935, "width": 0.12111437908496729, "height": 0.012579545454545482, "page": 7}], "section": "Method", "id": "5640"}, {"text": "In permuting Empaths seed terms, we found it retained high unsupervised agreement with LIWC (between 0.82 and 0.88).", "bboxes": [{"left": 0.5246633986928104, "top": 0.641715909090909, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6555530303030302, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}], "section": "Results", "id": "5641"}, {"text": "The broad reach of our dataset allows Empath to classify documents among a large number of categories.", "bboxes": [{"left": 0.08811928104575163, "top": 0.0959229797979798, "width": 0.3971699346405229, "height": 0.01257954545454544, "page": 7}, {"left": 0.08811928104575163, "top": 0.10976010101010102, "width": 0.29339215686274517, "height": 0.01257954545454544, "page": 7}], "section": "EVALUATION", "id": "5642"}, {"text": "Next we selected two parameters for Empath: the minimum cosine similarity for category inclusion and the seed words for each category (we xed the size of each category at a maximum of 200 words).", "bboxes": [{"left": 0.08811928104575163, "top": 0.461864898989899, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.4757020202020202, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.4895391414141414, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 7}, {"left": 0.08811928104575163, "top": 0.5033762626262626, "width": 0.13587908496732026, "height": 0.012579545454545427, "page": 7}], "section": "Method", "id": "5643"}, {"text": "Empath demonstrates an approach that crosses traditional text analysis metaphors with advances in deep learning.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.3359444444444445, "height": 0.012579545454545427, "page": 7}], "section": "DISCUSSION", "id": "5644"}, {"text": "Fortunately, LIWC has been extensively validated by researchers [33], so we can use it to benchmark Empaths predictions across the categories that they share in common.", "bboxes": [{"left": 0.08811928104575163, "top": 0.18649242424242424, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.20032954545454545, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.21416666666666667, "width": 0.3784003267973856, "height": 0.012579545454545454, "page": 7}], "section": "EVALUATION", "id": "5645"}, {"text": "To anchor this analysis, we collected benchmark Pearson correlations against LIWC for GI and EmoLex (two existing human validated lexicons).", "bboxes": [{"left": 0.08811928104575163, "top": 0.7398636363636364, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7537007575757576, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7675366161616162, "width": 0.16062254901960782, "height": 0.012579545454545427, "page": 7}], "section": "Method", "id": "5646"}, {"text": "Comparing Empath and LIWC", "bboxes": [{"left": 0.08811928104575163, "top": 0.08245454545454546, "width": 0.20675490196078433, "height": 0.011321969696969691, "page": 7}], "section": "EVALUATION", "id": "5647"}, {"text": "Here we compare the predictions of Empath and LIWC over 12 shared categories: sadness , anger , positive emotion , negative emotion , sexual , money , death , achievement , home , religion , work , and health .", "bboxes": [{"left": 0.08811928104575163, "top": 0.27706186868686866, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.2906098484848485, "width": 0.3971732026143791, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.3044469696969697, "width": 0.3971748366013072, "height": 0.012868686868686807, "page": 7}, {"left": 0.08811928104575163, "top": 0.3182840909090909, "width": 0.1501225490196078, "height": 0.012868686868686918, "page": 7}], "section": "EVALUATION", "id": "5648"}, {"text": "As we see in our results, this scenario does not happen often, and when it does happen the effect size is small.", "bboxes": [{"left": 0.08811928104575163, "top": 0.7128661616161617, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.7267032828282828, "width": 0.364171568627451, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "id": "5649"}, {"text": "Empath, like any data-driven system, is ultimately at the mercy of its data  garbage in, garbage out.", "bboxes": [{"left": 0.08811928104575163, "top": 0.822885101010101, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.28401470588235295, "height": 0.012579545454545538, "page": 8}], "section": "The role of human validation", "id": "5650"}, {"text": "While adding a crowd lter to Empath improves its overall correlations with LIWC, the improvement is not statistically signicant.", "bboxes": [{"left": 0.08811928104575163, "top": 0.45625252525252524, "width": 0.3971699346405229, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.47008964646464646, "width": 0.39716993464052286, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811928104575163, "top": 0.4839267676767677, "width": 0.07190359477124182, "height": 0.012579545454545482, "page": 8}], "section": "The role of human validation", "id": "5651"}, {"text": "Contrary to this critique, we have found that ction is a useful training dataset for Empath given its abundance of concrete descriptors and emotional terms.", "bboxes": [{"left": 0.5246633986928104, "top": 0.4770075757575758, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.490844696969697, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.5046818181818182, "width": 0.24756699346405242, "height": 0.012579545454545427, "page": 8}], "section": "The role of human validation", "id": "5652"}, {"text": "First, while Empath reports high Pearson correlations with LIWCs categories, it is possible that other more qualitative properties are important to lexical categories.", "bboxes": [{"left": 0.5246633986928104, "top": 0.7184785353535353, "width": 0.39716993464052297, "height": 0.012579545454545538, "page": 8}, {"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.7461527777777778, "width": 0.3006029411764706, "height": 0.012579545454545538, "page": 8}], "section": "Limitations", "id": "5653"}, {"text": "Second, we have not tested how well Empaths categories generalize beyond the core set it shares with LIWC.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.3360424836601307, "height": 0.012579545454545427, "page": 8}], "section": "Limitations", "id": "5654"}, {"text": "Data-driven: who is actually driving?", "bboxes": [{"left": 0.08811928104575163, "top": 0.8094166666666667, "width": 0.25639215686274514, "height": 0.011321969696969636, "page": 8}], "section": "The role of human validation", "id": "5655"}, {"text": "Empaths design decisions suggest a set of limitations, many of which we hope to address in future work.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6832575757575757, "width": 0.39716993464052297, "height": 0.012579545454545427, "page": 8}, {"left": 0.5246633986928104, "top": 0.6970946969696971, "width": 0.2869133986928105, "height": 0.012579545454545427, "page": 8}], "section": "Limitations", "id": "5656"}, {"text": "Social science aims to avoid Type I errors  false claims that statistically appear to be true.", "bboxes": [{"left": 0.08811928104575163, "top": 0.3245580808080808, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.338395202020202, "width": 0.1955571895424837, "height": 0.012579545454545427, "page": 9}], "section": "Statistical false positives", "id": "5657"}, {"text": "Empath aims to combine modern NLP techniques with the transparency of dictionaries like LIWC.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5462765151515152, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.5601136363636364, "width": 0.264187908496732, "height": 0.012579545454545427, "page": 9}], "section": "CONCLUSION", "id": "5658"}, {"text": "Finally, while ction provides a powerful model for generating lexical categories, we have also seen that, for certain topics (e.g. death in Google News), other corpora may have even greater potential.", "bboxes": [{"left": 0.08811928104575163, "top": 0.17202651515151515, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.18586363636363637, "width": 0.3971699346405229, "height": 0.012579545454545454, "page": 9}, {"left": 0.08811928104575163, "top": 0.19941161616161615, "width": 0.397171568627451, "height": 0.01286868686868689, "page": 9}, {"left": 0.08811928104575163, "top": 0.21353661616161618, "width": 0.1446699346405229, "height": 0.012579545454545454, "page": 9}], "section": "Limitations", "id": "5659"}, {"text": "Special thanks to our reviewers and colleagues at Stanford for their helpful feedback.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6988080808080809, "width": 0.3971699346405229, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.712645202020202, "width": 0.17174019607843138, "height": 0.012579545454545538, "page": 9}], "section": "CONCLUSION", "id": "5660"}, {"text": "ACKNOWLEDGMENTS", "bboxes": [{"left": 0.08811928104575163, "top": 0.6853396464646464, "width": 0.15820098039215685, "height": 0.011321969696969636, "page": 9}], "section": "CONCLUSION", "id": "5661"}], "uist-6": [{"text": "By exploiting the deformability of the skin, Skin-On interfaces provide novel input capabilities and haptic feedback that the users are familiar with.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6006856060606061, "width": 0.39718464052287594, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.6145227272727273, "width": 0.3971928104575164, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.6283598484848485, "width": 0.14984803921568635, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "id": "5662"}, {"text": "Skin is a fundamental biological interface to sense the world and communicate with others [33].", "bboxes": [{"left": 0.5246633986928104, "top": 0.41325757575757577, "width": 0.3971928104575164, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.42709343434343433, "width": 0.23393137254901963, "height": 0.012579545454545482, "page": 0}], "section": "INTRODUCTION", "id": "5663"}, {"text": "There is a long history of research into the design of articial skin in the eld of Robotics, either to help with environment exploration, or to endow robots with human-like sensing capabilities [5, 13, 48, 94].", "bboxes": [{"left": 0.5241601307189543, "top": 0.760439393939394, "width": 0.39769117647058827, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.7742765151515151, "width": 0.39719281045751653, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.7881136363636364, "width": 0.39990032679738563, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8019507575757575, "width": 0.1672630718954249, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "id": "5664"}, {"text": "Many researchers have explored the design of exible input although these studies did not use human skin as inspiration.", "bboxes": [{"left": 0.5246633986928104, "top": 0.7537007575757576, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.4000179738562092, "height": 0.012579545454545427, "page": 1}], "section": "Flexible sensors", "id": "5666"}, {"text": "We present an exploration of the design space of Skin-On interfaces.", "bboxes": [{"left": 0.08735457516339869, "top": 0.0814570707070707, "width": 0.4006470588235294, "height": 0.012579545454545468, "page": 1}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.05375490196078429, "height": 0.012579545454545468, "page": 1}], "section": "INTRODUCTION", "id": "5667"}, {"text": "Our work relates to on-skin interfaces in HCI, articial skin in robotics and exible input sensors.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8219835858585858, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811928104575163, "top": 0.8358207070707071, "width": 0.22629084967320262, "height": 0.012579545454545427, "page": 1}], "section": "RELATED WORK", "id": "5668"}, {"text": "In summary, designing articial skin has thus been largely studied in the eld of Robotic, but with a focus in reproducing the sensing capability of the skin [14] or its visual aspects [62] for safety, sensing or cosmetic aspects.", "bboxes": [{"left": 0.5246633986928104, "top": 0.5980883838383838, "width": 0.39773856209150327, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.611925505050505, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.6257626262626262, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 1}, {"left": 0.5246633986928104, "top": 0.6395997474747475, "width": 0.26317810457516333, "height": 0.012579545454545427, "page": 1}], "section": "Arti\ufb01cial skin", "id": "5669"}, {"text": "We assemble the insights from these three steps and present the implementation of several Skin-On interfaces and applications to demonstrate the added value of our approach (see Figure 1 for examples).", "bboxes": [{"left": 0.08735457516339869, "top": 0.5305037878787878, "width": 0.39793790849673205, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811764705882352, "top": 0.544340909090909, "width": 0.3998807189542483, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811764705882352, "top": 0.5581780303030303, "width": 0.39717483660130726, "height": 0.012579545454545538, "page": 1}, {"left": 0.08811764705882352, "top": 0.5720151515151515, "width": 0.1609624183006536, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "id": "5670"}, {"text": "In Robotics, an  articial sensitive skin  [55] imitates the sensing capabilities of human skin.", "bboxes": [{"left": 0.5246633986928104, "top": 0.3688598484848485, "width": 0.39988398692810456, "height": 0.012868686868686863, "page": 1}, {"left": 0.5246633986928104, "top": 0.3829861111111111, "width": 0.2002614379084967, "height": 0.012579545454545482, "page": 1}], "section": "Arti\ufb01cial skin", "id": "5671"}, {"text": "On-Skin or Skin-worn interfaces harness the human skin properties to create new forms of on-body interactions [50, 30, 28]", "bboxes": [{"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39988398692810456, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.39718300653594774, "height": 0.012579545454545427, "page": 1}], "section": "On-Skin interfaces", "id": "5672"}, {"text": "In summary, like On-Skin interfaces, Skin-On interfaces also aim to use the affordances of human skin.", "bboxes": [{"left": 0.5246633986928104, "top": 0.2827222222222222, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 1}, {"left": 0.5246633986928104, "top": 0.2965593434343434, "width": 0.2673464052287582, "height": 0.012579545454545427, "page": 1}], "section": "On-Skin interfaces", "id": "5673"}, {"text": "To go further in the direction of deformable material, some works use silicone or PDMS layers.", "bboxes": [{"left": 0.08761437908496732, "top": 0.0814570707070707, "width": 0.39767156862745096, "height": 0.012579545454545468, "page": 2}, {"left": 0.08753104575163399, "top": 0.09529419191919192, "width": 0.2522859477124183, "height": 0.012579545454545468, "page": 2}], "section": "Flexible sensors", "id": "5674"}, {"text": "To better understand which are the desirable properties of the human skin to reproduce within articial skin, we looked through the Biology literature [20, 36, 38] and gathered information about the visual, haptic and sensing properties of the skin (described below).", "bboxes": [{"left": 0.5241601307189543, "top": 0.2687032828282828, "width": 0.3976715686274508, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.28254040404040404, "width": 0.39717483660130715, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.29637752525252525, "width": 0.3998807189542485, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.31021464646464647, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246633986928104, "top": 0.3240517676767677, "width": 0.18175000000000008, "height": 0.012579545454545482, "page": 2}], "section": "Human Skin properties", "id": "5675"}, {"text": "These studies rely on using a relatively thin layer for sensing deformations and some other researchers have proposed to add thickness to sensors to enrich the vocabulary of gestures.", "bboxes": [{"left": 0.08761437908496732, "top": 0.2273737373737374, "width": 0.3976862745098039, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.24121085858585856, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.2550479797979798, "width": 0.4000179738562092, "height": 0.012579545454545482, "page": 2}], "section": "Flexible sensors", "id": "5676"}, {"text": "The skin is about 1.7 m 2 in area and approximately 4 kg in weight, thus accounting for about 5.5% of body mass.", "bboxes": [{"left": 0.08761437908496732, "top": 0.8271729797979798, "width": 0.39767647058823524, "height": 0.014580808080808083, "page": 2}, {"left": 0.08753594771241831, "top": 0.8430113636363636, "width": 0.36424673202614377, "height": 0.012579545454545538, "page": 2}], "section": "Human skin overview", "id": "5677"}, {"text": "Skin-On interfaces augment interactive systems (e.g. smartphones) with an articial skin.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5294318181818182, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.5432689393939394, "width": 0.19303431372549024, "height": 0.012579545454545427, "page": 2}], "section": "Flexible sensors", "id": "5678"}, {"text": "In summary, there have been some research aiming at creating deformable sensors, but none has looked at the skin for inspiration; moreover the gestures these sensors can detect are limited to particular ones (e.g. bending but no stretching, stretching with no pressure, or pressure deformation but no stretching etc.).", "bboxes": [{"left": 0.08811928104575163, "top": 0.37329166666666663, "width": 0.3998807189542484, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.38712878787878785, "width": 0.3974558823529412, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.40096590909090907, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4148030303030303, "width": 0.39988071895424837, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4286401515151515, "width": 0.3971764705882353, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4424772727272727, "width": 0.1274542483660131, "height": 0.012579545454545427, "page": 2}], "section": "Flexible sensors", "id": "5679"}, {"text": "SKIN-ON INTERFACES: A BIO-DRIVEN APPROACH", "bboxes": [{"left": 0.08811928104575163, "top": 0.5159633838383838, "width": 0.353952614379085, "height": 0.011321969696969747, "page": 2}], "section": "Flexible sensors", "id": "5680"}, {"text": "The results are illustrated on Figure 4-top.", "bboxes": [{"left": 0.5241601307189543, "top": 0.7398636363636364, "width": 0.2848120915032679, "height": 0.012579545454545427, "page": 3}], "section": "Results of study 1", "id": "5681"}, {"text": "Our exploration into simulating human skin properties starts with the replication of its sensory properties.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6453472222222222, "width": 0.39717647058823524, "height": 0.012579545454545427, "page": 3}, {"left": 0.08753267973856209, "top": 0.6591830808080807, "width": 0.29792973856209143, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "id": "5682"}, {"text": "While Silicone is the best available approximation of human skin, it is unclear, in an interaction context, whether similarity to human skin is the most important factor.", "bboxes": [{"left": 0.08735457516339869, "top": 0.4262967171717172, "width": 0.39793464052287586, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.4401338383838384, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.4539709595959596, "width": 0.3081258169934641, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "id": "5683"}, {"text": "Figure 3 illustrates the ve types of pigmentation compared: beige and brown colors representative of realistic human skin colors; white and black colors representative of usual device colors; green color to suggest something organic, but not necessarily human (e.g. alien or reptilian).", "bboxes": [{"left": 0.5246633986928104, "top": 0.22334722222222222, "width": 0.3994411764705883, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.23718434343434341, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 3}, {"left": 0.5246633986928104, "top": 0.25102146464646463, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.26485858585858585, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.27869570707070707, "width": 0.27584313725490206, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "id": "5684"}, {"text": "Moving on to reproducing the properties of the skin described above, we looked at common material used in other elds of research.", "bboxes": [{"left": 0.08811928104575163, "top": 0.14829924242424242, "width": 0.39716830065359476, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.16213636363636363, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 3}, {"left": 0.08811928104575163, "top": 0.17597348484848485, "width": 0.0794019607843137, "height": 0.012579545454545454, "page": 3}], "section": "Design choices regarding Skin-On interface material", "id": "5685"}, {"text": "We recruited 15 participants (10 males, mean age 21) from our university to test each sample.", "bboxes": [{"left": 0.5238986928104574, "top": 0.31520580808080806, "width": 0.39822549019607856, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.3290429292929293, "width": 0.20200653594771234, "height": 0.012579545454545482, "page": 3}], "section": "Participants and experimental design", "id": "5686"}, {"text": "We use different silicone products from Smooth-On Inc to reproduce the skin properties listed above.", "bboxes": [{"left": 0.08735457516339869, "top": 0.2942171717171717, "width": 0.3979379084967321, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811764705882352, "top": 0.3080542929292929, "width": 0.2819901960784314, "height": 0.012579545454545427, "page": 3}], "section": "Design choices regarding Skin-On interface material", "id": "5687"}, {"text": "Our rst experiment aims at understanding the impact of pigmentation on the perception of skin human-likeness and comfort, but also at detecting possible negative anthropomorphic effects.", "bboxes": [{"left": 0.08811928104575163, "top": 0.850270202020202, "width": 0.39987091503267974, "height": 0.012868686868686918, "page": 3}, {"left": 0.08811928104575163, "top": 0.8641073232323233, "width": 0.3998660130718954, "height": 0.012868686868686807, "page": 3}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.39718137254901964, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.04704575163398693, "height": 0.012579545454545427, "page": 3}], "section": "SENSORY INSPIRATION", "id": "5688"}, {"text": "Study 1: Replicating pigmentation", "bboxes": [{"left": 0.08811928104575163, "top": 0.8370909090909091, "width": 0.2376388888888889, "height": 0.011321969696969636, "page": 3}], "section": "SENSORY INSPIRATION", "id": "5689"}, {"text": "We study the impact of the strain/thickness on easiness and comfort of interaction, as well as human-likeness.", "bboxes": [{"left": 0.5238986928104574, "top": 0.3674179292929293, "width": 0.397921568627451, "height": 0.012868686868686863, "page": 4}, {"left": 0.5246633986928104, "top": 0.3815441919191919, "width": 0.324452614379085, "height": 0.012579545454545482, "page": 4}], "section": "Study 3: Replicating thickness", "id": "5690"}, {"text": "Non-parametric Friedman tests were conducted followed by post hoc comparison tests for the questions asked.", "bboxes": [{"left": 0.5246633986928104, "top": 0.17259722222222224, "width": 0.39773856209150327, "height": 0.012579545454545454, "page": 4}, {"left": 0.5246633986928104, "top": 0.18643434343434345, "width": 0.3305147058823531, "height": 0.012579545454545454, "page": 4}], "section": "Participants and experimental design", "id": "5691"}, {"text": "Figure 5 illustrates the four samples of texture we compared.", "bboxes": [{"left": 0.08811928104575163, "top": 0.44551641414141413, "width": 0.4000261437908497, "height": 0.012579545454545482, "page": 4}], "section": "Samples", "id": "5692"}, {"text": "We study different surface textures to mimic wrinkles of different body locations.", "bboxes": [{"left": 0.08735457516339869, "top": 0.3784709595959596, "width": 0.4006388888888889, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.39230808080808083, "width": 0.1418872549019608, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 1", "id": "5693"}, {"text": "Non-parametric Friedman tests were conducted followed by post hoc comparison tests for all the questions asked and found a main effect on the look alike question (chi-square = 7.4, p<0.05).", "bboxes": [{"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.39773856209150327, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.822885101010101, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8364330808080809, "width": 0.3971895424836601, "height": 0.012868686868686807, "page": 4}, {"left": 0.5242565359477125, "top": 0.8505593434343435, "width": 0.08346732026143788, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 3", "id": "5694"}, {"text": "The design was similar to study 1.", "bboxes": [{"left": 0.08761437908496732, "top": 0.5817474747474748, "width": 0.2147467320261438, "height": 0.012579545454545427, "page": 4}], "section": "Participants and experimental design", "id": "5695"}, {"text": "We expected that the black and white colors would be perceived as more interactive because of their similarity with devices, but natural skin pigmentation was associated to a higher degree of interactivity.", "bboxes": [{"left": 0.08735457516339869, "top": 0.26375126262626264, "width": 0.4006437908496732, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811764705882352, "top": 0.27758838383838386, "width": 0.39717483660130726, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811764705882352, "top": 0.2914255050505051, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.3052626262626263, "width": 0.19246568627450977, "height": 0.012579545454545427, "page": 4}], "section": "Results of study 1", "id": "5696"}, {"text": "We used a similar design than previous studies.", "bboxes": [{"left": 0.5238986928104574, "top": 0.5259633838383838, "width": 0.3092042483660131, "height": 0.012579545454545538, "page": 4}], "section": "Experimental design", "id": "5697"}, {"text": "Figure 7 illustrates the four different skin thicknesses we compared.", "bboxes": [{"left": 0.5246633986928104, "top": 0.4191616161616162, "width": 0.399874183006536, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4329987373737374, "width": 0.040787581699346376, "height": 0.012579545454545482, "page": 4}], "section": "Samples", "id": "5698"}, {"text": "Study 2: Replicating texture", "bboxes": [{"left": 0.08811928104575163, "top": 0.36500252525252525, "width": 0.1942859477124183, "height": 0.011321969696969691, "page": 4}], "section": "Results of study 1", "id": "5699"}, {"text": "Results of study 2", "bboxes": [{"left": 0.5246633986928104, "top": 0.15908333333333333, "width": 0.11644607843137256, "height": 0.01132196969696972, "page": 4}], "section": "Participants and experimental design", "id": "5700"}, {"text": "We also had the opportunity to observe that users spontaneously performed these gestures during the studies.", "bboxes": [{"left": 0.08735457516339869, "top": 0.822885101010101, "width": 0.4006437908496732, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8367222222222221, "width": 0.3463807189542484, "height": 0.012579545454545538, "page": 5}], "section": "INTERACTION INSPIRATION", "id": "5701"}, {"text": "The last part of our exploration focuses on the reproduction of the human skin sensing acuity.", "bboxes": [{"left": 0.5241601307189543, "top": 0.2807828282828283, "width": 0.3976830065359477, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.2946199494949495, "width": 0.2032189542483661, "height": 0.012579545454545427, "page": 5}], "section": "SENSING INSPIRATION", "id": "5702"}, {"text": "Our sensory exploration let us to form a series of guidelines for mimicking human skin for an interactive setup: for the pigmentation using a skin-like color; for the texture using a realistic skin pore and wrinkle structure; for the thickness , using a fat layer of 5mm to 10mm and a dermis of 1.2mm.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5375795454545454, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.5514166666666667, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.5649646464646465, "width": 0.3971895424836602, "height": 0.012868686868686918, "page": 5}, {"left": 0.08813562091503269, "top": 0.5788017676767677, "width": 0.39919934640522875, "height": 0.012868686868686807, "page": 5}, {"left": 0.08811928104575163, "top": 0.5929267676767677, "width": 0.38054901960784315, "height": 0.012579545454545427, "page": 5}], "section": "Sensory exploration results", "id": "5703"}, {"text": "We found that the human skin affords two main types of gestures: gestures of mediated communication between individuals from the social literature [33, 32] and traditional 2D multi-touch gestures, for interface control though extracted from On-Skin literature [100].", "bboxes": [{"left": 0.08735457516339869, "top": 0.7046414141414141, "width": 0.4006470588235294, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7184785353535353, "width": 0.39987418300653593, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.7323156565656566, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.7461527777777778, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.759989898989899, "width": 0.20301960784313722, "height": 0.012579545454545427, "page": 5}], "section": "INTERACTION INSPIRATION", "id": "5704"}, {"text": "Our results suggest that users tend to transpose the interactions they are doing with real skin to articial skin, and that articial skin leverages the expressive gestures and tactile expressions of pro-social emotions.", "bboxes": [{"left": 0.5246633986928104, "top": 0.1305151515151515, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.14435227272727272, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.15818939393939394, "width": 0.39717973856209154, "height": 0.012579545454545454, "page": 5}, {"left": 0.5246633986928104, "top": 0.17202651515151515, "width": 0.1579232026143791, "height": 0.012579545454545454, "page": 5}], "section": "INTERACTION INSPIRATION", "id": "5705"}, {"text": "Discrete or matrix .", "bboxes": [{"left": 0.5240784313725491, "top": 0.787375, "width": 0.1273872549019608, "height": 0.012868686868686807, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "id": "5706"}, {"text": "Skin has a wide range of mechanoreceptors used conjointly to detect touch and deformations.", "bboxes": [{"left": 0.5246633986928104, "top": 0.40337626262626264, "width": 0.39773856209150327, "height": 0.012579545454545482, "page": 5}, {"left": 0.5246633986928104, "top": 0.41721338383838386, "width": 0.22492156862745105, "height": 0.012579545454545482, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "id": "5707"}, {"text": "We choose to implement our sensor using a matrix layout sensing mutual capacitance.", "bboxes": [{"left": 0.5238986928104574, "top": 0.6832575757575757, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.6970946969696971, "width": 0.1786928104575164, "height": 0.012579545454545427, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "id": "5708"}, {"text": "Our next step in the design of the articial skin was to identify the types of gestures which are desirable for Skin-On interfaces.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6417462121212121, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6555833333333333, "width": 0.3998807189542484, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6694204545454546, "width": 0.03808986928104573, "height": 0.012579545454545427, "page": 5}], "section": "INTERACTION INSPIRATION", "id": "5709"}, {"text": "Resistive or capacitive .", "bboxes": [{"left": 0.5241601307189543, "top": 0.8779444444444444, "width": 0.15626307189542477, "height": 0.012868686868686918, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "id": "5710"}, {"text": "Conductive threads .", "bboxes": [{"left": 0.5241274509803922, "top": 0.3546666666666667, "width": 0.13369771241830064, "height": 0.012868686868686863, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "id": "5711"}, {"text": "To implement the electrode pattern described above, we need a conductive material that ts our requirements.", "bboxes": [{"left": 0.08761437908496732, "top": 0.41280681818181814, "width": 0.3976748366013072, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.42664393939393935, "width": 0.3088137254901961, "height": 0.012579545454545482, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "id": "5712"}, {"text": "Conductive silicone .", "bboxes": [{"left": 0.08758169934640524, "top": 0.6074936868686869, "width": 0.13560620915032684, "height": 0.012868686868686918, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "id": "5713"}, {"text": "Conductive fabric.", "bboxes": [{"left": 0.08758169934640524, "top": 0.850270202020202, "width": 0.11944281045751633, "height": 0.012579545454545427, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "id": "5714"}, {"text": "Choosing the electrodes material", "bboxes": [{"left": 0.08811928104575163, "top": 0.3992929292929293, "width": 0.2143725490196079, "height": 0.011321969696969691, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "id": "5715"}, {"text": "Conductive ink .", "bboxes": [{"left": 0.08758169934640524, "top": 0.5307613636363636, "width": 0.10638398692810454, "height": 0.012868686868686807, "page": 6}], "section": "Implementing arti\ufb01cial mechanoreceptors", "id": "5716"}, {"text": "We now present the steps needed to fabricate our articial skin (Figure 10).", "bboxes": [{"left": 0.5238986928104574, "top": 0.6065252525252525, "width": 0.39793790849673216, "height": 0.012579545454545427, "page": 6}, {"left": 0.5241274509803922, "top": 0.6203623737373737, "width": 0.08014869281045756, "height": 0.012579545454545538, "page": 6}], "section": "Skin-On Fabrication Process", "id": "5717"}, {"text": "We developed an Open Source and Open Hardware multitouch controller 1 with a total cost of $4.", "bboxes": [{"left": 0.08735457516339869, "top": 0.5497083333333334, "width": 0.40064379084967316, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811764705882352, "top": 0.5615429292929294, "width": 0.2728921568627451, "height": 0.014582070707070627, "page": 7}], "section": "Hardware Platform", "id": "5718"}, {"text": "The process pipeline relies on OpenCV to convert the mutual capacitance readings to touch coordinates.", "bboxes": [{"left": 0.08761437908496732, "top": 0.7832828282828282, "width": 0.3976748366013072, "height": 0.012579545454545427, "page": 7}, {"left": 0.08811928104575163, "top": 0.7971199494949495, "width": 0.2881666666666667, "height": 0.012579545454545538, "page": 7}], "section": "Data processing", "id": "5719"}, {"text": "To minimize the background noise, we perform an initial calibration.", "bboxes": [{"left": 0.5241601307189543, "top": 0.3222373737373737, "width": 0.3976715686274508, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.33607449494949493, "width": 0.07222058823529409, "height": 0.012579545454545482, "page": 7}], "section": "Data processing", "id": "5720"}, {"text": "The nal step is to detect users gestures.", "bboxes": [{"left": 0.08761437908496732, "top": 0.4683396464646465, "width": 0.2708218954248366, "height": 0.012579545454545427, "page": 7}], "section": "Open-toolkit for touch and gestures detection", "id": "5721"}, {"text": "Advanced gesture detection.", "bboxes": [{"left": 0.523671568627451, "top": 0.781084595959596, "width": 0.1877777777777777, "height": 0.012579545454545427, "page": 7}], "section": "Data processing", "id": "5722"}, {"text": "To support accurate spacial interpolation, we upscale the image 5x using the Lanczos-4 algorithm (Figure 12-c).", "bboxes": [{"left": 0.5241601307189543, "top": 0.44048106060606057, "width": 0.3976830065359477, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4543181818181818, "width": 0.33054084967320263, "height": 0.012579545454545482, "page": 7}], "section": "Data processing", "id": "5723"}, {"text": "Multi-touch point detection.", "bboxes": [{"left": 0.5239803921568628, "top": 0.6766780303030303, "width": 0.18075490196078436, "height": 0.012579545454545538, "page": 7}], "section": "Data processing", "id": "5724"}, {"text": "Touch accuracy.", "bboxes": [{"left": 0.5237516339869281, "top": 0.5861085858585858, "width": 0.10781209150326798, "height": 0.012579545454545538, "page": 7}], "section": "Data processing", "id": "5725"}, {"text": "To improve the visual appearance of the interface, the excess of silicone can be trimmed before being folded around the side of the hypodermis layer and glued with silicone glue (Figure 10-5).", "bboxes": [{"left": 0.2616437908496732, "top": 0.17957323232323233, "width": 0.22364215686274508, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.19341035353535355, "width": 0.39716993464052297, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.20724747474747474, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 7}, {"left": 0.08753267973856209, "top": 0.22108459595959595, "width": 0.22082026143790845, "height": 0.012579545454545482, "page": 7}], "section": "Skin-On Fabrication Process", "id": "5726"}, {"text": "The electrodes are then connected, i.e. they are soldered to the hardware sensing platform (Figure 10-4).", "bboxes": [{"left": 0.29920424836601306, "top": 0.1305151515151515, "width": 0.18879084967320264, "height": 0.012579545454545454, "page": 7}, {"left": 0.08811928104575163, "top": 0.14435227272727272, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 7}, {"left": 0.08758169934640524, "top": 0.15818939393939394, "width": 0.0913235294117647, "height": 0.012579545454545454, "page": 7}], "section": "Skin-On Fabrication Process", "id": "5727"}, {"text": "Gesture detection pilot study.", "bboxes": [{"left": 0.08753267973856209, "top": 0.19941161616161615, "width": 0.18665359477124183, "height": 0.012579545454545454, "page": 8}], "section": "Data processing", "id": "5728"}, {"text": "We rst describe the implementation of three Skin-on interface prototypes with different form factors shown in Figure 1.", "bboxes": [{"left": 0.08735457516339869, "top": 0.3809684343434343, "width": 0.4006437908496732, "height": 0.012579545454545482, "page": 8}, {"left": 0.08811764705882352, "top": 0.3948055555555555, "width": 0.39717483660130726, "height": 0.012579545454545482, "page": 8}, {"left": 0.08688888888888889, "top": 0.40864267676767674, "width": 0.012454248366013068, "height": 0.012579545454545482, "page": 8}], "section": "USE CASES", "id": "5729"}, {"text": "We also fabricated a Skin-On wristband to alleviate the limited input and output capabilities of smartwatches [65] (Figure 1-c).", "bboxes": [{"left": 0.08735457516339869, "top": 0.7222474747474747, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 8}, {"left": 0.08812091503267974, "top": 0.736084595959596, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 8}], "section": "Skin-On Touchpads", "id": "5730"}, {"text": "Skin-On interfaces leverage physical interaction by providing haptic feedback in line with gesture input.", "bboxes": [{"left": 0.5246633986928104, "top": 0.42550252525252524, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 8}, {"left": 0.5246633986928104, "top": 0.43933964646464646, "width": 0.2703774509803921, "height": 0.012579545454545482, "page": 8}], "section": "Leveraging physical interaction", "id": "5731"}, {"text": "Skin-On interfaces allow users to perform advanced gestures with a higher degree of control.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6034381313131313, "width": 0.39717483660130715, "height": 0.012579545454545538, "page": 8}, {"left": 0.5240784313725491, "top": 0.6172752525252525, "width": 0.2066960784313725, "height": 0.012579545454545427, "page": 8}], "section": "Increasing the degree of control", "id": "5732"}, {"text": "We built a Skin-On smartphone case (Figure ?? -bottom) providing advanced input and output capabilities on the back and side of a mobile device [47, 11, 80].", "bboxes": [{"left": 0.08735457516339869, "top": 0.4947260101010101, "width": 0.40063725490196084, "height": 0.01266666666666677, "page": 8}, {"left": 0.0877124183006536, "top": 0.5086502525252525, "width": 0.3975816993464052, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.5224873737373738, "width": 0.2440996732026144, "height": 0.012579545454545538, "page": 8}], "section": "USE CASES", "id": "5733"}, {"text": "Skin-on interfaces allow a wide range of interactions.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8090479797979797, "width": 0.36487908496732024, "height": 0.012579545454545538, "page": 8}], "section": "Increasing input bandwidth", "id": "5734"}, {"text": "Skin-On devices form factors", "bboxes": [{"left": 0.08811928104575163, "top": 0.4657449494949495, "width": 0.20281372549019605, "height": 0.011321969696969691, "page": 8}], "section": "USE CASES", "id": "5735"}, {"text": "We also built a Skin-On interface for built-in and external touchpads.", "bboxes": [{"left": 0.08735457516339869, "top": 0.6292866161616162, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 8}, {"left": 0.08811928104575163, "top": 0.6431224747474747, "width": 0.06958986928104575, "height": 0.012579545454545538, "page": 8}], "section": "Skin-On Touchpads", "id": "5736"}, {"text": "Skin-On interfaces provide natural physical affordances.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.3667990196078432, "height": 0.012579545454545427, "page": 8}], "section": "Communicating interaction", "id": "5737"}, {"text": "Skin-On smartphones", "bboxes": [{"left": 0.08811928104575163, "top": 0.48130050505050503, "width": 0.14226143790849677, "height": 0.011321969696969691, "page": 8}], "section": "USE CASES", "id": "5738"}, {"text": "Skin-On Wristband", "bboxes": [{"left": 0.08811928104575163, "top": 0.7087335858585858, "width": 0.12315686274509803, "height": 0.011321969696969636, "page": 8}], "section": "Skin-On Touchpads", "id": "5739"}, {"text": "Virtual agent embodiment .", "bboxes": [{"left": 0.08712581699346406, "top": 0.6315757575757576, "width": 0.17330228758169935, "height": 0.012868686868686918, "page": 9}], "section": "Applications for emotional communication", "id": "5740"}, {"text": "Technical evaluations .", "bboxes": [{"left": 0.5237516339869281, "top": 0.08116792929292929, "width": 0.1495375816993464, "height": 0.012868686868686877, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "id": "5741"}, {"text": "Additional Skin-On interfaces form factors .", "bboxes": [{"left": 0.523671568627451, "top": 0.3792916666666667, "width": 0.29127124183006536, "height": 0.012868686868686863, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "id": "5742"}, {"text": "Skin-on interfaces can also serve for improving small mobile devices such as smartwatches or connected objects.", "bboxes": [{"left": 0.08811928104575163, "top": 0.11667929292929292, "width": 0.3971699346405229, "height": 0.012578282828282841, "page": 9}, {"left": 0.08811928104575163, "top": 0.1305151515151515, "width": 0.34594934640522873, "height": 0.012579545454545454, "page": 9}], "section": "Increasing input bandwidth", "id": "5743"}, {"text": "While our paper focuses on common interactive systems (PC, smartphones, smartwatches), Skin-On interfaces could also be useful in a wide range of setups, including robots and connected objects, or for extending the capabilities of everyday life objects.", "bboxes": [{"left": 0.5238986928104574, "top": 0.5531729797979797, "width": 0.3999722222222223, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.567010101010101, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.5808472222222222, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 9}, {"left": 0.5246633986928104, "top": 0.5946830808080807, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 9}, {"left": 0.5246633986928104, "top": 0.608520202020202, "width": 0.0785539215686275, "height": 0.012579545454545538, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "id": "5744"}, {"text": "Mobile tactile expression.", "bboxes": [{"left": 0.08743627450980393, "top": 0.48565909090909093, "width": 0.17462745098039215, "height": 0.012579545454545482, "page": 9}], "section": "Applications for emotional communication", "id": "5745"}, {"text": "We now discuss future directions regarding the implementation, the concept and the approach.", "bboxes": [{"left": 0.08735457516339869, "top": 0.8782335858585859, "width": 0.40064379084967316, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.892070707070707, "width": 0.22739869281045755, "height": 0.012579545454545427, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "id": "5746"}, {"text": "Skin-On interfaces with output abilities .", "bboxes": [{"left": 0.5242565359477125, "top": 0.6849633838383838, "width": 0.27307352941176466, "height": 0.012868686868686918, "page": 9}], "section": "DISCUSSION AND FUTURE WORK", "id": "5747"}, {"text": "Touch gestures on Skin-On can convey expressive messages for computer mediated communication with humans or virtual characters.", "bboxes": [{"left": 0.08761437908496732, "top": 0.43688888888888894, "width": 0.39768137254901953, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.45072601010101015, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 9}, {"left": 0.08811928104575163, "top": 0.46456313131313137, "width": 0.07004738562091505, "height": 0.012579545454545427, "page": 9}], "section": "Applications for emotional communication", "id": "5748"}, {"text": "Pie menu whose location depends on the handedness of the phone grasp.", "bboxes": [{"left": 0.08811928104575163, "top": 0.0814570707070707, "width": 0.39717320261437916, "height": 0.012579545454545468, "page": 9}, {"left": 0.08811928104575163, "top": 0.09529419191919192, "width": 0.08318464052287582, "height": 0.012579545454545468, "page": 9}], "section": "Increasing input bandwidth", "id": "5749"}, {"text": "Uncanny Valley .", "bboxes": [{"left": 0.08753267973856209, "top": 0.18557449494949493, "width": 0.10515686274509804, "height": 0.01286868686868689, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "id": "5750"}, {"text": "This work was supported by the Engineering and Physical Sciences Research Council (grant number EPSRC EP/P004342/1, EP/M021882/1, EP/R02961X/1) and the Agence Nationale de la Recherche (ANR-17-CE33-0006 SocialTouch, ANR-16CE330023 GESTURE).", "bboxes": [{"left": 0.08761437908496732, "top": 0.822885101010101, "width": 0.4003790849673202, "height": 0.012579545454545427, "page": 10}, {"left": 0.08812418300653595, "top": 0.8367222222222221, "width": 0.3992156862745098, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 10}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.39988071895424837, "height": 0.012579545454545538, "page": 10}, {"left": 0.08811928104575163, "top": 0.8782335858585859, "width": 0.16343464052287582, "height": 0.012579545454545427, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "id": "5751"}, {"text": "Anthropomorphism and attachment to machines .", "bboxes": [{"left": 0.08712581699346406, "top": 0.4283510101010101, "width": 0.30740359477124185, "height": 0.012868686868686863, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "id": "5752"}, {"text": "Bio-driven approach .", "bboxes": [{"left": 0.08761437908496732, "top": 0.588104797979798, "width": 0.14344444444444443, "height": 0.012868686868686918, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "id": "5753"}, {"text": "ACKNOWLEDGEMENT", "bboxes": [{"left": 0.08811928104575163, "top": 0.8094166666666667, "width": 0.15820098039215685, "height": 0.011321969696969636, "page": 10}], "section": "DISCUSSION AND FUTURE WORK", "id": "5754"}, {"text": "We motivate our use of silicone to mimic the skin deformability with reference to relevant literature.", "bboxes": [{"left": 0.10363235294117647, "top": 0.268885101010101, "width": 0.38436764705882354, "height": 0.012579545454545427, "page": 1}, {"left": 0.10439869281045752, "top": 0.2827222222222222, "width": 0.2793333333333333, "height": 0.012579545454545427, "page": 1}], "section": "INTRODUCTION", "id": "5755"}, {"text": "Choosing an electrode pattern", "bboxes": [{"left": 0.5246633986928104, "top": 0.6697436868686869, "width": 0.19663071895424833, "height": 0.011321969696969636, "page": 5}], "section": "Implementing arti\ufb01cial mechanoreceptors", "id": "5756"}, {"text": "We prepare a rectangular mold of the size of the desired articial skin and place it on top of", "bboxes": [{"left": 0.7167287581699346, "top": 0.8782335858585859, "width": 0.20511274509803923, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.39717483660130715, "height": 0.012579545454545427, "page": 6}], "section": "Skin-On Fabrication Process", "id": "5757"}, {"text": "Once cured, the top layer is positioned on a pane, with the texture facing down.", "bboxes": [{"left": 0.7455882352941177, "top": 0.7184785353535353, "width": 0.17651960784313736, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246683006535947, "top": 0.7323156565656566, "width": 0.36330065359477137, "height": 0.012579545454545427, "page": 6}], "section": "Skin-On Fabrication Process", "id": "5758"}, {"text": "The epidermis layer is built by pouring DragonSkin silicone with beige pigments on a skin-like texture mold (Figure 10-1).", "bboxes": [{"left": 0.772016339869281, "top": 0.6555833333333333, "width": 0.1498202614379086, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6694204545454546, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6832575757575757, "width": 0.2502924836601308, "height": 0.012579545454545427, "page": 6}], "section": "Skin-On Fabrication Process", "id": "5759"}], "uist-7": [{"text": "Text entry is an important task for communication and productivity in augmented reality and virtual reality (AR/VR).", "bboxes": [{"left": 0.5241601307189543, "top": 0.6354570707070707, "width": 0.4003774509803921, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246584967320261, "top": 0.6492941919191919, "width": 0.4000130718954249, "height": 0.012579545454545427, "page": 0}], "section": "INTRODUCTION", "id": "5760"}, {"text": "In this paper, we investigate the use of hand-tracking to enable typing on any at surface at speeds comparable to typing on a physical keyboard.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 0}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.39716830065359465, "height": 0.012579545454545427, "page": 0}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.1359052287581699, "height": 0.012579545454545538, "page": 0}], "section": "INTRODUCTION", "id": "5761"}, {"text": "Facebook Reality Labs", "bboxes": [{"left": 0.4183333333333333, "top": 0.14791035353535353, "width": 0.17983496732026155, "height": 0.01509469696969698, "page": 0}], "section": "INTRODUCTION", "id": "5762"}, {"text": "Facebook Reality Labs", "bboxes": [{"left": 0.6861535947712418, "top": 0.14791035353535353, "width": 0.17983496732026139, "height": 0.01509469696969698, "page": 0}], "section": "INTRODUCTION", "id": "5763"}, {"text": "Facebook Reality Labs", "bboxes": [{"left": 0.15051307189542484, "top": 0.14791035353535353, "width": 0.17983660130718954, "height": 0.01509469696969698, "page": 0}], "section": "INTRODUCTION", "id": "5765"}, {"text": "Typing on a at surface is analogous to typing on a soft keyboard on a smartphone and inherits similar problems such as noisy input and systematic offsets [14] which can be addressed via statistical decoding methods [7, 10, 25] (see below).", "bboxes": [{"left": 0.5241601307189543, "top": 0.11140025252525253, "width": 0.4003774509803921, "height": 0.01257954545454544, "page": 1}, {"left": 0.5246633986928104, "top": 0.12523737373737373, "width": 0.39717810457516345, "height": 0.012579545454545454, "page": 1}, {"left": 0.5246633986928104, "top": 0.13907449494949495, "width": 0.39717320261437905, "height": 0.012579545454545454, "page": 1}, {"left": 0.5242483660130719, "top": 0.15291161616161617, "width": 0.3532369281045751, "height": 0.012579545454545454, "page": 1}], "section": "Surface and eyes-free keyboards", "id": "5766"}, {"text": "Many other text entry systems have been proposed for AR/VR applications, and for brevity, we concentrate on those that use a QWERTY keyboard layout.", "bboxes": [{"left": 0.5246633986928104, "top": 0.34260984848484843, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.35644696969696965, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 1}, {"left": 0.5246633986928104, "top": 0.37028409090909087, "width": 0.17770424836601306, "height": 0.012579545454545482, "page": 1}], "section": "AR/VR text entry", "id": "5767"}, {"text": "People type faster on a physical keyboard [3] than a soft keyboard and also faster on a surface [4] than in the air.", "bboxes": [{"left": 0.08811928104575163, "top": 0.1997007575757576, "width": 0.39987418300653593, "height": 0.012579545454545454, "page": 1}, {"left": 0.08811928104575163, "top": 0.21353661616161618, "width": 0.35005065359477133, "height": 0.012579545454545454, "page": 1}], "section": "INTRODUCTION", "id": "5768"}, {"text": "We are inspired by the work of Dudley and colleagues [4] that shows the high potential of human typing efciency and error rate given a text decoding oracle with knowledge of the text being typed.", "bboxes": [{"left": 0.08735457516339869, "top": 0.4286401515151515, "width": 0.3979428104575164, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4424772727272727, "width": 0.3974493464052288, "height": 0.012579545454545427, "page": 1}, {"left": 0.08811928104575163, "top": 0.4563131313131313, "width": 0.3998839869281046, "height": 0.012579545454545482, "page": 1}, {"left": 0.08811928104575163, "top": 0.4701502525252525, "width": 0.06425163398692811, "height": 0.012579545454545482, "page": 1}], "section": "INTRODUCTION", "id": "5769"}, {"text": "To evaluate the feasibility of using hand-tracking to enable touch typing on virtual keyboards, we rst collect a dataset of skeletal hand-tracking data from touch-typists transcribing short phrases while typing on a at surface.", "bboxes": [{"left": 0.08761437908496732, "top": 0.30997474747474746, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811274509803921, "top": 0.3238118686868687, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.3376489898989899, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.3514861111111111, "width": 0.27779411764705886, "height": 0.012579545454545427, "page": 2}], "section": "APPROACH", "id": "5770"}, {"text": "Statistical decoding of typing is analogous to text decoding in automatic speech recognition.", "bboxes": [{"left": 0.08811928104575163, "top": 0.1455050505050505, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 2}, {"left": 0.08811928104575163, "top": 0.15934217171717172, "width": 0.21118464052287583, "height": 0.012579545454545454, "page": 2}], "section": "Automatic speech recognition", "id": "5771"}, {"text": "Flat surface typing is a paradigm that exists today in tablet computers.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6370303030303031, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6508674242424242, "width": 0.07044934640522875, "height": 0.012579545454545538, "page": 2}], "section": "APPROACH", "id": "5772"}, {"text": "The network is trained with a batch size of 32, where each individual sample consists of input features { u i } Ti = 1 , where T depends on how long it took the participant to type the phrase.", "bboxes": [{"left": 0.5241601307189543, "top": 0.7381363636363637, "width": 0.3976715686274509, "height": 0.012579545454545427, "page": 2}, {"left": 0.5246584967320261, "top": 0.7497588383838384, "width": 0.39717973856209143, "height": 0.01776010101010106, "page": 2}, {"left": 0.5246633986928104, "top": 0.7655214646464646, "width": 0.39717483660130715, "height": 0.012868686868686918, "page": 2}, {"left": 0.5246633986928104, "top": 0.7796477272727272, "width": 0.04562581699346402, "height": 0.012579545454545427, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "id": "5773"}, {"text": "We use a temporal neural network as a motion model.", "bboxes": [{"left": 0.5238986928104574, "top": 0.30038383838383836, "width": 0.3616732026143793, "height": 0.012579545454545427, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "id": "5774"}, {"text": "To collect a dataset of skeletal hand-tracking data, we make use of a high quality marker-based hand-tracking system [12] which is not subject to the current tracking limitations of consumer head mounted hand-trackers.", "bboxes": [{"left": 0.08761437908496732, "top": 0.4282184343434343, "width": 0.39767156862745096, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.44205555555555553, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 2}, {"left": 0.08753267973856209, "top": 0.4558914141414141, "width": 0.4004640522875817, "height": 0.012579545454545482, "page": 2}, {"left": 0.08811928104575163, "top": 0.4697285353535353, "width": 0.23133496732026143, "height": 0.012579545454545482, "page": 2}], "section": "APPROACH", "id": "5775"}, {"text": "The input features to the network are frame-to-frame deltas of wrist position and rotation along with 3D ngertip positions.", "bboxes": [{"left": 0.5241601307189543, "top": 0.6475669191919192, "width": 0.39768627450980387, "height": 0.012579545454545427, "page": 2}, {"left": 0.5240784313725491, "top": 0.6614040404040404, "width": 0.4006127450980391, "height": 0.012579545454545427, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "id": "5776"}, {"text": "For temporal modeling, we opt to use a temporal convolutional network (TCN) rather than a recurrent model because a xed window of hand motion data is typically sufcient to make predictions about key presses.", "bboxes": [{"left": 0.5246633986928104, "top": 0.404790404040404, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.4186275252525252, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 2}, {"left": 0.5240784313725491, "top": 0.43246464646464644, "width": 0.39775490196078434, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246633986928104, "top": 0.44630176767676766, "width": 0.1978415032679739, "height": 0.012579545454545482, "page": 2}], "section": "MAPPING SKELETAL MOTION TO TEXT", "id": "5777"}, {"text": "The problem of generating text from hand motion has strong analogs to automatic speech recognition (ASR), and we use ASR as motivation to design a multi-component system with the following three pieces.", "bboxes": [{"left": 0.08761437908496732, "top": 0.7876641414141414, "width": 0.39767320261437905, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8015012626262626, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 2}, {"left": 0.08753267973856209, "top": 0.8153383838383839, "width": 0.39776307189542476, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.8291742424242424, "width": 0.17229411764705882, "height": 0.012579545454545427, "page": 2}], "section": "SYSTEM DESIGN", "id": "5778"}, {"text": "The motion model captures the mapping from nger trajectories to intended key presses.", "bboxes": [{"left": 0.5241601307189543, "top": 0.17180429292929292, "width": 0.4003774509803921, "height": 0.012579545454545482, "page": 2}, {"left": 0.5246584967320261, "top": 0.18564141414141413, "width": 0.20514542483660125, "height": 0.012579545454545482, "page": 2}], "section": "SYSTEM DESIGN", "id": "5779"}, {"text": "For decoding hand motion into text, we make no such concessions and investigate the accuracy we can achieve using practical language models and statistical decoding strategies presented in this work.", "bboxes": [{"left": 0.08811928104575163, "top": 0.574135101010101, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.5879722222222222, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 2}, {"left": 0.08811928104575163, "top": 0.6018093434343434, "width": 0.39716830065359476, "height": 0.012579545454545427, "page": 2}, {"left": 0.08811928104575163, "top": 0.6156464646464647, "width": 0.14813725490196078, "height": 0.012579545454545538, "page": 2}], "section": "APPROACH", "id": "5780"}, {"text": "In our study we collected data by having participants complete a text transcription task for short phrases.", "bboxes": [{"left": 0.5246633986928104, "top": 0.31209974747474745, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.32593686868686866, "width": 0.27559803921568626, "height": 0.012579545454545482, "page": 3}], "section": "DATA COLLECTION", "id": "5781"}, {"text": "We use a beam search implementation with beam compaction which maximizes the objective W = argmax W p total .", "bboxes": [{"left": 0.08735457516339869, "top": 0.7520138888888889, "width": 0.3979362745098039, "height": 0.012579545454545427, "page": 3}, {"left": 0.08753267973856209, "top": 0.7655618686868687, "width": 0.3366797385620915, "height": 0.015136363636363614, "page": 3}], "section": "TEXT DECODING WITH A LANGUAGE MODEL", "id": "5782"}, {"text": "Participants typed phrases up to 40 characters long drawn from two corpora; samples used to t or train models were randomly sampled from Daily Dialog [19], while samples for testing and evaluation were randomly sampled from Mackenzie and Soukeroff [23].", "bboxes": [{"left": 0.5246633986928104, "top": 0.652993686868687, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6668308080808081, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6806679292929294, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.6945037878787879, "width": 0.39717810457516345, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.7083409090909091, "width": 0.10281372549019618, "height": 0.012579545454545538, "page": 3}], "section": "DATA COLLECTION", "id": "5783"}, {"text": "Participants were given prompts with 3-6 word phrases containing only lowercase alphabetical characters and spaces.", "bboxes": [{"left": 0.5246633986928104, "top": 0.5209128787878787, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.53475, "width": 0.3725816993464053, "height": 0.012579545454545538, "page": 3}], "section": "DATA COLLECTION", "id": "5784"}, {"text": "A naive greedy approach to decoding is to generate a label for each frame t  T by taking ` t = argmax k v t ( k ) .", "bboxes": [{"left": 0.08753267973856209, "top": 0.4532424242424243, "width": 0.3977549019607843, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811437908496732, "top": 0.466790404040404, "width": 0.3459052287581699, "height": 0.015137626262626325, "page": 3}], "section": "TEXT DECODING WITH A LANGUAGE MODEL", "id": "5785"}, {"text": "Aside from the prompted phrase, no visual feedback was afforded in any of the blocks.", "bboxes": [{"left": 0.5240784313725491, "top": 0.822885101010101, "width": 0.40046078431372545, "height": 0.012579545454545427, "page": 3}, {"left": 0.5246633986928104, "top": 0.8367222222222221, "width": 0.17288562091503268, "height": 0.012579545454545538, "page": 3}], "section": "DATA COLLECTION", "id": "5786"}, {"text": "We recruited 20 participants who passed a pre-screening questionnaire designed to select for experienced typists.", "bboxes": [{"left": 0.5238986928104574, "top": 0.43034343434343436, "width": 0.40065032679738566, "height": 0.012579545454545482, "page": 3}, {"left": 0.5246633986928104, "top": 0.4441805555555556, "width": 0.34616503267973864, "height": 0.012579545454545482, "page": 3}], "section": "DATA COLLECTION", "id": "5787"}, {"text": "We can do better than greedy decoding by using a prex beam search decoder [13].", "bboxes": [{"left": 0.08735457516339869, "top": 0.5312941919191919, "width": 0.39793790849673205, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.5451313131313131, "width": 0.136171568627451, "height": 0.012579545454545427, "page": 3}], "section": "TEXT DECODING WITH A LANGUAGE MODEL", "id": "5788"}, {"text": "This allows the language model to steer decoding when the motion model is uncertain (See Figure 2).", "bboxes": [{"left": 0.43633660130718954, "top": 0.6614444444444445, "width": 0.05167156862745098, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6752815656565656, "width": 0.3971748366013072, "height": 0.012579545454545427, "page": 3}, {"left": 0.08811928104575163, "top": 0.6891186868686869, "width": 0.23493300653594773, "height": 0.012579545454545427, "page": 3}], "section": "TEXT DECODING WITH A LANGUAGE MODEL", "id": "5789"}, {"text": "Because contact-based text decoders are sensitive to accidental or missed touches, we took care to clean the contact-based training data from the touchpad captures.", "bboxes": [{"left": 0.08811928104575163, "top": 0.3179431818181818, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.331780303030303, "width": 0.39717320261437916, "height": 0.012579545454545482, "page": 4}, {"left": 0.08811928104575163, "top": 0.3456174242424242, "width": 0.28328431372549023, "height": 0.012579545454545482, "page": 4}], "section": "DATA COLLECTION", "id": "5790"}, {"text": "Our rst evaluation is to determine if participants are able to transfer their existing skills typing on a physical keyboard to typing on a at surface (with the help of our proposed decoder).", "bboxes": [{"left": 0.5246633986928104, "top": 0.531050505050505, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.5448876262626262, "width": 0.39718137254901953, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.5587247474747474, "width": 0.40002777777777776, "height": 0.012579545454545427, "page": 4}], "section": "Physical keyboard versus virtual keyboard", "id": "5791"}, {"text": "The testing dataset used for all decoders is the same, except that we only lter samples where the user felt they made a mistake, which means that the number of characters typed might not equal the number of contact events.", "bboxes": [{"left": 0.08761437908496732, "top": 0.5959419191919192, "width": 0.39767156862745096, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811274509803921, "top": 0.6097790404040404, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811274509803921, "top": 0.6236161616161616, "width": 0.39717320261437905, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811274509803921, "top": 0.6374532828282828, "width": 0.3117941176470588, "height": 0.012579545454545427, "page": 4}], "section": "DATA COLLECTION", "id": "5792"}, {"text": "We present two evaluations of our proposed text entry method using the data collected from our user study.", "bboxes": [{"left": 0.08735457516339869, "top": 0.8505593434343435, "width": 0.39794281045751634, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.2939950980392157, "height": 0.012579545454545538, "page": 4}], "section": "EVALUATION", "id": "5793"}, {"text": "Hand-tracking information was recorded while participants typed during all three blocks.", "bboxes": [{"left": 0.08811928104575163, "top": 0.14435227272727272, "width": 0.39717320261437916, "height": 0.012579545454545454, "page": 4}, {"left": 0.08811928104575163, "top": 0.15818939393939394, "width": 0.1925751633986928, "height": 0.012579545454545454, "page": 4}], "section": "DATA COLLECTION", "id": "5794"}, {"text": "This result is consistent with previous ndings on the performance envelopes of human typing [4] which described the high potential of typing speeds given a limited text decoding oracle.", "bboxes": [{"left": 0.5241601307189543, "top": 0.8367222222222221, "width": 0.4003856209150327, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.8505593434343435, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.8643964646464646, "width": 0.39718137254901964, "height": 0.012579545454545538, "page": 4}, {"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.04418954248366014, "height": 0.012579545454545427, "page": 4}], "section": "Physical keyboard versus virtual keyboard", "id": "5795"}, {"text": "Participants typed on physical keyboards with a mean UER of 1.72% (median: 1.19%) compared to a mean UER of 2.38% (median: 1.77%) when they typed on at surfaces with our decoder.", "bboxes": [{"left": 0.5246633986928104, "top": 0.7323156565656566, "width": 0.3971830065359476, "height": 0.012579545454545427, "page": 4}, {"left": 0.5234428104575163, "top": 0.7461527777777778, "width": 0.39974673202614375, "height": 0.012579545454545538, "page": 4}, {"left": 0.524124183006536, "top": 0.759989898989899, "width": 0.39798692810457514, "height": 0.012579545454545427, "page": 4}, {"left": 0.5246633986928104, "top": 0.7738270202020202, "width": 0.05453267973856213, "height": 0.012579545454545427, "page": 4}], "section": "Physical keyboard versus virtual keyboard", "id": "5796"}, {"text": "While the contact-based baselines can only be trained with known key/contact-point correspondences, our motion model is trained using CTC loss, which allows for sequence level labels and can deduce the frame-level alignment of the labels.", "bboxes": [{"left": 0.08735457516339869, "top": 0.7003484848484849, "width": 0.3979379084967321, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7141856060606061, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 4}, {"left": 0.08811928104575163, "top": 0.7280214646464646, "width": 0.3971732026143791, "height": 0.012579545454545538, "page": 4}, {"left": 0.08811928104575163, "top": 0.7418585858585859, "width": 0.4000261437908497, "height": 0.012579545454545427, "page": 4}], "section": "DATA COLLECTION", "id": "5797"}, {"text": "For our motion model we trained person-specic models that captured each users typing style from the training set portion of the data.", "bboxes": [{"left": 0.5246633986928104, "top": 0.39182449494949495, "width": 0.39717647058823535, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.40566161616161617, "width": 0.39717320261437905, "height": 0.012579545454545482, "page": 4}, {"left": 0.5246633986928104, "top": 0.4194987373737374, "width": 0.0763627450980392, "height": 0.012579545454545482, "page": 4}], "section": "EVALUATION", "id": "5798"}, {"text": "The resulting touchpad training dataset consists of samples where we have", "bboxes": [{"left": 0.08761437908496732, "top": 0.5192095959595959, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 4}, {"left": 0.08753104575163399, "top": 0.5330467171717171, "width": 0.10017320261437908, "height": 0.012579545454545538, "page": 4}], "section": "DATA COLLECTION", "id": "5799"}, {"text": "Our system differs from prior work in several ways.", "bboxes": [{"left": 0.08811928104575163, "top": 0.4223914141414141, "width": 0.3342467320261438, "height": 0.012579545454545482, "page": 5}], "section": "Comparison of Input Features and Motion Models", "id": "5800"}, {"text": "In our second experiment we investigate the contribution of continuous motion decoding versus discrete contact-based decoding.", "bboxes": [{"left": 0.08811928104575163, "top": 0.6312045454545454, "width": 0.3971732026143791, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.6450416666666666, "width": 0.39717320261437916, "height": 0.012579545454545538, "page": 5}, {"left": 0.08811928104575163, "top": 0.6588787878787878, "width": 0.0640669934640523, "height": 0.012579545454545427, "page": 5}], "section": "Comparison of Input Features and Motion Models", "id": "5801"}, {"text": "In our rst experiment we study the contribution of nger identity information.", "bboxes": [{"left": 0.08811928104575163, "top": 0.5267979797979798, "width": 0.3974558823529412, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.540635101010101, "width": 0.138047385620915, "height": 0.012579545454545538, "page": 5}], "section": "Comparison of Input Features and Motion Models", "id": "5802"}, {"text": "Using the cleaned training dataset, we obtain a sequence of keys pressed { w  i } and a sequence of 2D contact points from the touchpad { x  i } (See Figure 5).", "bboxes": [{"left": 0.5246633986928104, "top": 0.7675366161616162, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 5}, {"left": 0.5246633986928104, "top": 0.781084595959596, "width": 0.39717320261437916, "height": 0.013835858585858563, "page": 5}, {"left": 0.5246633986928104, "top": 0.7949217171717171, "width": 0.23283986928104572, "height": 0.013835858585858563, "page": 5}], "section": "Experiment 1: Comparison with Finger Identity Information", "id": "5803"}, {"text": "For our rst baseline, Gaussian contact decoding , we evaluate performance using the spatial modeling approach described by Zhu et al.", "bboxes": [{"left": 0.08811928104575163, "top": 0.8364330808080809, "width": 0.3971732026143791, "height": 0.012868686868686807, "page": 5}, {"left": 0.08811928104575163, "top": 0.8505593434343435, "width": 0.39774673202614386, "height": 0.012579545454545427, "page": 5}, {"left": 0.08811928104575163, "top": 0.8643964646464646, "width": 0.06075000000000001, "height": 0.012579545454545538, "page": 5}], "section": "Experiment 1: Comparison with Finger Identity Information", "id": "5804"}, {"text": "The previous experiments show that our continuous hand motion decoding method is more accurate than decoding of discrete contacts with either a simple Gaussian spatial model or a richer model with nger identity information.", "bboxes": [{"left": 0.5241601307189543, "top": 0.13676767676767676, "width": 0.4003839869281046, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.15060479797979798, "width": 0.399874183006536, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.1644419191919192, "width": 0.39745915032679746, "height": 0.012579545454545454, "page": 6}, {"left": 0.5246633986928104, "top": 0.17827777777777779, "width": 0.3082058823529412, "height": 0.012579545454545454, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "id": "5805"}, {"text": "In order to disentangle the benet of continuous motion decoding versus discrete contact decoding, we lter our test dataset similarly to our training set to contain only samples where we know the correspondences between contact events and ground truth character labels.In this ltered corresponded contacts dataset, there are no samples with missing or spurious contacts.", "bboxes": [{"left": 0.5246633986928104, "top": 0.5808093434343434, "width": 0.39988398692810456, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.5946464646464646, "width": 0.39717973856209154, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6084835858585859, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.6223207070707071, "width": 0.39717647058823546, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6361578282828283, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6499949494949495, "width": 0.39988071895424837, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.6638308080808081, "width": 0.03445098039215688, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "id": "5806"}, {"text": "At rst, we assumed that the discrete nature of contacts was an advantage for contact decoding.", "bboxes": [{"left": 0.5240784313725491, "top": 0.33803282828282827, "width": 0.3977630718954249, "height": 0.012579545454545482, "page": 6}, {"left": 0.5246633986928104, "top": 0.3518699494949495, "width": 0.20806209150326793, "height": 0.012579545454545482, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "id": "5807"}, {"text": "Repeating the evaluation of the previous two baselines on this ltered corresponded contacts dataset, we found a substantial drop in the mean UER for all methods (Table 1), suggesting that removing spurious or missing key-presses makes for an easier dataset.", "bboxes": [{"left": 0.5246633986928104, "top": 0.6990530303030302, "width": 0.39717810457516345, "height": 0.012579545454545538, "page": 6}, {"left": 0.5246633986928104, "top": 0.7128901515151516, "width": 0.39716666666666667, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7267272727272727, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.740564393939394, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 6}, {"left": 0.5246633986928104, "top": 0.7544002525252526, "width": 0.0892271241830066, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 2: Comparison with Corresponded Contacts", "id": "5808"}, {"text": "For our second baseline, Per Finger Gaussian contact decoding , we specically investigate if augmenting the spatial model input with nger identity information improves performance of the contact-based model.", "bboxes": [{"left": 0.08811928104575163, "top": 0.1163888888888889, "width": 0.39988725490196086, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.1302260101010101, "width": 0.3971732026143791, "height": 0.012868686868686863, "page": 6}, {"left": 0.08811928104575163, "top": 0.14435227272727272, "width": 0.39718137254901964, "height": 0.012579545454545454, "page": 6}, {"left": 0.08811928104575163, "top": 0.15818939393939394, "width": 0.18443954248366018, "height": 0.012579545454545454, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "id": "5809"}, {"text": "With greedy decoding, the 2D contact-based methods appeared to have higher variance, though the median UER was comparable across the three strategies.", "bboxes": [{"left": 0.08735457516339869, "top": 0.4852462121212121, "width": 0.3979379084967321, "height": 0.012579545454545482, "page": 6}, {"left": 0.08811928104575163, "top": 0.4990820707070707, "width": 0.39988235294117647, "height": 0.012579545454545371, "page": 6}, {"left": 0.08811928104575163, "top": 0.512919191919192, "width": 0.21466013071895423, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "id": "5810"}, {"text": "To use nger identity information, we make two changes to our Gaussian contact decoding baseline.", "bboxes": [{"left": 0.08761437908496732, "top": 0.2764330808080808, "width": 0.39767156862745096, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811274509803921, "top": 0.29027020202020204, "width": 0.2936797385620915, "height": 0.012579545454545427, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "id": "5811"}, {"text": "Using our touchpad evaluation dataset we compared the two baselines with our motion model approach (Figure 8).", "bboxes": [{"left": 0.08811928104575163, "top": 0.3946755050505051, "width": 0.39716993464052286, "height": 0.012579545454545427, "page": 6}, {"left": 0.08811928104575163, "top": 0.40851262626262624, "width": 0.3664558823529412, "height": 0.012579545454545482, "page": 6}], "section": "Experiment 1: Comparison with Finger Identity Information", "id": "5812"}, {"text": "In this interactive setting we constrained our beam search decoder to force convergence for any predictions older than 6 frames (0.1s) causing all beams to have a common prex.", "bboxes": [{"left": 0.5246633986928104, "top": 0.44942424242424245, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.46326136363636367, "width": 0.39717320261437916, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.4770984848484849, "width": 0.4000130718954249, "height": 0.012579545454545427, "page": 7}], "section": "Performance and interactive run-time", "id": "5813"}, {"text": "Typing on any at surface without the need to bring a physical keyboard would provide a valuable addition to AR/VR interaction.", "bboxes": [{"left": 0.5241601307189543, "top": 0.6354570707070707, "width": 0.3976830065359477, "height": 0.012579545454545538, "page": 7}, {"left": 0.5246699346405229, "top": 0.6492941919191919, "width": 0.39988562091503266, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.6631313131313132, "width": 0.029692810457516372, "height": 0.012579545454545427, "page": 7}], "section": "CONCLUSION", "id": "5814"}, {"text": "Many challenges remain to making this system practical for general text input.", "bboxes": [{"left": 0.5246633986928104, "top": 0.8782335858585859, "width": 0.39745588235294127, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.892070707070707, "width": 0.12283823529411775, "height": 0.012579545454545427, "page": 7}], "section": "CONCLUSION", "id": "5815"}, {"text": "Our motion model network is compact, containing just 180KB of weights, enabling efcient evaluation on a GPU.", "bboxes": [{"left": 0.5246633986928104, "top": 0.28212247474747476, "width": 0.39717320261437905, "height": 0.012579545454545427, "page": 7}, {"left": 0.5246633986928104, "top": 0.295959595959596, "width": 0.3398562091503269, "height": 0.012579545454545427, "page": 7}], "section": "Performance and interactive run-time", "id": "5816"}, {"text": "We created a toy text entry application with an Oculus Rift and the Unity game engine to demonstrate real-time touch typing on a at surface in VR using hand-tracking (Figure 9).", "bboxes": [{"left": 0.5238986928104574, "top": 0.40036616161616156, "width": 0.39793790849673205, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.4142032828282828, "width": 0.39717647058823546, "height": 0.012579545454545482, "page": 7}, {"left": 0.5246633986928104, "top": 0.428040404040404, "width": 0.3542908496732027, "height": 0.012579545454545482, "page": 7}], "section": "Performance and interactive run-time", "id": "5817"}]}